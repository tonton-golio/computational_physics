# Advanced Deep Learning

## Course overview

This course covers **state-of-the-art methods** in deep learning, including modern architectures, training techniques, theoretical foundations, and open research questions. Topics evolve yearly to reflect the rapidly advancing field.

- Architecture: from CNNs and autoencoders to transformers and diffusion models.
- Design: optimization algorithms, regularization strategies, and hyperparameter tuning.
- Analysis: generalization bounds, loss landscapes, and interpretability.
- Theory: approximation theory, neural tangent kernels, and adversarial robustness.

## Why this topic matters

- Deep learning has achieved superhuman performance in vision, language, and game-playing.
- Understanding *why* these models work (and when they fail) is an active research frontier.
- Designing training procedures and architectures requires principled methodology, not just trial and error.
- Adversarial robustness and uncertainty quantification are critical for deploying models safely.

## Key mathematical ideas

- Backpropagation and automatic differentiation.
- Optimization landscapes: convexity, saddle points, and implicit regularization.
- Generalization theory: PAC-Bayes bounds, double descent, and overparameterization.
- Information-theoretic perspectives on representation learning.
- Attention mechanisms and self-supervised learning objectives.

## Prerequisites

- Machine Learning A or equivalent introduction.
- Deep Learning fundamentals: feedforward networks, CNNs, backpropagation.
- Strong Python and PyTorch/TensorFlow skills.
- Linear algebra, multivariate calculus, and probability.

## Recommended reading

- Goodfellow, Bengio, and Courville, *Deep Learning*.
- Research papers from NeurIPS, ICML, and ICLR.
- Course notes announced each semester.

## Learning trajectory

This module covers architectures, training methodology, and theory:

- Artificial neural networks: MLPs, activation functions, universal approximation.
- Convolutional neural networks: convolution, pooling, modern architectures (ResNet, etc.).
- Autoencoders and VAEs: representation learning and generative modeling.
- Generative adversarial networks: adversarial training and mode collapse.
- U-Net and image segmentation architectures.
- Transformers: attention mechanisms, BERT, GPT, and vision transformers.
- Design and optimization: Adam variants, learning rate schedules, regularization.
- Analysis and theory: generalization, double descent, NTK, adversarial robustness.

## Visual and Simulation Gallery

[[simulation adl-attention-heatmap]]

[[simulation adl-optimizer-trajectories]]

[[figure adl-mlops-loop]]

[[figure adl-cnn-feature-map]]
