{
  "meta": {
    "generatedAt": "2026-02-18T20:54:55.720100+00:00",
    "embeddingModel": "text-embedding-3-small",
    "projection": "tsne-2d",
    "inputCount": 127,
    "perplexity": 21
  },
  "topics": [
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing"
    }
  ],
  "points": [
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "analysis-and-theory",
      "lessonTitle": "Analysis and Theory of Deep Learning",
      "x": 0.1764618307352066,
      "y": 0.418682336807251,
      "searchText": "analysis and theory of deep learning\n# analysis and theory of deep learning\n\n## the puzzle of generalization\n\nhere is the central mystery of deep learning. classical learning theory says: if your model has more parameters than training examples, it will memorize the data and fail on new examples. this is the fundamental bias-variance tradeoff, and it has been the bedrock of statistics for a century. then along comes a neural network with 100 million parameters, trained on 50,000 images, and it *generalizes beautifully*. it has more than enough capacity to memorize every training example \u2014 and it does memorize them \u2014 yet it still performs well on data it has never seen. why?\n\nunderstanding this contradiction is one of the deepest open problems in machine learning.\n\n## generalization bounds\n\n**pac-bayes bounds** provide some of the tightest generalization guarantees for neural networks. the idea is to measure how much the trained model differs from what you expected before seeing the data. for a posterior distribution $q$ over hypotheses and a prior $p$:\n\n$$\n\\mathbb{e}_{h \\sim q}[\\mathcal{l}(h)] \\leq \\mathbb{e}_{h \\sim q}[\\hat{\\mathcal{l}}(h)] + \\sqrt{\\frac{d_{\\text{kl}}(q \\| p) + \\ln(n/\\delta)}{2n}},\n$$\n\nwhere $\\hat{\\mathcal{l}}$ is the empirical loss, $n$ is the number of training examples, and $\\delta$ is the failure probability. the bound favors models that are both accurate on training data and close to the prior \u2014 models that did not have to change much to fit the data.\n\n**rademacher complexity** takes a different approach, measuring the ability of a function class to fit random noise:\n\n$$\n\\mathcal{r}_n(\\mathcal{f}) = \\mathbb{e}\\left[\\sup_{f \\in \\mathcal{f}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_i f(x_i)\\right],\n$$\n\nwhere $\\sigma_i \\in \\{-1, +1\\}$ are random signs. if your model class can easily fit random labels, it has high rademacher complexity and weak generalization guarantees. lower rademacher complexity implies better generalization.\n\n## double descent\n\nthe **double descent** phenomenon (belkin et al., 2019) is one of the most surprising empirical discoveries in modern machine learning. as you increase model size, the test error first follows the classic u-shape: it decreases, then increases as the model starts overfitting. but keep going past the point where the model can exactly fit the training data (the interpolation threshold), and something unexpected happens: the test error starts *decreasing again*.\n\nthis challenges the classical bias-variance tradeoff and suggests that overparameterization acts as an implicit regularizer. among all the functions that perfectly fit the training data, gradient descent finds a particularly smooth one. the phenomenon has been observed in three settings:\n\n- model-wise double descent: varying the number of parameters.\n- epoch-wise double descent: varying training time.\n- sample-wise double descent: varying the number of training examples.\n\n[[simulation adl-double-descent]]\n\n## the neural tangent kernel (ntk)\n\nin the **infinite-width limit**, something remarkable happens: a neural network trained with gradient descent behaves like a simple, well-understood algorithm \u2014 kernel regression with a fixed kernel called the **neural tangent kernel**:\n\n$$\n\\theta(\\mathbf{x}, \\mathbf{x}') = \\left\\langle \\nabla_\\theta f(\\mathbf{x}; \\theta_0), \\, \\nabla_\\theta f(\\mathbf{x}'; \\theta_0) \\right\\rangle,\n$$\n\nwhere $\\theta_0$ are the initial parameters. in this infinite-width regime, the network becomes a lazy student who never changes its mind about the shape of the world. the ntk remains approximately constant during training (**lazy training**), meaning the network adjusts its weights linearly without fundamentally reorganizing its internal representations:\n\n- training dynamics become linear, and convergence to a global minimum is guaranteed.\n- the trained network is equivalent to kernel ridge regression with the ntk.\n\n**why this matters and why it is not the whole story**: the ntk theory is elegant and provides convergence guarantees, but it describes an idealization. finite-width networks exhibit **feature learning**, where the internal representations themselves evolve during training. this feature learning \u2014 the network discovering new ways to see the data \u2014 is believed to be essential for the practical success of deep learning. the lazy regime explains convergence; the rich regime explains performance.\n\n## loss landscapes\n\nthe loss function of a deep network defines a surface in a space with millions of dimensions. what does this surface look like? the answer turns out to be far more forgiving than you might expect.\n\n**in 10,000 dimensions, almost every critical point is a saddle point, not a local minimum.** a local minimum requires the loss to curve upward in *every* direction simultaneously. in high dimensions, this is astronomically unlikely \u2014 at a random critical point, roughly half the directions curve up and half curve down, making it a saddle. this is why deep networks can be trained at all: gradient descent is almost never truly stuck, because there is almost always a direction that leads further downhill.\n\nkey properties of the loss landscape:\n- **local minima vs saddle points**: most critical points are saddles. the loss at local minima (when they exist) tends to be close to the global minimum.\n- **mode connectivity**: different solutions found by sgd are often connected by paths of nearly constant loss (**linear mode connectivity**). the valleys are not isolated; they form a connected web.\n- **sharpness and generalization**: flatter minima tend to generalize better than sharp minima. a sharp minimum means tiny perturbations to the parameters cause large changes in the loss \u2014 that fragility usually means the solution does not transfer well to new data. this motivates techniques like sharpness-aware minimization (sam).\n\n[[simulation adl-loss-landscape]]\n\n## what if the loss landscape were full of local minima?\n\nif the loss landscape were riddled "
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "ann",
      "lessonTitle": "Artificial Neural Networks",
      "x": 0.05111666023731232,
      "y": 0.4445441663265228,
      "searchText": "artificial neural networks\n# artificial neural networks\n\n## the simplest possible learner\n\nbefore we build anything complicated, let us train the simplest thing that can learn. a single artificial neuron takes a handful of numbers as input, multiplies each one by a weight, adds them up, and then asks: \"is the total big enough?\" that is all a neuron does. it is a tiny voting machine: each input casts a weighted vote, and the neuron makes a decision based on the tally.\n\nmathematically, the neuron computes a weighted sum of inputs plus a bias, then applies a nonlinear **activation function**:\n\n$$\ny = \\sigma\\!\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = \\sigma(\\mathbf{w}^t \\mathbf{x} + b).\n$$\n\nthe weights $w_i$ determine how much each input contributes, the bias $b$ shifts the decision boundary, and $\\sigma$ introduces nonlinearity. this abstraction is loosely inspired by biological neurons: dendrites receive signals, the cell body integrates them, and the axon fires when a threshold is exceeded.\n\n## the perceptron\n\nthe simplest neural network is a single neuron called the **perceptron**. for binary classification, the perceptron computes:\n\n$$\n\\hat{y} = \\begin{cases} 1 & \\text{if } \\mathbf{w}^t \\mathbf{x} + b > 0, \\\\ 0 & \\text{otherwise}. \\end{cases}\n$$\n\nthis defines a linear decision boundary (a hyperplane) in the input space. when the perceptron gets a prediction wrong, it adjusts its weights in the direction of the mistake:\n\n$$\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\, (y - \\hat{y}) \\, \\mathbf{x},\n$$\n\nwhere $\\eta$ is the learning rate. the perceptron converges for linearly separable data but cannot solve nonlinear problems like xor. this limitation motivated the development of multilayer networks.\n\n## your first training loop\n\nbefore we go any further, let us actually train something. here is the simplest possible neural network on the mnist handwritten digit dataset. we load the data, build a model, and train it in under 20 lines. run this, watch the loss go down, and then we will explain every piece:\n\n```python\ntrain_data = datasets.mnist(root='data', train=true, download=true,\n                            transform=transforms.totensor())\ntrain_loader = torch.utils.data.dataloader(train_data, batch_size=100, shuffle=true)\n\nmodel = nn.sequential(nn.flatten(), nn.linear(784, 10))\ncriterion = nn.crossentropyloss()\noptimizer = torch.optim.adam(model.parameters(), lr=0.001)\n\nfor epoch in range(5):\n    for batch_x, batch_y in train_loader:\n        optimizer.zero_grad()\n        output = model(batch_x)\n        loss = criterion(output, batch_y)\n        loss.backward()\n        optimizer.step()\n    print(f\"epoch {epoch+1}, loss: {loss.item():.4f}\")\n```\n<!--code-toggle-->\n```pseudocode\ntrain_data = load_dataset(\"mnist\", split=\"train\")\napply_transform(train_data, to_tensor)\ntrain_loader = create_dataloader(train_data, batch_size=100, shuffle=true)\n\nmodel = sequential(flatten, linear(784, 10))\ncriterion = cross_entropy_loss()\noptimizer = adam(model.parameters(), lr=0.001)\n\nfor epoch = 1 to 5:\n    for each (batch_x, batch_y) in train_loader:\n        zero_gradients(optimizer)\n        output = model(batch_x)\n        loss = criterion(output, batch_y)\n        backward(loss)\n        update_parameters(optimizer)\n    print(\"epoch\", epoch, \"loss:\", loss)\n```\n\nthat single linear layer already gets about 92% accuracy on digit recognition. but it can only draw straight decision boundaries. to do better, we need nonlinearity and depth.\n\n## activation functions\n\nwhy do we need the activation function $\\sigma$ at all? without it, stacking linear layers would still produce a linear function \u2014 no matter how many layers you add, the composition of linear functions is still linear. the activation function is what gives the network the ability to bend and curve its decision boundaries.\n\ncommon choices include:\n\n- **relu**: $f(x) = \\max(0, x)$. dead simple: if the input is positive, pass it through; if negative, output zero. fast and effective, but neurons can \"die\" (output zero for all inputs) if the bias drifts too negative.\n- **sigmoid**: $f(x) = 1 / (1 + e^{-x})$. squashes everything into $(0, 1)$, useful for output layers in binary classification. but it has a fatal flaw for deep networks: the derivative $f'(x) = f(x)(1 - f(x))$ is at most $0.25$, so gradients shrink exponentially through many layers. this is the **vanishing gradient problem**.\n- **tanh**: $f(x) = \\tanh(x)$. outputs in $(-1, 1)$, zero-centered. better gradient flow than sigmoid but still saturates for large $|x|$.\n- **swish**: $f(x) = x \\cdot \\sigma(x)$. smooth and non-monotonic. used in efficientnet and modern architectures; often outperforms relu in deep networks.\n\nrelu and its variants (leaky relu, gelu) dominate modern practice because they maintain strong gradients even in deep networks.\n\n[[simulation adl-activation-functions]]\n\n## what if we didn't have activation functions?\n\nwithout nonlinear activations, a 100-layer network would compute exactly the same thing as a single-layer network. the entire stack of matrix multiplications would collapse into one giant matrix multiplication. you could never learn anything more complex than a straight line through the data. activations are the ingredient that turns a linear calculator into a universal function approximator.\n\n## multilayer perceptrons\n\na **multilayer perceptron** (mlp) stacks multiple layers of neurons. the **universal approximation theorem** (cybenko, 1989; hornik, 1991) says something remarkable: a feedforward network with a single hidden layer containing sufficiently many neurons can approximate any continuous function on a compact set to arbitrary accuracy. the network does not need to be deep \u2014 it just needs to be wide enough.\n\nso why bother with depth? in practice, **deeper networks** (more layers, fewer neurons per layer) are vastly more efficient:\n- depth enables hierarchical feature extraction: early layers detect simple patterns, later layers compose them into complex concepts.\n- "
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "cnn",
      "lessonTitle": "Convolutional Neural Networks",
      "x": 0.0,
      "y": 0.48728278279304504,
      "searchText": "convolutional neural networks\n# convolutional neural networks\n\n## seeing edges by hand\n\nbefore any formulas, let us do a convolution by hand. take a tiny 5x5 image \u2014 say, a white square on a black background. now take a small 3x3 grid of numbers called a **sobel filter** (it looks like $[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]$). place this little grid on the top-left corner of the image and multiply each overlapping pair of numbers, then add them all up. that gives you one number. slide the grid one pixel to the right and repeat. keep sliding until you have covered the whole image. what you get is a new image where bright spots mark vertical edges \u2014 places where the intensity changes sharply from left to right. that sliding-and-multiplying operation is a **convolution**, and the little grid is a **kernel**.\n\nthe key insight: a neural network can *learn* what kernels to use. instead of hand-designing a sobel filter, the network discovers \u2014 through backpropagation \u2014 exactly which patterns to look for.\n\n## why convolutions?\n\nfully connected networks treat each pixel independently, ignoring the spatial structure of images. **convolutional neural networks** (cnns) exploit three key properties:\n\n- **translation equivariance**: a pattern detected in one part of the image can be recognized elsewhere without learning separate weights for each position. an edge is an edge, whether it appears in the top-left corner or the bottom-right.\n- **parameter sharing**: the same kernel weights are applied at every spatial location, dramatically reducing the parameter count.\n- **locality**: each output depends only on a small region of the input, capturing local patterns before combining them into global features.\n\nthese inductive biases make cnns far more efficient than mlps for image tasks. a fully connected layer connecting two 28x28 feature maps would require $784^2 \\approx 600{,}000$ parameters; a 3x3 convolutional layer needs only 9.\n\n## the convolution operation\n\nimagine you are sliding a little magnifying glass over the picture and asking, at every spot, \"how much does this tiny 3x3 window look like the pattern i am searching for?\" that sliding multiplication is convolution. formally, a 2d convolution slides a kernel $k$ of size $k \\times k$ across an input feature map $i$, computing at each position:\n\n$$\n(i * k)[i, j] = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} i[i + m, j + n] \\cdot k[m, n].\n$$\n\nkey parameters that control the sliding:\n- **stride**: the step size when sliding the kernel. stride 2 halves the spatial dimensions.\n- **padding**: adding zeros around the border. \"same\" padding preserves spatial dimensions; \"valid\" padding shrinks them.\n- **dilation**: inserting gaps between kernel elements to increase the receptive field without adding parameters.\n\n[[simulation adl-convolution-demo]]\n\n## kernel types\n\ndifferent kernels detect different things:\n- **averaging**: a uniform kernel that blurs the image by averaging nearby pixels.\n- **gaussian blur**: a weighted average with gaussian falloff, producing smoother blurring.\n- **edge detection**: kernels like sobel or laplacian that highlight intensity changes \u2014 exactly what we did by hand above.\n- **dilated (atrous) convolution**: expands the receptive field without increasing parameters or pooling.\n\nin a cnn, the network learns its own kernels through training. early kernels tend to look like edge and texture detectors; deeper kernels respond to increasingly complex patterns.\n\n## feature hierarchies\n\ndeep cnns learn a hierarchy of features, building complexity layer by layer the way a painter builds a picture:\n- **early layers** detect low-level features: edges, corners, color gradients. these are the brushstrokes.\n- **middle layers** combine these into mid-level patterns: textures, object parts, shapes. an ear, a wheel, a petal.\n- **deep layers** recognize high-level concepts: objects, scenes, semantic categories. a face, a car, a flower.\n\nthis progression from simple to complex is analogous to the ventral visual stream in the brain, where visual processing progresses from v1 (edges) to it cortex (object recognition). the network rediscovers a strategy that evolution took millions of years to build.\n\n## what if we didn't have convolutions?\n\nwithout convolutions, you would need a fully connected layer for every pixel-to-pixel connection. a single layer processing a modest 224x224 rgb image would need $224 \\times 224 \\times 3 \\times 224 \\times 224 \\times 3 \\approx 7$ billion parameters \u2014 for *one* layer. and the network would have to learn separately that an edge in the top-left is the same concept as an edge in the bottom-right. convolutions give you parameter sharing and spatial awareness for free.\n\n## pooling layers\n\n**pooling** reduces spatial dimensions and provides a degree of translation invariance:\n\n- **max pooling**: takes the maximum value in each window. preserves the strongest activation \u2014 the most prominent feature in each region.\n- **average pooling**: takes the mean value. smoother but may lose sharp features.\n- **global average pooling** (gap): averages each entire feature map to a single number. replaces fully connected layers at the end of modern architectures, reducing parameters and overfitting.\n\na 2x2 max pool with stride 2 halves both spatial dimensions, reducing the feature map size by 4x while keeping the strongest signals.\n\n## cnn model\n\n```python\nclass convolutionalnetwork(nn.module):\n    def __init__(self, im_shape=(1, 28, 28), n_classes=10):\n        super().__init__()\n        self.conv1 = nn.conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.linear(in_features=8*7*7, out_features=32)\n        self.fc2 = nn.linear(in_features=32, out_features=10)\n\n    def forward(self, x):\n        x = f.relu(self.conv1(x))        # (1,28,28) -> (4,28,28)\n        x = f.max_pool2d(x, 2, 2)        # (4,28,28) -> "
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "design-and-optimization",
      "lessonTitle": "Design and Optimization of Deep Learning",
      "x": 0.15895725786685944,
      "y": 0.406830757856369,
      "searchText": "design and optimization of deep learning\n# design and optimization of deep learning\n\n## the optimization problem\n\ntraining a deep network means finding the parameter values that make the loss as small as possible. imagine standing on a vast mountain range in the dark, and all you have is a flashlight that shows you the slope under your feet. you take a step downhill, check the slope again, and repeat. that is gradient descent. the question is: how big should each step be, and should you remember which direction you have been going?\n\nformally, we minimize a loss function $\\mathcal{l}(\\theta)$ over parameters $\\theta$. the choice of optimizer profoundly affects both how fast you get to a good solution and how good that solution ultimately is.\n\n## stochastic gradient descent\n\n**stochastic gradient descent** (sgd) updates parameters using a mini-batch gradient estimate:\n\n$$\n\\theta_{t+1} = \\theta_t - \\eta \\, \\hat{g}_t, \\qquad \\hat{g}_t = \\frac{1}{|b|} \\sum_{i \\in b} \\nabla_\\theta \\mathcal{l}_i(\\theta_t).\n$$\n\nwhy use a mini-batch instead of the full dataset? two reasons: it is faster (you update after seeing 100 examples instead of 60,000), and the noise in the mini-batch gradient actually helps escape shallow local minima \u2014 like shaking a ball on a bumpy surface helps it roll into deeper valleys.\n\n**sgd with momentum** accumulates a velocity term that smooths oscillations:\n\n$$\nv_{t+1} = \\mu v_t + \\hat{g}_t, \\qquad \\theta_{t+1} = \\theta_t - \\eta v_{t+1}.\n$$\n\nmomentum helps the optimizer build speed in consistent gradient directions while dampening oscillations in noisy directions. think of a heavy ball rolling downhill: it picks up speed in the consistent downhill direction and resists being deflected by small bumps. a typical value is $\\mu = 0.9$.\n\n[[simulation adl-optimizer-trajectories]]\n\n## adaptive learning rate methods\n\nthe next breakthrough was giving each parameter its own personalized learning rate. parameters with consistently large gradients should take smaller steps (they are already learning fast), while parameters with small or sparse gradients should take larger steps (they need more encouragement).\n\n**adam** (kingma and ba, 2015) combines momentum with per-parameter adaptive learning rates:\n\n$$\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t, \\qquad v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2,\n$$\n\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\qquad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}, \\qquad \\theta_{t+1} = \\theta_t - \\frac{\\eta \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}.\n$$\n\ndefault hyperparameters: $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$. the bias correction terms ($1 - \\beta^t$) compensate for the zero-initialization of the moment estimates during early training.\n\n**variants**:\n\n- **adamw**: decouples weight decay from the gradient update. in standard adam, $l_2$ regularization is scaled by the adaptive learning rate, weakening its effect for parameters with large gradients. adamw applies weight decay directly, which improves generalization.\n- **lamb**: layer-wise adaptive learning rates for large-batch training. scales the update by the ratio of parameter norm to update norm for each layer.\n- **adafactor**: memory-efficient by factorizing the second-moment matrix using row and column statistics instead of storing the full matrix.\n\n## which optimizer to choose\n\na practical decision framework:\n\n- **default choice**: adamw with weight decay 0.01-0.1. works well across most architectures and datasets.\n- **vision (cnns)**: sgd with momentum 0.9 + cosine schedule often matches or beats adam with proper tuning.\n- **transformers / nlp**: adamw is strongly preferred. sgd converges too slowly for attention-based models.\n- **large batch training**: lamb or lars for scaling to very large batch sizes (>8k).\n- **memory constrained**: adafactor reduces optimizer state memory by ~50%.\n\n```python\n# adamw with weight decay\noptimizer = torch.optim.adamw(model.parameters(), lr=3e-4, weight_decay=0.01)\n\n# sgd with momentum for vision\noptimizer = torch.optim.sgd(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n```\n<!--code-toggle-->\n```pseudocode\n// adamw with weight decay\noptimizer = adamw(model.parameters(), lr=3e-4, weight_decay=0.01)\n\n// sgd with momentum for vision\noptimizer = sgd(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n```\n\n## learning rate schedules\n\nthe learning rate is arguably the single most important hyperparameter. too high and training diverges; too low and it crawls. the best strategy is to change it during training:\n\n- **step decay**: reduce $\\eta$ by a factor at specified epochs. simple but requires manual tuning of milestones.\n- **cosine annealing**: $\\eta_t = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min})(1 + \\cos(\\pi t / t))$. smooth decay that naturally reaches a minimum at the end of training.\n- **warmup**: linearly increase $\\eta$ from zero over the first few thousand steps to stabilize early training, especially important for large batch sizes and transformers. without warmup, the initial random gradients can push parameters far from their initialization before the model has learned anything useful.\n- **one-cycle policy**: warmup then cosine decay; often yields faster convergence.\n\n[[simulation adl-lr-schedule-comparison]]\n\n```python\n# cosine annealing with warmup\nscheduler = torch.optim.lr_scheduler.cosineannealinglr(optimizer, t_max=100, eta_min=1e-6)\n\n# one-cycle policy\nscheduler = torch.optim.lr_scheduler.onecyclelr(optimizer, max_lr=0.01, total_steps=1000)\n```\n<!--code-toggle-->\n```pseudocode\n// cosine annealing\nscheduler = cosine_annealing(optimizer, t_max=100, eta_min=1e-6)\n\n// one-cycle policy\nscheduler = one_cycle(optimizer, max_lr=0.01, total_steps=1000)\n```\n\n## regularization techniques\n\ndeep networks have an enormous capacity to memorize training data. regularization techniques fight overfitting by constraining what the network can learn, forcing it to find simpler patterns that generalize.\n\n**dropout** (srivastava "
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "gan",
      "lessonTitle": "Generative Adversarial Networks",
      "x": 0.16256782412528992,
      "y": 0.541470468044281,
      "searchText": "generative adversarial networks\n# generative adversarial networks\n\n## the adversarial framework\n\npicture a boxing ring with two fighters. in one corner stands the **forger** \u2014 a network that takes random noise and tries to paint a convincing fake image. in the other corner stands the **detective** \u2014 a network that looks at images and tries to figure out which ones are real and which are forgeries. they train simultaneously, each getting better in response to the other. the forger studies why the detective caught him and paints more convincingly next time. the detective studies the forger's latest tricks and sharpens her eye. round after round, the forgeries become more and more indistinguishable from reality.\n\nthat is a **generative adversarial network** (gan). the **generator** $g$ maps random noise $\\mathbf{z} \\sim p_z(\\mathbf{z})$ to synthetic data $g(\\mathbf{z})$, while the **discriminator** $d$ tries to distinguish real data from generated samples. training proceeds as a minimax game:\n\n$$\n\\min_g \\max_d \\; \\mathbb{e}_{\\mathbf{x} \\sim p_{\\text{data}}}[\\log d(\\mathbf{x})] + \\mathbb{e}_{\\mathbf{z} \\sim p_z}[\\log(1 - d(g(\\mathbf{z})))].\n$$\n\nat the nash equilibrium, the generator produces samples indistinguishable from real data, and the discriminator outputs $d(\\mathbf{x}) = 1/2$ everywhere \u2014 it genuinely cannot tell the difference. in practice, training alternates between updating $d$ (sharpen the detective) and updating $g$ (improve the forger).\n\n## architecture\n\na basic gan for image generation uses:\n\n- **generator**: takes a latent vector $\\mathbf{z} \\in \\mathbb{r}^{d}$ (typically $d = 100$) and maps it through fully connected or transposed convolutional layers to produce an image. batch normalization and relu activations are standard in intermediate layers, with a tanh output.\n\n- **discriminator**: takes an image and outputs a single scalar (real/fake probability) through convolutional layers with leakyrelu activations and a sigmoid output.\n\n**dcgan** (deep convolutional gan) established key architectural guidelines: replace pooling with strided convolutions, use batch normalization in both networks, remove fully connected hidden layers, and use relu in the generator but leakyrelu in the discriminator.\n\n```python\nclass generator(nn.module):\n    def __init__(self, latent_dim=100):\n        super().__init__()\n        self.net = nn.sequential(\n            nn.linear(latent_dim, 256),\n            nn.batchnorm1d(256), nn.relu(),\n            nn.linear(256, 512),\n            nn.batchnorm1d(512), nn.relu(),\n            nn.linear(512, 784), nn.tanh()\n        )\n    def forward(self, z):\n        return self.net(z).view(-1, 1, 28, 28)\n```\n<!--code-toggle-->\n```pseudocode\nclass generator:\n    init(latent_dim=100):\n        net = sequential(\n            linear(latent_dim, 256), batch_norm(256), relu,\n            linear(256, 512), batch_norm(512), relu,\n            linear(512, 784), tanh\n        )\n    forward(z):\n        return reshape(net(z), shape=(-1, 1, 28, 28))\n```\n\n## training challenges\n\ngan training is notoriously unstable. the two-player game introduces failure modes that single-loss training never encounters:\n\n**mode collapse** happens when the forger discovers that one particular type of forgery reliably fools the detective, and stops trying anything else. the generator produces only a few distinct outputs, ignoring the diversity of the training distribution. it is like a forger who can perfectly copy one painting but cannot paint anything else.\n\n**vanishing gradients** happen when the detective becomes too good too fast. if $d(g(\\mathbf{z})) \\approx 0$ for all generated samples, the gradient of $\\log(1 - d(g(\\mathbf{z})))$ vanishes, and the generator gets no useful learning signal. the forger has been so thoroughly defeated that she cannot even tell *how* to improve. a practical fix is to train $g$ to maximize $\\log d(g(\\mathbf{z}))$ instead.\n\n**training instability** means the two-player game may not converge at all. the losses oscillate rather than decrease, with the generator and discriminator taking turns dominating.\n\n[[simulation adl-gan-training-dynamics]]\n\n## improved loss functions\n\n**wasserstein gan** (wgan) replaces the js divergence implicit in the original loss with the earth mover distance \u2014 a measure of how much \"work\" is needed to transform one distribution into another. the discriminator (called a \"critic\") outputs an unbounded score:\n\n$$\n\\min_g \\max_{d \\in \\mathcal{d}} \\; \\mathbb{e}_{\\mathbf{x}}[d(\\mathbf{x})] - \\mathbb{e}_{\\mathbf{z}}[d(g(\\mathbf{z}))],\n$$\n\nwhere $\\mathcal{d}$ is the set of 1-lipschitz functions. the lipschitz constraint is enforced via **gradient penalty** (wgan-gp):\n\n$$\n\\lambda \\, \\mathbb{e}_{\\hat{\\mathbf{x}}}[(\\|\\nabla_{\\hat{\\mathbf{x}}} d(\\hat{\\mathbf{x}})\\|_2 - 1)^2],\n$$\n\nwhere $\\hat{\\mathbf{x}}$ is interpolated between real and generated samples. wgan provides more meaningful loss curves (the loss actually correlates with sample quality) and significantly more stable training.\n\n## conditional gans\n\n**conditional gans** (cgan) give the generator and discriminator additional information $\\mathbf{y}$ (e.g., class labels):\n\n$$\n\\min_g \\max_d \\; \\mathbb{e}_{\\mathbf{x}}[\\log d(\\mathbf{x}, \\mathbf{y})] + \\mathbb{e}_{\\mathbf{z}}[\\log(1 - d(g(\\mathbf{z}, \\mathbf{y}), \\mathbf{y}))].\n$$\n\nthis enables controlled generation: producing images of a specific digit, translating between image domains (pix2pix), or generating data with desired physical properties.\n\n## vae vs gan: two philosophies of generation\n\nvaes and gans represent fundamentally different approaches to the same problem. a vae is a careful cartographer: it draws a smooth, continuous map of the data landscape and can reliably generate plausible outputs from any point on that map. the price is that its outputs tend to be blurry \u2014 the smoothness averages out sharp details.\n\na gan is an arms race: the forger-detective competition drives the generator to produce crisp, realistic outputs, but the process "
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "home",
      "lessonTitle": "Advanced Deep Learning",
      "x": 0.0969606339931488,
      "y": 0.3970875144004822,
      "searchText": "advanced deep learning\n# advanced deep learning\n\n## roadmap\n\nthis course takes you on a journey from a single artificial neuron to the cutting edge of deep learning theory. here is the path:\n\n**a single neuron** learns to draw a line between two classes. stack many of them into a **multilayer network** and you can approximate any function. give the network **eyes** with convolutional filters and it starts to see edges, textures, and objects. teach it to label **every pixel** with a u-shaped encoder-decoder, and it can outline a tumor in a brain scan. now give it the tools to **design and train** properly: the right optimizer, the right initialization, the right regularization. with those tools in hand, let the network **dream**: a variational autoencoder learns a smooth landscape of possibilities, while a gan pits a forger against a detective until the forgeries become indistinguishable from reality. replace convolutions with **attention** and the network can read a sentence, translate a language, or caption an image. finally, step back and ask the deepest question: **why does any of this work at all?** that is the theory.\n\n$$\n\\text{neuron} \\;\\to\\; \\text{mlp} \\;\\to\\; \\text{cnn} \\;\\to\\; \\text{u-net} \\;\\to\\; \\text{training toolkit} \\;\\to\\; \\text{vae / gan} \\;\\to\\; \\text{transformer} \\;\\to\\; \\text{theory}\n$$\n\n## course overview\n\nthis course covers **state-of-the-art methods** in deep learning, including modern architectures, training techniques, theoretical foundations, and open research questions. topics evolve yearly to reflect the rapidly advancing field.\n\n- architecture: from cnns and autoencoders to transformers and diffusion models.\n- design: optimization algorithms, regularization strategies, and hyperparameter tuning.\n- analysis: generalization bounds, loss landscapes, and interpretability.\n- theory: approximation theory, neural tangent kernels, and adversarial robustness.\n\n## why this topic matters\n\n- deep learning has achieved superhuman performance in vision, language, and game-playing.\n- understanding *why* these models work (and when they fail) is an active research frontier.\n- designing training procedures and architectures requires principled methodology, not just trial and error.\n- adversarial robustness and uncertainty quantification are critical for deploying models safely.\n\n## key mathematical ideas\n\n- backpropagation and automatic differentiation.\n- optimization landscapes: convexity, saddle points, and implicit regularization.\n- generalization theory: pac-bayes bounds, double descent, and overparameterization.\n- information-theoretic perspectives on representation learning.\n- attention mechanisms and self-supervised learning objectives.\n\n## prerequisites\n\n- machine learning a or equivalent introduction.\n- deep learning fundamentals: feedforward networks, cnns, backpropagation.\n- strong python and pytorch/tensorflow skills.\n- linear algebra, multivariate calculus, and probability.\n\n## recommended reading\n\n- goodfellow, bengio, and courville, *deep learning*.\n- research papers from neurips, icml, and iclr.\n- course notes announced each semester.\n\n## learning trajectory\n\nthis module is structured to build concepts progressively:\n\n1. **artificial neural networks**: perceptrons, activation functions, mlps, backpropagation, and the universal approximation theorem. you start with a single neuron and build up to networks that can learn any function.\n2. **convolutional neural networks**: convolution operations, pooling, feature hierarchies, residual connections (resnet), and transfer learning. the network gets eyes and learns to see.\n3. **u-net and segmentation**: encoder-decoder with skip connections, dice loss, and medical imaging applications. the natural next step after cnns: pixel-level understanding using the same building blocks with a clever u-shaped pipe.\n4. **design and optimization**: sgd, adam, learning rate schedules, regularization (dropout, batch norm), and initialization. the professional toolkit you need before building anything more ambitious.\n5. **variational autoencoders**: probabilistic encoder-decoder, elbo derivation, reparameterization trick, and latent space structure. the network learns to dream by mapping data onto a smooth landscape of possibilities.\n6. **generative adversarial networks**: adversarial training, dcgan, wgan, mode collapse, and evaluation metrics. a forger and a detective train together until the forgeries are perfect.\n7. **transformers**: attention mechanisms, multi-head attention, bert, gpt, vision transformers, and scaling laws. the architecture that replaced everything by letting every part of the input talk to every other part.\n8. **analysis and theory**: generalization bounds, double descent, neural tangent kernels, loss landscapes, adversarial robustness, and interpretability. the deepest question: why does deep learning work at all?\n\n## visual and simulation gallery\n\n[[figure adl-mlops-loop]]\n\n[[figure adl-cnn-feature-map]]\n"
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "transformers",
      "lessonTitle": "Transformers and Attention Mechanisms",
      "x": 0.09334664791822433,
      "y": 0.5145787000656128,
      "searchText": "transformers and attention mechanisms\n# transformers and attention mechanisms\n\n## why attention?\n\nrecurrent neural networks (rnns) process sequences one token at a time, creating two fundamental limitations:\n- **sequential processing**: each step depends on the previous, preventing parallelization during training.\n- **long-range dependencies**: information from early tokens must pass through many time steps to reach later ones, causing vanishing gradients and making it difficult to capture distant relationships.\n\nthe **attention mechanism** solves both problems in one stroke: it allows every position to directly attend to every other position in a single computation step. no waiting in line. no game of telephone where the signal degrades over distance. every word can talk to every other word directly.\n\n## the attention mechanism\n\nhere is the core intuition. you are at a loud party trying to hear your friend across the room. your brain does not process every sound equally \u2014 it automatically turns up the volume on your friend's voice and turns down the background chatter. that is attention: a learned mechanism for focusing on the relevant parts of the input while ignoring the rest.\n\nin a neural network, attention works through three roles: **queries**, **keys**, and **values**. think of it as a database lookup. the **query** is \"what am i looking for?\" the **key** is \"what do i have to offer?\" the **value** is \"here is the actual information.\" attention computes a soft match between each query and all keys, then returns a weighted combination of the corresponding values.\n\nformally, given queries $q$, keys $k$, and values $v$, **scaled dot-product attention** computes:\n\n$$\n\\text{attention}(q, k, v) = \\text{softmax}\\!\\left(\\frac{qk^t}{\\sqrt{d_k}}\\right) v,\n$$\n\nwhere $d_k$ is the dimension of the keys. the scaling factor $\\sqrt{d_k}$ prevents the dot products from growing too large, which would push the softmax into saturated regions with tiny gradients.\n\nthe attention weights $\\text{softmax}(qk^t / \\sqrt{d_k})$ form a matrix where each row sums to one. you can picture it as every word in a sentence voting on which other words matter most to it. the word \"it\" might vote heavily for \"the cat\" when trying to determine what \"it\" refers to.\n\n```python\ndef scaled_dot_product_attention(q, k, v, mask=none):\n    d_k = q.size(-1)\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not none:\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n    weights = f.softmax(scores, dim=-1)\n    return torch.matmul(weights, v)\n```\n<!--code-toggle-->\n```pseudocode\nfunction scaled_dot_product_attention(q, k, v, mask=none):\n    d_k = dimension(q, -1)\n    scores = matmul(q, transpose(k)) / sqrt(d_k)\n    if mask is not none:\n        scores = fill_where(scores, mask == 0, -inf)\n    weights = softmax(scores, dim=-1)\n    return matmul(weights, v)\n```\n\n## multi-head attention\n\na single attention function can only capture one type of relationship at a time. but language is rich: words relate to each other syntactically (subject-verb), semantically (synonyms), positionally (nearby words), and logically (cause-effect). **multi-head attention** runs several attention functions in parallel, each looking for a different type of pattern:\n\n$$\n\\text{multihead}(q, k, v) = \\text{concat}(\\text{head}_1, \\ldots, \\text{head}_h) w^o,\n$$\n\nwhere each head computes attention on a different linear projection:\n\n$$\n\\text{head}_i = \\text{attention}(q w_i^q, k w_i^k, v w_i^v).\n$$\n\none head might learn to attend to adjacent words. another might focus on long-range syntactic dependencies. another might track coreference. the network learns which types of relationships are useful.\n\n## the transformer architecture\n\nthe **transformer** (vaswani et al., 2017) replaces recurrence entirely with attention. it is the architecture that changed everything \u2014 enabling modern language models, vision transformers, and protein structure prediction.\n\n**encoder block** (repeated $n$ times):\n1. multi-head self-attention.\n2. add & layer normalization.\n3. position-wise feedforward network.\n4. add & layer normalization.\n\n**decoder block** (repeated $n$ times):\n1. masked multi-head self-attention (causal mask prevents attending to future tokens \u2014 you cannot cheat by looking at the answer).\n2. multi-head cross-attention (attends to encoder output).\n3. position-wise feedforward network.\n4. add & layer normalization at each step.\n\n**positional encoding** injects sequence order information since attention is permutation-invariant. without it, the transformer would treat \"dog bites man\" and \"man bites dog\" identically. the standard approach uses sine and cosine functions of different frequencies:\n\n$$\n\\text{pe}_{(pos, 2i)} = \\sin(pos / 10000^{2i/d}), \\qquad \\text{pe}_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d}).\n$$\n\n[[simulation adl-attention-heatmap]]\n\n## what if we didn't have attention?\n\nwithout attention, a recurrent network must pass information from word 1 to word 100 through 99 sequential steps. by the time the signal arrives, it has been transformed, attenuated, and mixed with everything in between. long-range dependencies become almost impossible to learn. attention gives every word a direct phone line to every other word \u2014 no intermediaries, no signal loss.\n\n## bert: bidirectional encoder representations\n\n**bert** (devlin et al., 2019) uses only the encoder stack with two pre-training objectives:\n\n- **masked language modeling** (mlm): randomly mask 15% of input tokens and predict them from context. the network must understand language deeply enough to fill in the blanks.\n- **next sentence prediction** (nsp): predict whether two sentences are consecutive.\n\nbert's bidirectional attention allows each token to attend to both left and right context, unlike autoregressive models that can only look backward. fine-tuning bert on downstream tasks (classification, ner, qa) achieved state-of-the-art results across nlp benchmarks.\n\n## gpt: "
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "unet",
      "lessonTitle": "U-Net Architecture",
      "x": 0.0002542570873629302,
      "y": 0.4430484175682068,
      "searchText": "u-net architecture\n# u-net architecture\n\n## the pixel-level challenge\n\nsuppose you are a doctor looking at a brain scan, and you need to outline exactly where a tumor is \u2014 not just \"there is a tumor in this image,\" but the precise boundary of every single pixel that belongs to the tumor. this is **image segmentation**: assigning a class label to every pixel in an image, producing a segmentation mask with the same spatial dimensions as the input.\n\nthe challenge is that you need two things simultaneously. you need the **big picture** \u2014 enough context to know that this blob is a tumor and not normal tissue. and you need **fine detail** \u2014 the exact pixel boundaries where the tumor ends and healthy tissue begins. how do you build a network that sees both the forest and the trees at the same time? you build a u-shaped pipe.\n\napplications span many domains:\n- **medical imaging**: organ and tumor segmentation in ct/mri scans.\n- **autonomous driving**: identifying roads, pedestrians, vehicles, and obstacles.\n- **satellite imagery**: land use classification, building detection, deforestation monitoring.\n- **microscopy**: cell counting and boundary detection in biological research.\n\n## encoder-decoder architecture\n\nsegmentation requires both **global context** (what object is this?) and **fine spatial detail** (where exactly are its boundaries?). this creates a fundamental tension:\n\n- **downsampling** (pooling, strided convolutions) builds a large receptive field and captures high-level semantics, but it loses spatial resolution. each pooling layer throws away positional information.\n- **upsampling** (transposed convolutions, bilinear interpolation) recovers spatial resolution, but the fine details that were lost during downsampling are gone.\n\nan **encoder-decoder** architecture addresses this by first compressing the input through an encoder (like a classification cnn) and then expanding it through a decoder that progressively restores spatial dimensions. but there is a problem: by the time the information reaches the bottleneck, the fine details have been squeezed out.\n\n## skip connections: the key insight\n\nthe innovation of **u-net** (ronneberger et al., 2015) is **skip connections** that directly connect encoder layers to their corresponding decoder layers at each resolution level. instead of forcing all information through the narrow bottleneck, skip connections give the decoder a shortcut to the fine-grained details from the encoder.\n\nat each level, the encoder feature maps are concatenated with the decoder feature maps. the decoder gets both the high-level \"what\" from the bottleneck and the low-level \"where\" from the encoder \u2014 and it learns to combine them.\n\n## what if we didn't have skip connections?\n\nwithout skip connections, the decoder would have to reconstruct fine spatial details from only the compressed bottleneck representation. it is like trying to describe the exact shape of a cloud to a friend over the phone, and then asking them to draw it. by the time the description passes through the bottleneck of language, the fine details are lost. skip connections are like also sending your friend a photograph \u2014 they get both your verbal description (global context) and the actual visual details.\n\n## the u-shape\n\nthe u-net gets its name from its symmetric u-shaped architecture. a typical configuration for 128x128 input:\n\n**encoder (contracting path)**:\n1. 128x128 x 1 channel $\\to$ 128x128 x 64 (two 3x3 convolutions)\n2. pool $\\to$ 64x64 x 64 $\\to$ 64x64 x 128 (two 3x3 convolutions)\n3. pool $\\to$ 32x32 x 128 $\\to$ 32x32 x 256 (two 3x3 convolutions)\n4. pool $\\to$ 16x16 x 256 $\\to$ 16x16 x 512 (bottleneck)\n\n**decoder (expanding path)**:\n5. upsample $\\to$ 32x32 x 512 $\\to$ concat with encoder level 3 $\\to$ 32x32 x 256\n6. upsample $\\to$ 64x64 x 256 $\\to$ concat with encoder level 2 $\\to$ 64x64 x 128\n7. upsample $\\to$ 128x128 x 128 $\\to$ concat with encoder level 1 $\\to$ 128x128 x 64\n8. final 1x1 convolution $\\to$ 128x128 x $c$ (number of classes)\n\nthe left side of the u compresses, the bottom is the bottleneck, and the right side expands back to full resolution. the horizontal connections across the u are the skip connections.\n\n[[simulation adl-unet-architecture]]\n\n## u-net model\n\n```python\nclass unet(nn.module):\n    def __init__(self, in_ch=1, out_ch=10):\n        super().__init__()\n        # encoder\n        self.enc1 = self._block(in_ch, 16)\n        self.enc2 = self._block(16, 32)\n        self.enc3 = self._block(32, 64)\n        self.bottleneck = self._block(64, 128)\n        self.pool = nn.maxpool2d(2, 2)\n        # decoder\n        self.up3 = nn.convtranspose2d(128, 64, 2, stride=2)\n        self.dec3 = self._block(128, 64)   # 64+64 from skip\n        self.up2 = nn.convtranspose2d(64, 32, 2, stride=2)\n        self.dec2 = self._block(64, 32)    # 32+32 from skip\n        self.up1 = nn.convtranspose2d(32, 16, 2, stride=2)\n        self.dec1 = self._block(32, 16)    # 16+16 from skip\n        self.out_conv = nn.conv2d(16, out_ch, 1)\n\n    def _block(self, in_c, out_c):\n        return nn.sequential(\n            nn.conv2d(in_c, out_c, 3, padding=1), nn.relu(),\n            nn.conv2d(out_c, out_c, 3, padding=1), nn.relu()\n        )\n\n    def forward(self, x):\n        # encoder\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool(e1))\n        e3 = self.enc3(self.pool(e2))\n        b = self.bottleneck(self.pool(e3))\n        # decoder with skip connections\n        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n        return self.out_conv(d1)\n```\n<!--code-toggle-->\n```pseudocode\nclass unet:\n    init(in_ch=1, out_ch=10):\n        // encoder blocks: each has two 3x3 convolutions + relu\n        enc1 = conv_block(in_ch, 16)\n        enc2 = conv_block(16, 32)\n        enc3 = conv_block(32, 64)\n        bottleneck = conv_block(64, 128)\n        pool = max_pool_2d(size=2)\n        // decoder: ups"
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "vae",
      "lessonTitle": "Variational Autoencoders",
      "x": 0.2264055609703064,
      "y": 0.45101550221443176,
      "searchText": "variational autoencoders\n# variational autoencoders\n\n## standard autoencoders\n\nan **autoencoder** compresses input $\\mathbf{x}$ to a low-dimensional latent representation $\\mathbf{z}$ through an encoder $f_\\phi$, then reconstructs the input through a decoder $g_\\theta$:\n\n$$\n\\mathbf{z} = f_\\phi(\\mathbf{x}), \\qquad \\hat{\\mathbf{x}} = g_\\theta(\\mathbf{z}).\n$$\n\ntraining minimizes reconstruction loss: $\\mathcal{l} = \\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|^2$.\n\nthe problem: the latent space of a standard autoencoder is a mess. it is **discontinuous and unstructured** \u2014 nearby points in latent space may decode to wildly different outputs, and large regions of the latent space correspond to nothing meaningful at all. if you pick a random point in latent space and try to decode it, you will likely get garbage. the autoencoder learned to compress and decompress, but it did not learn a *map* of all possible outputs.\n\n## the vae idea\n\na **variational autoencoder** (vae) fixes this by changing one thing: instead of mapping each input to a single point in latent space, the encoder outputs the parameters of a probability distribution \u2014 a little cloud of uncertainty around where the input *should* be in latent space:\n\n$$\nq_\\phi(\\mathbf{z} | \\mathbf{x}) = \\mathcal{n}(\\mathbf{z}; \\boldsymbol{\\mu}_\\phi(\\mathbf{x}), \\text{diag}(\\boldsymbol{\\sigma}^2_\\phi(\\mathbf{x}))).\n$$\n\nthe decoder then reconstructs from a sample $\\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})$. by forcing these little clouds to stay close to a standard normal prior $p(\\mathbf{z}) = \\mathcal{n}(\\mathbf{0}, \\mathbf{i})$, the vae ensures that the latent space is smooth and well-organized. every region of latent space decodes to something meaningful, and walking smoothly through latent space produces smooth transformations in the output.\n\n[[simulation adl-pca-demo]]\n\n## elbo derivation\n\nwhy does the vae loss function look the way it does? we want to maximize the marginal log-likelihood $\\log p_\\theta(\\mathbf{x})$ \u2014 the probability that our model assigns to the data we actually observe. but computing this directly requires integrating over all possible latent codes $\\mathbf{z}$, which is intractable.\n\nthe trick is to derive a tractable lower bound. starting from the log-marginal likelihood and introducing an approximate posterior $q_\\phi(\\mathbf{z}|\\mathbf{x})$:\n\n$$\n\\log p_\\theta(\\mathbf{x}) = \\log \\int p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z}) \\, d\\mathbf{z}.\n$$\n\napplying jensen's inequality (or equivalently, decomposing the kl divergence):\n\n$$\n\\log p_\\theta(\\mathbf{x}) = \\underbrace{\\mathbb{e}_{q_\\phi}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - d_{\\text{kl}}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}))}_{\\text{elbo}} + d_{\\text{kl}}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p_\\theta(\\mathbf{z}|\\mathbf{x})).\n$$\n\nsince the last kl term is non-negative, the **evidence lower bound** (elbo) is a lower bound on the log-likelihood. maximizing the elbo simultaneously:\n1. maximizes the expected reconstruction quality (first term).\n2. minimizes the gap between the approximate and true posterior (last kl term vanishes when $q_\\phi = p_\\theta(\\mathbf{z}|\\mathbf{x})$).\n\nthe loss function is therefore:\n\n$$\n\\mathcal{l}_{\\text{vae}} = -\\mathbb{e}_{q_\\phi}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] + d_{\\text{kl}}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z})).\n$$\n\nthe first term says \"reconstruct well.\" the second term says \"do not stray too far from the prior.\" together they produce a model that generates well and has a smooth latent space.\n\n## the reparameterization trick\n\nthis is one of the most beautiful ideas in the entire course. here is the problem: the encoder outputs a distribution, and the decoder needs a sample from that distribution. but sampling is a random operation \u2014 and you cannot backpropagate through randomness. the gradient of \"pick a random number\" with respect to the distribution parameters is undefined.\n\nthe reparameterization trick solves this with a simple, elegant maneuver. instead of sampling directly from the learned distribution, we sample a \"frozen\" piece of noise $\\boldsymbol{\\epsilon}$ from a fixed standard normal, and then *deterministically* transform it using the learned parameters:\n\n$$\n\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon}, \\qquad \\boldsymbol{\\epsilon} \\sim \\mathcal{n}(\\mathbf{0}, \\mathbf{i}).\n$$\n\nnow all the randomness lives in $\\boldsymbol{\\epsilon}$, which does not depend on any learnable parameters. the gradient flows cleanly through $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$, and we can train the whole system end-to-end with standard backpropagation. the frozen noise is just a source of randomness that the network reshapes; the shaping is what we optimize.\n\n## kl divergence term\n\nfor gaussian encoder and standard normal prior, the kl divergence has a closed-form solution:\n\n$$\nd_{\\text{kl}}(q_\\phi \\| p) = -\\frac{1}{2} \\sum_{j=1}^{d} \\left(1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right).\n$$\n\nwhat is this term actually doing? it is the universe telling the network \"do not be too sure of yourself.\" without the kl term, the encoder would collapse each input to a single point (zero variance) at some arbitrary location \u2014 perfectly memorizing each training example but creating a latent space full of gaps. the kl penalty forces each encoding to be a spread-out cloud that overlaps with the prior, filling the latent space with meaning.\n\n## what if we didn't have the kl term?\n\nwithout the kl regularizer, the vae degenerates into a regular autoencoder. the encoder would learn to place each training example at a unique, precise point in latent space with zero variance. reconstruction would be perfect, but sampling would be useless \u2014 random points in latent space would decode to nothing coherent. the kl term is the price of generativity.\n\n## reconstruction vs kl tradeoff: beta-vae\n\nthe standard vae loss weights reconstruction and kl terms equally. **beta-vae** introduces an explicit tradeoff:\n\n$$\n\\mat"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "dimensionality-reduction",
      "lessonTitle": "Dimensionality Reduction",
      "x": 0.2334832400083542,
      "y": 0.34494486451148987,
      "searchText": "dimensionality reduction\n# dimensionality reduction\n\nhere is a problem that comes up everywhere in science: you have data with hundreds or thousands of features, but the interesting structure lives on a much lower-dimensional surface. dimensionality reduction finds that surface. it lets you compress data without losing the signal, strip away noise, and \u2014 perhaps most importantly \u2014 see patterns that are invisible in the raw high-dimensional space.\n\n## pca: finding the directions that matter\n\nsuppose you photograph a 3d object from many angles. each photo has millions of pixels, but the photos are not independent \u2014 they all depict the same object, just rotated. the \"real\" information is only three-dimensional (the rotation angles). principal component analysis does exactly this kind of detective work: it looks at your data and asks, \"which directions carry the most variation, and which directions are mostly noise?\"\n\ntechnically, pca computes the covariance matrix of your centered data and finds its eigenvectors. the eigenvector with the largest eigenvalue points in the direction of greatest variance; the second eigenvector points in the direction of greatest remaining variance, perpendicular to the first; and so on.\n\nfor centered data matrix $x$:\n$$\nc=\\frac{1}{n-1}x^\\top x,\\quad z=x v_k\n$$\n\nhere $v_k$ contains the top $k$ eigenvectors of the covariance matrix $c$, and $z$ is your data projected into $k$ dimensions. each column of $v_k$ is a principal component \u2014 a direction in feature space ranked by how much variance it explains.\n\nuse pca when you need a fast, interpretable, linear embedding. it is the right first step for preprocessing before downstream models, for explained-variance diagnostics (\"how many dimensions do we actually need?\"), and as a sanity check before trying fancier methods.\n\n## t-sne: seeing your clusters\n\nt-sne is a nonlinear method designed for one thing: producing beautiful 2d visualizations where similar points cluster together. think of it as placing your data points on a rubber sheet, then stretching and compressing the sheet so that points that were close together in high-dimensional space stay close, while points that were far apart get pushed further away.\n\nthe result is often stunning \u2014 you can *see* clusters that were hidden in 100 dimensions. but there is a loud warning you must internalize: **the distances on a t-sne plot are not meaningful**. two clusters that appear far apart might actually be close in the original space, and the size of clusters can be misleading. use t-sne for visual exploration, never for quantitative distance comparisons.\n\n## umap: t-sne's faster cousin\n\numap achieves similar goals to t-sne \u2014 preserving local neighborhoods to reveal cluster structure \u2014 but it scales better to large datasets and often preserves more of the global organization. if t-sne gives you beautiful clusters but you cannot tell how the clusters relate to each other, umap may give you a more faithful big-picture layout. that said, the same warnings about interpreting distances apply.\n\n## autoencoders: learned compression\n\nimagine a kid who has to describe a painting to a friend over the phone using only ten words. the friend then tries to redraw the painting from those ten words alone. if the redrawing is good, those ten words captured the essence of the painting. the ten-word description is the **latent representation**, and the kid-and-friend pair is an **autoencoder**.\n\nan autoencoder is a neural network trained to reconstruct its input through a narrow bottleneck. the encoder compresses the input to a low-dimensional code; the decoder tries to reconstruct the original from that code.\n\n$$\n\\mathbf{z}=f_\\text{enc}(\\mathbf{x}),\\quad \\hat{\\mathbf{x}}=f_\\text{dec}(\\mathbf{z}),\\quad \\mathcal{l}=\\|\\mathbf{x}-\\hat{\\mathbf{x}}\\|^2\n$$\n\nin words: we encode input $\\mathbf{x}$ to a compressed code $\\mathbf{z}$, decode it back to $\\hat{\\mathbf{x}}$, and minimize the reconstruction error. if the bottleneck has dimension $d$ and reconstruction quality is high, the data's intrinsic dimensionality is at most $d$.\n\nunlike pca, autoencoders can capture curved, nonlinear manifolds. architecture choices (depth, width, activation functions) control the kind of embedding you get. and because they are neural networks, they can be trained end-to-end with a downstream task. for the generative extension (variational autoencoders), see [advanced deep learning \u2014 vaes](/topics/advanced-deep-learning/vae).\n\n## when to use which?\n\nhere is a practical decision guide:\n\n- **you need speed and linear interpretability** \u2014 use pca. it is deterministic, fast, and gives you explained-variance scores to decide how many dimensions to keep.\n- **you need detailed 2d cluster visualizations** \u2014 use t-sne. set perplexity carefully and run it multiple times with different seeds to check that the clusters are stable.\n- **you need a fast nonlinear embedding that scales** \u2014 use umap. it handles large datasets well and often gives clearer global structure than t-sne.\n- **pca underperforms and you need a learnable, nonlinear embedding** \u2014 use autoencoders. especially valuable when you want to train the embedding end-to-end with a downstream model.\n\n## interactive simulations\n\n[[simulation aml-pca-correlated-data]]\n\n[[simulation aml-explained-variance]]\n\n[[simulation aml-tsne-umap-comparison]]\n\n## practical warnings\n\n- standardize features before pca \u2014 otherwise the component with the largest scale dominates.\n- t-sne and umap output can vary with random seed and hyperparameters. always run multiple times.\n- do not over-interpret apparent distances between far-apart clusters in t-sne.\n- autoencoder embeddings depend on architecture and training. validate that the bottleneck dimension captures meaningful structure before trusting the embedding.\n\n## check your understanding\n\n- can you explain pca to a friend using the \"photographs of a 3d object\" analogy?\n- what is the one picture in your head that captures why t-sne"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "generative-adversarial-networks",
      "lessonTitle": "Generative Adversarial Networks (Extension)",
      "x": 0.16027119755744934,
      "y": 0.5418382883071899,
      "searchText": "generative adversarial networks (extension)\n# generative adversarial networks (extension)\n\nimagine two kids locked in an arms race. one is a forger who paints fake banknotes. the other is a detective who inspects them. every time the detective catches a fake, the forger studies what gave it away and makes a better fake next time. every time a fake slips past, the detective sharpens their eye. over time, both get astonishingly good \u2014 and the fakes become nearly indistinguishable from the real thing. that is the core idea behind generative adversarial networks.\n\n## the adversarial game\n\na gan trains two neural networks simultaneously:\n\n- a **generator** $g$ takes random noise $z$ and produces a synthetic sample $g(z)$. its goal is to fool the discriminator.\n- a **discriminator** $d$ receives both real samples from the training data and fakes from the generator. its goal is to tell them apart.\n\nthe two play a min-max game:\n\n$$\n\\min_g \\max_d \\; \\mathbb{e}_{x\\sim p_{data}}[\\log d(x)] + \\mathbb{e}_{z\\sim p_z}[\\log(1-d(g(z)))]\n$$\n\nthink of this as a zero-sum tennis match. the discriminator tries to maximize its score (correctly classifying real vs. fake), while the generator tries to minimize it (making fakes that the discriminator calls real). at equilibrium \u2014 if you ever reach it \u2014 the generator produces samples so realistic that the discriminator can do no better than random guessing.\n\nin plain english: the first term rewards the discriminator for giving high scores to real data. the second term rewards it for giving low scores to fakes \u2014 but the generator is simultaneously trying to make that second term as small as possible by generating convincing samples.\n\n## mode collapse: when the forger gets lazy\n\nhere is the most infamous failure mode. the forger discovers that painting only happy faces always fools the detective \u2014 so they never learn to paint landscapes, animals, or buildings. the generator has found a narrow pocket of \"safe\" outputs and stubbornly refuses to explore the full diversity of the data distribution.\n\nthis is **mode collapse**: the generator produces high-quality but low-diversity samples, capturing only a few modes of the real distribution. you asked for a model that can generate any face, and you got one that generates the same three faces over and over.\n\n## stabilizing training\n\ngan training is notoriously finicky. the generator and discriminator are locked in a delicate dance, and small imbalances can cascade.\n\n**wasserstein objective** replaces the log-probability game with a smoother distance metric (the earth-mover distance), giving the generator more useful gradients even when the discriminator is very confident.\n\n**gradient penalty** adds a regularization term that prevents the discriminator from becoming too sharp, keeping the gradients informative for the generator.\n\n**spectral normalization** constrains the discriminator's weights to control its lipschitz constant, stabilizing the adversarial dynamics.\n\nthese are engineering solutions to a fundamental tension: the two networks must improve at roughly the same rate, or one overpowers the other and learning stalls.\n\n## where this goes deeper\n\nthis page gives you the intuition for adversarial training. gans are a bridge from classical applied ml to the wild world of deep generative modeling. for architecture details (dcgan, stylegan, conditional gans), training recipes, and the relationship to other generative approaches (vaes, diffusion models), see [advanced deep learning \u2014 generative models](/topics/advanced-deep-learning/gan).\n\n## check your understanding\n\n- can you explain the gan training loop to a friend using the forger-and-detective story?\n- what is the one picture in your head that captures mode collapse?\n- what experiment would tell you whether your gan is suffering from mode collapse rather than simply needing more training?\n"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "graph-neural-networks",
      "lessonTitle": "Graph Neural Networks",
      "x": 0.04408210888504982,
      "y": 0.5022554397583008,
      "searchText": "graph neural networks\n# graph neural networks\n\nwhat if your data has friendships?\n\nnot all data fits neatly into a table or a sequence. molecules are atoms connected by bonds. social networks are people connected by relationships. protein structures are amino acids connected in 3d space. in all these cases, the connections carry as much information as the entities themselves. graph neural networks learn on this structure directly, without forcing relational data into a flat grid.\n\n## why graphs\n\nmany systems we care about are fundamentally relational:\n\n- **molecules**: atoms are nodes, bonds are edges. predicting molecular properties requires understanding the connectivity, not just a list of atom types.\n- **social and communication networks**: who talks to whom shapes information flow, influence, and community structure.\n- **recommendation systems**: users and items form a bipartite graph, and similar users connect to similar items.\n- **physical simulations**: particles interact through local forces, forming interaction graphs.\n\na graph $g=(v,e)$ has nodes $v$ (entities) and edges $e$ (relationships). each node can carry a feature vector, and edges can have their own features (bond type, interaction strength, distance).\n\n## message passing: the gossip network\n\nthe central idea in gnns is beautifully simple. every node is gossiping with its neighbors, then updating its own diary based on what it heard.\n\nmore precisely, a single gnn layer does two things:\n\n1. **aggregate**: each node collects messages from its neighbors.\n2. **update**: each node updates its own representation using the collected messages and its current state.\n\n$$\nm_v^{(l+1)}=\\bigoplus_{u\\in\\mathcal{n}(v)}\\phi^{(l)}(h_v^{(l)},h_u^{(l)},e_{uv})\n$$\n$$\nh_v^{(l+1)}=\\psi^{(l)}(h_v^{(l)},m_v^{(l+1)})\n$$\n\nin words: node $v$ gathers messages from all its neighbors $u \\in \\mathcal{n}(v)$ using a message function $\\phi$. the aggregation $\\bigoplus$ (sum, mean, or max) must be permutation-invariant \u2014 it should not matter what order you read the messages. then node $v$ updates its hidden state $h_v$ using an update function $\\psi$ that combines the old state with the aggregated messages.\n\nafter $l$ layers of message passing, each node's representation encodes information from its $l$-hop neighborhood. one layer captures immediate neighbors; two layers capture neighbors-of-neighbors; and so on.\n\n## gcn normalization\n\nthe graph convolutional network (gcn) is one of the simplest and most widely used gnn variants. for adjacency matrix $a$:\n\n$$\n\\tilde{a}=a+i,\\quad\nh^{(l+1)}=\\sigma\\left(\\tilde{d}^{-1/2}\\tilde{a}\\tilde{d}^{-1/2}h^{(l)}w^{(l)}\\right)\n$$\n\nthe key idea: we add self-loops ($a+i$, so each node also receives its own message), then normalize by the degree matrix $\\tilde{d}$ so that high-degree nodes do not dominate simply because they have more neighbors. the result is a weighted average of neighbor features, followed by a linear transformation and nonlinearity \u2014 essentially a localized convolution on the graph.\n\n## a tiny example you can compute by hand\n\nconsider four nodes in a triangle-plus-tail graph: nodes a, b, c form a triangle, and node d connects only to c.\n\ninitial features: $h_a=[1,0]$, $h_b=[0,1]$, $h_c=[1,1]$, $h_d=[0,0]$.\n\nwith a simple mean-aggregation gnn layer (no learned weights, just averaging):\n\n- node a's neighbors are b, c. message: mean of $[0,1]$ and $[1,1]$ = $[0.5, 1.0]$.\n- node b's neighbors are a, c. message: mean of $[1,0]$ and $[1,1]$ = $[1.0, 0.5]$.\n- node c's neighbors are a, b, d. message: mean of $[1,0]$, $[0,1]$, $[0,0]$ = $[0.33, 0.33]$.\n- node d's neighbor is only c. message: $[1,1]$.\n\nafter one layer, each node knows about its immediate neighbors. after two layers, node d would know about a and b through c \u2014 information has propagated across the graph.\n\n## oversmoothing: when everyone sounds the same\n\nthere is a catch with stacking many gnn layers. after too many rounds of gossip, everyone ends up with the same opinion. this is **oversmoothing**: node representations converge to indistinguishable vectors as you add layers, because information has diffused uniformly across the graph. it is like a party where everyone has talked to everyone else so many times that all opinions have blended into a single consensus.\n\nin practice, most gnns work best with 2\u20134 layers. beyond that, you need architectural tricks (skip connections, normalization, or attention mechanisms) to preserve node identity.\n\n## interactive simulations\n\n[[simulation aml-graph-convolution-intuition]]\n\n[[simulation aml-graph-adjacency-demo]]\n\n[[simulation aml-graph-message-passing]]\n\n## practical notes\n\n- the aggregation function must be permutation-invariant \u2014 sum, mean, and max are the standard choices.\n- task formulation matters: node-level tasks (classify each node), edge-level tasks (predict whether an edge exists), and graph-level tasks (predict a property of the whole graph) require different readout strategies.\n- for graph-level tasks, you need a pooling step that aggregates all node representations into a single graph vector.\n\n## check your understanding\n\n- can you explain message passing to a friend using the gossip analogy?\n- what is the one picture in your head that shows why too many gnn layers cause oversmoothing?\n- what experiment would test whether your gnn benefits from 2 layers versus 4?\n"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "home",
      "lessonTitle": "Applied Machine Learning",
      "x": 0.10610253363847733,
      "y": 0.3636208474636078,
      "searchText": "applied machine learning\n# applied machine learning\n\n## course overview\n\napplied machine learning studies how to design models that generalize well to new data while remaining computationally practical. this module progresses from how models learn, through classical methods and data understanding, to neural network architectures for structured, sequential, and relational data.\n\n## why this order?\n\nwe start where every ml system starts: with loss functions and optimization. before you can appreciate any model, you need to understand what \"learning\" actually means \u2014 a model adjusts its knobs to make a number (the loss) go down. once you feel that in your bones, we move to decision trees and ensembles, which are the most intuitive and practically powerful models for structured data. you can literally draw them on a napkin. next comes dimensionality reduction, because before building fancier models you need to understand what your data looks like in high-dimensional space \u2014 and how to squash it down so you can see patterns. with that foundation, we introduce neural networks: the general-purpose function approximators behind modern deep learning. finally, we offer three taster lessons on specialized architectures \u2014 recurrent networks for sequences, graph networks for relational data, and gans for generative modeling \u2014 each of which gets full treatment in the advanced deep learning module.\n\n## what you will learn\n\n- **loss functions and optimization** \u2014 how models learn from data, and why the choice of loss function shapes everything downstream.\n- **decision trees and ensemble methods** \u2014 strong, interpretable baselines for structured data that you should always try before reaching for neural networks.\n- **dimensionality reduction** \u2014 how to compress and visualize high-dimensional spaces so you can see what your model sees.\n- **neural network fundamentals** \u2014 the building blocks (layers, activations, backpropagation, regularization) behind all modern deep learning.\n- **recurrent neural networks** (extension) \u2014 extending neural networks to sequential data like time series and language.\n- **graph neural networks** (extension) \u2014 extending neural networks to relational data that lives on graphs.\n- **generative adversarial networks** (extension) \u2014 a bridge from classical ml to modern deep generative modeling.\n\nthe three extension lessons are tasters \u2014 they give you the core intuition and one worked idea, then point you to [advanced deep learning](/topics/advanced-deep-learning) for the full treatment.\n\n## why this topic matters\n\nmachine learning drives modern scientific discovery, from molecular property prediction to climate modeling. understanding loss functions and optimization dynamics is essential for training any model effectively. tree ensembles remain the strongest baselines for tabular data across many domains. dimensionality reduction is critical for exploratory data analysis and feature engineering. neural networks are the foundation of nearly all modern architectures, and graph neural networks open ml to relational data that cannot be expressed as flat tables.\n\n## prerequisites\n\n- linear algebra (vectors, matrices, eigenvalues/eigenvectors).\n- probability and statistics.\n- multivariable calculus (gradients and chain rule).\n- introductory python or javascript-level programming familiarity.\n\n## relation to other modules\n\nif you have taken applied statistics, you will recognize some topics here (decision trees, pca, model evaluation). this module covers them in greater depth with a focus on ml methodology rather than statistical inference. for deeper coverage of specific architectures (cnns, transformers, vaes), see [advanced deep learning](/topics/advanced-deep-learning).\n"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "loss-optimization",
      "lessonTitle": "Loss Functions and Optimization",
      "x": 0.17976494133472443,
      "y": 0.3782540559768677,
      "searchText": "loss functions and optimization\n# loss functions and optimization\n\nimagine you are blindfolded on a bumpy mountain, and your only goal is to reach the lowest valley. you can feel the slope under your feet, so each step you take is downhill. that is gradient descent \u2014 and the terrain you are walking on is the loss function. everything in machine learning starts here: you pick a number that measures how wrong your predictions are (the loss), and then you adjust your model's parameters to make that number smaller.\n\n## classification losses\n\nwhen the task is to assign labels \u2014 spam or not spam, cat or dog \u2014 we need a loss that punishes wrong labels.\n\n**zero-one loss** is the most natural: it counts the number of mistakes. but it is flat everywhere except at the decision boundary, which means it gives the optimizer no slope to follow. you cannot walk downhill on a plateau.\n\n**hinge loss** fixes this by enforcing a margin. it says: \"not only should you get the label right, you should be confident about it.\" this is the loss behind support vector machines.\n\n**binary cross-entropy** takes a probabilistic view. it says: \"tell me the probability you assign to the correct class, and i will punish you logarithmically for being wrong.\" if you predict 0.99 for the correct class, the penalty is tiny. if you predict 0.01, the penalty is enormous.\n\n$$\n\\mathcal{l}_{bce}=-\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i\\log\\hat{p}_i+(1-y_i)\\log(1-\\hat{p}_i)\\right)\n$$\n\nin words: for each sample, we take the log of the predicted probability for the true class, and average across the dataset. confident correct predictions contribute near-zero loss; confident wrong predictions blow up.\n\n## regression losses\n\nwhen predicting continuous values \u2014 house prices, temperatures, molecular energies \u2014 we need a different kind of penalty.\n\nthink of it this way: suppose you are estimating earthquake damage across a city. **mse (mean squared error)** squares every error before averaging. a house where you are off by $100k gets 100 times the penalty of a house where you are off by $10k. mse hates large errors and will bend over backwards to fix them, even at the cost of making small errors slightly worse.\n\n**mae (mean absolute error)** treats all errors proportionally. off by $10k? penalty of $10k. off by $100k? penalty of $100k. it does not obsess over outliers. but it has a kink at zero, which makes optimization slightly trickier.\n\n**huber loss** gives you the best of both worlds. it behaves like mse for small errors (smooth, easy to optimize) and like mae for large errors (robust to outliers).\n\n## optimization dynamics\n\nnow back to our blindfolded hiker. gradient descent updates every parameter by taking a step proportional to the downhill slope:\n\n$$\n\\theta_{t+1}=\\theta_t-\\eta \\nabla_{\\theta}\\mathcal{l}(\\theta_t)\n$$\n\nhere $\\eta$ is the learning rate \u2014 your step size. this single number controls everything. if $\\eta$ is too large, you overshoot the valley and bounce around wildly, possibly diverging. if $\\eta$ is too small, you inch forward painfully slowly and might get stuck in a shallow dip that is not the true minimum.\n\nmodern optimizers improve on vanilla gradient descent. **momentum** remembers past steps and keeps rolling in a consistent direction, like a ball with inertia. **rmsprop** adapts the step size per parameter, taking bigger steps in flat directions and smaller steps in steep ones. **adam** combines both ideas and is the default starting point for most practitioners.\n\n## validation and overfitting\n\nhere is the most important discipline in all of machine learning: always separate your data into three pools.\n\nyour **training set** is where the model learns \u2014 it adjusts parameters by minimizing the loss on these samples. your **validation set** is your mirror \u2014 you check your model's performance here after each round of training, but you never train on it. your **test set** is sacred. you touch it once, at the very end, to get an honest estimate of how the model performs on data it has never influenced.\n\nwatch the training and validation loss curves together. in early training, both go down \u2014 the model is learning real patterns. at some point the training loss keeps falling but the validation loss starts creeping up. that is the moment overfitting begins: the model is memorizing noise in the training data instead of learning generalizable structure. the best parameters correspond to the lowest validation loss.\n\n### k-fold cross-validation\n\nwhen data is limited, holding out a validation set feels wasteful. k-fold cross-validation makes every sample count.\n\nhere is a concrete example. suppose you have five data points: [2, 5, 7, 11, 13]. with $k=5$ (leave-one-out), you train five times. the first time, you hold out 2 and train on [5, 7, 11, 13]. the second time you hold out 5 and train on [2, 7, 11, 13]. and so on. each sample gets exactly one turn as the validator. you average all five validation scores to estimate how well your model generalizes.\n\nin general:\n1. split the data into $k$ equally sized folds.\n2. train on $k-1$ folds, validate on the remaining fold.\n3. rotate the held-out fold and repeat $k$ times.\n4. average the $k$ validation scores to estimate generalization error.\n\ntypical choices are $k=5$ or $k=10$. leave-one-out ($k=n$) gives low bias but high variance and is computationally expensive.\n\n### time-series cross-validation\n\nstandard k-fold is cheating for temporal data \u2014 it lets the model peek into the future when predicting the past. instead, we use chronological splits that respect the arrow of time.\n\n**expanding window**: train on all data up to time $t$, validate on $t+1,\\ldots,t+h$. slide $t$ forward and repeat. your training set grows each round.\n\n**rolling window**: train on a fixed-size window ending at $t$, validate on the next $h$ steps. older data is dropped as the window advances. use this when you believe recent patterns matter more than distant history.\n\nboth approaches prevent data leakage b"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "neural-networks-fundamentals",
      "lessonTitle": "Neural Network Fundamentals",
      "x": 0.062322281301021576,
      "y": 0.45789051055908203,
      "searchText": "neural network fundamentals\n# neural network fundamentals\n\nneural networks are parameterized function approximators that learn representations directly from data. instead of hand-engineering features, you give the network raw input and let it discover the patterns. this page introduces the building blocks you need before studying specialized architectures like rnns and gnns.\n\n## the multilayer perceptron\n\na feedforward network (mlp) with $l$ layers computes:\n\n$$\n\\mathbf{h}^{(l)}=\\sigma\\bigl(w^{(l)}\\mathbf{h}^{(l-1)}+\\mathbf{b}^{(l)}\\bigr),\\quad l=1,\\ldots,l\n$$\n\nin words: each layer takes the previous layer's output, applies a linear transformation (multiply by weights $w$, add bias $b$), then passes the result through a nonlinear activation function $\\sigma$. the input is $\\mathbf{h}^{(0)}=\\mathbf{x}$, and the final layer produces predictions \u2014 logits for classification, continuous values for regression.\n\nwhy does stacking layers help? each layer composes a new set of features from the previous ones. the first layer might detect edges; the second, shapes; the third, objects. depth lets the network build increasingly abstract representations.\n\n## activation functions\n\nwithout a nonlinear activation, stacking layers would be pointless \u2014 a chain of linear transformations is just one big linear transformation. the activation function is what gives neural networks their power.\n\n**relu** ($\\max(0,x)$) is the workhorse. it is simple, fast, and its gradient is either 0 or 1, which helps with training. the downside: if a neuron's input is always negative, its output is permanently zero \u2014 a \"dead neuron\" that never recovers.\n\n**leaky relu** ($\\max(\\alpha x, x)$ with small $\\alpha>0$) fixes the dead neuron problem by giving a small slope to negative inputs.\n\n**sigmoid** ($1/(1+e^{-x})$) squashes output to $(0,1)$, making it natural for probability outputs. but for large $|x|$, the gradient nearly vanishes, making deep networks hard to train.\n\n**tanh** ($\\tanh(x)$) is a zero-centered version of sigmoid with the same vanishing gradient issue.\n\n**gelu / silu** are smooth approximations to relu used in modern architectures like transformers. they give slightly better training dynamics in practice.\n\nstart with relu for hidden layers unless you have a specific reason to choose otherwise.\n\n## backpropagation\n\nhere is the key question: the network has thousands (or millions) of parameters, and the loss is a single number. how do we figure out which parameters to adjust, and by how much?\n\nimagine you are a principal grading a school's exam results. the students did poorly, but you do not know if the problem was the math teacher, the english teacher, or the p.e. coach. so you trace the blame backward: the final grades depend on the teachers' contributions, which depend on the curriculum, which depends on the department heads. you pass the blame backward through the chain until you know exactly how much each person contributed to the failure.\n\nthat is backpropagation. using the chain rule of calculus, we propagate error gradients from the output layer back through every layer to every parameter:\n\n$$\n\\frac{\\partial\\mathcal{l}}{\\partial w^{(1)}}=\\frac{\\partial\\mathcal{l}}{\\partial\\mathbf{h}^{(2)}}\\cdot\\frac{\\partial\\mathbf{h}^{(2)}}{\\partial\\mathbf{h}^{(1)}}\\cdot\\frac{\\partial\\mathbf{h}^{(1)}}{\\partial w^{(1)}}\n$$\n\neach term in this chain tells us: \"how much does a small change in this layer's output affect the next?\" multiplied together, they give the gradient of the loss with respect to the first layer's weights. modern frameworks (pytorch, jax) compute these gradients automatically, but understanding the chain rule structure still matters for diagnosing training failures \u2014 when gradients explode or vanish, it is usually because one of these terms is too large or too small.\n\n## regularization\n\nneural networks have many parameters, and given enough capacity they will happily memorize the training data \u2014 including the noise. regularization techniques fight this.\n\n**dropout** randomly zeroes out a fraction of activations during training. this forces the network to learn redundant representations \u2014 no single neuron can become a critical bottleneck. it acts as an implicit ensemble: each training step uses a different random sub-network.\n\n**weight decay** ($l_2$ regularization) adds $\\lambda\\|\\theta\\|_2^2$ to the loss, penalizing large weights. this encourages smoother functions that are less likely to memorize noise.\n\n**batch normalization** normalizes activations within each mini-batch to zero mean and unit variance. this stabilizes training and allows higher learning rates.\n\n**early stopping** monitors validation loss and halts training when it starts increasing. it is the simplest regularization and often the most effective \u2014 your model is telling you it has learned all the real patterns and is now memorizing noise.\n\n## universal approximation\n\na remarkable theorem (cybenko, 1989) says that a single hidden layer with enough neurons can approximate any continuous function on a compact set. so why use deep networks? because a single wide layer can approximate anything in theory, but it may need an astronomically large number of neurons. depth is a swiss army knife \u2014 deep networks with fewer neurons per layer learn more efficiently, because each layer composes features hierarchically. width is a sledgehammer; depth is elegance.\n\n## architecture landscape\n\nneural networks specialize by changing how layers are connected. here is the family portrait:\n\n| architecture | structure | best for |\n|---|---|---|\n| mlp | fully connected layers \u2014 every neuron talks to every neuron | tabular data, baselines |\n| cnn | convolutional filters that slide over the input \u2014 looking through a moving window | images, spatial data |\n| rnn / lstm | recurrent connections that carry a hidden state through time | sequences, time series |\n| gnn | message passing between neighbors on a graph | relational / graph data |\n| transformer | self-atte"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "recurrent-neural-networks",
      "lessonTitle": "Recurrent Neural Networks (Extension)",
      "x": 0.09725473821163177,
      "y": 0.4696885049343109,
      "searchText": "recurrent neural networks (extension)\n# recurrent neural networks (extension)\n\nsome data only makes sense in order. the sentence \"the cat sat on the mat\" means something; \"mat the on sat cat the\" does not. temperature readings, stock prices, sensor streams, musical notes \u2014 whenever the meaning depends on sequence, we need a model that remembers what came before. that is what recurrent neural networks do.\n\n## core recurrence\n\na standard feedforward network processes each input independently, with no memory. an rnn adds a crucial ingredient: a hidden state $h_t$ that carries information from one timestep to the next.\n\nat each timestep $t$:\n$$\nh_t=f(w_{xh}x_t + w_{hh}h_{t-1} + b_h),\\quad\ny_t=g(w_{hy}h_t+b_y)\n$$\n\nin words: the hidden state $h_t$ is a blend of the current input $x_t$ and the previous hidden state $h_{t-1}$, passed through a nonlinearity. the output $y_t$ is then computed from the hidden state. the same weights $w_{xh}$, $w_{hh}$, and $w_{hy}$ are reused at every timestep \u2014 the network \"unrolls\" through time but shares parameters.\n\nthis means an rnn can, in principle, handle sequences of any length. it reads one token at a time, updates its internal state, and carries a compressed summary of everything it has seen so far.\n\n## the vanishing gradient problem\n\nhere is the catch. imagine a game of telephone where a message passes through 50 people. by the time it reaches the last person, the message is garbled beyond recognition. the same thing happens in an rnn: when we backpropagate through many timesteps, the gradients get multiplied by the same weight matrix over and over. if that matrix has eigenvalues less than 1, the gradients shrink exponentially \u2014 they *vanish*. the network forgets early inputs because the error signal from the end of the sequence never makes it back to the beginning.\n\nthe opposite can also happen: if eigenvalues are greater than 1, gradients *explode*, and training becomes unstable. gradient clipping (capping the gradient norm) is a practical fix for explosions, but vanishing gradients require a more fundamental architectural change.\n\n## gated variants: lstm and gru\n\nthe solution is to give the network a notebook \u2014 a persistent memory cell with explicit gates that control what to remember, what to forget, and what to output.\n\n**lstm (long short-term memory)** introduces a cell state $c_t$ that runs alongside the hidden state. three gates control it: the *forget gate* decides what old information to erase, the *input gate* decides what new information to write, and the *output gate* decides what part of the cell state to expose as the hidden state. because the cell state can pass through timesteps with only additive updates (no repeated matrix multiplication), gradients flow much more easily over long sequences.\n\n**gru (gated recurrent unit)** is a simplified variant with two gates instead of three. it merges the cell state and hidden state into one, using a *reset gate* and an *update gate*. grus are faster to train and often perform comparably to lstms. when in doubt, try both.\n\n## a concrete example\n\nconsider predicting the next word. given \"the cat sat on the ___\", a simple rnn can likely predict \"mat\" \u2014 the context is short and recent. but now consider: \"the author, who grew up in paris and studied literature at the sorbonne before moving to new york where she worked as a journalist for twenty years, finally published her ___\". to predict \"book\" or \"novel,\" the model must remember \"author\" and \"published\" across a gap of 30+ words. a vanilla rnn's hidden state would have long forgotten \"author\" by the time it reaches \"her.\" an lstm, with its explicit memory cell, can carry that information across the entire sentence.\n\n## where this topic goes deeper\n\nthis page gives you the core intuition for sequence modeling. for full architectural details \u2014 bidirectional rnns, attention mechanisms, the evolution from lstms to transformers, and practical sequence modeling workflows \u2014 see [advanced deep learning \u2014 sequence models](/topics/advanced-deep-learning/ann).\n\n## practical checklist\n\n- normalize and window your sequence data carefully \u2014 sequence models are sensitive to scale and length.\n- always use chronological validation splits, never random shuffling.\n- start with an lstm or gru. switch to a transformer if sequences are long and you have enough data.\n- track both short-horizon and long-horizon error metrics to understand where your model struggles.\n\n## check your understanding\n\n- can you explain the vanishing gradient problem using the telephone game analogy?\n- what is the one picture in your head that shows how an lstm gate decides what to remember?\n- what experiment would reveal whether your sequence model is actually using long-range context or just predicting from the last few tokens?\n"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "trees-ensembles",
      "lessonTitle": "Decision Trees and Ensemble Methods",
      "x": 0.22282807528972626,
      "y": 0.30005818605422974,
      "searchText": "decision trees and ensemble methods\n# decision trees and ensemble methods\n\nhave you ever played twenty questions? you think of an animal, and your friend asks yes-or-no questions to narrow it down. \"does it have fur?\" \"is it bigger than a cat?\" \"does it live in water?\" each question splits the remaining possibilities into two groups. that is exactly what a decision tree does \u2014 except it asks the questions that split the data *best*.\n\n## decision trees\n\na decision tree recursively partitions feature space by choosing, at each node, the question (feature and threshold) that produces the purest child groups. but how do we measure \"pure\"?\n\n**gini impurity** asks: if you picked two random samples from this group, what is the probability they belong to different classes?\n\n$$\ng(s)=1-\\sum_{k=1}^{k}p_k^2\n$$\n\nwhen every sample in a group belongs to the same class, $g=0$ (perfectly pure). when classes are evenly mixed, $g$ is at its maximum. gini is fast to compute and works well in practice.\n\n**entropy** comes from information theory and asks a different question: how surprised are you by a random sample from this group?\n\n$$\nh(s)=-\\sum_{k=1}^{k}p_k\\log p_k\n$$\n\nif every sample is class a, you are never surprised \u2014 entropy is zero. if the group is a 50/50 mix, every sample is maximally surprising \u2014 entropy is at its peak. in practice, gini and entropy give very similar trees. gini is the default in most libraries because it avoids the logarithm.\n\ntrees are wonderfully interpretable \u2014 you can draw them on a whiteboard and explain every prediction. but a single deep tree will memorize the training data, fitting noise along with signal. that is where ensembles come in.\n\n## ensemble methods\n\nthe key insight behind ensembles is that combining many imperfect models can produce something much better than any individual.\n\n**random forests (bagging)** take the \"wisdom of crowds\" approach. imagine 100 people each given a slightly different, incomplete map of a city. individually, each person will get lost sometimes. but if you ask all 100 for directions and go with the majority vote, you almost never get lost. random forests train many trees, each on a random subset of the data and a random subset of features. the trees make independent errors, and averaging cancels those errors out. bagging reduces variance \u2014 the predictions become more stable without significantly increasing bias.\n\n**gradient boosting (adaboost, xgboost, lightgbm)** takes the opposite approach. instead of training many trees in parallel, it trains them sequentially. each new tree focuses specifically on the mistakes the previous trees made. think of it as one very determined hiker who, after every wrong turn, studies exactly where they went wrong and adjusts their strategy. boosting reduces bias \u2014 the model becomes progressively better at capturing complex patterns, though you need to be careful not to overfit by boosting too many rounds.\n\n## bias-variance intuition\n\nhere is the core tradeoff in all of machine learning, and trees make it beautifully concrete.\n\na very shallow tree (say, depth 2) is like a tourist with a simple rule: \"if you are north of the river, go east.\" it is too simple to capture the real layout of the city \u2014 that is **high bias**, underfitting. a very deep tree that memorizes every turn you have ever taken is the opposite problem: it works perfectly on streets you have seen but fails on new ones \u2014 that is **high variance**, overfitting.\n\nbagging (random forests) fights variance by averaging many high-variance trees \u2014 each tree overfits differently, and the errors cancel. boosting fights bias by building a sequence of simple trees that progressively correct each other's mistakes. in practice, gradient boosting with careful tuning (early stopping, regularization) is one of the most powerful methods for structured data.\n\n## interactive simulations\n\n[[simulation aml-tree-split-impurity]]\n\n[[simulation aml-tree-ensemble-xor]]\n\n## model selection notes\n\n- always use cross-validation for small tabular datasets \u2014 a single train/test split is too noisy.\n- key hyperparameters to tune: `max_depth`, `min_samples_leaf`, `n_estimators`, and learning rate (for boosting).\n- when you use predicted probabilities for decisions (not just class labels), run calibration checks \u2014 tree ensembles can produce poorly calibrated probabilities.\n\n## check your understanding\n\n- can you explain to a non-technical friend what a decision tree does using the twenty-questions game?\n- what is the one picture in your head that shows why averaging many noisy predictions reduces error?\n- what experiment would reveal whether your tree ensemble is overfitting?\n"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "advanced-fitting-calibration",
      "lessonTitle": "Advanced Fitting and Calibration",
      "x": 0.3222605884075165,
      "y": 0.16957026720046997,
      "searchText": "advanced fitting and calibration\n# advanced fitting and calibration\n\n## beyond simple fitting\n\nin the chi-square chapter, we fit models with a handful of parameters and gaussian errors. real experimental data is often messier: multiple overlapping signals, correlated parameters, systematic uncertainties from the instrument itself, and competing models that all look plausible. this chapter builds on that foundation to handle the complications that arise in serious data analysis.\n\n## multi-component models\n\nmany physical measurements involve a **signal plus background** decomposition. you are looking for a small peak sitting on top of a large, slowly-varying background. the observed data follow a model:\n\n$$\nf(x; \\boldsymbol{\\theta}) = s(x; \\boldsymbol{\\theta}_s) + b(x; \\boldsymbol{\\theta}_b),\n$$\n\nwhere $s$ describes the signal of interest and $b$ accounts for background contributions. the total parameter vector $\\boldsymbol{\\theta} = (\\boldsymbol{\\theta}_s, \\boldsymbol{\\theta}_b)$ is estimated simultaneously.\n\nthe intuition: you are trying to hear a conversation in a noisy room. you need to model both the conversation (signal) and the room noise (background) at the same time. if you model the background poorly, it leaks into your signal estimate and distorts your conclusions.\n\nwhen the background shape is known from control measurements or simulation, its parameters may be **constrained** by adding penalty terms to the objective function. this is equivalent to bayesian fitting with informative priors on the background parameters \u2014 a direct connection to the bayesian framework from the previous chapter.\n\n## profile likelihood\n\nin a model with many parameters, some are interesting (the signal strength, a physical constant) and others are nuisance (background slope, detector efficiency). you want confidence intervals on the interesting ones that correctly account for uncertainty in the nuisance parameters.\n\nthe **profile likelihood** does this by optimizing over the nuisance parameters at every point:\n\n$$\nl_p(\\boldsymbol{\\psi}) = \\max_{\\boldsymbol{\\lambda}} l(\\boldsymbol{\\psi}, \\boldsymbol{\\lambda}).\n$$\n\nthink of it as asking: \"for this particular value of the interesting parameter, what is the *best* the model can do if i freely adjust everything else?\" you trace out a curve of best-case likelihoods, and that curve gives you confidence intervals.\n\na confidence region at level $\\alpha$ is defined by:\n\n$$\n-2 \\ln \\frac{l_p(\\boldsymbol{\\psi})}{l_p(\\hat{\\boldsymbol{\\psi}})} \\leq \\chi^2_{k, \\alpha},\n$$\n\nwhere $k = \\dim(\\boldsymbol{\\psi})$. this correctly propagates parameter correlations into the uncertainty \u2014 something that simply reading off the diagonal of the covariance matrix would miss if parameters are correlated.\n\n## goodness of fit\n\nafter fitting, you must assess whether the model actually describes the data. a beautiful fit to a wrong model is worse than no fit at all.\n\nthe **chi-squared statistic** $\\chi^2 = \\sum_i (y_i - f(x_i; \\hat{\\boldsymbol{\\theta}}))^2 / \\sigma_i^2$ should follow a $\\chi^2$ distribution with $n - p$ degrees of freedom if the model is correct. the **reduced chi-squared** $\\chi^2_\\nu = \\chi^2 / (n - p)$ should be approximately 1. this is the same diagnostic we used in the chi-square chapter, now applied to more complex models.\n\nwhat do the numbers mean?\n\n- $\\chi^2_\\nu \\gg 1$: the model does not describe the data, or the uncertainties are underestimated. something is wrong.\n- $\\chi^2_\\nu \\approx 1$: the model describes the data within the quoted uncertainties. good.\n- $\\chi^2_\\nu \\ll 1$: the fit is *too* good \u2014 the uncertainties are probably overestimated. this is suspicious in a different way.\n\nthe **p-value** gives the probability of obtaining a $\\chi^2$ at least as large as observed, assuming the model is correct. small p-values (typically $< 0.05$) suggest the model is inadequate.\n\n## model comparison\n\nwhen multiple models could describe the data, you need principled criteria for deciding which one to use. the temptation is to pick the model with the lowest $\\chi^2$, but a model with more parameters will *always* fit at least as well. you need to penalize complexity.\n\nthe **likelihood ratio test** (which we met in the hypothesis testing chapter) compares nested models. the test statistic $\\lambda = -2\\ln(l_0 / l_1)$ follows a $\\chi^2$ distribution with degrees of freedom equal to the difference in number of parameters. it asks: does the extra parameter buy enough improvement to justify its inclusion?\n\nfor non-nested models (where neither is a special case of the other), **information criteria** balance fit quality against complexity:\n\n$$\n\\text{aic} = -2\\ln l + 2p, \\qquad \\text{bic} = -2\\ln l + p\\ln n,\n$$\n\nwhere $p$ is the number of parameters and $n$ the number of data points. lower values are better. bic penalizes complexity more heavily than aic and tends to favor simpler models, especially for large datasets.\n\n## calibration\n\nall the fitting methods above assume you know the relationship between what your instrument reads and the physical quantity you care about. **calibration** establishes that relationship using reference standards.\n\na typical calibration procedure:\n\n1. **reference measurements**: measure known standards spanning the range of interest. these are your anchor points.\n2. **fit the calibration curve**: determine the function $r = g(s; \\boldsymbol{\\theta})$ that maps the true physical signal $s$ to the instrument response $r$.\n3. **invert for unknowns**: given a new measurement $r_{\\text{obs}}$, solve for the physical quantity: $r_{\\text{obs}} = g(s; \\hat{\\boldsymbol{\\theta}})$.\n4. **propagate uncertainties**: include both the statistical uncertainty from the measurement and the systematic uncertainty from the calibration curve itself. this uses the error propagation formulas from chapter 3.\n\n## control channels\n\nhow do you pin down the background model without contaminating the signal region? **control channels** (or sidebands) are data regions "
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "anova",
      "lessonTitle": "Analysis of Variance (ANOVA)",
      "x": 0.36337098479270935,
      "y": 0.05634421110153198,
      "searchText": "analysis of variance (anova)\n# analysis of variance (anova)\n\n## the idea behind anova\n\nthe t-test from the previous chapter compares two groups. but experiments often involve three, four, or a dozen groups. you could run t-tests on every pair, but this creates a multiple-testing problem \u2014 with enough comparisons, you will find \"significant\" differences by sheer chance.\n\n**analysis of variance** solves this with a single test: do *any* of the group means differ? despite its name, anova works by comparing variability *between* groups to variability *within* groups. the key insight: if group means differ substantially, the between-group variance will be large relative to the within-group variance. if all groups come from the same population, the between-group and within-group variances should be similar.\n\n## one-way anova\n\nfor $k$ groups with $n_i$ observations each, the model is:\n\n$$\ny_{ij} = \\mu + \\alpha_i + \\varepsilon_{ij}, \\qquad \\varepsilon_{ij} \\sim \\mathcal{n}(0, \\sigma^2),\n$$\n\nwhere $\\mu$ is the grand mean, $\\alpha_i$ is the effect of group $i$, and $\\varepsilon_{ij}$ is random error.\n\nthe total variability decomposes cleanly:\n\n$$\n\\text{ss}_{\\text{total}} = \\text{ss}_{\\text{between}} + \\text{ss}_{\\text{within}}.\n$$\n\nthis decomposition is the heart of anova \u2014 it splits the total variation in the data into the part explained by group differences and the part left over as noise.\n\nthe **f-statistic** is the ratio of the two:\n\n$$\nf = \\frac{\\text{ss}_{\\text{between}} / (k - 1)}{\\text{ss}_{\\text{within}} / (n - k)} = \\frac{\\text{ms}_{\\text{between}}}{\\text{ms}_{\\text{within}}}.\n$$\n\nunder $h_0: \\alpha_1 = \\cdots = \\alpha_k = 0$, the statistic follows an $f(k-1, n-k)$ distribution. a large $f$ means the group differences are large relative to the noise \u2014 evidence that at least one group is different.\n\n**assumptions** (check these before trusting the result):\n\n- independence of observations.\n- normality within each group (check with q-q plots or the shapiro-wilk test).\n- homogeneity of variances (**homoscedasticity**); checked with levene's test or bartlett's test.\n\n## two-way anova\n\nwhat if you are varying two factors simultaneously \u2014 say, drug type *and* dosage? two-way anova handles this:\n\n$$\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}.\n$$\n\nthis decomposes variability into three sources:\n\n- main effect of $a$: do levels of factor $a$ differ on average?\n- main effect of $b$: do levels of factor $b$ differ on average?\n- **interaction** $a \\times b$: does the effect of $a$ depend on the level of $b$?\n\nthe interaction term is often the most scientifically interesting finding. for example, a drug might work well at high doses but not at low doses \u2014 that is an interaction between drug and dosage.\n\n## factorial designs\n\na **full factorial design** tests all combinations of factor levels. for factors with $a$ and $b$ levels, there are $a \\times b$ treatment combinations. factorial designs are efficient because every observation contributes information about every factor.\n\n**balanced designs** (equal sample sizes per cell) simplify the analysis and make the f-tests exact. when balance is lost (due to missing data or unequal groups), the sums of squares are no longer orthogonal, and you need to choose between type i, ii, or iii sums of squares \u2014 a subtlety that trips up many practitioners.\n\n## post-hoc tests\n\nanova tells you *that* at least one group differs, but not *which* ones. to find the specific differences, use post-hoc (after-the-fact) comparisons. these control the **family-wise error rate** \u2014 the probability of making *any* false discovery across all comparisons:\n\n- **tukey's hsd**: compares all pairs of group means; controls the simultaneous confidence level. the go-to choice for pairwise comparisons.\n- **bonferroni correction**: divides $\\alpha$ by the number of comparisons; conservative but general-purpose.\n- **scheff\u00e9's method**: allows arbitrary contrasts (not just pairwise); most conservative.\n- **dunnett's test**: compares each treatment to a single control group. useful when you have one reference condition.\n\n## what if the assumptions fail? kruskal-wallis\n\nanova relies on normality and equal variances. but real data is often skewed, heavy-tailed, or heteroscedastic. what then?\n\nthe **kruskal-wallis test** is a non-parametric alternative to one-way anova. instead of comparing means, it compares **ranks**: all observations are ranked together, and the test asks whether the mean ranks differ across groups.\n\n$$\nh = \\frac{12}{n(n+1)} \\sum_{i=1}^{k} n_i (\\bar{r}_i - \\bar{r})^2,\n$$\n\nwhere $\\bar{r}_i$ is the mean rank in group $i$. under $h_0$, $h \\sim \\chi^2(k-1)$ approximately.\n\nbecause it works with ranks rather than raw values, the kruskal-wallis test is robust to outliers, skewness, and non-constant variance. the trade-off is reduced power when the anova assumptions *are* met \u2014 you pay a price for making fewer assumptions. if the kruskal-wallis test is significant, pairwise comparisons can be performed with the **dunn test** (with bonferroni correction).\n"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "bayesian-statistics",
      "lessonTitle": "Bayesian Statistics",
      "x": 0.41440799832344055,
      "y": 0.15294580161571503,
      "searchText": "bayesian statistics\n# bayesian statistics\n\n## a different way of thinking\n\nthroughout this course, we have used **frequentist** methods: p-values, confidence intervals, maximum likelihood. these treat probability as a long-run frequency \u2014 if you repeat the experiment many times, how often does this outcome occur?\n\n**bayesian statistics** takes a fundamentally different view. probability represents a *degree of belief*. you start with some prior belief about a parameter, observe data, and update that belief. the result is a **posterior distribution** \u2014 a complete description of what you know about the parameter after seeing the data.\n\nneither approach is \"correct\" \u2014 they answer different questions. frequentist methods ask \"how surprising is this data if the hypothesis is true?\" bayesian methods ask \"given the data, what should i believe about the parameter?\" both are useful, and the best practitioners are comfortable with both.\n\n## bayes' theorem\n\nthe mathematical engine of bayesian statistics is **bayes' theorem**. for a hypothesis $h$ and observed data $d$:\n\n$$\np(h|d) = \\frac{p(d|h) \\cdot p(h)}{p(d)}\n$$\n\neach term plays a specific role:\n\n- $p(h|d)$ is the **posterior**: your updated belief about $h$ after seeing the data. this is what you want.\n- $p(d|h)$ is the **likelihood**: the probability of the data given the hypothesis. this is the same likelihood function we used in maximum likelihood estimation \u2014 the connection between bayesian and frequentist methods runs deep.\n- $p(h)$ is the **prior**: your belief about $h$ before seeing the data. this is where existing knowledge (or ignorance) enters.\n- $p(d)$ is the **evidence** (or marginal likelihood): a normalization constant ensuring the posterior integrates to 1. it is often the hardest part to compute.\n\nthe formula reads as a recipe: **posterior $\\propto$ likelihood $\\times$ prior**. the data update your belief, with the likelihood acting as the bridge.\n\n## example: is this coin fair?\n\nsuppose someone hands you a coin and you want to know whether it is fair. let $\\theta$ be the probability of heads.\n\n**prior**: you have no strong reason to think the coin is biased, so you start with a uniform prior: $p(\\theta) = 1$ for $\\theta \\in [0, 1]$. every value of $\\theta$ is equally plausible before you see any data.\n\n**data**: you flip the coin 10 times and get 7 heads and 3 tails.\n\n**likelihood**: from the binomial distribution (chapter 2), the probability of this outcome is:\n\n$$\np(d|\\theta) = \\binom{10}{7} \\theta^7 (1-\\theta)^3\n$$\n\n**posterior**: applying bayes' theorem:\n\n$$\np(\\theta|d) \\propto \\theta^7 (1-\\theta)^3\n$$\n\nthis is a beta(8, 4) distribution. its peak (the most probable value) is at $\\theta = 7/10 = 0.7$ \u2014 the same as the maximum likelihood estimate. but the bayesian answer gives you *more*: the full shape of the distribution tells you how uncertain you are. the 95% credible interval (the bayesian analog of a confidence interval) runs roughly from 0.40 to 0.93.\n\nnow flip the coin 100 more times and get 70 heads. the posterior tightens dramatically \u2014 more data means more certainty. the prior matters less and less as data accumulate. this is a reassuring property: with enough data, reasonable people with different priors converge to the same conclusion.\n\n## choosing priors\n\nthe prior is the most controversial part of bayesian statistics. where does it come from?\n\n- **informative priors** encode genuine prior knowledge. if previous experiments have measured a quantity, use that result as your prior. this is one of the great strengths of bayesian methods \u2014 they provide a principled way to combine old and new evidence.\n- **weakly informative priors** gently constrain parameters to physically reasonable ranges without strongly favoring specific values. for example, a half-normal prior on a variance parameter ensures it stays positive without committing to a particular magnitude.\n- **non-informative (flat) priors** attempt to \"let the data speak.\" a uniform prior on $\\theta$ says every value is equally plausible. this sounds objective, but it is not quite: a flat prior on $\\theta$ is *not* flat on $\\theta^2$ or $\\ln\\theta$. the choice of parameterization matters.\n\nin practice, the prior is most important when data are scarce. with abundant data, the likelihood dominates and the prior washes out. if your conclusions depend sensitively on the prior, that is a signal that the data are insufficient to answer the question \u2014 which is itself a useful thing to know.\n\n## bayesian vs frequentist confidence\n\na 95% **confidence interval** (frequentist) means: if you repeated the experiment many times, 95% of the intervals would contain the true value. it says nothing about *this particular* interval.\n\na 95% **credible interval** (bayesian) means: given the data and the prior, there is a 95% probability that the parameter lies in this interval. this is often the statement people *think* a confidence interval makes.\n\nfor large samples with weak priors, the two intervals are numerically similar. the philosophical difference becomes practically important for small samples or when strong prior information is available.\n"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "chi-square-method",
      "lessonTitle": "Chi-Square Method",
      "x": 0.34631264209747314,
      "y": 0.171587273478508,
      "searchText": "chi-square method\n# chi-square method\n\nwe now have the tools to describe data (chapter 1), model it with distributions (chapter 2), understand why errors are gaussian (chapter 3), and simulate complex scenarios (chapter 4). the next step is to connect models to data quantitatively: given a model and a dataset, how well does the model actually describe the data? and what are the best-fit parameters?\n\nthis is the domain of the **chi-square method** \u2014 the workhorse of model fitting in the physical sciences.\n\n## linear regression\n\nthe simplest model is a straight line through data. linear regression finds the line that best describes the relationship between a dependent variable and one or more independent variables:\n\n$$\ny = \\beta_0 + \\beta_1 x + \\epsilon\n$$\n\nwhere $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\epsilon$ is the error term. this is a starting point, but it treats all data points equally. what if some measurements are much more precise than others? we need a way to weight the fit.\n\n[[simulation applied-stats-sim-1]]\n\n## the chi-square statistic\n\nthe chi-square statistic extends linear regression by incorporating measurement uncertainties directly. each data point contributes to the sum in proportion to its precision \u2014 precise measurements pull the fit strongly, while noisy ones have less influence:\n\n$$\n\\chi^2(\\theta) = \\sum_i^n \\frac{(y_i - f(x_i,\\theta))^2}{\\sigma_i^2}\n$$\n\nthis is just the sum of squared residuals, each divided by the variance of that measurement. notice the connection to maximum likelihood: for gaussian errors, minimizing $\\chi^2$ is *exactly* equivalent to maximizing the likelihood (as we saw in the chapter on pdfs).\n\nthe number of **degrees of freedom** is:\n\n$$\nn_\\text{dof} = n_\\text{samples} - n_\\text{fit parameters}\n$$\n\n### interpreting the fit\n\nhow do you know if a fit is good? the $\\chi^2$ value should be roughly equal to the number of degrees of freedom. more precisely, we compute the **p-value** \u2014 the probability of getting a $\\chi^2$ at least this large if the model is correct:\n\n$$\n\\text{prob}(\\chi^2 = 65.7, n_\\text{dof}=42) = 0.011\n$$\n\na small p-value (say, below 0.05) suggests the model does not describe the data well. but be careful: it could also mean the error bars are underestimated.\n\nif errors are large, different models will all fit similarly well \u2014 the data cannot distinguish between them. with small, precise errors, even slight model deficiencies produce large $\\chi^2$ values. this is a feature, not a bug: better data demands better models.\n\na useful diagnostic is to plot the **residuals** $\\frac{y_i-f(x_i, \\theta)}{\\sigma_i}$ \u2014 these should scatter like a standard normal distribution (mean 0, standard deviation 1) with no visible pattern. trends in the residuals indicate the model is missing something.\n\n```python\nchi2_prob = stats.chi2.sf(chi2_value, n_dof)\n```\n\nnote: the weighted mean from chapter 1 is actually a special case of chi-squared fitting \u2014 it is just fitting a constant to the data.\n\n### chi-square for binned data\n\nfor large datasets, it is often practical to bin the data into a histogram first:\n\n$$\n\\chi^2 = \\sum_{i\\in \\text{bins}} \\frac{(o_i-e_i)^2}{e_i}\n$$\n\nwhere $o_i$ is the observed count and $e_i$ is the expected count in each bin. empty bins should be excluded (they cause division by zero), at the cost of some loss of resolution. tools like `iminuit` handle this automatically.\n\n### why chi-square is powerful\n\n[[simulation applied-stats-sim-5]]\n\nthe power of $\\chi^2$ comes from its geometry. near the minimum, the $\\chi^2$ surface is approximately parabolic \u2014 like a bowl. the curvature of that bowl directly gives you the uncertainties on the fitted parameters. a steep, narrow bowl means the parameter is tightly constrained; a shallow, wide bowl means large uncertainty. this parabolic structure is what makes $\\chi^2$-based confidence intervals so straightforward.\n\n### uncertainties in $x$\n\nso far we have assumed errors only in $y$. if both $x$ and $y$ have uncertainties, the procedure is iterative: fit without $x$ errors first, then fold them in using error propagation (from chapter 3):\n\n$$\n\\sigma_{y_i}^{\\text{new}} = \\sqrt{\\sigma_{y_i}^2 + \\left( \\frac{\\partial y}{\\partial x}\\bigg|_{x_i} \\sigma_{x_i} \\right)^2}\n$$\n\nrepeat the fit with the updated errors. this converges quickly \u2014 usually one or two iterations suffice.\n\n### reporting errors\n\nwhen you report a result, distinguish between **statistical** uncertainty (from the finite size of your dataset) and **systematic** uncertainty (from imperfect knowledge of the experimental setup). the standard format is:\n\n$$\na = (0.24 \\pm 0.05_\\text{stat} \\pm 0.07_\\text{syst}) \\times 10^4 \\; \\text{kg}\n$$\n\n## looking ahead\n\nthe chi-square method tells you how well a model fits the data and gives you parameter uncertainties. but it does not directly answer the question: is there a real effect, or is what i see just noise? that is the territory of **hypothesis testing**, which we take up in the next chapter. there, we will formalize the idea of comparing competing explanations for the data \u2014 building directly on the likelihood and $\\chi^2$ framework developed here.\n"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "design-of-experiments",
      "lessonTitle": "Basic Design of Experiments",
      "x": 0.3566570580005646,
      "y": 0.039880383759737015,
      "searchText": "basic design of experiments\n# basic design of experiments\n\nso far, we have developed tools for analyzing data *after* it has been collected. but the quality of any statistical analysis depends critically on how the data was gathered in the first place. a well-designed experiment can answer your question with 50 observations; a poorly designed one might not answer it with 5000.\n\nexperimental design is where you invest thought *before* spending time and money on data collection. the payoff is enormous.\n\n## principles of experimental design\n\nthree foundational principles guide every well-designed experiment:\n\n- **randomization**: randomly assign experimental units to treatments to eliminate systematic bias. without randomization, apparent treatment effects might simply reflect pre-existing differences between groups. **practical tip**: use a random number generator, not convenience or judgment. human \"random\" assignment is notoriously non-random.\n- **replication**: repeat measurements to estimate variability and increase precision. a single measurement tells you nothing about reliability. **practical tip**: determine how many replicates you need *before* starting (we will see how in the power analysis section below).\n- **blocking**: group experimental units by a known source of variability to reduce noise. if you know that batches, days, or operators introduce variability, account for it by design rather than hoping it averages out.\n\n## completely randomized design (crd)\n\nthe simplest design assigns all experimental units to treatments purely at random. the crd is appropriate when units are homogeneous and no blocking variable is identified.\n\nthe model is identical to one-way anova:\n\n$$\ny_{ij} = \\mu + \\tau_i + \\varepsilon_{ij},\n$$\n\nwhere $\\tau_i$ is the treatment effect.\n\n**advantages**: simple to implement and analyze.\n**limitation**: if units vary substantially (e.g., different ages, batches, or instruments), the within-group variance is inflated and power drops. when you suspect heterogeneity, do not ignore it \u2014 block on it.\n\n## randomized block design (rbd)\n\nwhen a nuisance variable is known (e.g., batch, day, or subject), blocking removes its effect from the error term:\n\n$$\ny_{ij} = \\mu + \\tau_i + \\beta_j + \\varepsilon_{ij},\n$$\n\nwhere $\\beta_j$ is the block effect. each treatment appears exactly once in each block.\n\n**advantage**: removes block-to-block variability from the error term, increasing the f-statistic for the treatment effect. the signal stays the same but the noise goes down.\n\nthe **relative efficiency** of blocking compares the precision of rbd to crd:\n\n$$\n\\text{re} = \\frac{\\text{ms}_{\\text{blocks}} + (b-1)\\,\\text{ms}_{\\text{error,rbd}}}{b\\,\\text{ms}_{\\text{error,rbd}}},\n$$\n\nwhere $b$ is the number of blocks. values greater than 1 indicate that blocking was beneficial. in practice, blocking almost always helps \u2014 it rarely hurts and often improves power substantially.\n\n**practical tip**: when in doubt, block. if the blocking variable turns out to be unimportant, you lose very little (just one degree of freedom per block). if it *is* important and you failed to block, you lose much more.\n\n## power and sample size\n\nbefore starting an experiment, you should answer: how many observations do i need? the answer depends on four quantities that form a connected system \u2014 changing one affects the others.\n\n**statistical power** is the probability of correctly rejecting $h_0$ when a true effect exists:\n\n$$\n\\text{power} = 1 - \\beta = p(\\text{reject } h_0 \\mid h_1 \\text{ true}).\n$$\n\npower depends on:\n\n- **effect size** ($\\delta$): the magnitude of the difference you want to detect. smaller effects need more data.\n- **sample size** ($n$): more observations increase power.\n- **significance level** ($\\alpha$): relaxing $\\alpha$ increases power at the cost of more false positives.\n- **variability** ($\\sigma$): less noise increases power. this is where good experimental design (blocking, precise instruments) pays off.\n\nfor a two-sample t-test, the required sample size per group to achieve power $1 - \\beta$ at level $\\alpha$ for detecting a difference $\\delta$ is approximately:\n\n$$\nn \\approx \\frac{2(z_{\\alpha/2} + z_\\beta)^2 \\sigma^2}{\\delta^2}.\n$$\n\n**cohen's conventions** for effect size $d = \\delta/\\sigma$: small ($d = 0.2$), medium ($d = 0.5$), large ($d = 0.8$). these are rough benchmarks \u2014 always think about what effect size is scientifically meaningful in your specific context rather than blindly adopting conventions.\n\n**practical tips for power analysis**:\n\n- always determine sample size *before* starting the experiment. running until you get a significant result is a recipe for false positives.\n- use **pilot studies** to estimate $\\sigma$ when it is unknown. even a small pilot (10-20 observations) gives a rough variance estimate that makes your power calculation much more reliable than guessing.\n- **pre-register** the analysis plan to avoid p-hacking \u2014 the temptation to try many analyses and report only the significant ones.\n- consider **multiple testing corrections** when evaluating many endpoints. the more tests you run, the more likely one will be \"significant\" by chance.\n"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "error-propagation",
      "lessonTitle": "Central Limit Theorem and Error Propagation",
      "x": 0.4247332513332367,
      "y": 0.24785123765468597,
      "searchText": "central limit theorem and error propagation\n# central limit theorem and error propagation\n\n## central limit theorem\n\nin the previous chapter, we saw that many different distributions arise in nature \u2014 binomial, poisson, exponential, and others. yet when we report experimental results, we almost always quote gaussian uncertainties. why should that be valid?\n\nthe **central limit theorem** (clt) provides the answer. it says: take $n$ independent samples from *any* distribution (as long as it has finite variance), compute their mean, and that mean will be approximately gaussian. the more samples you average, the better the approximation. the original distribution can be uniform, exponential, bimodal, or anything else \u2014 the average still converges to a bell curve.\n\n[[simulation applied-stats-sim-3]]\n\nthis is why gaussian error analysis is so widely applicable. your measurement is typically the result of many small, independent influences added together. each influence might follow some complicated distribution, but their sum \u2014 your measurement \u2014 will be gaussian. the clt is the license that lets us use gaussian statistics almost everywhere.\n\nthere are caveats. for the clt to work well, the contributing distributions should have similar standard deviations. if one source of variation dwarfs all others, the sum will look like that dominant source, not a gaussian. distributions with very heavy tails (like the cauchy distribution, which has no finite variance) never converge \u2014 no amount of averaging will tame them.\n\n**the practical rule**: if each contribution has finite variance and no single contribution dominates, the sum is approximately gaussian. this is the foundation on which the rest of the course builds.\n\n## error propagation\n\nnow suppose you have measured some input quantities $x_i$, each with uncertainty $\\sigma(x_i)$, and you compute a derived quantity $y(x_i)$. what is the uncertainty on $y$?\n\nthe intuition is simple: a small wiggle in the input causes a wiggle in the output, and the size of that output wiggle depends on how steeply $y$ changes with respect to $x$. if $y$ varies gently with $x$, input errors stay small. if $y$ varies steeply, input errors get amplified.\n\nthink of measuring the area of a circle by measuring its radius. the area is $a = \\pi r^2$, so $da/dr = 2\\pi r$. a 1% error in $r$ becomes roughly a 2% error in $a$ \u2014 the derivative amplifies it. for the volume of a sphere ($v = \\frac{4}{3}\\pi r^3$), the same 1% error in $r$ becomes a 3% error in $v$. the steeper the function, the bigger the amplification.\n\nformally, for a function of one variable:\n\n$$\n\\sigma(y) = \\frac{\\partial y}{\\partial x}\\sigma(x_i)\n$$\n\nthis works well when $y$ is smooth around $x_i$ \u2014 specifically, when the slope is roughly constant over the uncertainty range in $x_i$.\n\nfor multiple variables with correlations, the general formula uses the covariance matrix $v_{ij}$ (which we met in the introduction chapter):\n\n$$\n\\sigma_y^2 = \\sum_{i,j}^n \\frac{\\partial y}{\\partial x_i} \\frac{\\partial y}{\\partial x_j} v_{ij}\n$$\n\nif there are no correlations between the inputs, only the diagonal terms survive \u2014 each input contributes independently to the total uncertainty. this lets you identify which parameter dominates the error budget and focus your effort on improving that measurement.\n\n### addition\n\n$$\ny = x_1 + x_2 \\implies \\sigma_y^2 = \\sigma_{x_1}^2 + \\sigma_{x_2}^2 + 2v_{x_1, x_2}\n$$\n\nerrors add in quadrature (when uncorrelated). this is why combining independent measurements always improves precision.\n\n### multiplication\n\n$$\ny = x_1 x_2 \\implies \\sigma_y^2 = (x_2\\sigma_{x_1})^2 + (x_1\\sigma_{x_2})^2 + 2x_1 x_2 v_{x_1, x_2}\n$$\n\ndividing by $y^2$ gives the relative uncertainties. an interesting consequence: by engineering *negative* error correlations, we can make errors partially cancel. harrison's gridiron pendulum exploits this \u2014 it uses two metals with different thermal expansion coefficients arranged so that temperature changes cancel out, keeping the pendulum length stable.\n\n### when analytical propagation fails\n\nthe formulas above assume the function $y(x)$ is smooth and approximately linear over the range of uncertainty. when that breaks down \u2014 for instance, when $y$ has a sharp threshold or a discontinuity \u2014 we need a different approach: **simulation**.\n\nchoose random inputs $x_i$ from their error distributions, compute $y$ for each draw, and look at the resulting spread in $y$. this monte carlo approach (which we develop fully in the next chapter on simulation methods) handles arbitrary functions, non-gaussian inputs, and complex correlations. if $y(x)$ is not smooth, the output distribution may not be gaussian even when the inputs are \u2014 the simulation will reveal this automatically.\n\n[[simulation applied-stats-sim-8]]\n"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "home",
      "lessonTitle": "Applied Statistics",
      "x": 0.32466983795166016,
      "y": 0.2490188330411911,
      "searchText": "applied statistics\n# applied statistics\n\n## what this course is really about\n\nstatistics isn't about numbers. it's about learning the truth when the world is noisy and you only get to peek once.\n\nevery measurement you'll ever make is contaminated by randomness \u2014 your instruments are imperfect, your samples are finite, and the universe just won't sit still. this course hands you the tools to cut through that noise and figure out what's actually going on. not by memorizing formulas, but by understanding *why* each technique exists and *when* it's the right one to reach for.\n\n- **classical statistics**: estimation, hypothesis testing, confidence intervals.\n- **applied statistics**: model selection, validation, interpretation, and communication of results.\n- **the real skill**: knowing when each method is appropriate and what its assumptions demand.\n\n> **a bayesian sneak preview.** most of this course uses frequentist methods \u2014 p-values, confidence intervals, maximum likelihood. but there is another way of thinking about probability: as a *degree of belief* that updates when new evidence arrives. that's the bayesian view, and it's closer to how your brain actually works when it learns from data. we cover it formally in lesson 11, but you'll see hints of it much earlier. when we talk about likelihood in lesson 2, or priors in lesson 12, remember: bayesian reasoning is quietly running in the background the whole time.\n\n## course map\n\nhere is the big picture \u2014 the forest before the trees.\n\n```\nfoundations                      comparing groups\n  1. introduction \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       7. anova\n  2. distributions & mle \u2500\u2500\u2524       8. experimental design\n  3. clt & error propagation\n  4. simulation            \u2502     real-world structures\n                           \u2502       9.  mixed models\ntools for noisy data       \u2502      10. longitudinal data\n  5. chi-square fitting \u2500\u2500\u2500\u2518\n  6. hypothesis testing          advanced tools\n                                  11. bayesian statistics\n                                  12. advanced fitting\n                                  13. machine learning\n```\n\nthe arrows between these stages matter as much as the stages themselves. the likelihood function introduced in lesson 2 shows up in chi-square fitting (5), hypothesis testing (6), bayesian statistics (11), and machine learning (13). the clt (3) justifies why gaussian methods work everywhere. mixed models (9) generalize anova (7) to messy, grouped data. and by lesson 13, you'll realize that every \"new\" machine learning trick is secretly something you already learned.\n\n## meet alex\n\nthroughout this course, you'll follow **alex**, a physicist who measures things for a living and keeps running into the same problem: the universe refuses to give a clean answer. alex measures the gravitational acceleration and gets $9.81 \\pm 0.03$ m/s$^2$ \u2014 but is that uncertainty right? alex compares two treatments and sees a difference \u2014 but is it real or just noise? every chapter, alex faces a new puzzle. you'll solve it together.\n\n## why this topic matters\n\n- every experimental science relies on statistical reasoning to distinguish signal from noise.\n- choosing the wrong test or violating assumptions leads to false conclusions \u2014 and in medicine, engineering, or policy, false conclusions have real consequences.\n- modern datasets demand regression, anova, mixed models, and longitudinal methods.\n- statistical literacy is essential for reading and producing scientific literature.\n\n## key mathematical ideas\n\n- probability distributions and likelihood functions \u2014 the language of uncertainty.\n- parametric and non-parametric hypothesis tests \u2014 asking \"is this real?\"\n- linear models: regression, anova, and their generalizations \u2014 the workhorses.\n- random effects and hierarchical/mixed models \u2014 handling messy, grouped data.\n- experimental design: randomization, blocking, and power analysis \u2014 getting the data right before you analyze it.\n\n## prerequisites\n\n- introductory statistics: variation, estimation, confidence intervals, hypothesis tests.\n- one-way anova and simple linear regression.\n- basic familiarity with python or r.\n\n## recommended reading\n\n- draper and smith, *applied regression analysis*.\n- agresti, *statistical methods for the social sciences*.\n- feynman, *the character of physical law* \u2014 for the spirit of thinking clearly about uncertainty.\n- feynman, *surely you're joking, mr. feynman!* \u2014 because the best statisticians are the most curious experimentalists.\n- course notes and documentation.\n\n## learning trajectory\n\nthis module builds like a chain \u2014 each topic relies on what came before and sets up what comes next. the connections between chapters are as important as the chapters themselves.\n\n- **lesson 1 \u2014 introduction and general concepts**: you have a pile of numbers \u2014 what can you learn from them? we start with the tools for summarizing data: means, medians, spread, and correlation. these descriptive tools appear in every later chapter. *next up: where do those numbers come from?*\n- **lesson 2 \u2014 probability density functions**: the mathematical models behind data \u2014 distributions and maximum likelihood estimation. the likelihood concept introduced here becomes the foundation for fitting, testing, and bayesian analysis. *next up: why do errors always seem to be gaussian?*\n- **lesson 3 \u2014 clt and error propagation**: why gaussian statistics work, and how uncertainties flow through calculations. connects distributions to the practical error analysis used everywhere else. *next up: what happens when the math gets too hard for pen and paper?*\n- **lesson 4 \u2014 simulation methods**: when analytical formulas break down, monte carlo methods take over. now that you can fake the universe a million times, watch what happens when you ask \"is this real or just luck?\" *next up: fitting models to real data.*\n- **lesson 5 \u2014 chi-square method**: fitting models to data with uncertainties. connects maximum likelihood (from lesson 2) to practical regression "
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "hypothesis-testing",
      "lessonTitle": "Hypothesis Testing and Limits",
      "x": 0.3728737533092499,
      "y": 0.11081315577030182,
      "searchText": "hypothesis testing and limits\n# hypothesis testing and limits\n\n## the logic of hypothesis testing\n\nthink of hypothesis testing as a courtroom trial. the defendant (your null hypothesis) is presumed innocent until proven guilty. you gather evidence (data), and if the evidence is overwhelming enough, you reject the presumption of innocence. notice the asymmetry: you never *prove* innocence \u2014 you either find enough evidence to convict, or you don't. statistical testing works the same way.\n\nthe **null hypothesis** $h_0$ is the boring explanation \u2014 nothing interesting is happening, there is no effect, the drug does not work. the **alternative hypothesis** $h_1$ says something real is going on. your job is to ask: if $h_0$ were true, how surprising would my data be?\n\nthe answer is the **p-value**: the probability of observing data at least as extreme as what you got, assuming $h_0$ is true. a small p-value means the data would be very surprising under $h_0$, so you reject it. the threshold for \"surprising enough\" is the **significance level** $\\alpha$ (typically 0.05).\n\nwe can perform either **one-tailed** or **two-tailed** tests depending on the alternative hypothesis. a one-tailed test asks \"is the effect in *this* direction?\"; a two-tailed test asks \"is there *any* effect at all?\"\n\n## the testing toolkit\n\nwith the logic in place, we now build up a toolkit of specific tests, each suited to different situations.\n\n### one-sample z test\n\nthe simplest test, but rarely used in practice since it requires knowing the population standard deviation \u2014 which you almost never do. the procedure illustrates the general framework:\n\n1. state the null hypothesis: $h_0: \\mu = 100$.\n2. state the alternative hypothesis: $h_1: \\mu > 100$.\n3. choose a significance level: $\\alpha = 0.05$.\n4. find the rejection region from the z-table. an area of 0.05 corresponds to a z-score of 1.645.\n5. calculate the test statistic:\n\n$$\nz = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\n$$\n\n6. if $z$ exceeds the critical value, reject $h_0$.\n\n### student's t-test\n\nin practice, you estimate the standard deviation from the data itself, which introduces extra uncertainty. the **t-test** accounts for this by using the heavier-tailed $t$-distribution (which we met in the chapter on pdfs) instead of the gaussian.\n\na student's t-test determines if there is a significant difference between means. there are two flavors:\n\n- **one-sample t-test**: compares the mean of a single sample to a known or hypothesized value. is this batch of lightbulbs lasting the claimed 1000 hours?\n- **two-sample t-test**: compares the means of two independent groups. do patients on drug a recover faster than patients on drug b?\n\nthe test assumes approximately normal data and (for the two-sample version) equal variances. the test statistic is the difference between the sample mean and the hypothesized mean, divided by the standard error, compared against a $t$-distribution with the appropriate degrees of freedom.\n\n[[simulation applied-stats-sim-4]]\n\n### non-parametric tests\n\nwhat if your data is clearly not normal? non-parametric tests make fewer assumptions about the underlying distribution.\n\nthe **kolmogorov-smirnov test** (k-s test) compares a sample with a reference distribution (one-sample) or compares two samples (two-sample). it works by measuring the maximum distance between the cumulative distribution functions. it returns a p-value indicating whether the samples are drawn from the same distribution.\n\nthe **runs test** checks whether a sequence of two-valued data is random. given a sequence like:\n\n> $+ + + + - - - + + + - - + + + + + + - - - -$\n\nit counts the number of \"runs\" (consecutive sequences of the same value). too few runs suggest the data is clustered; too many suggest alternation. either way, the sequence is not random.\n\n## comparing models: the likelihood ratio test\n\nthe tests above compare data to a single hypothesis. but often you want to compare two competing models: does adding an extra parameter significantly improve the fit? this is where the **likelihood ratio test** connects hypothesis testing to the chi-square fitting framework from the previous chapter.\n\nthe test statistic is:\n\n$$\nd = -2\\ln\\frac{\\mathcal{l}_\\text{null}}{\\mathcal{l}_\\text{alt}} = -2\\ln(\\mathcal{l}_\\text{null}) + 2\\ln(\\mathcal{l}_\\text{alt})\n$$\n\nunder $h_0$, this follows a $\\chi^2$ distribution with degrees of freedom equal to the difference in the number of parameters. a model with more parameters will *always* fit better (or at least as well), so the test asks: does the improvement justify the added complexity?\n\n## confidence intervals: ranges of belief\n\na p-value gives a binary answer: reject or not. a **confidence interval** gives something richer \u2014 a range of plausible values for the parameter.\n\na 95% confidence interval means: if you repeated the entire experiment many times, 95% of the intervals you compute would contain the true parameter. it is *not* a statement about the probability that the true value lies in this particular interval (that is the bayesian interpretation, which we will see later).\n\nthe interval is constructed from the point estimate (e.g., sample mean), the standard error, and the desired confidence level. wider intervals give more confidence but less precision \u2014 there is always a trade-off.\n\nconfidence intervals connect directly to hypothesis testing: if a hypothesized value falls outside the 95% confidence interval, you would reject it at the 5% significance level.\n\n## a warning: simpson's paradox\n\nbefore moving on, here is a cautionary tale about what can go wrong when you test hypotheses without considering the full picture.\n\n**simpson's paradox** occurs when a trend that appears in separate groups of data reverses when the groups are combined. it is not just a mathematical curiosity \u2014 it has fooled researchers and policymakers.\n\na famous real-world example: in 1973, uc berkeley was accused of gender bias in graduate admissions. the overall ad"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "introduction-concepts",
      "lessonTitle": "Introduction and General Concepts",
      "x": 0.38596415519714355,
      "y": 0.2048111855983734,
      "searchText": "introduction and general concepts\n# introduction and general concepts\n\nyou have a pile of numbers. maybe they're pendulum timings, patient blood pressures, or photon counts from a distant star. before you build any model or run any test, you need to answer two questions: *where* does the data cluster, and *how much does it scatter?*\n\nthese sound simple. they aren't. the \"center\" of a dataset is not a single concept, and neither is \"spread.\" different situations demand different summaries. picking the wrong one can lead you astray before you've even started your analysis.\n\nalex, our experimentalist, has just measured the gravitational acceleration ten times and gotten values between 9.78 and 9.84 m/s$^2$. what should alex report as \"the\" value? and how uncertain should alex be? that's what this chapter is about.\n\n## measures of central tendency\n\nthe \"center\" of a dataset depends on what question you're asking. here is the menu, ordered from most familiar to most specialized.\n\n### arithmetic mean\n\nthe most familiar average. you add up all the values and divide by how many there are:\n\n$$\n\\hat{\\mu} = \\bar{x} = \\langle x \\rangle = \\frac{1}{n}\\sum_i^n x_i\n$$\n\n```python\ndef arithmetic_mean(arr):\n    return np.sum(arr) / len(arr)\n    # or equivalently: np.mean(arr)\n```\n\nthe arithmetic mean is sensitive to extreme values \u2014 a single outlier can drag it far from where most of the data sits. if alex's ten measurements include one wild reading of 10.5 (a bumped table, perhaps), the mean shifts noticeably. that's not always what you want.\n\n### geometric mean\n\nwhen your data are multiplicative in nature (growth rates, ratios, concentrations spanning orders of magnitude), the geometric mean is more appropriate. it is the $n$-th root of the product:\n\n$$\n\\bar{x}_\\text{geo} = \\left( \\prod_i^n x_i\\right)^{1/n} = \\exp\\left(\\frac{1}{n}\\sum_{i}^n\\ln x_i \\right)\n$$\n\nthis is equivalent to taking the arithmetic mean in log-space, which is why it works well for data that are log-normally distributed. if your data span several orders of magnitude, the geometric mean lives where the data actually cluster \u2014 not where the arithmetic mean gets dragged by the largest values.\n\n```python\ndef geometric_mean(arr):\n    # a sum of logs is less prone to\n    # under-/over-flow than a product.\n    return np.exp(np.mean(np.log(arr)))\n```\n\n### median\n\nthe middle value when the data are sorted. unlike the mean, the median is robust to outliers \u2014 even wild values at the extremes do not shift it much. if alex had that one bumped-table reading of 10.5, the median wouldn't flinch.\n\n```python\ndef median(arr):\n    s = np.sort(arr)\n    n = len(s)\n    if n % 2 == 1:\n        return s[n // 2]\n    return (s[n // 2 - 1] + s[n // 2]) / 2\n```\n\n### mode\n\nthe most frequently occurring value. for continuous data, the mode is estimated from a histogram or kernel density estimate. it tells you where the data piles up the most \u2014 useful when your distribution has a clear peak.\n\n### harmonic mean\n\n$$\n\\bar{x}_\\text{harm} = \\left[\\frac{1}{n}\\sum_i^n \\frac{1}{x_i}\\right]^{-1}\n$$\n\nthe harmonic mean is the right choice when you're averaging *rates*. suppose you drive 60 km/h for the first half of a trip and 40 km/h for the second half. the harmonic mean (48 km/h) gives the correct average speed, not the arithmetic mean (50 km/h). the difference matters.\n\n```python\ndef harmonic_mean(x):\n    return (np.sum(x**(-1)) / len(x))**(-1)\n```\n\n### truncated mean\n\na compromise between the sensitivity of the mean and the robustness of the median: compute the arithmetic mean after throwing away the most extreme values on both ends.\n\n```python\ndef truncated_mean(arr, k):\n    arr_sorted = np.sort(arr)\n    return np.mean(arr_sorted[k:-k])\n```\n\n> **try this at home.** take ten measurements of anything \u2014 your walking time to class, the temperature outside, the number of words per sentence in a book. compute the mean and the median. are they close? if not, you probably have outliers or a skewed distribution. that's already telling you something about the data.\n\n## measures of spread\n\nknowing the center is only half the story. two datasets can have the same mean but look completely different \u2014 one tightly clustered, the other scattered across a huge range. you need a number that captures *how much the data wanders*.\n\n### standard deviation\n\nstandard deviation measures how much data points typically deviate from the mean. think of it as the \"average distance\" from the center:\n\n$$\n\\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_i^n (x_i - \\mu)^2}\n$$\n\nthis formula assumes you know the true mean $\\mu$. in practice, you use the sample mean $\\bar{x}$, which introduces a subtle bias. here is the surprising thing: the sample mean is always a little closer to the data than the true mean is, so the raw formula systematically underestimates the spread. **bessel's correction** compensates by dividing by $n-1$ instead of $n$, accounting for the lost degree of freedom:\n\n$$\n\\tilde{\\sigma} = \\sqrt{\\frac{1}{n-1}\\sum_i(x_i-\\bar{x})^2}\n$$\n\n### weighted mean\n\nwhen measurements come with different uncertainties, you should trust the precise ones more. the weighted mean does exactly this \u2014 measurements with smaller error bars get larger weights:\n\n$$\n\\hat{\\mu} = \\frac{\\sum x_i / \\sigma_i^2}{\\sum 1 / \\sigma_i^2}, \\qquad \\hat{\\sigma}_\\mu = \\sqrt{\\frac{1}{\\sum 1/\\sigma_i^2}}\n$$\n\nthe uncertainty on the weighted mean decreases with the number of samples:\n\n$$\n\\hat{\\sigma}_\\mu = \\hat{\\sigma}/\\sqrt{n}.\n$$\n\nalex has three measurements of the same length: $10.2 \\pm 0.1$, $10.5 \\pm 0.5$, and $10.1 \\pm 0.2$ cm. the weighted mean pulls toward 10.2, because that measurement is the most precise. this is your first taste of a powerful idea: *let the data tell you how much to trust each piece of information*. you'll see this idea again when we meet likelihood in lesson 2 and chi-square fitting in lesson 5.\n\n## from individual variables to relationships\n\nso far you've described single variables in isolation: their centers and th"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "longitudinal-data",
      "lessonTitle": "Longitudinal Data and Repeated Measures",
      "x": 0.28239139914512634,
      "y": 0.0582607239484787,
      "searchText": "longitudinal data and repeated measures\n# longitudinal data and repeated measures\n\n## what makes longitudinal data special\n\nin the previous chapter, we handled grouped data where measurements within a cluster are correlated. **longitudinal data** is a specific and important case: the same subjects are measured repeatedly *over time*. this creates two fundamental challenges:\n\n- **within-subject correlation**: measurements from the same individual are not independent. a person who scores high on monday will probably score high on tuesday.\n- **time structure**: the spacing and ordering of observations carry information. a measurement today is more correlated with yesterday's than with last month's.\n\nignoring these features and applying standard methods (like ordinary regression) treats each measurement as if it came from a different person \u2014 a mistake that produces incorrect standard errors and misleading p-values.\n\n## repeated measures anova\n\nthe classical approach extends anova to handle within-subject factors. for a single within-subject factor with $k$ time points:\n\n$$\ny_{ij} = \\mu + \\pi_i + \\tau_j + \\varepsilon_{ij},\n$$\n\nwhere $\\pi_i$ is the subject effect and $\\tau_j$ is the time effect.\n\n**sphericity assumption**: the variances of all pairwise differences between time points must be equal. in plain terms, the change from time 1 to time 2 should be as variable as the change from time 2 to time 3, and so on. mauchly's test checks this assumption. when sphericity is violated (which is common):\n\n- **greenhouse-geisser correction**: reduces the degrees of freedom to control type i error. conservative \u2014 it may miss real effects.\n- **huynh-feldt correction**: less conservative alternative.\n\n**limitation**: repeated measures anova requires complete data (no missing time points) and equally spaced measurements. in practice, subjects drop out, miss visits, or are measured on irregular schedules. this is where mixed models take over.\n\n## linear mixed models for longitudinal data\n\n**linear mixed models** (lmms) overcome these limitations by modeling each subject's trajectory directly:\n\n$$\ny_{ij} = \\beta_0 + \\beta_1 t_{ij} + u_{0i} + u_{1i} t_{ij} + \\varepsilon_{ij},\n$$\n\nwhere $u_{0i}$ is a **random intercept** (each subject starts at a different level) and $u_{1i}$ is a **random slope** (each subject changes at a different rate). this builds directly on the mixed model framework from the previous chapter, adding the time dimension.\n\nthe random effects are assumed multivariate normal:\n\n$$\n\\begin{pmatrix} u_{0i} \\\\ u_{1i} \\end{pmatrix} \\sim \\mathcal{n}\\!\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_0^2 & \\rho\\sigma_0\\sigma_1 \\\\ \\rho\\sigma_0\\sigma_1 & \\sigma_1^2 \\end{pmatrix}\\right).\n$$\n\n**advantages over repeated measures anova**:\n\n- handles missing data naturally (uses all available observations, not just complete cases).\n- accommodates unequally spaced time points.\n- models individual trajectories, not just group means.\n- allows complex correlation structures.\n\n## growth curve models\n\nwhen the outcome follows a nonlinear trajectory over time (growth spurts, learning curves, disease progression), polynomial or nonlinear growth curves can be fit within the mixed-model framework:\n\n$$\ny_{ij} = \\beta_0 + \\beta_1 t_{ij} + \\beta_2 t_{ij}^2 + u_{0i} + u_{1i} t_{ij} + \\varepsilon_{ij}.\n$$\n\nthe fixed effects $(\\beta_0, \\beta_1, \\beta_2)$ describe the population-average trajectory \u2014 the \"typical\" curve. the random effects $(u_{0i}, u_{1i})$ capture how each individual deviates from that average.\n\n## autocorrelation\n\nhere is an everyday example of autocorrelation: if today is warm, tomorrow is probably warm too. weather does not jump randomly from day to day \u2014 it has *memory*. the same is true of many measurement sequences: stock prices, heart rate readings, reaction times across trials.\n\n**autocorrelation** quantifies how a signal correlates with itself at different time lags:\n\n$$\nr(\\tau) = \\frac{1}{n} \\sum_{t=1}^{n-\\tau} (x_t - \\bar{x})(x_{t+\\tau} - \\bar{x}).\n$$\n\nin longitudinal data, residuals from adjacent time points are often more correlated than residuals from distant ones. the mixed model can account for this through the correlation structure of the errors. common choices:\n\n- **compound symmetry**: constant correlation between all pairs (equivalent to a random intercept \u2014 all time separations are treated equally).\n- **ar(1)**: correlation decays exponentially with time lag: $\\text{cor}(\\varepsilon_{ij}, \\varepsilon_{ik}) = \\phi^{|j-k|}$. this is often the most realistic choice for equally-spaced longitudinal data.\n- **unstructured**: separate correlation for each pair. the most flexible but parameter-intensive \u2014 only feasible when the number of time points is small.\n\nmodel selection among correlation structures uses **aic** or **bic** (the information criteria we will encounter again in the chapter on advanced fitting).\n\n## handling missing data\n\nlongitudinal studies inevitably have missing observations \u2014 subjects drop out, miss appointments, or are lost to follow-up. the critical question is *why* data are missing, because the mechanism determines which analyses are valid:\n\n- **mcar** (missing completely at random): missingness is unrelated to any data. the data you have are a random subset of what you would have collected. standard methods remain valid, though less powerful.\n- **mar** (missing at random): missingness depends on observed data but not on the missing values themselves. a patient might drop out because of a recorded side effect, but not because of their unrecorded future outcome. lmms under maximum likelihood provide valid inference.\n- **mnar** (missing not at random): missingness depends on the unobserved values themselves. patients drop out *because* they are getting worse, and you never see how much worse. this is the hardest case, requiring specialized models (selection models, pattern-mixture models).\n\nlmms estimated by maximum likelihood or reml (the estimat"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "machine-learning-data-analysis",
      "lessonTitle": "Machine Learning and Data Analysis",
      "x": 0.2676611542701721,
      "y": 0.2630707323551178,
      "searchText": "machine learning and data analysis\n# machine learning and data analysis\n\n## overview\n\nthroughout this course, we have built up a toolkit: fitting models to data (chi-square method), testing hypotheses, designing experiments, and handling complex data structures. **machine learning** extends this toolkit with algorithms that learn patterns from data without being explicitly programmed for each case.\n\nthe connection to what we have already learned is closer than it might seem. logistic regression is maximum likelihood estimation (chapter 2) applied to classification. regularization is the frequentist cousin of bayesian priors (chapter 11). cross-validation is a practical implementation of the model comparison ideas from the advanced fitting chapter. ml does not replace statistics \u2014 it builds on it.\n\nin experimental physics and data analysis, ml methods complement traditional techniques for three core tasks: **classification** (separating signal from background), **regression** (predicting continuous quantities), and **clustering** (discovering structure in unlabelled data).\n\nml algorithms fall into two broad categories. **supervised learning** trains on labelled examples and predicts labels for new data. **unsupervised learning** finds structure in data without labels.\n\n## supervised learning: classification\n\ngiven training data $\\{(\\mathbf{x}_i, y_i)\\}$ where $y_i \\in \\{0, 1\\}$ labels signal vs. background, a classifier learns a decision boundary in feature space. this is the ml version of the hypothesis testing problem from chapter 6: is this event signal or background?\n\n### logistic regression\n\n**logistic regression** models the probability of the positive class as:\n\n$$\np(y=1 | \\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{w}^t \\mathbf{x} - b}},\n$$\n\nwhere $\\mathbf{w}$ and $b$ are learned by maximizing the likelihood \u2014 the same mle principle from chapter 2, just applied to a different model. despite its simplicity, logistic regression is effective when classes are approximately linearly separable and serves as a useful baseline before trying more complex methods.\n\n### decision trees and ensembles\n\n**decision trees** recursively partition feature space by selecting the feature and threshold that best separates classes at each node. they are interpretable (you can read off the decision rules) but prone to overfitting \u2014 they memorize noise in the training data.\n\ntwo ensemble strategies fix this:\n\n- **random forests** reduce overfitting by averaging predictions from many trees, each trained on a bootstrap sample with a random subset of features. the averaging smooths out the noise that individual trees memorize.\n- **boosted decision trees** (bdt) build an ensemble sequentially, with each new tree focusing on the examples the previous ones got wrong. gradient boosting (e.g., xgboost) is among the most powerful classifiers for tabular data and is widely used in particle physics for event selection.\n\n## the fisher linear discriminant\n\nthe **fisher discriminant** finds the single linear combination of features that best separates two classes. it connects directly to the ideas of variance decomposition from anova (chapter 7): just as anova separates between-group from within-group variance, fisher finds the direction that maximizes the ratio of between-class to within-class scatter.\n\nthe discriminant direction is:\n\n$$\n\\mathbf{w} = s^{-1}(\\boldsymbol{\\mu}_0 - \\boldsymbol{\\mu}_1)\n$$\n\nwhere $\\boldsymbol{\\mu}_0$ and $\\boldsymbol{\\mu}_1$ are the class means and $s$ is the **pooled within-class covariance matrix**:\n\n$$\ns = \\frac{1}{n}(s_0 + s_1)\n$$\n\na new observation $\\mathbf{x}$ is classified by projecting onto $\\mathbf{w}$ and comparing to a threshold. this method assumes normally distributed classes with equal covariance matrices \u2014 the same assumptions that underlie anova. when those assumptions are violated, quadratic discriminant analysis or nonlinear classifiers should be used instead.\n\n## supervised learning: regression\n\nfor predicting continuous targets, the same algorithms adapt. linear regression minimizes squared residuals (the chi-square method from chapter 5 without the weighting by uncertainties). **ridge** and **lasso** regression add penalty terms:\n\n$$\n\\hat{\\mathbf{w}} = \\arg\\min_{\\mathbf{w}} \\sum_i (y_i - \\mathbf{w}^t\\mathbf{x}_i)^2 + \\lambda \\|\\mathbf{w}\\|_p^p.\n$$\n\nthese penalties serve the same purpose as the nuisance parameter constraints in the advanced fitting chapter \u2014 they prevent the model from over-adapting to noise:\n\n- **lasso** ($p=1$) performs automatic feature selection by driving irrelevant coefficients to zero. useful when you suspect many features are unimportant.\n- **ridge** ($p=2$) shrinks all coefficients toward zero without eliminating any. better when features are correlated.\n\n## neural networks\n\na feedforward neural network with $l$ layers computes:\n\n$$\n\\mathbf{h}^{(l)} = \\sigma\\bigl(w^{(l)} \\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)}\\bigr), \\quad l = 1, \\ldots, l,\n$$\n\nwhere $\\sigma$ is a nonlinear activation function (relu, sigmoid, or tanh) and $\\mathbf{h}^{(0)} = \\mathbf{x}$. the parameters are optimized by **backpropagation** using gradient descent on a loss function.\n\nneural networks can approximate any continuous function (universal approximation theorem) and excel when feature engineering is difficult \u2014 when you do not know in advance which combinations of inputs matter. the price is that they require large training sets, careful regularization, and produce models that are harder to interpret than tree-based methods.\n\n## unsupervised learning\n\nsometimes you do not have labels \u2014 you just have data and want to find structure in it.\n\n**k-means clustering** partitions data into $k$ groups by iteratively assigning points to the nearest centroid and updating centroids. it requires specifying $k$ in advance and assumes roughly spherical clusters. think of it as an automated grouping procedure \u2014 like anova in reverse, where you discover the groups rather than being told what th"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "probability-density-functions",
      "lessonTitle": "Probability Density Functions",
      "x": 0.35884302854537964,
      "y": 0.2045113891363144,
      "searchText": "probability density functions\n# probability density functions\n\nin the previous chapter we summarized data that was already in hand. now we ask a deeper question: what *process* generated that data? if we can describe the underlying mechanism with a mathematical function, we unlock the ability to predict, extrapolate, and quantify uncertainty. that mathematical function is the **probability density function**.\n\n## probability density functions (pdfs)\n\na **probability density function** describes a continuous random variable. the pdf itself is not a probability \u2014 it is a *density*. the probability that the variable falls within an interval comes from integrating the pdf over that interval:\n\n$$\np(a \\leq x \\leq b) = \\int_{a}^{b} f(x) \\, dx\n$$\n\nwe may also consider the **cumulative distribution function** (cdf), which accumulates probability from $-\\infty$:\n\n$$\nf_x(x) = \\int_{-\\infty}^{x} f(x') \\, dx'.\n$$\n\nthe cdf is always non-decreasing, starts at 0, and ends at 1. it answers the question: what fraction of outcomes fall below $x$?\n\n## common distributions\n\ndifferent physical processes give rise to different distributions. here are the ones you will encounter most often, organized from discrete counting processes to the continuous workhorse of statistics.\n\n### binomial\n\nsuppose you flip a coin $n$ times, each with probability $p$ of landing heads. how many heads should you expect? the binomial distribution answers this:\n\n$$\n\\begin{aligned}\nf(n;n,p) &= \\frac{n!}{n!(n-n)!}p^n(1-p)^{n-n}\\\\\n\\langle f(n;n,p)\\rangle &= np \\\\\n\\sigma^2 &= np(1-p)\n\\end{aligned}\n$$\n\nnote that this is a **discrete** distribution \u2014 it counts whole events (0 heads, 1 head, 2 heads, ...).\n\n### poisson\n\nnow imagine you have a huge number of trials ($n\\rightarrow \\infty$), each with a tiny probability of success ($p\\rightarrow 0$), but the *expected count* stays finite ($np\\rightarrow\\lambda$). the binomial simplifies to the **poisson** distribution:\n\n$$\nf(n, \\lambda) = \\frac{\\lambda^n}{n!}e^{-\\lambda}\n$$\n\n**example**: a large number of people go into traffic every day ($n\\rightarrow\\infty$), the probability of any one person being killed is tiny ($p\\rightarrow 0$), but some number of fatalities occur every year ($\\lambda\\neq 0$).\n\nthe poisson distribution has a remarkable property: its mean and variance are both equal to $\\lambda$. this means the error on a poisson count is simply the square root of that count \u2014 a fact you will use constantly when working with histograms. if a histogram bin contains $n$ events, its statistical uncertainty is $\\sqrt{n}$, provided the count is large enough ($n \\gtrsim 5{-}20$) for the gaussian approximation to hold.\n\nthe sum of independent poissons is itself a poisson: $\\lambda = \\lambda_a + \\lambda_b$. and when $\\lambda$ is large ($\\lambda \\gtrsim 20$), the poisson approaches a gaussian.\n\n### gaussian\n\nthe **normal distribution** is the central character of statistics:\n\n$$\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right]\n$$\n\nwhy does it appear everywhere? the central limit theorem (covered in the next chapter) gives the answer: whenever you average many independent contributions, the result tends toward a gaussian regardless of what the individual contributions look like. this is why measurement errors, which arise from the sum of many small disturbances, are almost always gaussian.\n\n[[simulation applied-stats-sim-2]]\n\n### student's t-distribution\n\nthe gaussian works beautifully when you have plenty of data. but with small samples, the estimated mean and standard deviation are themselves uncertain, and the gaussian underestimates the tails. the **student's t-distribution** accounts for this extra uncertainty by having heavier tails. as the sample size grows, the $t$-distribution converges to the gaussian. for **low statistics** work, always prefer the $t$-distribution.\n\n## maximum likelihood estimation\n\nwe have now met the major distributions. a natural question follows: given observed data, how do we figure out *which* distribution (and which parameter values) generated it? this is the problem of **estimation**, and the most powerful general method is **maximum likelihood estimation** (mle).\n\nthe idea is intuitive. suppose you flip a coin 10 times and get 7 heads. what value of $p$ (the probability of heads) makes this outcome most plausible? for $p = 0.5$, getting 7 heads is possible but not the most likely result. for $p = 0.7$, it is. mle formalizes this: find the parameter values that make the observed data as probable as possible.\n\nmathematically, the **likelihood** is the probability of the data as a function of the parameters $\\theta$:\n\n$$\n\\mathcal{l}(\\theta) = \\prod_i f(x_i; \\theta)\n$$\n\nthis is just the joint probability of all the observations, treated as a function of $\\theta$ rather than of the data. multiplying many small probabilities together causes numerical problems, so in practice we maximize the **log-likelihood** instead:\n\n$$\n\\ln\\mathcal{l}(\\theta) = \\sum_i \\ln f(x_i; \\theta)\n$$\n\nfor gaussian-distributed data, minimizing $-2\\ln\\mathcal{l}$ is equivalent to minimizing the familiar $\\chi^2$ statistic \u2014 a connection we will exploit heavily in the chapter on the chi-square method.\n\n### why mle works well\n\nmle estimators have three desirable properties that make them the default choice in most applications. first, they are **consistent**: as you collect more data, the estimate homes in on the true value, like an archer who gets closer to the bullseye with every shot. second, they are **asymptotically normal**: for large samples, the distribution of the estimator becomes gaussian, which makes it easy to construct confidence intervals. third, they are **efficient**: among all consistent estimators, mle achieves the smallest possible variance \u2014 the **cram\u00e9r-rao bound**. no other method can do better in the large-sample limit.\n\n### the fitting procedure\n\nputting mle into practice follows a clear recipe:\n\n1. **choose a model**: dec"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "random-effects",
      "lessonTitle": "Random Effects and Mixed Models",
      "x": 0.28766030073165894,
      "y": 0.05404961481690407,
      "searchText": "random effects and mixed models\n# random effects and mixed models\n\n## fixed vs random effects\n\nin standard anova and regression, all effects are **fixed**: they represent specific levels of interest (e.g., drug a vs drug b). you chose those levels deliberately, and your conclusions apply only to them.\n\nbut sometimes the levels in your study are a random sample from a larger population. if you study 10 hospitals, you probably do not care about *those specific 10* \u2014 you want conclusions that generalize to all hospitals. that is a **random effect**.\n\n- **fixed effect**: the specific levels are of direct interest. inference applies only to those levels.\n- **random effect**: the levels are sampled from a population. inference generalizes to the entire population of levels.\n\nthe distinction matters because it changes what you are estimating. with fixed effects, you estimate individual level means. with random effects, you estimate the *variance* across levels \u2014 how much variability exists in the population.\n\nthink of it this way: if you are studying the effect of a drug and you test it in several hospitals, the drug effect is fixed (you chose that drug deliberately) but the hospital effect is random (each hospital is one sample from a population of hospitals). different hospitals have different patient populations, equipment, and practices \u2014 this creates variability that is not about the drug itself but about the context. you need to account for it without treating each hospital as a separate, deliberate choice.\n\n## the mixed-effects model\n\na **mixed model** contains both fixed and random effects. the simplest version is a random-intercept model:\n\n$$\ny_{ij} = \\mu + \\beta x_{ij} + u_i + \\varepsilon_{ij},\n$$\n\nwhere $\\beta$ is a fixed effect (e.g., treatment), $u_i \\sim \\mathcal{n}(0, \\sigma_u^2)$ is a random intercept for group $i$, and $\\varepsilon_{ij} \\sim \\mathcal{n}(0, \\sigma_\\varepsilon^2)$ is the residual error.\n\nthe random effect $u_i$ captures what makes each group different. in a study of children's heights across schools, $u_i$ captures the fact that children in some schools are systematically taller (perhaps due to nutrition, demographics, or geography). it is like accounting for family traits in a height study \u2014 members of the same family share genetic and environmental factors that make them more similar to each other than to random strangers.\n\n## intraclass correlation coefficient (icc)\n\nhow similar are observations within the same group compared to observations from different groups? the **icc** quantifies this:\n\n$$\n\\text{icc} = \\frac{\\sigma_u^2}{\\sigma_u^2 + \\sigma_\\varepsilon^2}.\n$$\n\n- icc $\\approx 0$: the grouping does not matter. observations within a group are no more similar than observations from different groups. you could ignore the grouping and use standard methods.\n- icc $\\approx 1$: nearly all variability is between groups. within-group observations are very similar \u2014 the grouping structure dominates.\n\nthe icc is your diagnostic: when it is substantial (say, above 0.05-0.10), ignoring the grouping structure will produce incorrect standard errors and p-values. this is where mixed models become necessary rather than optional.\n\n## variance components\n\nin a **variance components model**, all factors are random:\n\n$$\ny_{ijk} = \\mu + a_i + b_{j(i)} + \\varepsilon_{ijk},\n$$\n\nwhere $a_i \\sim \\mathcal{n}(0, \\sigma_a^2)$ and $b_{j(i)} \\sim \\mathcal{n}(0, \\sigma_b^2)$ represent nested random effects. the notation $b_{j(i)}$ means that the $b$ levels are nested within the $a$ levels \u2014 classrooms within schools, or wells within experimental plates.\n\n### estimating variance components with reml\n\nhow do we estimate $\\sigma_a^2$, $\\sigma_b^2$, and $\\sigma_\\varepsilon^2$? ordinary maximum likelihood (which we used for fitting distributions in earlier chapters) has a bias problem here: it estimates the fixed effects first and then estimates the variances from the residuals, but it does not account for the degrees of freedom lost in estimating those fixed effects. the result is that ml systematically *underestimates* variance components \u2014 the same issue that bessel's correction fixes for the sample variance.\n\n**restricted maximum likelihood** (reml) solves this. it separates the likelihood into two parts: one for the fixed effects and one for the variance components. by estimating variances only from the second part \u2014 which is free of the fixed effects \u2014 reml produces unbiased variance estimates. it is the standard method for mixed models.\n\nthe practical difference: for large datasets, ml and reml give nearly identical results. for small datasets (where getting the variance right matters most), reml is noticeably better.\n\n## applications to clustered data\n\nmixed models are essential when data have a **hierarchical structure**:\n\n- students nested within classrooms nested within schools.\n- repeated measurements nested within patients nested within hospitals.\n- cells nested within wells nested within experimental plates.\n\nignoring this structure and treating all observations as independent leads to three problems:\n\n- **underestimated standard errors** (pseudo-replication). you think you have more independent information than you actually do.\n- **inflated type i error rates**. you reject $h_0$ too often.\n- **misleading p-values**. effects look significant when they are not.\n\nthe mixed model correctly partitions variability across levels of the hierarchy, producing valid inference even with unbalanced designs and missing data. it is the natural extension of anova (from the previous chapters) to data with grouped or clustered structure.\n\n[[simulation applied-stats-sim-6]]\n"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "simulation-fitting",
      "lessonTitle": "Simulation Methods",
      "x": 0.4244898855686188,
      "y": 0.2759094834327698,
      "searchText": "simulation methods\n# simulation methods\n\nthe central limit theorem told us that averages of random samples tend toward a gaussian. error propagation showed us how uncertainties flow through calculations. but both of those tools rely on analytical formulas that assume smoothness, linearity, or known distributions. what happens when those assumptions break down?\n\nthe answer is to let a computer do the experiment for you. instead of deriving a formula, you generate millions of random samples, push them through your calculation, and look at the result. this is the core idea behind **monte carlo simulation** \u2014 named after the famous casino, because it runs on random numbers.\n\n## producing random numbers\n\nevery monte carlo method starts with random numbers drawn from a specific distribution. computers generate **uniform** random numbers natively, so the challenge is converting those into samples from whatever distribution you need. two methods cover most cases: the **transformation method** and the **accept-reject method**.\n\n## transformation method\n\nthe transformation method is elegant and efficient when it works. the idea: if you can invert the cdf (the cumulative distribution function we met in the chapter on pdfs), you can transform uniform random numbers into any distribution you want.\n\nthe recipe:\n\n1. verify the pdf is normalized.\n2. compute the cumulative distribution function (cdf).\n3. invert the cdf.\n\n$$\nf(x) = \\int_{-\\infty}^x f(x') \\, dx'\n$$\n\ndraw a uniform random number $p \\in [0,1]$ and compute $x = f^{-1}(p)$. the resulting $x$ follows the target distribution.\n\n### example: exponential distribution\n\nconsider the exponential distribution, which models waiting times between poisson events:\n\n$$\nf(x) = \\lambda \\exp(-\\lambda x), \\quad x \\in [0, \\infty)\n$$\n\nthis is normalized. the cdf is:\n\n$$\nf(x) = 1 - \\exp(-\\lambda x)\n$$\n\ninverting gives:\n\n$$\nf^{-1}(p) = -\\frac{\\ln(1-p)}{\\lambda}\n$$\n\nto sample: draw $p$ uniformly from $[0,1]$ and compute $x = f^{-1}(p)$. each $x$ is a random draw from the exponential distribution.\n\n## accept-reject method\n\nthe transformation method requires an invertible cdf, which is not always available. the **accept-reject method** (also called the von neumann method) is more general: it works for any distribution you can evaluate, even if you cannot integrate or invert it.\n\nthe idea: draw samples from a simple proposal distribution and keep only those that \"pass\" a random acceptance test. given a target pdf $f(x)$ and a proposal distribution $g(x)$ with $f(x) \\leq m \\cdot g(x)$ for some constant $m$:\n\n1. sample $x$ from $g(x)$.\n2. sample $u$ uniformly from $[0, 1]$.\n3. accept $x$ if $u \\leq f(x) / (m \\cdot g(x))$; otherwise reject and repeat.\n\nthe accepted samples follow the target distribution exactly. the efficiency depends on how tightly the proposal $g$ envelops the target $f$ \u2014 a loose envelope wastes many samples.\n\n```python\ndef accept_reject_sample(target_pdf, proposal_sample, proposal_pdf, m, num_samples):\n    samples = []\n    while len(samples) < num_samples:\n        x = proposal_sample()\n        u = random.uniform(0, 1)\n        if u <= target_pdf(x) / (m * proposal_pdf(x)):\n            samples.append(x)\n    return samples\n```\n\n## why monte carlo scales well\n\nhere is the key advantage of monte carlo methods. remember from the clt that the uncertainty on a mean decreases as $1/\\sqrt{n}$, where $n$ is the number of samples. this convergence rate holds *regardless of the dimensionality* of the problem. by contrast, deterministic numerical integration (e.g., the trapezoidal rule) converges as $1/n^{2/d}$, where $d$ is the number of dimensions. in one dimension, deterministic methods win. but as $d$ grows, their convergence collapses while monte carlo stays at $1/\\sqrt{n}$.\n\nthis is why monte carlo is the tool of choice for high-dimensional problems \u2014 integrating over many parameters, propagating errors through complex models, or simulating physical processes with many degrees of freedom. it connects directly to the clt: every monte carlo estimate is an average, and the clt guarantees that average will be approximately gaussian with a calculable uncertainty.\n"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "agentbased",
      "lessonTitle": "Agent-Based Models and Stochastic Simulation",
      "x": 0.5819147825241089,
      "y": 0.5007127523422241,
      "searchText": "agent-based models and stochastic simulation\n# agent-based models and stochastic simulation\n\nno bird in a flock has a map of the formation. no driver on a highway knows the state of every other car. no neuron in your brain has a blueprint of your thoughts. and yet: the flock moves as one, traffic jams form and dissolve in waves, and your brain somehow thinks.\n\nin all these cases, the amazing thing is the same: **no agent has a global plan, yet order emerges**. simple local rules \u2014 follow your nearest neighbor, slow down if the car ahead is close, fire if enough of your neighbors fire \u2014 produce complex, coordinated global behavior. that is the central insight of agent-based modeling, and it connects directly to everything we have studied: the ising spins following local rules that produce phase transitions, the sandpile grains following local rules that produce power-law avalanches, the network nodes following local rules that produce scale-free structure.\n\nthe power of \"stupid\" local rules producing \"smart\" global behavior is one of the deepest themes in complex physics.\n\n## from equations to agents\n\nin many systems, the relevant actors are discrete individuals, not continuous fields. **agent-based models** (abms) simulate autonomous agents following local rules, and emergent collective behavior arises from their interactions.\n\nabms are particularly powerful when:\n\n- the population is **heterogeneous** (agents differ in attributes or behavior).\n- spatial structure matters (local interactions dominate over global averages).\n- stochasticity at the individual level drives macroscopic phenomena.\n\n## cellular automata: the game of life\n\n**conway's game of life** is the canonical deterministic agent-based model. on a 2d grid, each cell is alive or dead, and its state updates synchronously based on its eight neighbors:\n\n- a live cell **survives** if it has exactly 2 or 3 live neighbors; otherwise it dies (of loneliness or overcrowding).\n- a dead cell **is born** if it has exactly 3 live neighbors (just the right amount of community).\n\n[[simulation game-of-life]]\n\nwatch the simulation and something astonishing happens: from a random initial pattern, you see gliders that translate across the grid, oscillators that pulse with fixed periods, and complex structures that interact, collide, and sometimes produce entirely new structures. the game of life is actually turing-complete \u2014 it can in principle simulate any computation. all from two rules on a grid.\n\ndespite these simple rules, predicting the long-term behavior of a given initial configuration is in general impossible without actually running the simulation. this is the hallmark of complexity: simple rules, unpredictable outcomes.\n\n## stochastic simulation and the gillespie algorithm\n\nwhen reactions involve small numbers of molecules, deterministic rate equations (odes) fail to capture the inherent randomness. the **gillespie algorithm** (stochastic simulation algorithm) provides exact trajectories from the chemical master equation.\n\nat each step:\n\n1. compute all reaction **propensities** $a_i$ (rate $\\times$ number of reactant combinations).\n2. compute the total propensity $a_0 = \\sum_i a_i$.\n3. draw the **waiting time** to the next reaction: $\\delta t \\sim \\text{exp}(a_0)$.\n4. select **which reaction** fires with probability $a_i / a_0$.\n5. update the state and repeat.\n\nthe algorithm generates sample paths that are statistically exact solutions of the master equation. the price is that you simulate one reaction at a time \u2014 but the payoff is that you capture fluctuations that deterministic equations completely miss. in small systems (a few dozen molecules in a cell), these fluctuations can drive qualitatively different behavior: switches, oscillations, extinctions.\n\n## predator-prey dynamics\n\nthe **lotka-volterra model** describes the interaction between predators (foxes) and prey (rabbits). the ode (mean-field) version predicts sustained oscillations:\n\n$$\n\\frac{dr}{dt} = \\alpha r - \\beta r f, \\qquad \\frac{df}{dt} = \\delta r f - \\gamma f.\n$$\n\nbut now put this on a spatial grid as an agent-based model:\n\n- rabbits reproduce with some probability at each step.\n- foxes eat nearby rabbits and reproduce; they die if they go too long without eating.\n- both species move randomly on a spatial grid.\n\nthe agent-based version reveals phenomena invisible to the odes: spatial clustering (predators chase prey in traveling waves), local extinctions (an island of rabbits gets wiped out even though the global population is fine), and stochastic fluctuations that can drive one species to *global* extinction \u2014 something the deterministic equations say is impossible.\n\nthis is a recurring lesson: mean-field equations tell you what happens on average, but agents live in a world of fluctuations, space, and individual histories. the average can be misleading.\n\n## random walks and diffusion\n\n**random walks** are the simplest stochastic models \u2014 and they keep showing up. we saw them in the first-return time analysis of soc (chapter 8). here they are again as the foundation of diffusion.\n\na walker on a lattice takes steps in random directions at each time step. key results for an unbiased random walk in $d$ dimensions:\n\n- mean displacement: $\\langle \\mathbf{r}(t) \\rangle = 0$ (no net drift).\n- mean-squared displacement: $\\langle r^2(t) \\rangle = 2d \\, d \\, t$ (spreads as $\\sqrt{t}$).\n- **recurrence**: the walker returns to the origin with probability 1 in 1d and 2d, but not in 3d and higher (**polya's theorem**). a drunk person will eventually find their way home on a 2d street grid, but a drunk bird in 3d space may wander forever.\n\nthe **langevin equation** provides a continuous-time description:\n\n$$\n\\frac{dx}{dt} = -\\frac{\\partial u}{\\partial x} + \\sqrt{2d} \\, \\xi(t),\n$$\n\nwhere $\\xi(t)$ is gaussian white noise with $\\langle \\xi(t) \\xi(t') \\rangle = \\delta(t - t')$.\n\n## flocking, traffic, and the theme of emergence\n\nlet us tie together the applications that make agent-"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "criticalPhenomena",
      "lessonTitle": "Critical Phenomena",
      "x": 0.6880315542221069,
      "y": 0.6163880228996277,
      "searchText": "critical phenomena\n# critical phenomena\n\n## the miracle of forgetting details\n\nhere is one of the most astonishing facts in all of physics. take a magnet near its curie temperature. take a fluid near its liquid-gas critical point. take a binary alloy near its unmixing transition. these systems are made of completely different stuff \u2014 iron atoms with quantum spins, water molecules with hydrogen bonds, copper and zinc atoms with metallic interactions. their microscopic physics could not be more different.\n\nand yet, near their critical points, they all behave *identically*.\n\nthe magnetization of iron near $t_c$ follows the exact same power law as the density difference between liquid and gas water near its critical point. the susceptibility of the magnet diverges with the same exponent as the compressibility of the fluid. the correlation length grows the same way in both systems.\n\nhow is this possible? how can systems that are so different at the microscopic level produce the same macroscopic behavior at the critical point? the answer is that near a critical point, the system develops correlations on *all* length scales. the correlation length $\\xi$ diverges to infinity, and the system \"forgets\" its microscopic details. all that matters is the spatial dimensionality $d$ and the symmetry of the order parameter. everything else washes out.\n\nthis is **universality**, and it is the deepest result in the theory of phase transitions.\n\n## universality\n\nsystems in the same **universality class** share identical critical exponents. what determines the class? just two things:\n\n1. the **spatial dimensionality** $d$.\n2. the **symmetry of the order parameter** (scalar, vector, tensor, etc.).\n\nthe liquid-gas critical point and the 3d ising ferromagnet belong to the same universality class because both have a scalar order parameter in 3 dimensions \u2014 even though one involves molecules bouncing around and the other involves quantum spins on a lattice. nature does not care about the details. she cares about symmetry and dimension.\n\n## critical exponents and scaling laws\n\nnear the critical temperature $t_c$, thermodynamic quantities diverge or vanish as power laws. the **reduced temperature** $t = (t - t_c)/t_c$ measures distance from criticality.\n\nkey critical exponents:\n\n- **order parameter**: $m \\sim |t|^\\beta$ for $t < t_c$.\n- **susceptibility**: $\\chi \\sim |t|^{-\\gamma}$.\n- **heat capacity**: $c \\sim |t|^{-\\alpha}$.\n- **correlation length**: $\\xi \\sim |t|^{-\\nu}$.\n- **correlation function at $t_c$**: $g(r) \\sim r^{-(d-2+\\eta)}$.\n\nthese exponents are not independent. **scaling relations** connect them:\n\n$$\n\\alpha + 2\\beta + \\gamma = 2 \\qquad \\text{(rushbrooke)},\n$$\n\n$$\n\\gamma = \\nu(2 - \\eta) \\qquad \\text{(fisher)},\n$$\n\n$$\n\\nu d = 2 - \\alpha \\qquad \\text{(josephson / hyperscaling)}.\n$$\n\nthese are not just empirical correlations. they follow from the mathematical structure of scale invariance. if you know any two independent exponents, you can compute all the others.\n\n## mean-field theory and its limitations\n\nrecall from the mean-field chapter: mean-field theory replaces fluctuations with their average, yielding the landau free energy:\n\n$$\nf(m) = a_0 + a_2 t \\, m^2 + a_4 m^4 + \\cdots\n$$\n\nminimizing gives mean-field exponents: $\\beta = 1/2$, $\\gamma = 1$, $\\alpha = 0$, $\\nu = 1/2$.\n\nmean-field theory is exact above the **upper critical dimension** $d_c = 4$ for the ising model. below $d_c$, fluctuations are too strong to ignore, and the actual exponents differ from mean-field predictions \u2014 sometimes dramatically.\n\n| exponent | mean-field | 2d ising | 3d ising |\n|----------|-----------|----------|----------|\n| $\\beta$  | 1/2       | 1/8      | 0.326    |\n| $\\gamma$ | 1         | 7/4      | 1.237    |\n| $\\nu$    | 1/2       | 1        | 0.630    |\n\nlook at those numbers. in 2d, $\\beta$ drops from $1/2$ to $1/8$ \u2014 the magnetization vanishes *much* more slowly than mean-field predicts. the susceptibility diverges almost twice as fast ($\\gamma = 7/4$ vs. $1$). fluctuations matter enormously.\n\n[[simulation phase-transition-ising-1d]]\n\nin this simulation, you can watch how the ising model behaves near its critical point: enormous clusters of aligned spins form and dissolve, the correlation length grows, and the system fluctuates wildly between magnetized and unmagnetized states. this is what criticality looks like.\n\n## the renormalization group\n\nthe **renormalization group** (rg) provides the theoretical framework for understanding universality and scaling. the idea, due to kenneth wilson, is to systematically zoom out:\n\n1. **block spins**: group neighboring spins into blocks and define a new effective spin for each block.\n2. **rescale**: shrink the lattice back to its original size.\n3. **renormalize**: adjust the coupling constants so the partition function is preserved.\n\nthis defines a flow in the space of coupling constants. **fixed points** of the rg flow correspond to scale-invariant systems \u2014 systems that look the same at every scale. these are exactly the critical points.\n\nthe critical exponents are determined by the eigenvalues of the linearized rg transformation at the fixed point. **relevant** directions (which grow under iteration) drive the system away from criticality \u2014 they correspond to temperature and external field. **irrelevant** directions (which shrink) do not affect the critical behavior \u2014 they correspond to microscopic details like lattice structure, interaction range, etc.\n\nthis is why universality works: the irrelevant directions all flow to zero, and only the relevant ones survive. two systems with different microscopic details but the same relevant variables flow to the same fixed point and therefore share the same critical exponents.\n\n## correlation functions\n\nthe **two-point correlation function** measures how fluctuations at two points are statistically related:\n\n$$\ng(\\mathbf{r}) = \\langle s(\\mathbf{0}) \\, s(\\mathbf{r}) \\rangle - \\langle s \\rangle^2.\n$$\n\naway from $t_c$, correlations de"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "econophysics",
      "lessonTitle": "Econophysics",
      "x": 0.35898762941360474,
      "y": 0.30531764030456543,
      "searchText": "econophysics\n# econophysics\n\nremember the sandpile from chapter 8? you drop grains one at a time, and most of the time nothing happens, but occasionally a massive avalanche rearranges the whole pile. the same mathematics shows up in stock prices.\n\nremember the scale-free networks from chapter 9? a few hubs dominate, and the degree distribution follows a power law. the same pattern appears in the distribution of stock returns: small fluctuations are common, but extreme moves (crashes and rallies) are far more frequent than a normal distribution would predict.\n\nremember the ising model from the very beginning? spins interact with their neighbors and collectively decide to align \u2014 a phase transition. traders interact with each other (through news, rumors, herding) and collectively decide to sell \u2014 a market crash.\n\n**econophysics** applies the methods of statistical mechanics and complex systems to financial markets. the central discovery is that markets are not the well-behaved random walks that classical finance assumes. they are complex systems with heavy tails, long-range correlations, and avalanche-like dynamics.\n\n## brownian motion and stock prices\n\nthe simplest model treats log-returns as a random walk. let $s_t$ be the closing price and $x_t = \\log(s_t)$. the variance at lag $\\tau$ is:\n\n$$\n\\text{var}(\\tau) = \\langle (x_{t+\\tau} - x_t)^2 \\rangle.\n$$\n\nfor **geometric brownian motion** (the foundation of the black-scholes model), variance grows linearly with lag:\n\n$$\n\\text{var}(\\tau) \\propto \\tau.\n$$\n\nthis would mean stock returns are uncorrelated, gaussian, and well-behaved. it is a beautiful theory. it is also wrong.\n\n[[simulation stock-variance]]\n\nrun this simulation with real stock data and compare it with the prediction from geometric brownian motion. you will see two major deviations: (1) **heavy tails** \u2014 large moves are far more frequent than a gaussian predicts (the famous \"fat tails\"), and (2) **volatility clustering** \u2014 large changes tend to follow large changes, and calm periods follow calm periods. the market has memory.\n\n## the hurst exponent\n\nthe **hurst exponent** $h$ measures long-range dependence in time series. for the rescaled range $r/s$ of a time series with $n$ data points:\n\n$$\n\\frac{r}{s} \\sim c \\, n^h \\quad \\text{as } n \\to \\infty,\n$$\n\nwhere $c$ is a constant.\n\nthe value of $h$ reveals the nature of correlations:\n\n- $h = 0.5$: uncorrelated random walk (pure brownian motion). each step is independent of the past.\n- $h > 0.5$: **persistent** (trending) behavior. an up move is more likely to be followed by another up move. positive long-range correlations.\n- $h < 0.5$: **anti-persistent** (mean-reverting) behavior. an up move is more likely to be followed by a down move. negative long-range correlations.\n\nfor self-similar time series, $h$ is directly related to the fractal dimension (chapter 7): $d = 2 - h$. a brownian motion trace has fractal dimension $1.5$; a persistent time series is smoother ($d < 1.5$); an anti-persistent one is rougher ($d > 1.5$).\n\n[[simulation hurst-exponent]]\n\nestimate the hurst exponent from data in this simulation. real financial time series typically show $h$ close to $0.5$ for the returns themselves (they are nearly uncorrelated), but $h$ significantly above $0.5$ for the absolute returns or volatility (volatility is persistent \u2014 this is the clustering effect).\n\n## the fear-factor model\n\nhere is a simple model that captures the asymmetry in stock returns. let $q$ be the probability of the stock moving up under normal conditions. now introduce a **collective fear event** with probability $p$ that forces all stocks down simultaneously \u2014 think of a market panic, a financial crisis, a sudden loss of confidence.\n\nthe effective probabilities become:\n\n- stock goes up: $(1-p)q$.\n- stock goes down: $p + (1-p)(1-q)$.\n\nfor a neutral random walk (equal probability of up and down):\n\n$$\n(1-p)q = p + (1-p)(1-q) \\implies q = \\frac{1}{2(1-p)}.\n$$\n\nas the fear probability $p$ increases, the required upward probability $q$ must increase to compensate. this creates an asymmetry that mimics the observed **negative skewness** in stock returns: the market rises slowly most of the time (to compensate for the fear premium) but crashes quickly and dramatically when the collective fear event hits.\n\nthe variance of returns includes a term proportional to $p$ that represents **systematic risk** \u2014 risk that cannot be diversified away because it affects everyone simultaneously.\n\n## bet hedging\n\n**bet hedging** is a strategy where an organism (or investor) sacrifices expected performance to reduce variance. it is the mathematical foundation of portfolio diversification.\n\nin a stochastic growth model, consider a population with growth rate $r_t$ drawn from a distribution at each time step. the long-run growth rate is not the arithmetic mean but the **geometric mean**:\n\n$$\n\\bar{r}_{\\text{long-run}} = \\langle \\ln r_t \\rangle = \\langle r_t \\rangle - \\frac{1}{2} \\text{var}(r_t) + \\cdots\n$$\n\nthis **arithmetic-geometric inequality** means that variance *always* reduces long-run growth. an organism (or investor) that reduces its variance \u2014 by hedging its bets across strategies \u2014 can outperform a specialist that maximizes expected growth, especially when the noise is large.\n\n[[simulation bet-hedging]]\n\ntry the simulation: compare a \"specialist\" strategy (high expected return, high variance) with a \"diversified\" strategy (lower expected return, lower variance). run both for many time steps. the specialist might win in the short run, but the diversifier almost always wins in the long run. variance is not just risk \u2014 it is a *drag* on growth.\n\nthe key insight: the optimal strategy depends on the **noise size** relative to the expected return. in a calm, predictable world, specialize. in a noisy, uncertain world, diversify. the mathematics is the same whether you are a bacterium hedging against environmental fluctuations or a portfolio manager hedging against market crashes.\n\n> "
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "home",
      "lessonTitle": "Complex Physics",
      "x": 0.5968502759933472,
      "y": 0.45899632573127747,
      "searchText": "complex physics\n# complex physics\n\n## course overview\n\ncomplex physics studies systems composed of many interacting components whose collective behavior cannot be predicted from individual parts alone. these systems exhibit **emergent phenomena** such as phase transitions, self-organization, power-law distributions, and fractal geometry.\n\n- classical physics: few-body systems, exact solutions, linear superposition.\n- complex physics: many-body interactions, statistical descriptions, nonlinear dynamics, universality.\n\ncore approach:\n\n1. identify the relevant degrees of freedom and their interactions.\n2. use statistical mechanics to connect microscopic rules to macroscopic observables.\n3. recognize universal scaling behavior near critical points.\n4. simulate complex systems computationally when analytical solutions fail.\n\n## the road map\n\nimagine you are watching a pot of water boil. first we will understand why anything happens at all: why does heat spread evenly, why do molecules share energy the way they do? that is **statistical mechanics**. then we will ask: how can we let a computer roll the dice for us when the math gets too hard? that is the **metropolis algorithm**. next comes the dramatic part: what happens when everything suddenly changes, when a magnet loses its magnetism or a liquid becomes a gas? those are **phase transitions**, and we will build a simple theory (**mean-field**) to understand them, then solve a toy model exactly with the **transfer matrix**.\n\nbut here is where it gets truly beautiful. we will discover that completely different systems, magnets and fluids and gases, all behave *identically* near their critical points. that is **universality and critical phenomena**. the same patterns keep showing up: in **percolation** (when a connected path suddenly spans a random medium), in **self-organized criticality** (when a sandpile tunes itself to the edge of chaos), in **networks** (when the rich get richer and power laws emerge), in **agent-based models** (when dumb local rules produce smart global behavior), and even in **econophysics** (when stock markets crash like avalanches).\n\nby the end of this course, you will see these patterns everywhere. nature reuses her tricks, and we are going to learn them.\n\n## why this topic matters\n\n- phase transitions underpin phenomena from magnetism to superconductivity.\n- percolation theory models fluid flow through porous media, disease spreading, and network resilience.\n- self-organized criticality explains power laws in earthquakes, forest fires, and biological evolution.\n- network science describes the structure of the internet, social systems, and gene regulation.\n- agent-based models capture emergent collective behavior from simple local rules.\n- econophysics applies statistical mechanics to financial markets and wealth distributions.\n\n## key mathematical ideas\n\n- partition functions, free energy, and thermodynamic observables.\n- order parameters, critical exponents, and scaling relations.\n- renormalization group and universality classes.\n- power-law distributions and fractal dimensions.\n- monte carlo sampling and markov chain methods.\n- graph theory and network metrics.\n\n## prerequisites\n\n- probability and statistical reasoning.\n- calculus and linear algebra.\n- basic thermodynamics and classical mechanics.\n- familiarity with python for computational exercises.\n\n## recommended reading\n\n- thurner, hanel, and klimek, *introduction to the theory of complex systems*.\n- sethna, *statistical mechanics: entropy, order parameters, and complexity*.\n- newman, *networks: an introduction*.\n\n## learning trajectory\n\nthis module is organized from equilibrium foundations to complex emergent behavior:\n\n- **statistical mechanics** \u2014 why does a room full of air molecules share energy so democratically? we start with the rules that govern huge numbers of particles.\n- **metropolis algorithm** \u2014 let the computer do the hard work of exploring impossible-to-calculate probabilities.\n- **phase transitions** \u2014 the moment everything changes: how a magnet suddenly finds its direction.\n- **mean-field results** \u2014 a beautifully simple (and sometimes wrong) theory where every spin feels the average of all the others.\n- **1d ising & transfer matrix** \u2014 one dimension, one exact solution, and a clever matrix trick that makes it all work.\n- **critical phenomena** \u2014 the miracle of universality: why magnets, fluids, and gases all forget their differences at the critical point.\n- **percolation and fractals** \u2014 when random connections suddenly span the world, and the geometry turns out to be fractal.\n- **self-organized criticality** \u2014 sandpiles, avalanches, and systems that tune themselves to the edge without anyone turning the knob.\n- **networks** \u2014 the rich get richer, power laws appear, and six degrees of separation connect us all.\n- **agent-based models** \u2014 dumb local rules, smart global behavior: flocking birds, traffic jams, and the game of life.\n- **econophysics** \u2014 stock markets crash like sandpiles, and the same mathematics describes both.\n\n## visual and simulation gallery\n\n[[figure complex-sandpile-image]]\n\n[[figure complex-percolation-video]]\n"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "meanFieldResults",
      "lessonTitle": "Mean-Field Results",
      "x": 0.6988773345947266,
      "y": 0.6946330070495605,
      "searchText": "mean-field results\n# mean-field results\n\nimagine every spin in our magnet feels an average field from all its neighbors, like a crowd where everyone is trying to face the same way because they see everyone else doing it. we derived the mean-field hamiltonian in the previous chapter. now let us see what comes out of it.\n\nthe punchline first \u2014 here are the key results:\n\n## mf: z, m, tc, f & critical exponents\n\n$$\n\\begin{align*}\n    z_{\\mathrm{mf}}\n    &=\n    \\exp\n    \\left(\n        - \\beta\\frac{j n z}{2} m^2\n    \\right)\n    \\left[\n    2\\cosh\n    \\left(\n        \\beta j z m + \\beta h\n    \\right)\n    \\right]^n\n    \\\\\n    m\n    &= \\langle s_i \\rangle\n    =\n    \\tanh\n    \\left(\n        \\beta j z m + \\beta h\n    \\right)\n    \\\\\n    t_c &= \\frac{jz}{k_b}\n    \\\\\n    f_{\\mathrm{mf}} &=\n    \\frac{jnz}{2}m^2\n    - \\frac{n}{\\beta} \\ln\n    \\left[\n        2 \\cosh \\left(\\beta jzm + \\beta h \\right)\n    \\right]\n\\end{align*}\n$$\n\nnow let us earn these results.\n\n## mf: z, m, tc, f & critical exponents (derivation)\n\n### mean-field partition function\n\nthe partition function of the ising model is a sum over all $2^n$ spin configurations:\n$$\n\\begin{align*}\n    z\n    &=\n    \\sum_{\\{s_i\\}}\n    e^{-\\beta \\mathcal{h}(\\{s_i\\})}\n    =\n    \\sum_{n}^{2^n}\n    e^{-\\beta e_n}.\n\\end{align*}\n$$\n\nnow we plug in the mean-field hamiltonian. the crucial simplification is that the spins have decoupled \u2014 the sum over all configurations *factorizes*:\n$$\n\\begin{align*}\n    z_{\\mathrm{mf}}\n    &=\n    \\sum_{s_1 = \\pm 1} \\cdots \\sum_{s_n = \\pm 1}\n    \\exp\n    \\left[\n        - \\beta\\frac{j n z}{2} m^2\n        + \\beta \\left( j z m + h \\right) \\sum_i s_i\n    \\right]\n    \\\\&=\n    \\exp\n    \\left(\n        - \\beta\\frac{j n z}{2} m^2\n    \\right)\n    \\prod_{i=1}^n\n    \\left[\n    \\sum_{s_i = \\pm 1}\n    \\exp\n    \\left(\n        \\beta (j z m + h) s_i\n    \\right)\n    \\right]\n    \\\\&=\n    \\exp\n    \\left(\n        - \\beta\\frac{j n z}{2} m^2\n    \\right)\n    \\left[\n    2\\cosh\n    \\left(\n        \\beta j z m + \\beta h\n    \\right)\n    \\right]^n.\n\\end{align*}\n$$\n\neach spin contributes a factor of $2\\cosh(\\beta(jzm + h))$, and since they are all independent in mean-field, these factors multiply. this is why mean-field theory is solvable: the terrible many-body problem has been reduced to $n$ copies of a one-body problem.\n\n### self-consistent equation of magnetization\n\nthe magnetization per spin $m = \\langle s_i \\rangle$ can be calculated directly:\n$$\n\\begin{align*}\n    m\n    &= \\langle s_i \\rangle\n    \\\\&=\n    \\frac{1}{z_{\\mathrm{mf}}}\n    \\sum_{\\{s_i\\}}\n    s_i \\, e^{-\\beta \\mathcal{h}_\\mathrm{mf}}\n    \\\\&=\n    \\frac\n    {\n    \\displaystyle\n    \\sum_{s_i = \\pm 1}\n    s_i\n    \\exp\\left[\\beta (j z m + h) s_i\\right]\n    }\n    {\n    \\displaystyle\n    \\sum_{s_i = \\pm 1}\n    \\exp\\left[\\beta (j z m + h) s_i\\right]\n    }\n    \\\\&=\n    \\frac{2\\sinh(\\beta j z m + \\beta h)}{2\\cosh(\\beta j z m + \\beta h)}\n    \\\\&=\n    \\tanh(\\beta j z m + \\beta h).\n\\end{align*}\n$$\n\nthis is the **self-consistency equation**: $m$ appears on both sides! the magnetization depends on itself through the mean field. with no external field ($h = 0$):\n$$\n    m = \\tanh(\\beta j z m).\n$$\n\nthis equation cannot be solved in closed form, but we can understand it graphically. plot $y = m$ and $y = \\tanh(\\beta jzm)$ and look for intersections:\n\n- at **high temperature** ($\\beta jz < 1$), the $\\tanh$ curve is too shallow \u2014 it only crosses $y = m$ at $m = 0$. the only solution is zero magnetization. the system is disordered.\n- at **low temperature** ($\\beta jz > 1$), the $\\tanh$ curve is steep enough to cross $y = m$ at three points: $m = 0$ and two symmetric nonzero values $\\pm m_0$. the system can be magnetized!\n\nthe transition between these two regimes is the **phase transition**.\n\n### critical temperature of mean-field approximation\n\nthe critical temperature is where the number of solutions changes from one to three. this happens when the slope of $\\tanh(\\beta jzm)$ at $m = 0$ equals the slope of $y = m$ (which is 1).\n\nsince $\\tanh(x) \\approx x$ for small $x$:\n$$\n    \\left.\\frac{\\mathrm{d}}{\\mathrm{d}m} \\tanh(\\beta jzm)\\right|_{m=0}\n    = \\beta jz = 1.\n$$\n\nsolving for temperature:\n$$\n    t_c = \\frac{jz}{k_\\mathrm{b}}.\n$$\n\nthis makes physical sense: stronger coupling $j$ means higher $t_c$ (harder to disorder), and more neighbors $z$ means higher $t_c$ (more peer pressure to stay aligned). however, note that mean-field theory predicts a phase transition in *every* dimension, including 1d \u2014 and we know from the exact solution that there is no phase transition in the 1d ising model. mean-field theory overestimates the tendency to order because it ignores the correlated fluctuations that destroy order in low dimensions.\n\n### free energy of mean-field approximation\n\nfrom the partition function:\n$$\n\\begin{align*}\n    f_\\mathrm{mf}\n    &=\n    - \\frac{1}{\\beta} \\ln z_\\mathrm{mf}\n    \\\\&=\n    \\frac{jnz}{2}m^2\n    - \\frac{n}{\\beta} \\ln\n    \\left[\n        2 \\cosh (\\beta jzm + \\beta h)\n    \\right].\n\\end{align*}\n$$\n\nas a check, we can recover the self-consistency equation by differentiating with respect to $h$:\n$$\n    m = -\\frac{1}{n}\\frac{\\partial f_\\mathrm{mf}}{\\partial h}\n    = \\tanh(\\beta jzm + \\beta h). \\quad \\checkmark\n$$\n\n### critical exponents from landau expansion\n\nto understand the behavior near the transition, we expand the free energy in powers of $m$. introduce the dimensionless temperature $\\theta = t/t_c = 1/(\\beta jz)$ and dimensionless free energy $f_\\mathrm{mf} = f_\\mathrm{mf}/(jzn)$. for $h = 0$ and small $m/\\theta$:\n\nusing $\\cosh(x) \\approx 1 + x^2/2 + x^4/24 + \\cdots$ and $\\ln(1+x) \\approx x - x^2/2 + \\cdots$:\n$$\n\\begin{align*}\n    f_\\mathrm{mf}\n    &=\n    \\frac{1}{2}m^2\\left(1 - \\frac{1}{\\theta}\\right)\n    + \\frac{1}{12}\\frac{m^4}{\\theta^3}\n    - \\theta \\ln 2\n    + \\mathcal{o}(m^6).\n\\end{align*}\n$$\n\nthis is a **landau free energy** \u2014 a polynomial in the order parameter $m$. the coefficient of $m^2$ changes sign at $\\theta = 1$ (i.e., $t = t_c$). that sign change is the phase transition.\n\n### sta"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "metropolisAlgorithm",
      "lessonTitle": "Metropolis Algorithm",
      "x": 0.6572655439376831,
      "y": 0.6683178544044495,
      "searchText": "metropolis algorithm\n# metropolis algorithm\n\nwe just learned that the partition function $z$ is the key to everything. but here is the problem: for an ising model with $n$ spins, there are $2^n$ possible configurations. for a modest $10 \\times 10$ lattice, that is about $10^{30}$ states. you could not sum over them all if you had every computer on earth running until the heat death of the universe.\n\nso what do we do? we cheat \u2014 brilliantly. instead of summing over all states, we let the computer wander through state space, visiting states with the correct boltzmann probabilities. this is the monte carlo method, and the metropolis algorithm is its most famous implementation.\n\n## ising model\n\nbefore we get to the algorithm, let us set up the playground. ernst ising introduced a beautifully simple model of ferromagnetism: put a spin on every site of a lattice, and let each spin point either up ($s_i = +1$) or down ($s_i = -1$). the energy is\n$$\n    \\mathcal{h}\n    =\n    -j \\sum_{\\langle i j \\rangle} s_i s_j - h \\sum_i s_i.\n$$\nhere $\\langle i j \\rangle$ means we sum over nearest-neighbor pairs (each pair counted once), $j > 0$ is the coupling strength that rewards neighboring spins for pointing the same way, and $h$ is an external magnetic field.\n\nthink of it like peer pressure: every spin wants to agree with its neighbors (when $j > 0$). when $j < 0$ the interaction is antiferromagnetic \u2014 spins prefer to alternate. when $j$ varies from pair to pair, you get a spin glass, which is a whole other can of worms.\n\nthe magnetization is simply the average spin:\n$$\n    m = \\frac{1}{n} \\sum_{i=1}^n s_i.\n$$\n\nising himself solved the 1d case exactly (no phase transition \u2014 we will see why in the transfer matrix chapter). lars onsager solved the 2d case in a celebrated tour de force. the 3d ising model remains unsolved analytically to this day. this is why we need computers.\n\nwe can spot the critical temperature of the 2d ising phase transition by looking for a divergence in the susceptibility $\\chi$, which we approximate as the variance of the magnetization:\n$$\n    \\chi = \\langle m^2 \\rangle - \\langle m \\rangle^2.\n$$\n\n## monte carlo method\n\n*monte carlo is a district of monaco famous for its casinos. according to lore, nicholas metropolis suggested the name \u2014 sampling random numbers to solve physics problems felt a lot like gambling.*\n\nthe idea is to estimate thermal averages without summing over all states. the statistical mechanical average of an observable $a$ is\n$$\n\\begin{align*}\n    \\langle a \\rangle\n    &=\n    \\sum_i a_i p_i\n    \\\\&=\n    \\frac{\\sum_i a_i \\exp(-\\beta e_i)}{\\sum_i \\exp(-\\beta e_i)}.\n\\end{align*}\n$$\n\nif we could generate a sequence of states $\\{s^{(1)}, s^{(2)}, \\ldots\\}$ where state $i$ appears with probability $p_i$, then we could approximate\n$$\n    \\langle a \\rangle\n    \\approx\n    \\frac{1}{m} \\sum_{k=1}^{m} a(s^{(k)}).\n$$\n\nthat is all monte carlo does: replace an impossible sum with a sample average. the metropolis algorithm tells us *how* to generate those samples correctly.\n\n## markov process and master equation\n\nthe key assumption is that the system evolves as a **markov process**: what happens next depends only on the current state, not on the history of how we got here. under this assumption, the probability of being in state $i$ evolves according to the **master equation**:\n$$\n    \\frac{\\mathrm{d}p_i}{\\mathrm{d}t}\n    =\n    \\sum_j \\left( w_{ij}p_j - w_{ji}p_i \\right).\n$$\nhere $w_{ij}$ is the transition rate from state $j$ to state $i$. the first term is probability flowing *into* state $i$; the second is probability flowing *out*.\n\nin steady state ($\\mathrm{d}p_i/\\mathrm{d}t = 0$), the total inflow equals total outflow:\n$$\n    \\sum_j w_{ij}p_j = \\sum_j w_{ji}p_i.\n$$\n\nbut this is not enough. this condition still allows circular flows (probability sloshing around in loops), which would violate the spirit of thermal equilibrium.\n\n## detailed balance\n\nto ensure true equilibrium, we impose a stronger condition called **detailed balance**: the flow between *every pair* of states must individually balance.\n$$\n    w_{ij}p_j = w_{ji}p_i.\n$$\n\nno net current between any two states. no loops. this is the condition we need.\n\n## metropolis-hastings algorithm\n\nnow we combine detailed balance with the boltzmann distribution. in equilibrium, $p_i \\propto e^{-\\beta e_i}$, so\n$$\n    \\frac{w_{ij}}{w_{ji}}\n    =\n    \\frac{p_i}{p_j}\n    =\n    \\exp\\left(-\\beta \\delta e_{ij}\\right),\n$$\nwhere $\\delta e_{ij} = e_i - e_j$ is the energy change when transitioning from state $j$ to state $i$.\n\nthe metropolis algorithm turns this into a simple recipe:\n\n1. **propose** a random move (e.g., flip a random spin).\n2. **calculate** the energy difference $\\delta e$.\n3. **if** $\\delta e \\leq 0$ (the move lowers the energy): accept it. always.\n4. **if** $\\delta e > 0$ (the move raises the energy): accept it with probability $e^{-\\beta \\delta e}$. draw a random number $r \\in [0,1]$; if $r < e^{-\\beta \\delta e}$, accept. otherwise reject and keep the old state.\n\nthat is it. this simple accept/reject rule guarantees that after enough steps, the system samples states according to the boltzmann distribution. you do not need to know $z$. you do not need to enumerate all states. you just need to compute energy differences, which are cheap.\n\n## watching the simulation\n\nhere is where things get exciting. run the metropolis algorithm on a 2d ising model and watch the screen.\n\nat **high temperature** the spins flip like mad \u2014 the thermal energy overwhelms the coupling, and you see pure noise, a salt-and-pepper mess of up and down spins. the magnetization bounces around zero.\n\nnow **cool it down slowly**. at first, nothing dramatic \u2014 just a bit less noise. but then, as you approach the critical temperature $t_c \\approx 2.27 \\, j/k_\\mathrm{b}$, something remarkable happens: **whole regions of aligned spins appear**. domains of up-spins and down-spins form, grow, merge. the fluctuations become huge. the susceptibility $\\chi$ sh"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "Networks",
      "lessonTitle": "Networks",
      "x": 0.5958528518676758,
      "y": 0.5271850824356079,
      "searchText": "networks\n# networks\n\nnew web pages do not link randomly. they link to pages that are already popular \u2014 google, wikipedia, youtube. a new paper does not cite random old papers. it cites the famous ones, the ones everyone else is already citing. a new actor does not get cast with random other actors. they get cast alongside established stars.\n\nthis is the **\"rich get richer\"** effect, and it produces power laws everywhere. the distribution of web links, citation counts, social connections, airport traffic \u2014 they all follow the same pattern: a few hubs with enormous numbers of connections, and a vast majority with very few.\n\nit is the same kind of power law we saw in the sandpile (chapter 8) and in percolation clusters (chapter 7). nature loves power laws, and networks are one of her favorite places to put them.\n\n## graph theory basics\n\na **network** (graph) consists of nodes (vertices) connected by edges (links). networks provide a natural language for describing complex systems where relationships between components matter as much as the components themselves.\n\nexamples of real-world networks:\n\n- **social networks**: individuals connected by friendships or interactions.\n- **world wide web**: pages connected by hyperlinks.\n- **biological networks**: proteins connected by interactions, genes by regulatory links.\n- **infrastructure**: power grids, transportation systems, the internet.\n\n## network metrics\n\nkey quantities that characterize a network:\n\n- **degree** $k_i$: the number of edges connected to node $i$. the **degree distribution** $p(k)$ describes the probability that a randomly chosen node has degree $k$. this single function tells you more about a network's structure than almost anything else.\n- **clustering coefficient**: measures the fraction of a node's neighbors that are also connected to each other. high clustering means \"my friends know each other\" \u2014 cliquishness.\n- **betweenness centrality**: the fraction of shortest paths between all pairs of nodes that pass through a given node. high centrality means the node is a bottleneck \u2014 remove it, and many paths break.\n- **connectedness**: how many nodes must be removed to disconnect the network?\n- **modularity**: the strength of division into communities. zero modularity means no meaningful partition.\n\nthe **amplification factor** involves the second moment of the degree distribution:\n\n$$\n\\langle \\mathcal{a} \\rangle = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} - 1.\n$$\n\nthis quantity is crucial: it diverges for scale-free networks with exponent $\\gamma \\leq 3$, with profound consequences for epidemic spreading and network robustness.\n\n## small-world networks\n\nhere is a surprising fact about social networks: any two people on earth are connected by roughly six intermediaries. that is the **\"six degrees of separation\"** phenomenon (milgram, 1967).\n\n**small-world networks** (watts and strogatz, 1998) explain how this is possible by combining two properties:\n\n- **high clustering**: like a regular lattice, neighbors of a node tend to be connected (your friends know each other).\n- **short path lengths**: like a random graph, any two nodes are connected by a short chain (six degrees).\n\nthe watts-strogatz model starts from a regular ring lattice and randomly rewires each edge with probability $p$. the remarkable finding: even a tiny rewiring probability ($p \\sim 0.01$) dramatically reduces the average path length while preserving high clustering. a few long-range shortcuts are enough to make the whole network small-world.\n\n## scale-free networks\n\n**scale-free networks** have a degree distribution that follows a power law:\n\n$$\np(k) \\sim k^{-\\gamma},\n$$\n\ntypically with $2 < \\gamma < 3$. this means a few nodes (hubs) have enormously many connections, while most nodes have few. there is no \"typical\" number of connections \u2014 the distribution is scale-free.\n\nexamples of scale-free networks:\n\n- the world wide web ($\\gamma \\approx 2.1$ for in-degree).\n- protein interaction networks.\n- citation networks.\n- software dependency graphs.\n\n[[simulation scale-free-network]]\n\nwatch the network grow in this simulation. new nodes attach preferentially to well-connected hubs, and the power-law degree distribution emerges naturally. the resulting network looks nothing like a regular grid or a random graph \u2014 it has a few giant hubs surrounded by many weakly connected nodes.\n\n## preferential attachment\n\nthe **barabasi-albert model** (1999) explains how scale-free networks arise through **preferential attachment**: new nodes connect preferentially to existing nodes that already have many connections.\n\nalgorithm:\n\n1. start with $m_0$ connected nodes.\n2. add new nodes one at a time, each connecting to $m$ existing nodes.\n3. the probability of connecting to node $i$ is proportional to its degree: $\\pi(k_i) = k_i / \\sum_j k_j$.\n\nthis produces a power-law degree distribution with exponent $\\gamma = 3$. the rich-get-richer mechanism is all you need \u2014 the power law emerges as a mathematical consequence, just as the power law in the sandpile (chapter 8) emerges from the self-organized dynamics.\n\n## dynamics on networks\n\nnetworks are not just static structures; processes unfold on them:\n\n- **epidemic spreading** (sir/sis models): on scale-free networks, the epidemic threshold vanishes in the thermodynamic limit. even weakly infectious diseases can spread through the hubs. this is why super-spreaders matter so much.\n- **diffusion and synchronization**: random walks on networks explore hubs quickly; coupled oscillators synchronize more easily on networks with high connectivity.\n- **information cascades**: content spreads through social networks via threshold mechanisms \u2014 one viral post can reach millions through the hub structure.\n\n## robustness and attacks\n\nnetworks respond very differently to random failures versus targeted attacks:\n\n- **random failure**: removing random nodes has little effect on scale-free networks because most nodes have low degree \u2014 you are unlikely"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "percolation_and_fractals",
      "lessonTitle": "Percolation and Fractals",
      "x": 0.6718800663948059,
      "y": 0.5759598612785339,
      "searchText": "percolation and fractals\n# percolation and fractals\n\nsuppose you are painting a wall with random dots of ink. at first they are isolated islands \u2014 tiny splotches scattered here and there with gaps between them. keep adding dots. some start to merge into small clusters. keep going. more clusters connect, forming larger and larger blobs.\n\nthen, at a critical density, something extraordinary happens: a single cluster suddenly spans from one side of the wall all the way to the other. one more dot of ink, and the whole geometry changes. that sudden appearance of a giant connected cluster is a **phase transition** \u2014 but it is geometric, not energetic. no temperature is involved, no energy is minimized. it is purely about connectivity. beautiful, isn't it?\n\nthis is **percolation**, and it turns out to have the same mathematical structure as the thermal phase transitions we studied earlier \u2014 critical exponents, scaling laws, universality, and all.\n\n## percolation theory\n\nin **site percolation** on a lattice, each site is independently occupied with probability $p$ and empty with probability $1-p$. occupied neighbors form clusters.\n\n- for small $p$, only small isolated clusters exist.\n- at the **percolation threshold** $p_c$, a giant cluster first spans the system.\n- for $p > p_c$, the giant cluster contains a finite fraction of all sites.\n\nthe percolation threshold depends on the lattice geometry:\n\n| lattice | $p_c$ (site) |\n|---------|-------------|\n| square | 0.5927 |\n| triangular | 0.5000 |\n| honeycomb | 0.6962 |\n\n[[simulation percolation]]\n\ntry the simulation: start with $p$ well below the threshold and slowly increase it. watch the clusters grow, merge, and then suddenly \u2014 a single cluster connects the entire system. the transition is sharp and dramatic, just like the magnetic phase transition, even though no energy or temperature is involved.\n\n## critical exponents in percolation\n\nnear the threshold, key quantities follow power laws in $|p - p_c|$, just like thermal phase transitions near $t_c$:\n\n- **order parameter** (fraction in spanning cluster): $p_\\infty \\sim (p - p_c)^\\beta$ for $p > p_c$.\n- **mean cluster size** (excluding the spanning cluster): $\\langle s \\rangle \\sim |p - p_c|^{-\\gamma}$.\n- **correlation length** (typical cluster radius): $\\xi \\sim |p - p_c|^{-\\nu}$.\n\nin 2d: $\\beta = 5/36$, $\\gamma = 43/18$, $\\nu = 4/3$. these exponents are universal within the percolation universality class \u2014 they do not depend on whether you use a square, triangular, or honeycomb lattice. sound familiar? it is the same miracle of universality we saw for thermal phase transitions (chapter 6), just in a different universality class.\n\n## the bethe lattice\n\nthe **bethe lattice** (cayley tree) is an infinite tree where each node has exactly $z$ neighbors. percolation on the bethe lattice is exactly solvable:\n\n$$\np_c = \\frac{1}{z - 1}.\n$$\n\nfor $z = 3$: $p_c = 1/2$. the bethe lattice provides the **mean-field theory** for percolation and gives exact critical exponents $\\beta = 1$, $\\gamma = 1$, $\\nu = 1/2$ \u2014 the same mean-field exponents we saw in the ising model, because mean-field theory always gives the same exponents regardless of the specific system.\n\n[[simulation bethe-lattice]]\n\nin this simulation you can build a bethe lattice and watch how occupied sites form clusters. because every node looks the same (no loops, no boundaries), the math simplifies enormously \u2014 it is the percolation equivalent of mean-field theory.\n\n## fractals and self-similarity\n\nat the percolation threshold, the spanning cluster is not a smooth blob \u2014 it is a tangled, wispy, infinitely detailed object full of holes at every scale. zoom in, and you see the same kind of structure as when you zoom out. this is a **fractal**: a geometric object that exhibits self-similarity across scales.\n\nthe **fractal dimension** $d_f$ characterizes how the mass of an object scales with its linear size:\n\n$$\nm(l) \\sim l^{d_f}.\n$$\n\nfor ordinary solid objects in $d$ dimensions, $d_f = d$ (a square has $d_f = 2$, a cube has $d_f = 3$). for fractals, $d_f$ is typically non-integer \u2014 the object is \"too thin\" to fill its embedding space, but \"too thick\" to be a lower-dimensional object.\n\nexamples:\n\n- koch snowflake: $d_f = \\log 4 / \\log 3 \\approx 1.26$.\n- sierpinski triangle: $d_f = \\log 3 / \\log 2 \\approx 1.58$.\n- percolation cluster at $p_c$ in 2d: $d_f = 91/48 \\approx 1.896$.\n\n[[simulation fractal-dimension]]\n\nplay with this simulation to see how fractal dimension is measured. the object has more \"stuff\" than a line ($d_f > 1$) but less than a plane ($d_f < 2$). it lives in between \u2014 and that fractional dimension is what makes it a fractal.\n\n## the mandelbrot set\n\nthe **mandelbrot set** is defined in the complex plane as the set of values $c$ for which the iteration\n\n$$\nz_{n+1} = z_n^2 + c, \\qquad z_0 = 0,\n$$\n\nremains bounded. the boundary of the mandelbrot set is a fractal with infinite detail at every scale.\n\nthe **escape-time algorithm** colors each point $c$ by the number of iterations needed for $|z_n|$ to exceed a threshold (typically 2), producing the iconic visualizations of the set.\n\n[[simulation mandelbrot-fractal]]\n\nzoom into the boundary of the mandelbrot set. no matter how far you zoom, you keep finding new structure \u2014 spirals, tendrils, miniature copies of the whole set. this infinite self-similarity from a one-line formula ($z \\to z^2 + c$) is one of the most stunning examples of complexity emerging from simplicity.\n\n## box-counting dimension\n\nthe **box-counting method** provides a practical way to estimate the fractal dimension of any set:\n\n1. cover the set with boxes of side length $\\epsilon$.\n2. count the number $n(\\epsilon)$ of boxes needed.\n3. the fractal dimension is $d_f = -\\lim_{\\epsilon \\to 0} \\frac{\\log n(\\epsilon)}{\\log \\epsilon}$.\n\nin practice, $\\log n(\\epsilon)$ is plotted against $\\log(1/\\epsilon)$, and $d_f$ is estimated from the slope of the linear region.\n\n> **key intuition.** percolation is a geometric phase transi"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "phaseTransitions",
      "lessonTitle": "Phase Transitions",
      "x": 0.6810548305511475,
      "y": 0.6657238602638245,
      "searchText": "phase transitions\n# phase transitions\n\npick up a refrigerator magnet. it sticks to your fridge because trillions of tiny atomic magnets inside it have collectively decided to point the same direction. now heat that magnet with a blowtorch. at first nothing much changes. but at a certain temperature \u2014 the curie temperature \u2014 the magnet suddenly stops being a magnet. the atomic spins are still there, still interacting, but the thermal jiggling has overwhelmed their desire to align. above that temperature it is just a lump of iron with no north or south pole. below it, the whole thing points the same way.\n\nthat sudden appearance of a preferred direction is what we call an **order parameter** \u2014 a quantity that is zero in the disordered phase and nonzero in the ordered phase. for a magnet, the order parameter is the magnetization $m$. for a liquid-gas transition, it is the density difference. the order parameter is the signature that tells you something dramatic has happened.\n\n## ising model simulation\n\nwatch the ising model in action. at high temperature, the spins are a random mess \u2014 the magnetization is zero on average. as you cool toward $t_c$, domains of aligned spins start forming and growing. at $t_c$ itself, the fluctuations are enormous: the system cannot decide which way to point. below $t_c$, one direction wins, and the magnetization becomes nonzero. that is the phase transition.\n\n[[simulation ising-model]]\n\nin this simulation, you are watching millions of spins make a collective decision \u2014 no leader, no blueprint, just nearest-neighbor interactions. the fact that global order emerges from purely local rules is one of the deepest surprises in physics.\n\n## mean-field hamiltonian\n\ncan we build a simple theory of this transition? the exact hamiltonian of the ising model couples every spin to its neighbors:\n$$\n    \\mathcal{h}\n    =\n    -j \\sum_{\\langle i j \\rangle} s_i s_j - h \\sum_i s_i.\n$$\n\nthe trouble is the $s_i s_j$ coupling \u2014 it ties every spin to its neighbors, and those neighbors to *their* neighbors, creating a tangled web of correlations.\n\nthe **mean-field approximation** cuts this knot with a beautifully simple idea: instead of feeling the actual fluctuating spins of its neighbors, each spin feels only the *average* field from all of them. imagine being in a crowd where everyone is trying to face the same direction. you do not look at each individual \u2014 you just feel the general pull of the crowd.\n\n## mean-field hamiltonian (derivation)\n\nto make the mean-field idea precise, we decompose each spin into its mean and a fluctuation:\n$$\n    s_i = \\langle s_i \\rangle + \\delta s_i = m + \\delta s_i,\n$$\nwhere $m = \\langle s_i \\rangle$ is the magnetization per spin (all spins are equivalent by symmetry).\n\nthe product of two neighboring spins becomes:\n$$\n\\begin{align*}\n    s_i s_j\n    &=\n    (m + \\delta s_i)(m + \\delta s_j)\n    \\\\&=\n    m^2 + m \\, \\delta s_j + \\delta s_i \\, m + \\delta s_i \\, \\delta s_j\n    \\\\& \\approx\n    m^2 + m(s_j - m) + (s_i - m) m\n    \\\\&=\n    -m^2 + m(s_i + s_j).\n\\end{align*}\n$$\n\nwe dropped the $\\delta s_i \\, \\delta s_j$ term \u2014 this is the mean-field approximation. we are saying that the correlated fluctuations between two spins are small enough to ignore. (this is a good approximation when each spin has many neighbors, and a terrible one in low dimensions \u2014 but we will worry about that later.)\n\nsubstituting into the hamiltonian and carefully handling the nearest-neighbor sums (each spin has $z$ neighbors, and each pair is counted once):\n\n**first term:**\n$$\n    j m^2 \\sum_{\\langle i j \\rangle} = \\frac{j n z}{2} m^2.\n$$\n\n**second term:**\n$$\n    - j \\sum_{\\langle i j \\rangle} m(s_i + s_j) = - j z m \\sum_{i} s_i.\n$$\n\n**mean-field hamiltonian:**\n$$\n\\begin{align*}\n    \\mathcal{h}_\\mathrm{mf}\n    &=\n    \\frac{j n z}{2} m^2\n    - (jzm + h) \\sum_i s_i.\n\\end{align*}\n$$\n\nlook at what happened: the spins have decoupled! each spin $s_i$ now sees an effective field $(jzm + h)$ that depends on the *average* magnetization $m$, not on the actual values of its neighbors. this makes the problem exactly solvable \u2014 each spin is independent, and we just need to find $m$ self-consistently.\n\n> **key intuition.** a phase transition is the moment when a system collectively chooses an ordered state. the order parameter measures the degree of this collective choice. mean-field theory captures the essential physics by replacing the complicated many-body problem with a simpler one where each particle feels only the average effect of all the others.\n\n> **feynman challenge.** here is a thought experiment. imagine 100 people in a room, each trying to decide whether to stand or sit. if they make the decision independently, roughly half will stand and half will sit. but now add a rule: each person looks at their two nearest neighbors and feels a slight urge to do the same thing. can you convince yourself that below some \"critical level of conformity,\" the crowd splits 50/50, but above it, nearly everyone stands (or sits)? that is a phase transition in a social system.\n\n---\n\n*we have the mean-field hamiltonian, and it looks clean and solvable. but what actually comes out of it? what is the critical temperature? what does the free energy look like? and what goes wrong with the stability analysis near the transition? let us find out in the mean-field results.*\n"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "selfOrganizedCriticality",
      "lessonTitle": "Self-Organized Criticality",
      "x": 0.6118678450584412,
      "y": 0.5550162196159363,
      "searchText": "self-organized criticality\n# self-organized criticality\n\nyou drop one grain of sand on a sandpile and \u2014 nothing happens. you drop another. nothing. another. still nothing. then you drop one more grain and *whoosh* \u2014 half the pile slides off in a massive avalanche. the next grain? nothing again.\n\nhere is what is remarkable: nobody tuned the slope of that pile. nobody set a parameter to a critical value. the pile did it by itself. through the simple process of adding grains and letting them tumble, the sandpile has driven itself to the exact point where tiny inputs can cause huge outputs. no one turned the knob \u2014 the system found the critical point on its own.\n\nthat is **self-organized criticality** (soc), and it may explain why power laws show up everywhere in nature, from earthquakes to forest fires to the electrical activity of your brain.\n\n## the concept of soc\n\nin everything we have studied so far \u2014 the ising model, percolation \u2014 criticality required **fine-tuning**: we had to set the temperature to exactly $t_c$ or the occupation probability to exactly $p_c$. miss by a little, and the beautiful power laws disappear.\n\nsoc is different. these systems spontaneously evolve toward a critical state without any external tuning. they naturally drive themselves to the edge.\n\nkey signatures of soc:\n\n- power-law distributed event sizes (avalanches of all scales).\n- $1/f$ noise in temporal fluctuations.\n- fractal spatial structure (just like the percolation clusters from chapter 7).\n- separation of time scales: slow driving (one grain at a time) and fast relaxation (avalanches).\n\n## the sandpile model (btw)\n\nthe **bak-tang-wiesenfeld sandpile model** (1987) is the paradigmatic example of soc.\n\non a 2d grid, each site $i$ has a height $z_i$. sand is added one grain at a time at random sites. when $z_i$ exceeds a threshold $z_c$ (typically 4 for a square lattice), the site **topples**:\n\n$$\nz_i \\to z_i - 4, \\qquad z_j \\to z_j + 1 \\quad \\text{for each neighbor } j.\n$$\n\ntoppling can trigger neighbors to topple in turn, creating an **avalanche** that can cascade across the entire system. grains that topple off the boundary are lost (open boundary conditions).\n\n[[simulation sandpile-model]]\n\nwatch the simulation: drop grains one at a time and see what happens. most of the time, nothing dramatic \u2014 the grain just sits there. but occasionally, a single grain triggers a cascade that rearranges the entire pile. small avalanches are common, large ones are rare, and the distribution follows a power law:\n\n$$\np(s) \\sim s^{-\\tau},\n$$\n\nwith $\\tau \\approx 1.1$ in 2d. the distribution of avalanche areas, durations, and lifetimes also follow power laws. there is no characteristic scale \u2014 avalanches of all sizes occur.\n\nafter a transient, the system reaches a statistically steady state. the average rate of sand input equals the average rate of sand falling off the edges. the pile maintains itself at the critical slope \u2014 not too steep, not too shallow. self-organized criticality.\n\n## the bak-sneppen model\n\nthe **bak-sneppen model** (1993) applies soc to biological evolution. consider $n$ species arranged on a ring, each with a random fitness value $f_i \\in [0, 1]$.\n\nat each time step:\n\n1. find the species with the **lowest fitness**.\n2. replace its fitness and the fitness of its two neighbors with new random values from $[0, 1]$.\n\nthe model self-organizes to a critical state where most fitness values exceed a threshold $f_c \\approx 0.667$ (in 1d). below this threshold, species are unstable and trigger cascading replacements \u2014 the avalanches of the model.\n\n[[simulation bak-sneppen]]\n\nthe analogy to evolution is compelling: the weakest species goes extinct and gets replaced, but this disrupts its neighbors (who depended on it ecologically), potentially triggering a cascade of extinctions and replacements. punctuated equilibrium \u2014 long periods of stasis interrupted by bursts of change \u2014 emerges naturally.\n\n## first-return times of random walks\n\nthe connection between soc and random walks provides theoretical insight. consider a 1d random walk starting at the origin. the **first-return time** $t$ is the number of steps until the walker returns to the origin.\n\nfor an unbiased random walk:\n\n$$\np(t = 2n) \\sim n^{-3/2},\n$$\n\na power law with exponent $-3/2$. this result connects to soc because avalanches in many soc models can be mapped to random-walk first-return problems. the avalanche \"starts\" when the system leaves the critical state and \"ends\" when it returns \u2014 just like a random walk departing from and returning to the origin.\n\n[[simulation random-walk-first-return]]\n\nrun this simulation and watch the random walk wander away from zero and then return. short excursions are common, long ones are rare \u2014 and the distribution of return times follows a power law. no tuning required.\n\n## non-equilibrium steady states\n\nsoc systems are fundamentally different from the equilibrium systems we studied earlier. they are **out of equilibrium**: energy (or sand, or fitness) is continuously injected and dissipated.\n\nthis distinguishes soc from equilibrium critical phenomena:\n\n- no detailed balance.\n- no free energy or partition function.\n- the steady state is maintained by the balance of driving and dissipation.\n- fluctuations (avalanches) are the mechanism of transport, not just noise around an average.\n\n## power laws in nature\n\nsoc has been proposed as an explanation for power-law distributions observed in:\n\n- **earthquakes**: the gutenberg-richter law $\\log n(m) = a - bm$ relates earthquake magnitude $m$ to frequency. small quakes happen all the time; huge ones are rare but inevitable.\n- **forest fires**: fire size distributions in some models follow power laws. a single spark can burn one tree or an entire forest.\n- **neuronal avalanches**: cascades of neural activity in cortical circuits show power-law size distributions. your brain may operate near a critical point.\n- **solar flares**: the energy distribution of solar flares follo"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "statisticalMechanics",
      "lessonTitle": "Statistical Mechanics",
      "x": 0.7463685274124146,
      "y": 0.6600000858306885,
      "searchText": "statistical mechanics\n# statistical mechanics\n\nsuppose you have a huge box full of air molecules bouncing around. billions upon billions of them, all flying in random directions, colliding, exchanging energy. why is the temperature the same everywhere? why doesn't all the energy suddenly rush to one corner, leaving the other side frozen? that is the miracle we are going to explain.\n\nthe answer is statistics. when you have an enormous number of particles, the laws of probability take over, and they are ruthlessly democratic. the system explores every possible arrangement of energy among its particles, and the most probable arrangement wins by an overwhelming margin. that is the core insight of statistical mechanics: we do not need to track every particle. we just need to count states.\n\n## microcanonical ensemble\n\nthe central assumption of statistical mechanics is the principle of *equal a priori probabilities*: all microstates that share a given total energy are equally likely. there is no favoritism. nature does not prefer one arrangement of molecular speeds over another, as long as the energy adds up.\n\nwith this assumption, the probability of finding the system in a particular microstate $i$ is simply\n$$\n    p_i = \\frac{1}{\\omega},\n$$\nwhere $\\omega$ is the total number of microstates with energy $e$.\n\nthink of it this way: if you have a jar with $\\omega$ lottery tickets and each one is equally likely, the chance of drawing any particular ticket is $1/\\omega$. that is all this equation says.\n\nludwig boltzmann connected this counting of states to thermodynamics through his famous entropy formula:\n$$\n    s = k_\\mathrm{b} \\ln \\omega.\n$$\nhere $k_\\mathrm{b}$ is boltzmann's constant. more microstates means more entropy. a system left to itself will find its way to the macrostate with the most microstates, because that is simply the most probable outcome.\n\ntemperature then emerges as a statistical quantity:\n$$\n    \\frac{1}{t}\n    = \\frac{\\partial s}{\\partial e}\n    = k_\\mathrm{b} \\frac{\\partial}{\\partial e} \\ln \\omega.\n$$\n\ntemperature tells you how fast the number of available states grows as you add energy. if adding a little energy opens up a huge number of new states, the system is cold. if the number of states barely changes, the system is hot.\n\n## canonical ensemble\n\n### temperature of system and reservoir become the same in equilibrium\n\nnow let us zoom in on a small piece of a much larger system. imagine a teacup of water sitting in a room. the teacup is our \"system\" and the room is our \"reservoir\" (or \"heat bath\"). they can exchange energy, but the total energy is fixed:\n$$\n    e_\\mathrm{t} = e_\\mathrm{s} + e_\\mathrm{r}.\n$$\n\nthe number of microstates for the whole arrangement factors into a product:\n$$\n    \\omega(e_\\mathrm{s}, e_\\mathrm{r})\n    = \\omega_\\mathrm{s}(e_\\mathrm{s}) \\, \\omega_\\mathrm{r}(e_\\mathrm{r}),\n$$\nso the total entropy is the sum:\n$$\n    s_\\mathrm{t} = s_\\mathrm{s} + s_\\mathrm{r}.\n$$\n\nthe probability of finding a particular split of energy is\n$$\n    p(e_\\mathrm{s}, e_\\mathrm{r})\n    = \\frac{\\omega_\\mathrm{s}(e_\\mathrm{s}) \\, \\omega_\\mathrm{r}(e_\\mathrm{r})}{\\omega(e_\\mathrm{t})}.\n$$\n\nthe most probable state (thermodynamic equilibrium) maximizes this probability. setting the derivative to zero:\n$$\n\\begin{align*}\n    0 &=\n        \\frac{\\partial \\ln p(e_\\mathrm{s}, e_\\mathrm{r})}{\\partial e_\\mathrm{s}}\n        \\\\&=\n        \\frac{\\partial}{\\partial e_\\mathrm{s}}\n        \\ln \\omega_\\mathrm{s}(e_\\mathrm{s})\n        +\n        \\frac{\\partial}{\\partial e_\\mathrm{r}}\n        \\frac{\\partial e_\\mathrm{r}}{\\partial e_\\mathrm{s}}\n        \\ln \\omega_\\mathrm{r}(e_\\mathrm{r})\n        \\\\&=\n        \\frac{1}{k_\\mathrm{b}}\n        \\frac{\\partial s_\\mathrm{s}}{\\partial e_\\mathrm{s}}\n        -\n        \\frac{1}{k_\\mathrm{b}}\n        \\frac{\\partial s_\\mathrm{r}}{\\partial e_\\mathrm{r}}\n        \\\\&=\n        \\frac{1}{t_\\mathrm{s}}\n        -\n        \\frac{1}{t_\\mathrm{r}}.\n\\end{align*}\n$$\n\nthe conclusion is simple and beautiful: in equilibrium, the system and reservoir have the same temperature.\n$$\n    t_\\mathrm{s} = t_\\mathrm{r}\n$$\n\nthis is not something we assumed. it *followed* from counting states. temperature equality is the most probable outcome, and for large systems it is overwhelmingly probable.\n\n### boltzmann distribution\n\nwhen the system is in a specific microstate $i$ with energy $e_i$, the reservoir has energy $e_\\mathrm{t} - e_i$. since the reservoir is enormous, we can taylor-expand its entropy:\n$$\n\\begin{align*}\n    p_i\n    &\\propto\n        \\omega_\\mathrm{r}(e_\\mathrm{t} - e_i)\n    \\\\&=\n        \\exp\n        \\left[ \\frac{1}{k_\\mathrm{b}} s_r(e_\\mathrm{t} - e_i) \\right]\n    \\\\&\\approx\n        \\exp \\left[ \\frac{1}{k_\\mathrm{b}}\n        \\left(s_\\mathrm{r}(e_\\mathrm{t})\n        - \\left.\n        \\frac{\\mathrm{d}s_\\mathrm{r}}{\\mathrm{d}e}\n        \\right|_{e=e_\\mathrm{t}}e_i\n        \\right)\n        \\right]\n    \\\\&\\propto\n        \\exp \\left(\n        -\\frac{e_i}{k_\\mathrm{b}t}\n        \\right).\n\\end{align*}\n$$\n\nthis is the **boltzmann distribution**: the probability of finding the system in state $i$ falls off exponentially with the energy of that state. high-energy states are exponentially rare. low-energy states are exponentially favored. temperature controls how steep the falloff is.\n\n### partition function, free energy, and thermodynamic observables\n\nthe **partition function** is the normalization constant that makes probabilities add up to one:\n$$\n\\begin{align*}\n    z &= \\sum_i \\exp \\left(-\\frac{e_i}{k_\\mathrm{b}t}\\right)\n        \\\\&= \\sum_i \\exp \\left(-\\beta e_i\\right).\n\\end{align*}\n$$\nhere $\\beta = 1/(k_\\mathrm{b}t)$. the probability of state $i$ is then\n$$\n    p_i = \\frac{e^{-\\beta e_i}}{z}.\n$$\n\ndo not underestimate this humble sum. once you have $z$, you can extract *every* thermodynamic quantity by taking derivatives. it is the master key.\n\nthe **(helmholtz) free energy** is\n$$\n    f = -k_\\mathrm{b}t \\ln z = -\\frac{1}{\\beta}\\ln z = \\langle e \\rangle - ts.\n$$\n\nthe **average energy** f"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "transferMatrix",
      "lessonTitle": "1D Ising Model and Transfer Matrix Method",
      "x": 0.7039777040481567,
      "y": 0.6596906185150146,
      "searchText": "1d ising model and transfer matrix method\n# 1d ising model and transfer matrix method\n\nthink of a one-dimensional chain of spins as a book. each page (spin) can be \"up\" or \"down.\" the energy cost depends only on whether two neighboring pages match or not. the transfer matrix is a clever way of multiplying the probabilities page by page until you reach the end of the book \u2014 and then the eigenvalues pop out and hand you the exact answer.\n\nthis is one of the rare cases in statistical mechanics where we can solve the many-body problem *exactly*. no approximations, no monte carlo, no mean-field hand-waving. just linear algebra.\n\n## hamiltonian\n\nthe hamiltonian of the 1d ising model with periodic boundary conditions ($s_{n+1} = s_1$) is\n$$\n    \\mathcal{h}\n    =\n    -j\\sum_{i=1}^{n} s_i s_{i+1} - h \\sum_{i=1}^{n} s_i.\n$$\n\n## partition function\n\nthe partition function is a sum over all $2^n$ configurations:\n$$\n\\begin{align*}\n    z\n    &=\n    \\sum_{s_1 = \\pm 1} \\cdots \\sum_{s_n = \\pm 1}\n    \\exp\n    \\left(\n        \\beta j\\sum_{i=1}^{n} s_i s_{i+1}\n        + \\beta h \\sum_{i=1}^{n} s_i\n    \\right).\n\\end{align*}\n$$\n\nthe trick is to split the external field term equally between each pair of neighbors:\n$$\n    \\beta h \\sum_i s_i = \\beta h \\sum_i \\frac{s_i + s_{i+1}}{2},\n$$\nso the whole exponent factorizes into a product of identical \"bond\" terms:\n$$\n    z =\n    \\sum_{s_1 = \\pm 1} \\cdots \\sum_{s_n = \\pm 1}\n    \\prod_{i=1}^n\n    \\exp\n    \\left(\n        \\beta j s_i s_{i+1}\n        + \\beta h \\frac{s_i+s_{i+1}}{2}\n    \\right).\n$$\n\neach factor depends only on two neighboring spins. that is a matrix element waiting to happen.\n\n## transfer matrix\n\ndefine the transfer matrix $t$ with elements\n$$\n    t_{s_i, s_{i+1}}\n    =\n    \\exp\n    \\left(\n        \\beta j s_i s_{i+1}\n        + \\beta h \\frac{s_i+s_{i+1}}{2}\n    \\right).\n$$\n\nwritten out explicitly as a $2 \\times 2$ matrix:\n$$\n    t\n    =\n    \\begin{bmatrix}\n    e^{\\beta(j+h)} & e^{-\\beta j} \\\\\n    e^{-\\beta j} & e^{\\beta(j-h)}\n    \\end{bmatrix}.\n$$\n\nnow here is where the magic happens. the partition function involves summing products of these matrix elements over all intermediate spins \u2014 but that is exactly what matrix multiplication does! summing over $s_2$ gives $(t^2)_{s_1, s_3}$, then summing over $s_3$ gives $(t^3)_{s_1, s_4}$, and so on:\n$$\n\\begin{align*}\n    z\n    &=\n    \\sum_{s_1 = \\pm 1} \\cdots \\sum_{s_n = \\pm 1}\n    t_{s_1, s_2} t_{s_2, s_3} \\cdots t_{s_n, s_1}\n    \\\\&=\n    \\sum_{s_1 = \\pm 1}\n    (t^n)_{s_1, s_1}\n    \\\\&=\n    \\mathrm{tr}(t^n).\n\\end{align*}\n$$\n\nthe partition function is the trace of the $n$-th power of the transfer matrix. we have converted the statistical mechanics problem into a linear algebra problem.\n\n## diagonalization and eigenvalues\n\nsince $t$ is a real symmetric matrix, it is diagonalizable: $t = pdp^{-1}$, where $d$ is the diagonal matrix of eigenvalues. then\n$$\n    t^n = pd^n p^{-1},\n$$\nand using the cyclic property of the trace:\n$$\n    z = \\mathrm{tr}(t^n) = \\mathrm{tr}(d^n) = \\lambda_1^n + \\lambda_2^n.\n$$\n\nthe entire partition function reduces to the eigenvalues of a $2 \\times 2$ matrix. that is the power of the transfer matrix method.\n\n## finding the eigenvalues\n\nthe characteristic equation $\\det(t - \\lambda i) = 0$ gives:\n$$\n    \\lambda^2\n    -\n    2\\lambda \\, e^{\\beta j} \\cosh(\\beta h)\n    +\n    2 \\sinh(2\\beta j)\n    = 0.\n$$\n\nsolving the quadratic:\n$$\n    \\lambda_{\\pm}\n    =\n    e^{\\beta j} \\cosh(\\beta h)\n    \\pm\n    \\sqrt{\n        e^{2\\beta j} \\sinh^2(\\beta h)\n        + e^{-2\\beta j}\n    }.\n$$\n\nsince $\\lambda_+ > \\lambda_-$ always, in the thermodynamic limit ($n \\to \\infty$) the smaller eigenvalue becomes negligible:\n$$\n    z \\approx \\lambda_+^n.\n$$\n\nthe free energy per spin is\n$$\n    f = -\\frac{1}{\\beta n} \\ln z = -\\frac{1}{\\beta} \\ln \\lambda_+.\n$$\n\n## the punchline: no phase transition in 1d\n\nfor $h = 0$, the eigenvalues simplify to\n$$\n    \\lambda_+ = e^{\\beta j} + e^{-\\beta j} = 2\\cosh(\\beta j), \\qquad\n    \\lambda_- = e^{\\beta j} - e^{-\\beta j} = 2\\sinh(\\beta j).\n$$\n\nboth eigenvalues are positive and analytic (smooth) functions of temperature for all $t > 0$. since the free energy $f = -(1/\\beta)\\ln\\lambda_+$ is an analytic function of $t$, there is *no singularity* at any finite temperature. no singularity means no phase transition.\n\nthis is the exact confirmation of what we suspected: in one dimension, thermal fluctuations always win. a single domain wall (a place where neighboring spins disagree) costs energy $2j$ but increases entropy by $k_\\mathrm{b} \\ln n$ (it can be placed at any of $n$ bonds). for large enough $n$, the entropy always beats the energy, and domain walls proliferate, destroying any long-range order.\n\nmean-field theory predicted a phase transition in 1d \u2014 and that prediction is wrong. this is a concrete example of why mean-field theory fails in low dimensions: it ignores the fluctuations that matter most.\n\n> **key intuition.** the transfer matrix converts the partition function into a trace of a matrix power: $z = \\mathrm{tr}(t^n)$. in the thermodynamic limit, only the largest eigenvalue matters. if the largest eigenvalue is analytic (no singularity), there is no phase transition. in 1d, the largest eigenvalue is always smooth \u2014 so there is no phase transition at finite temperature. entropy always wins over energy in one dimension.\n\n> **feynman challenge.** for the 1d ising model with $h = 0$, compute the average energy per spin $\\langle e \\rangle / n = -\\partial \\ln \\lambda_+ / \\partial \\beta$. you should get $-j\\tanh(\\beta j)$. check that this interpolates between $0$ (high $t$) and $-j$ (low $t$). does it make physical sense?\n\n---\n\n*we solved the one-dimensional case exactly \u2014 beautiful, but it has no phase transition. in real life, things are higher-dimensional and messy. so now we zoom out and ask a much deeper question: what do all critical points have in common, regardless of the system? that is the subject of critical phenomena and universality.*\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "channelsandpipes",
      "lessonTitle": "Channels and Pipes",
      "x": 0.7871440052986145,
      "y": 0.394442081451416,
      "searchText": "channels and pipes\n# channels and pipes\n\n## the beauty of exact solutions\n\nmost of the time, the navier-stokes equation is too hard to solve analytically. but for a few special geometries \u2014 flows between parallel plates and through circular pipes \u2014 the equation simplifies enough that we can solve it exactly, by hand. these aren't just textbook exercises. they're the foundations for understanding everything from blood flow in arteries to glaciers sliding down valleys.\n\nthe key simplification: for **steady** ($\\partial/\\partial t = 0$), **fully developed** (the flow doesn't change along the channel), **unidirectional** flow, the nonlinear advection term $(\\mathbf{v} \\cdot \\nabla)\\mathbf{v}$ vanishes. the navier-stokes equation reduces to a balance between the pressure gradient (or gravity) and viscous friction.\n\n## pressure-driven channel flow \u2014 the parabolic profile\n\nimagine two infinite parallel plates separated by a distance $2a$, with fluid pushed through by a pressure gradient $g = -dp/dx$. the no-slip boundary condition (fluid velocity is zero at the walls) and symmetry give:\n$$\nv_x(y) = \\frac{g}{2\\eta}(a^2 - y^2)\n$$\n\nthe velocity profile is a **parabola**: fastest in the center, zero at the walls. this is **poiseuille flow** between parallel plates.\n\nthe shape of this parabola tells you about the fluid. for a newtonian fluid ($n = 1$), it's a perfect parabola. for a **shear-thinning** material ($n < 1$), the profile becomes more \"blunted\" \u2014 flatter in the middle, dropping sharply near the walls. for a **shear-thickening** material ($n > 1$), the profile becomes more \"pointed\" \u2014 sharper in the center.\n\nglaciers are a dramatic example: ice behaves as a shear-thickening fluid ($n > 1$ in glen's flow law), so the velocity profile across a glacier valley is remarkably flat in the middle and drops off steeply near the valley walls. gladys the glacier knows her flow law.\n\n## gravity-driven planar flow \u2014 the waterfall problem\n\nnow tilt the channel. instead of a pressure gradient pushing the fluid, gravity does the work. think of rain flowing down a tilted roof, or a thin film of water on an inclined plane.\n\nfor a uniform film of thickness $a$ on a plane inclined at angle $\\theta$:\n$$\n0 = g_x + \\nu \\frac{\\partial^2 v_x}{\\partial y^2}, \\qquad 0 = g_y - \\frac{1}{\\rho}\\frac{\\partial p}{\\partial y}\n$$\n\nwhere $g_x = g_0 \\sin\\theta$ drives the flow downhill. for a newtonian fluid, the velocity profile is again parabolic:\n$$\nv_x(y) = \\frac{g_0 \\sin\\theta}{2\\nu}(2ay - y^2)\n$$\n\nmaximum velocity is at the free surface ($y = a$), zero at the wall ($y = 0$). by measuring the velocity profile \u2014 for instance, by tracking particles on the surface \u2014 you can estimate the power-law exponent $n$ and learn about the fluid's rheology.\n\n## laminar pipe flow \u2014 the hagen-poiseuille result\n\nthe most practically important exact solution: steady flow through a circular pipe of radius $a$ driven by a pressure gradient $g$. the velocity profile in cylindrical coordinates is:\n$$\nv_z(r) = \\frac{g}{4\\eta}(a^2 - r^2)\n$$\n\nagain parabolic \u2014 fastest on the centerline, zero at the pipe wall. the **volume flow rate** (total discharge) is:\n$$\nq = \\int_0^a v_z(r) \\, 2\\pi r \\, dr = \\frac{\\pi g a^4}{8\\eta}\n$$\n\nthis is the **hagen-poiseuille law**: flow rate scales with the *fourth power* of the pipe radius. double the pipe diameter and you get 16 times the flow. this is why your arteries care so much about even a small amount of plaque buildup \u2014 a 10% reduction in radius cuts flow by over 34%.\n\nthe reynolds number for pipe flow is re $= ud/\\nu$, where $u$ is the mean velocity and $d = 2a$ is the diameter. for re $\\lesssim 2300$, the flow is laminar and the poiseuille solution holds. above that, the flow transitions to turbulence and everything gets much more complicated.\n\n## what we just learned\n\nfor simple geometries with steady, unidirectional flow, the navier-stokes equation yields exact analytical solutions. channel flow and pipe flow both produce parabolic velocity profiles. the hagen-poiseuille law reveals the dramatic fourth-power dependence of flow rate on pipe radius. non-newtonian fluids modify the profile shape according to their power-law exponent.\n\n## what's next\n\nwe've seen flows driven by pressure and gravity in confined geometries. the next chapter takes us to the surface: gravity waves on water, shallow-water equations, and the physics of ocean waves.\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "contapprox",
      "lessonTitle": "Continuum Approximation",
      "x": 0.6895467042922974,
      "y": 0.4364204406738281,
      "searchText": "continuum approximation\n# continuum approximation\n\n## the big idea\n\nhere's the deal: everything is made of atoms. water, steel, glaciers, cheese \u2014 it's all discrete little particles bouncing around. but if you try to track every atom in a glass of water (about $10^{25}$ of them), you'll be dead before your computer finishes the first timestep.\n\nso we do something audacious. we *ignore the atoms*. we pretend the material is perfectly smooth and continuous, like a mathematical function that you can differentiate everywhere. this is the **continuum approximation**, and it's the foundation of everything in this course.\n\nit's the same trick you use every day without thinking about it. when you say \"it's raining,\" you're ignoring individual raindrops. when you look at a photograph, you're ignoring individual pixels. when you check the temperature outside, you're ignoring the fact that some air molecules are screaming along at 600 m/s while others are barely moving. you're *averaging*, and that averaging is the continuum approximation.\n\nbut when does this trick actually work? and when does it break down? that's what this chapter is about.\n\nas benny lautrup puts it: \"science originates from curiosity and bad eyesight.\" the continuum approximation is the formal version of *not looking too closely*.\n\n## density fluctuations \u2014 when is smooth smooth enough?\n\nlet's start with something concrete. you have a box of gas. the density is:\n$$\n\\rho = \\frac{n m}{v}\n$$\nwhere $n$ is the number of molecules, $v$ is the volume, and $m$ is the mass of each molecule.\n\nnow here's the question you should be asking: *if i measure the density in a tiny box versus a slightly different tiny box right next to it, will i get the same answer?*\n\nnot exactly, no. there will be fluctuations \u2014 some boxes have a few more molecules, some have a few less. from basic statistics, the relative fluctuation goes like:\n$$\n\\frac{\\delta \\rho}{\\rho} = \\frac{\\delta n}{n} = \\frac{1}{\\sqrt{n}}\n$$\n\nso if you want a relative precision of $\\epsilon = 10^{-3}$ (one part in a thousand), you need at least $n > \\epsilon^{-2} = 10^6$ molecules in your box. those molecules occupy a certain volume, and the side length of the smallest box that gives you that precision is:\n$$\nl_{\\text{micro}} = \\epsilon^{-2/3} l_{\\text{mol}}\n$$\nwhere $l_{\\text{mol}}$ is the typical spacing between molecules:\n$$\nl_{\\text{mol}} = \\left( \\frac{v}{n} \\right)^{1/3} = \\left( \\frac{m_{\\text{mol}}}{\\rho n_a} \\right)^{1/3}\n$$\n\nfor air at sea level, $l_{\\text{mol}} \\approx 3 \\times 10^{-9}$ m. so $l_{\\text{micro}} \\approx 3 \\times 10^{-7}$ m \u2014 about 300 nanometers. anything bigger than that, and the continuum approximation gives you density to better than 0.1%. that's *tiny*. the continuum approximation works spectacularly well for everyday situations.\n\n## macroscopic smoothness \u2014 the slow-change rule\n\nhaving enough molecules in each box is necessary but not sufficient. you also need the density to change *gradually* from one box to the next. if the density jumps wildly between neighboring cells, \"smooth\" is a lie and your derivatives are meaningless.\n\nwe require that the relative change in density between adjacent cells is also less than $\\epsilon$:\n$$\n\\left| \\frac{\\partial \\rho}{\\partial x} \\right| < \\frac{\\epsilon \\, \\rho}{l_{\\text{micro}}}\n$$\n\nthis defines a macroscopic length scale:\n$$\nl_{\\text{macro}} = \\epsilon^{-1} l_{\\text{micro}}\n$$\n\nif your physical situation varies on length scales larger than $l_{\\text{macro}}$, the continuum approximation is valid. for air with $\\epsilon = 10^{-3}$, that's about $l_{\\text{macro}} \\approx 0.3$ mm. still tiny.\n\nbut here's the catch: at *interfaces* between different materials (the surface of a water droplet, the edge of a steel beam), properties change over distances comparable to $l_{\\text{mol}}$ \u2014 far too sharp for the continuum approximation. so we represent these as **surface discontinuities** and handle them separately with boundary conditions.\n\n## velocity fluctuations \u2014 the thermal jitter problem\n\nfor fluids, there's another wrinkle. molecules don't just sit still \u2014 they're buzzing around with thermal energy. the root-mean-square molecular speed is:\n$$\nv_{\\text{mol}} = \\sqrt{\\frac{3 r t}{m_{\\text{mol}}}}\n$$\n\nfor air at room temperature, that's about 500 m/s. if you're measuring the *bulk velocity* of a gentle breeze (say 5 m/s), the thermal jitter is 100 times larger than the signal you're trying to measure.\n\nto keep the velocity fluctuations below $\\epsilon$, you need a bigger box:\n$$\nl_{\\text{micro}}^* = \\left( \\frac{v_{\\text{mol}}}{v} \\right)^{2/3} l_{\\text{micro}}\n$$\n\nfor a 5 m/s breeze in air, $l_{\\text{micro}}^* \\approx 100 \\, l_{\\text{micro}}$. still small enough for the continuum approximation to hold in most practical situations \u2014 but now you see that slow-moving flows need *bigger* averaging volumes than fast ones.\n\n## mean free path \u2014 the drunk crowd at closing time\n\nthere's one more way to think about when the continuum approximation works, and it's perhaps the most vivid.\n\npicture ten thousand people leaving a bar at closing time. they're stumbling around, bouncing off each other, changing direction with every collision. the average distance someone travels before bumping into the next person is the **mean free path** $\\ell$.\n\nfor air at sea level, the mean free path is about $\\ell \\approx 68$ nm \u2014 molecules travel less than a ten-thousandth of a millimeter before colliding. if your measuring stick (the length scale of your problem) is much longer than the mean free path, you can safely average over many collisions and treat the material as continuous.\n\nthe rule of thumb: the continuum approximation holds when\n$$\n\\text{kn} = \\frac{\\ell}{l} \\ll 1\n$$\n\nwhere $l$ is the characteristic length of your problem and kn is the **knudsen number**. when kn $\\ll 1$, molecules collide so frequently within your length scale that they've thoroughly \"mixed\" their information, and the continuum picture is reliable.\n\nwhen kn $\\sim 1$"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "coursequote",
      "lessonTitle": "Course Overview",
      "x": 0.5797865986824036,
      "y": 0.3544272184371948,
      "searchText": "course overview\n# course overview\n\n\n\n#course quotes\nthe following are quotes from the coursed deemed worthy to save.\n\n*if the berg is full of water or if its full of iceberg, it doesnt matter! \njust imagine an iceberg made of water, a so called waterberg.*\n\n*for a concept, the stress deviator doesn't exactly live up to its name*\n\n*with that in mind, consider a one cubic meter of gouda cheese*"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "creepingflow",
      "lessonTitle": "Creeping Flow",
      "x": 0.7943198680877686,
      "y": 0.4324425458908081,
      "searchText": "creeping flow\n# creeping flow\n\n## when the fluid forgets its own history\n\nimagine swimming through honey. you push forward, but the moment you stop pushing, you stop moving. there's no gliding, no coasting, no inertia to speak of. the honey is so viscous that it instantly dampens any momentum. the fluid *forgets its own history* \u2014 it has no memory of what it was doing a moment ago.\n\nthis is **creeping flow** (also called **stokes flow**): flow at very low reynolds numbers, where viscosity completely dominates over inertia. it describes heavy oils, biological flows at the cellular scale, magma oozing through rock, and \u2014 importantly for us \u2014 glaciers.\n\n## the stokes equations \u2014 navier-stokes without the hard part\n\nwhen re $\\ll 1$, the advective term $(\\mathbf{v} \\cdot \\nabla)\\mathbf{v}$ in the navier-stokes equation becomes negligible. for steady flow ($\\partial \\mathbf{v}/\\partial t = 0$), the equations simplify dramatically:\n$$\n\\nabla p = \\eta \\nabla^2 \\mathbf{v}, \\qquad \\nabla \\cdot \\mathbf{v} = 0\n$$\n\nthis is the **stokes equation**: pressure gradient balances viscous forces. it's *linear* \u2014 which means superposition works, solutions are unique, and the mathematical machinery is much more tractable than for the full navier-stokes equation.\n\nthe linearity has a strange consequence: creeping flow is **time-reversible**. if you reverse all the forces, the flow runs backwards through exactly the same states. this is why microorganisms can't swim by reciprocal motions (the \"scallop theorem\") \u2014 a scallop opening and closing its shell would go nowhere at re $\\ll 1$.\n\n## drag and lift \u2014 how the fluid pushes back\n\nplace a body in a creeping flow. the fluid exerts a force on the body through the no-slip boundary condition at its surface:\n$$\n\\mathbf{r} = \\oint_s \\sigma \\cdot d\\mathbf{s} = \\mathbf{d} + \\mathbf{l}\n$$\n\nthis reaction force splits into:\n- **drag** $\\mathbf{d}$: the component in the direction of the flow \u2014 it resists the body's motion through the fluid.\n- **lift** $\\mathbf{l}$: the component perpendicular to the flow.\n- there can also be a **torque** that makes the body spin.\n\nthe drag itself has two contributions: **viscous drag** (shear stresses on the surface \u2014 the fluid \"rubbing\" against the body) and **pressure drag** (the pressure difference between the front and back of the body \u2014 the \"suction\" effect).\n\n## the sphere in stokes flow \u2014 a classic result\n\nfor a sphere of radius $a$ moving at speed $u$ through a creeping flow, the drag is:\n$$\nd = 6\\pi \\eta a u\n$$\n\nthis is **stokes' drag law**, and it's one of the most useful results in fluid mechanics. it tells you:\n- drag is proportional to velocity (not velocity squared, as in high-re turbulent flow).\n- drag is proportional to radius (not radius squared).\n- drag is proportional to viscosity.\n\nstokes' law is how we measure the viscosity of fluids (drop a known sphere and time its fall), estimate the settling rate of sediment in water, and understand why fog droplets hang in the air so much longer than raindrops.\n\nin creeping flow, the isobars (lines of constant pressure) stretch far out into the flow \u2014 much further than in nearly ideal (high-re) flow. this is because viscous effects are felt over long distances when inertia is absent.\n\n## non-newtonian creeping flow\n\nfor non-newtonian fluids in creeping flow, we return to cauchy's equation and use a power-law constitutive relation instead of the linear newtonian one. the effective viscosity depends on the strain rate:\n$$\n\\eta_{\\text{eff}} = k \\dot{\\gamma}^{n-1}\n$$\n\nfor glaciers, $n \\approx 3$ (glen's flow law), making ice a strongly shear-thinning material at glacier scales \u2014 it flows more easily where the shear rates are high, which is near the base and the valley walls.\n\n## what we just learned\n\ncreeping flow occurs when viscosity overwhelms inertia (re $\\ll 1$). the stokes equation is linear, time-reversible, and analytically tractable. stokes' drag law gives the force on a sphere in creeping flow. non-newtonian creeping flows, governed by power-law rheology, describe glaciers and other geophysical systems.\n\n## what's next\n\nto solve the stokes equation for realistic geometries (like gladys the glacier flowing through an irregular valley), we need to reformulate it in **weak form** \u2014 a step that prepares the equation for numerical solution by the finite element method.\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "dynamics",
      "lessonTitle": "The Heartbeat Equation \u2014 Dynamics of Continua",
      "x": 0.7310194373130798,
      "y": 0.41063758730888367,
      "searchText": "the heartbeat equation \u2014 dynamics of continua\n# the heartbeat equation \u2014 dynamics of continua\n\n## one equation to rule them all\n\nhere's something beautiful: whether you're modeling a glacier grinding through a valley, honey dripping off a spoon, or a steel beam vibrating after being struck \u2014 the fundamental equation is *the same*. it's the cauchy momentum equation, and it's the **heartbeat** of continuum mechanics.\n\neverything we've done so far \u2014 stress tensors, strain tensors, hooke's law \u2014 was building toward this. now we put it all together.\n\n## the ingredients \u2014 mass, momentum, and forces\n\nbefore we get to the big equation, let's make sure we know what we're tracking. for a chunk of material occupying volume $v$:\n\n**mass** \u2014 how much stuff is there:\n$$\nm = \\int_v \\rho \\, dv\n$$\n\n**momentum** \u2014 how much \"oomph\" does it carry:\n$$\n\\mathbf{p} = \\int_v \\rho \\, \\mathbf{v} \\, dv\n$$\n\n**angular momentum** \u2014 how much is it spinning:\n$$\n\\mathbf{l} = \\int_v \\mathbf{x} \\times \\rho \\, \\mathbf{v} \\, dv\n$$\n\n**kinetic energy** \u2014 how much energy is in the motion:\n$$\nk = \\int_v \\frac{1}{2} \\rho \\, v^2 \\, dv\n$$\n\n## conservation of mass \u2014 stuff doesn't disappear\n\nthe simplest conservation law: the mass in a volume $v$ can only change if stuff flows in or out through the surface $s$:\n$$\n\\frac{d}{dt} \\int_v \\rho \\, dv = -\\oint_s \\rho \\, \\mathbf{v} \\cdot \\hat{n} \\, da\n$$\n\nusing the divergence theorem to convert the surface integral into a volume integral:\n$$\n\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho \\, \\mathbf{v}) = 0\n$$\n\nthis is the **continuity equation**. it says: the rate at which density changes at a point equals the rate at which mass flows away from that point. nothing more, nothing less.\n\n## the material derivative \u2014 riding the flow\n\nhere's a subtlety that trips up everyone the first time. there are two ways to watch a flowing river:\n\n- **stand on the bank** (eulerian view): you watch the water flow past you. at your fixed location, the velocity changes over time as different parcels of water arrive.\n- **jump in a boat** (lagrangian view): you ride along with the water. the velocity you experience changes because you're moving to new locations.\n\nthe **material derivative** $d/dt$ captures the boat-rider's perspective:\n$$\n\\frac{dq}{dt} = \\frac{\\partial q}{\\partial t} + (\\mathbf{v} \\cdot \\nabla) q\n$$\n\nthe first term is the *local* change (what happens at your fixed point). the second term is the *advective* change (what changes because you moved to a new location). together, they give the total rate of change *experienced by a moving parcel of material*.\n\nthink of it this way: imagine you're in a hot air balloon drifting east. the temperature at your location changes for two reasons: (1) the air around you might be heating up (local change), and (2) you're drifting into a region that was already warmer (advective change).\n\nin the lagrangian picture, conservation of mass becomes elegantly simple:\n$$\n\\frac{dm}{dt} = 0\n$$\n\nand the material derivative of density:\n$$\n\\frac{d\\rho}{dt} = -\\rho (\\nabla \\cdot \\mathbf{v})\n$$\n\nfor an incompressible material ($\\nabla \\cdot \\mathbf{v} = 0$), the density of each parcel doesn't change as it moves \u2014 which makes sense, since incompressibility means volumes don't change.\n\n## transport of any quantity \u2014 the general recipe\n\nthe material derivative works for *any* quantity, not just density. if $q$ is some specific (per-unit-mass) quantity like temperature or chemical concentration, then the total amount in a volume is $q = \\int_v \\rho \\, q \\, dv$, and:\n$$\n\\frac{dq}{dt} = \\int_v \\rho \\frac{dq}{dt} \\, dv\n$$\n\nthis is the general transport equation. imagine a box full of some quantity $q$, drifting with the flow. what enters and leaves through the boundaries changes $q$:\n$$\n\\frac{\\partial (\\rho q)}{\\partial t} + \\nabla \\cdot (\\rho q \\, \\mathbf{v}) = \\rho \\frac{dq}{dt}\n$$\n\nthis works because the mass conservation terms cancel, leaving only the \"ride along\" derivative.\n\n## cauchy's equation \u2014 the heartbeat\n\nnow we're ready. newton's second law says momentum changes equal forces. for a continuum:\n$$\n\\rho \\frac{d\\mathbf{v}}{dt} = \\mathbf{f} + \\nabla \\cdot \\sigma\n$$\n\nthat's it. that's the heartbeat equation. let's unpack it:\n\n- **left side**: mass per unit volume times acceleration (in the material derivative sense \u2014 following the flow).\n- **$\\mathbf{f}$**: body forces, like gravity ($\\rho \\, \\mathbf{g}$).\n- **$\\nabla \\cdot \\sigma$**: the divergence of the stress tensor \u2014 the net force per unit volume from all the internal stresses acting on the material.\n\nthis is **cauchy's equation**, and it's universal. it doesn't care whether you're dealing with a solid, a liquid, or anything in between. the difference between solids and fluids comes in *later*, when you specify what $\\sigma$ looks like:\n\n- **for an elastic solid**: $\\sigma = \\lambda \\, \\text{tr}(\\varepsilon) \\, \\mathbf{i} + 2\\mu \\, \\varepsilon$ \u2192 you get the **navier-cauchy equation**.\n- **for a viscous fluid**: $\\sigma = -p\\,\\mathbf{i} + 2\\eta \\, \\dot{\\varepsilon}$ \u2192 you get the **navier-stokes equation**.\n\nsame heartbeat. different constitutive law. that's the deep unity of continuum mechanics.\n\n## what we just learned\n\nconservation of mass gives the continuity equation. the material derivative lets us track quantities while riding along with the flow. cauchy's equation is newton's second law for continua \u2014 and it's the same equation for both solids and fluids. the only thing that changes is the constitutive relation between stress and strain.\n\n## what's next\n\nnow we know the general equation of motion. let's see what happens when we specialize to fluids \u2014 starting with fluids that aren't moving at all. pressure, buoyancy, floating icebergs: fluids at rest.\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "elasticity",
      "lessonTitle": "Elasticity",
      "x": 0.7161388993263245,
      "y": 0.3037623465061188,
      "searchText": "elasticity\n# elasticity\n\n## the cheese test\n\nfrom basic mechanics, you know hooke's law: $f = -kx$. a spring, one dimension, done. but the world isn't one-dimensional. when you squeeze a block of gouda cheese, it doesn't just compress downward \u2014 it *bulges outward*. when you stretch a rubber band lengthwise, it gets thinner in the middle. forces and deformations couple across dimensions, and the simple spring constant $k$ isn't enough anymore.\n\nwhy cheese? because it's *perfect* for building intuition. it's solid enough to hold its shape (unlike honey), but soft enough that you can actually *see* the deformation with your eyes (unlike steel). a one cubic meter block of gouda is our mental laboratory for this chapter. put a weight on top and watch what happens.\n\n## young's modulus \u2014 how stiff is your cheese?\n\nyoung's modulus $e$ answers a simple question: *how hard do i have to pull (per unit area) to stretch the material by a certain fraction of its length?*\n\n$$\ne = \\frac{\\sigma_{xx}}{\\varepsilon_{xx}} = \\frac{f/a}{\\delta l / l} = k \\frac{l}{a}\n$$\n\nit's the spring constant, but normalized by geometry \u2014 force per unit area divided by relative deformation. this lets you compare materials regardless of their shape or size:\n\n| material | young's modulus |\n|----------|----------------|\n| diamond  | $\\sim 1000$ gpa |\n| steel    | $\\sim 200$ gpa  |\n| bone     | $\\sim 15$ gpa   |\n| wood     | $\\sim 10$ gpa   |\n| gouda    | $\\sim 0.3$ gpa  |\n| rubber   | $\\sim 0.01$ gpa |\n\n## poisson's ratio \u2014 the sideways squeeze\n\nnow put that weight on the cheese. it compresses vertically, but it also *bulges outward* horizontally. poisson's ratio $\\nu$ measures this coupling:\n\n$$\n\\nu = -\\frac{\\varepsilon_{\\text{transverse}}}{\\varepsilon_{\\text{longitudinal}}}\n$$\n\nfor most solid materials, $\\nu$ is between 0.2 and 0.5. a poisson's ratio of 0.5 means the material is **incompressible** \u2014 it changes shape but not volume (rubber is close to this). a poisson's ratio of 0 means the dimensions are decoupled \u2014 squeezing vertically has no effect on horizontal dimensions (cork is close to this, which is why it works so well as a bottle stopper).\n\n## generalized hooke's law \u2014 the full 3d picture\n\nin the general case, hooke's law becomes a tensor equation:\n$$\n\\sigma_{ij} = c_{ijkl} \\, \\varepsilon_{kl}\n$$\n\nwhere $c_{ijkl}$ is a rank-4 **stiffness tensor** that relates all components of stress to all components of strain. for a general anisotropic material, this has 21 independent components. for an **isotropic** material (same properties in all directions), it collapses to just two parameters \u2014 the lame coefficients $\\lambda$ and $\\mu$:\n$$\n\\sigma_{ij} = \\lambda \\, \\varepsilon_{kk} \\, \\delta_{ij} + 2\\mu \\, \\varepsilon_{ij}\n$$\n\nthe lame coefficients relate to young's modulus and poisson's ratio by:\n$$\n\\mu = \\frac{e}{2(1+\\nu)}, \\qquad \\lambda = \\frac{e\\nu}{(1+\\nu)(1-2\\nu)}\n$$\n\n## what happens when you stretch too far?\n\nhooke's law is a *linear* relationship \u2014 double the stress, double the strain. but you know from experience that this can't be the whole story. stretch the rubber band far enough and it snaps. squeeze the cheese hard enough and it crumbles.\n\nreal materials have a **yield point** where the linear relationship breaks down. beyond it, the material deforms *permanently* \u2014 it doesn't spring back. this is **plastic deformation**, and it's where materials science gets interesting. we won't dive deep into plasticity in this course, but it's important to know that linear elasticity has limits. the von mises criterion from the previous chapter tells you *when* those limits are reached.\n\n## work and energy in a deformed material\n\nwhen you deform a material, you do work on it. that work gets stored as elastic potential energy (like compressing a spring). the work per unit volume is:\n$$\nw = \\sigma : \\varepsilon = \\sum_{ij} \\sigma_{ij} \\, \\varepsilon_{ij}\n$$\n\nthe \"$:$\" operator is the **double contraction** \u2014 you multiply corresponding elements and sum them all up. the units work out to $\\text{pa} = \\text{j/m}^3$: energy per unit volume, exactly what you'd expect for stored elastic energy.\n\n## linear elastostatics \u2014 when nothing moves\n\nif a material is in static equilibrium (no acceleration), the forces must balance everywhere:\n$$\n-\\nabla \\cdot \\sigma = \\mathbf{f}\n$$\n\nwhere $\\mathbf{f}$ is the body force density (like gravity). combined with hooke's law ($\\sigma = \\lambda \\, \\text{tr}(\\varepsilon) \\, \\mathbf{i} + 2\\mu \\, \\varepsilon$) and the strain-displacement relation, this gives the **navier-cauchy equation** for elastostatics. it's analytically solvable for simple geometries, and it's what we'll solve numerically with fem for everything else.\n\n## beam profiles and slender rods\n\nmany engineering structures \u2014 bridges, buildings, bones \u2014 can be modeled as slender beams. when a beam bends under load, the top is compressed and the bottom is stretched (or vice versa). there's a **neutral axis** in the middle where the strain is zero.\n\nthe classic euler-bernoulli beam theory gives the deflection $w(x)$ of a beam under load:\n$$\nei \\frac{d^4 w}{dx^4} = q(x)\n$$\n\nwhere $i$ is the second moment of area (a geometric property of the cross-section) and $q(x)$ is the distributed load. this single equation governs how bridges sag, diving boards flex, and tree branches bend in the wind.\n\n## vibrations and sound \u2014 when elasticity meets dynamics\n\npush a material and let go. if it's elastic, it springs back \u2014 and *overshoots*. then it springs back again. this oscillation propagates through the material as a **wave**.\n\nthere are two kinds of elastic waves:\n\n- **p-waves** (pressure/longitudinal): the material compresses and expands along the direction of propagation, like a slinky being pushed and pulled. the displacement is parallel to the wave vector $\\vec{k}$.\n- **s-waves** (shear/transverse): the material shears perpendicular to the direction of propagation, like a rope being wiggled. the displacement is perpendicular to $\\vec{k}$, with tw"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "examdisp",
      "lessonTitle": "The Grand Finale \u2014 A Detective Story in Seven Acts",
      "x": 0.722987949848175,
      "y": 0.36790749430656433,
      "searchText": "the grand finale \u2014 a detective story in seven acts\n# the grand finale \u2014 a detective story in seven acts\n\n## how to use this chapter\n\neverything below is a compressed retelling of the entire course, organized around real problems. think of each section as a detective story: there's a physical mystery, and the tools we've built are the detective's kit. if any section feels unfamiliar, go back to the corresponding chapter and re-read it with fresh eyes.\n\n---\n\n## act 1: hydrostatics and buoyancy \u2014 why the titanic sank (and why the iceberg didn't)\n\n**the mystery:** an iceberg floats with about 90% of its volume below the waterline. why exactly 90%? and what forces keep it stable?\n\n**the toolkit:**\n\n*continuum approximation.* before anything else, we check: can we treat ice and water as continua? the molecular separation length:\n$$\nl_{\\text{mol}} = \\left(\\frac{m_{\\text{mol}}}{\\rho n_a}\\right)^{1/3}\n$$\n\nfor water: $l_{\\text{mol}} \\approx 3 \\times 10^{-10}$ m. our iceberg is $\\sim 100$ m across. the knudsen number is vanishingly small \u2014 the continuum approximation is spectacularly valid.\n\n*pressure and the hydrostatic equation:*\n$$\n\\nabla p = \\rho \\, \\mathbf{g}, \\qquad \\mathbf{f}_{\\text{pressure}} = -\\oint_s p \\, d\\mathbf{s}\n$$\n\n*archimedes' principle:*\n$$\n\\mathbf{f} = \\mathbf{f}_g + \\mathbf{f}_b = \\int_v (\\rho_{\\text{ice}} - \\rho_{\\text{water}}) \\, \\mathbf{g} \\, dv\n$$\n\nfor equilibrium, the mass of displaced water equals the mass of the iceberg. since $\\rho_{\\text{ice}} \\approx 917$ kg/m$^3$ and $\\rho_{\\text{water}} \\approx 1025$ kg/m$^3$, the fraction below the surface is $\\rho_{\\text{ice}}/\\rho_{\\text{water}} \\approx 0.895$ \u2014 about 90%. as the course quote says: *\"just imagine an iceberg made of water, a so-called waterberg.\"*\n\n*stability:* the iceberg stays upright because its center of buoyancy $\\mathbf{x}_b$ is above its center of gravity $\\mathbf{x}_g$, creating a restoring moment when it tilts.\n\n**key equations:** continuum approximation scales ($l_{\\text{mol}}$, $l_{\\text{micro}}$, $l_{\\text{macro}}$), hydrostatic equation, archimedes' principle, moment balance for floating body stability.\n\n---\n\n## act 2: stress and strain \u2014 the glacier's crevasses\n\n**the mystery:** glaciers develop deep cracks (crevasses) at their surfaces. why at the surface and not deeper? and why do they stop at a certain depth?\n\n**the toolkit:**\n\n*the cauchy stress tensor* describes forces throughout the ice. near the surface, the ice is being pulled apart by the glacier's flow \u2014 tensile stress. deeper down, the weight of overlying ice creates compressive stress that overwhelms the tension.\n\n*the strain tensor* captures how the ice deforms. the eigenvectors of strain point in the directions of maximum stretching \u2014 which is where crevasses open.\n\n*the stress deviator* $s = \\sigma - p\\,\\mathbf{i}$ strips away the confining pressure and reveals the \"shape-changing\" stress. crevasses form where the deviatoric stress exceeds the tensile strength of ice.\n\n*the von mises criterion:* $\\sigma_{\\text{vm}} = \\sqrt{3j_2}$ predicts the onset of failure independent of coordinate choice.\n\n**key equations:** cauchy stress tensor, principal stresses, stress deviator, invariants, von mises criterion.\n\n---\n\n## act 3: elasticity and vibrations \u2014 the earthquake's double punch\n\n**the mystery:** during an earthquake, you feel two distinct jolts. the first is a sharp, compressive \"thump.\" a few seconds later comes a rolling, sideways shake. why two?\n\n**the toolkit:**\n\n*hooke's law in 3d:* $\\sigma_{ij} = \\lambda\\,\\varepsilon_{kk}\\,\\delta_{ij} + 2\\mu\\,\\varepsilon_{ij}$\n\n*elastic wave speeds:*\n$$\nc_p = \\sqrt{\\frac{\\lambda + 2\\mu}{\\rho}}, \\qquad c_s = \\sqrt{\\frac{\\mu}{\\rho}}\n$$\n\np-waves (primary/pressure waves) are compressive and travel faster. s-waves (secondary/shear waves) are transverse and travel slower. for typical rock with $\\nu \\approx 1/3$: $c_p = 2\\,c_s$.\n\nthe first jolt is the p-wave arriving; the second is the s-wave. the time delay between them is proportional to the distance to the earthquake. with three seismograph stations, you can triangulate the epicenter.\n\n*the block of gouda:* cheese has a young's modulus of about 0.3 gpa \u2014 stiff enough to demonstrate elastic wave propagation but soft enough to deform visibly. it's the perfect teaching material.\n\n**key equations:** generalized hooke's law, lame coefficients, p- and s-wave speeds, relationship between $e$, $\\nu$, and wave speed.\n\n---\n\n## act 4: the cauchy equation \u2014 one heartbeat, many rhythms\n\n**the mystery:** why do the equations for elastic solids and viscous fluids look so similar?\n\n**the toolkit:**\n\n*cauchy's equation:*\n$$\n\\rho \\frac{d\\mathbf{v}}{dt} = \\mathbf{f} + \\nabla \\cdot \\sigma\n$$\n\nthis is the *same equation* for every continuous material. the constitutive law (the relationship between $\\sigma$ and deformation) is what distinguishes them:\n\n| material | constitutive law | result |\n|----------|-----------------|--------|\n| elastic solid | $\\sigma = \\lambda\\,\\text{tr}(\\varepsilon)\\,\\mathbf{i} + 2\\mu\\,\\varepsilon$ | navier-cauchy eq. |\n| viscous fluid | $\\sigma = -p\\,\\mathbf{i} + 2\\eta\\,\\dot{\\varepsilon}$ | navier-stokes eq. |\n| power-law fluid | $\\sigma = -p\\,\\mathbf{i} + 2k\\dot{\\gamma}^{n-1}\\dot{\\varepsilon}$ | glen's flow law (glaciers) |\n\nthe material derivative $d/dt = \\partial/\\partial t + (\\mathbf{v} \\cdot \\nabla)$ connects the eulerian and lagrangian perspectives.\n\n**key equations:** material derivative, cauchy's equation, continuity equation, constitutive relations.\n\n---\n\n## act 5: (nearly) ideal flows \u2014 bernoulli's garden hose\n\n**the mystery:** pinch a garden hose and the water jets out faster. why? and why does an airplane stay up?\n\n**the toolkit:**\n\n*euler's equations* for inviscid flow. *bernoulli's theorem:*\n$$\nh = \\frac{1}{2}v^2 + gz + \\frac{p}{\\rho} = \\text{const along a streamline}\n$$\n\npinch the hose \u2192 area decreases \u2192 velocity increases (continuity) \u2192 pressure decreases (bernoulli). this pressure difference is also what generates lift on an airplan"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "fem",
      "lessonTitle": "Finite Element Method",
      "x": 0.6498655676841736,
      "y": 0.31560805439949036,
      "searchText": "finite element method\n# finite element method\n\n## the rubber sheet, revisited \u2014 in triangles\n\nremember the rubber sheet from the tensor chapter? imagine you need to calculate exactly how it deforms under some complicated load. the shape is irregular, the forces are messy, and there's no analytical solution.\n\nhere's the idea: instead of solving the whole sheet at once, **cut it into tiny triangles**. on each triangle, assume the solution is simple \u2014 maybe linear or quadratic. then stitch all the triangles together, demanding that the solutions agree at the shared edges. that's the **finite element method** (fem), and it's how we actually compute the deformation of real glaciers, airplane wings, bridges, and biological tissues.\n\n## the big picture \u2014 approximate the solution, not the equation\n\nfem is fundamentally different from finite differences. in finite differences, you approximate the *derivatives* in the equation (replacing $\\partial^2 u / \\partial x^2$ with $(u_{i+1} - 2u_i + u_{i-1})/h^2$). in fem, you approximate the *solution itself* as a combination of simple functions, then find the best combination.\n\nconsider a differential equation $a(u) = f$. we approximate the solution as:\n$$\nu^n(x) = \\sum_{i=1}^n a_i \\, \\phi_i(x)\n$$\n\nwhere the $\\phi_i$ are **basis functions** (simple, known functions \u2014 usually polynomials that are nonzero only near a single mesh node) and the $a_i$ are unknown coefficients we need to find.\n\nsubstituting gives a **residual**:\n$$\nr^n(x) = a(u^n) - f\n$$\n\nif $u^n$ were the exact solution, the residual would be zero everywhere. it won't be, so our job is to choose the $a_i$ to make the residual as small as possible.\n\n## how small? \u2014 the method of weighted residuals\n\ndifferent choices for \"as small as possible\" give different methods:\n\n### least squares method\nminimize the total squared residual:\n$$\n\\pi = \\int_0^l (r^n)^2 \\, dx\n$$\n\nsetting $\\partial \\pi / \\partial a_i = 0$ gives $n$ equations for $n$ unknowns. this is clean and intuitive \u2014 you're literally minimizing the error in an $l^2$ sense.\n\n### collocation method\nforce the residual to be exactly zero at $n$ specific points:\n$$\nr^n(x_i) = 0, \\qquad i = 1, \\ldots, n\n$$\n\nthis is useful when you care about accuracy in specific locations. imagine working at a fastener company and getting complaints that screws keep breaking at the head. you don't need the stress field everywhere \u2014 you need it *right below the screwhead*. collocation lets you concentrate accuracy where it matters.\n\n### galerkin's method \u2014 the winner\nthe most widely used approach: force the residual to be **orthogonal** to every basis function:\n$$\n\\int_0^l r^n(x) \\, \\phi_i(x) \\, dx = 0, \\qquad i = 1, \\ldots, n\n$$\n\nthe intuition: the error $e = u - u^n$ and the residual $r$ are related (if one is zero, so is the other). making $r$ orthogonal to the approximation space is the closest we can get to minimizing the error without knowing the true solution.\n\nthe galerkin recipe:\n1. **compute the residual**: $r^n = a(u^n) - f$\n2. **force orthogonality**: $\\int r^n \\, \\phi_i \\, dx = 0$ for each $i$\n3. **solve** the resulting system of equations for the $a_i$\n\n## the \"finite element\" part \u2014 choosing the basis functions\n\nthe general galerkin framework doesn't tell you *how* to choose the $\\phi_i$. this is where fem adds its magic:\n\n1. **mesh the domain** \u2014 divide it into small elements (triangles in 2d, tetrahedra in 3d).\n2. **define local basis functions** \u2014 on each element, the basis functions are simple polynomials (linear, quadratic, etc.). each $\\phi_i$ is nonzero only in the elements surrounding node $i$, and zero everywhere else. this is the \"finite\" in \"finite element\" \u2014 the basis functions have finite support.\n3. **assemble** \u2014 the integrals over the whole domain become sums of integrals over individual elements. each element contributes to a small block of the global matrix.\n\nthe result: a large but **sparse** linear system $\\mathbf{k}\\mathbf{a} = \\mathbf{f}$, where $\\mathbf{k}$ is the stiffness matrix and $\\mathbf{f}$ is the load vector. sparse means most entries are zero (because each basis function only overlaps with its neighbors), which makes the system efficient to solve even for millions of unknowns.\n\n## minimum potential energy \u2014 the variational perspective\n\nfor elastic problems, there's an elegant alternative viewpoint. the true displacement minimizes the **potential energy**:\n$$\nw(u) = \\frac{1}{2}\\int_\\omega \\varepsilon(u) : \\sigma(\\varepsilon(u)) \\, dv - \\int_\\omega \\mathbf{f} \\cdot u \\, dv\n$$\n\nany perturbation from the true solution increases $w$. this is a **variational problem**: instead of solving a differential equation, we minimize a functional. the two formulations are equivalent (for linear problems, the galerkin equations *are* the optimality conditions for the energy functional), but the variational perspective gives physical insight \u2014 nature chooses the configuration that minimizes energy.\n\nthe technique: instead of varying a scalar to minimize a function, we vary a *function* to minimize a *functional*. the calculus of variations gives us the tools, and the result is identical to the galerkin weak form.\n\n## interactive fem 1d bar demo\n\n[[simulation fem-1d-bar-sim]]\n\n## what we just learned\n\nthe finite element method approximates solutions to differential equations by dividing the domain into small elements with simple local basis functions. galerkin's method makes the residual orthogonal to the approximation space, producing a sparse linear system. for elastic problems, this is equivalent to minimizing potential energy. fem handles complex geometries, arbitrary boundary conditions, and heterogeneous materials \u2014 making it the workhorse of modern computational mechanics.\n\n## what's next\n\nwe have the theory. now let's get our hands dirty with actual computation. the next chapter introduces the python packages \u2014 especially fenics \u2014 that let you set up and solve fem problems in remarkably few lines of code.\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "fluids",
      "lessonTitle": "Fluids at Rest",
      "x": 0.7609919905662537,
      "y": 0.35998567938804626,
      "searchText": "fluids at rest\n# fluids at rest\n\n## what makes a fluid a fluid?\n\npick up a block of cheese and set it on the table. it holds its shape. now pour a glass of water on the table. it spreads everywhere.\n\nthat's the fundamental difference between solids and fluids. a solid resists being deformed \u2014 apply a shear stress and it pushes back with a restoring force, like a spring. a fluid *can't* resist shear. apply even the tiniest shear stress and it flows. it might flow slowly (honey) or quickly (water), but it flows.\n\nmore precisely: a **fluid** is a material with zero shear modulus. it deforms continuously under any applied shear stress. fluids resist compression (you can't easily squeeze water into a smaller volume), but they don't resist shearing.\n\nthis means fluids respond to stress in a very particular way:\n- **normal stress** (compression/tension) \u2192 yes, fluids push back through **pressure**.\n- **shear stress** \u2192 no resistance in equilibrium. any shear causes flow.\n\nat rest, the only stress a fluid can sustain is pressure \u2014 equal in all directions, like being squeezed uniformly from all sides. the stress tensor for a fluid at rest is:\n$$\n\\sigma_{ij} = -p \\, \\delta_{ij}\n$$\n\nthat's it: negative pressure (compression) equally on every face of every tiny cube.\n\n## pressure \u2014 the weight of everything above\n\nwhat *is* pressure? at a molecular level, it's the result of billions of molecules bombarding a surface. but in the continuum picture, we don't need molecules \u2014 we just need the fact that the fluid pushes back against compression.\n\nthe force on a small surface element $d\\mathbf{s}$ due to pressure is:\n$$\nd\\mathbf{f} = -p \\, d\\mathbf{s}\n$$\n\nthe minus sign means the force points *inward* \u2014 the fluid pushes against the surface, trying to expand. the total pressure force on a body submerged in fluid is:\n$$\n\\mathbf{f}_b = -\\oint_s p \\, d\\mathbf{s}\n$$\n\nin a fluid at rest in a gravitational field, the pressure increases with depth. you've felt this in a swimming pool \u2014 your ears pop as you dive deeper. the condition for static equilibrium is:\n$$\n\\nabla p = \\rho \\, \\mathbf{g}\n$$\n\nthis is the **hydrostatic equation**: pressure gradient equals body force density. for a constant-density fluid with gravity pointing down:\n$$\np(z) = p_0 + \\rho g (z_0 - z)\n$$\n\npressure increases linearly with depth. every 10 meters of water adds about 1 atmosphere of pressure.\n\n## equation of state \u2014 how pressure relates to density\n\nfor many applications, we need to know how pressure relates to other properties of the fluid, like density and temperature. this is the **equation of state**.\n\nfor an ideal gas: $p = \\rho r t / m_{\\text{mol}}$. for liquids, the relationship is more complex \u2014 the **bulk modulus** $k = -v \\, dp/dv$ tells you how much the pressure changes when you compress the fluid. water has a bulk modulus of about 2.2 gpa, which is why it's nearly incompressible under everyday conditions.\n\nfor advanced applications (like modeling phase transitions), the **van der waals equation** accounts for molecular interactions and finite molecular size \u2014 but for most of this course, we'll treat fluids as either ideal gases or incompressible liquids.\n\n## buoyancy \u2014 why icebergs float\n\ndrop an object into a fluid. gravity pulls it down. but the pressure of the fluid pushes up on its bottom surface more than it pushes down on its top surface (because pressure increases with depth). the result is an upward **buoyancy force**.\n\nthe gravitational force on the body:\n$$\n\\mathbf{f}_g = \\int_v \\rho_{\\text{body}} \\, \\mathbf{g} \\, dv\n$$\n\nthe buoyancy force from the surrounding fluid pressure:\n$$\n\\mathbf{f}_b = -\\oint_s p \\, d\\mathbf{s}\n$$\n\ncombining them gives **archimedes' principle**:\n$$\n\\mathbf{f} = \\mathbf{f}_g + \\mathbf{f}_b = \\int_v (\\rho_{\\text{body}} - \\rho_{\\text{fluid}}) \\, \\mathbf{g} \\, dv\n$$\n\n*the buoyant force equals the weight of the displaced fluid.* if the object is less dense than the fluid, it floats. if it's denser, it sinks.\n\nfor a uniform gravitational field:\n$$\n\\mathbf{f} = (m_{\\text{body}} - m_{\\text{displaced fluid}}) \\, \\mathbf{g}_0\n$$\n\nhere's a beautiful way to think about it: imagine replacing the object with an equal volume of fluid. that \"fluid object\" would be in perfect equilibrium \u2014 the buoyancy exactly balances its weight. now swap in the real object: if it's lighter, there's a net upward force; if heavier, a net downward force.\n\nas the course quote goes: *\"if the berg is full of water or if it's full of iceberg, it doesn't matter! just imagine an iceberg made of water, a so-called waterberg.\"*\n\n## stability \u2014 will it tip over?\n\nfloating isn't enough. a floating body must also be *stable* \u2014 if you nudge it, it should rock back to its original position, not capsize.\n\nbeyond the buoyant force balance, there must also be a balance of **moments**. the gravitational moment acts through the **center of gravity** $\\mathbf{x}_g$, and the buoyancy moment acts through the **center of buoyancy** $\\mathbf{x}_b$:\n\n$$\n\\mathbf{x}_g = \\frac{1}{m}\\int_v \\mathbf{x} \\, \\rho_{\\text{body}} \\, dv, \\qquad \\mathbf{x}_b = \\frac{1}{m_{\\text{displaced}}}\\int_v \\mathbf{x} \\, \\rho_{\\text{fluid}} \\, dv\n$$\n\nthe total moment is:\n$$\n\\mathbf{m}_{\\text{total}} = (\\mathbf{x}_g - \\mathbf{x}_b) \\times m \\, \\mathbf{g}_0\n$$\n\nfor stability, this moment must be *restoring*: tilting the body should create a moment that pushes it back upright. this is why ships have heavy keels (to lower $\\mathbf{x}_g$) and wide hulls (to raise $\\mathbf{x}_b$ when tilted).\n\n## what we just learned\n\na fluid at rest sustains only pressure \u2014 no shear. the hydrostatic equation relates pressure to depth. archimedes' principle tells us the buoyancy force, and the relative positions of the centers of gravity and buoyancy determine whether a floating body is stable.\n\n## what's next\n\nwe've studied fluids sitting still. now we set them in motion. what happens when you turn on a hose, stir a cup of tea, or watch the wind blow? the next chapter brings us to ideal flows, eu"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "fluidsinmotion",
      "lessonTitle": "Fluids in Motion",
      "x": 0.7621496915817261,
      "y": 0.4005424380302429,
      "searchText": "fluids in motion\n# fluids in motion\n\n## the garden hose experiment\n\nbefore a single equation, do this experiment in your head. you're holding a garden hose with water flowing steadily out the end. now pinch the hose partway shut with your thumb. what happens?\n\nthe water speeds up. a narrow, fast jet shoots out.\n\n*why?* because the same amount of water has to get through a smaller opening in the same amount of time. water isn't appearing or disappearing \u2014 it's being conserved. that simple observation \u2014 **what goes in must come out** \u2014 is all the continuity equation is saying. the rest is just making that idea precise.\n\n## ideal flows \u2014 the simplest starting point\n\nlet's start with the most idealized fluid imaginable: **incompressible** (you can't squeeze it), **inviscid** (no internal friction), and **irrotational** (no local spinning). this is an \"ideal flow.\" real fluids are never quite this perfect, but ideal flow gives us beautiful results that are surprisingly useful.\n\nfor an ideal fluid at rest, we had $\\sigma = -p\\,\\mathbf{i}$. this remains true in motion \u2014 the only stress is pressure:\n$$\n\\sigma = -p\\,\\mathbf{i} \\qquad \\rightarrow \\qquad \\nabla \\cdot \\sigma = -\\nabla p\n$$\n\nplugging into cauchy's equation, and using incompressibility ($\\nabla \\cdot \\mathbf{v} = 0$), we get the **euler equations**:\n$$\n\\nabla \\cdot \\mathbf{v} = 0, \\qquad \\frac{d\\mathbf{v}}{dt} = \\mathbf{g} - \\frac{\\nabla p}{\\rho}\n$$\n\nfour equations (one scalar continuity equation, three components of the momentum equation) for four unknowns ($v_x$, $v_y$, $v_z$, $p$). the system is closed.\n\n[[simulation elastic-wave]]\n\none striking consequence of incompressibility: taking the divergence of the momentum equation gives a **poisson equation** for pressure. this means pressure is *non-local* \u2014 change the pressure somewhere, and it instantly adjusts everywhere. that's because we've assumed the fluid is perfectly incompressible: pressure waves travel at infinite speed (in reality they travel at the speed of sound, which is very fast but not infinite).\n\n## bernoulli's theorem \u2014 conservation of energy along a streamline\n\nback to the garden hose. the water speeds up when you pinch the opening. but where does the extra kinetic energy come from? it comes from the pressure: the pressure drops where the velocity increases. this trade-off between pressure and velocity is **bernoulli's theorem**.\n\ndefine the **bernoulli function**:\n$$\nh = \\frac{1}{2}v^2 + \\phi + \\frac{p}{\\rho}\n$$\n\nwhere $\\phi$ is the gravitational potential (e.g., $gz$). along a streamline in steady flow:\n$$\n\\frac{dh}{dt} = 0\n$$\n\n$h$ is constant along a flowline. it's an energy conservation statement: kinetic energy + potential energy + pressure energy = constant. divide everything by $g$ and you get what engineers call the **total head** \u2014 a quantity measured in meters that you can literally see as a height.\n\nthe gradient of $h$ connects to vorticity:\n$$\n\\nabla h = \\mathbf{v} \\times (\\nabla \\times \\mathbf{v}) - \\frac{\\partial \\mathbf{v}}{\\partial t}\n$$\n\nin steady, irrotational flow, $\\nabla h = 0$ everywhere \u2014 meaning $h$ is constant not just along streamlines but throughout the entire flow. this is the strongest form of bernoulli's theorem.\n\n## vorticity \u2014 the local spin\n\nimagine dropping a tiny ice skater into a flowing river. at some points, the current is uniform and she glides straight. at other points, the flow is faster on one side than the other, and she starts to **spin**. the rate at which she spins is the **vorticity**:\n$$\n\\boldsymbol{\\omega} = \\nabla \\times \\mathbf{v}\n$$\n\nvorticity is the curl of the velocity field \u2014 it measures the local rotation rate. if the vorticity is zero everywhere, the flow is **irrotational** and things simplify enormously (we can use a velocity potential). if vorticity is nonzero, the flow has internal structure \u2014 think of the swirling eddies behind a rock in a stream.\n\ntaking the curl of euler's equation gives the **vorticity equation**:\n$$\n\\frac{\\partial \\boldsymbol{\\omega}}{\\partial t} = \\nabla \\times (\\mathbf{v} \\times \\boldsymbol{\\omega})\n$$\n\na key consequence: **if a flow starts irrotational, it stays irrotational** (for an ideal fluid). vorticity can't spontaneously appear in an inviscid flow \u2014 you need viscosity (friction) or boundaries to create it. this is **kelvin's circulation theorem**, and it explains why potential flow is such a useful approximation far from surfaces.\n\n## circulation and stokes' theorem\n\nthe **circulation** $\\gamma$ around a closed curve is the line integral of velocity:\n$$\n\\gamma = \\oint_c \\mathbf{v} \\cdot d\\mathbf{l}\n$$\n\nby stokes' theorem, this equals the flux of vorticity through any surface bounded by the curve:\n$$\n\\gamma = \\int_s \\boldsymbol{\\omega} \\cdot d\\mathbf{s}\n$$\n\ncirculation is the \"total spin\" enclosed by a loop. kelvin's theorem says that for an ideal fluid, the circulation around a material loop (one that moves with the fluid) is conserved. vorticity isn't created or destroyed \u2014 it's just transported and stretched by the flow.\n\n## what we just learned\n\nideal flows obey the euler equations. bernoulli's theorem provides energy conservation along streamlines. vorticity measures local rotation, and in ideal flows, an irrotational state is preserved. these are the building blocks for understanding everything from airplane wings to ocean currents.\n\n## what's next\n\nideal flows are elegant but incomplete \u2014 they can't explain why the water in your cup eventually stops spinning after you stir it. that's because real fluids have *viscosity*: internal friction that dissipates energy. the next chapter introduces viscous flow and the navier-stokes equation.\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "frontpagetext",
      "lessonTitle": "Welcome",
      "x": 0.6132580637931824,
      "y": 0.37161505222320557,
      "searchText": "welcome\n# welcome\n\n## the glacier on your doorstep\n\nright now, as you read this, a massive slab of ice the size of denmark is sliding into the ocean. inside it, every tiny piece of ice is pushing and pulling on its neighbors \u2014 squeezing here, stretching there, cracking where the stress gets too much. the ice doesn't know it's a glacier. it just follows the rules.\n\ntoday we start learning those rules.\n\ncontinuum mechanics is the physics of *stuff* \u2014 solids that bend and break, fluids that pour and swirl, and everything in between. it's the reason bridges hold up, rivers flow downhill, honey spirals off a spoon, and earthquakes rattle cities hundreds of kilometers from a fault line.\n\n## what you'll learn\n\nin this course, you'll become fluent in the mechanical behavior of continuous matter \u2014 from viscous fluids to elastic solids. we'll build up from first principles:\n\n- **how to describe deformation** \u2014 the stress and strain tensors, linear elasticity, and why materials push back when you push them.\n- **how to write the equations of motion** \u2014 the navier-cauchy equation for solids, the euler and navier-stokes equations for fluids.\n- **how to solve them** \u2014 analytically for the beautiful simple cases, numerically for everything else.\n\nthe emphasis is on real systems: glaciers, oceans, icebergs, earthquakes, cheese. yes, cheese. you'll see.\n\n## the philosophy\n\nwe won't start with equations. we'll start with *pictures*. every concept in this course begins with something you can see, touch, or imagine \u2014 a rubber band, a block of cheese, a drop of honey. the equations come after, as the punchline to a question you already have.\n\nif at any point the math feels disconnected from reality, stop and go back to the physical picture. the math is the servant of the physics, not the other way around.\n\n## course structure\n\ntwo mandatory assignments (which don't count toward your grade \u2014 they're for your own learning). an oral exam at the end. but the real payoff is this: after this course, you'll never look at a river, a rubber tire, or a cube of cheese the same way again.\n\nlet's begin.\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "gravitywaves",
      "lessonTitle": "Gravity Waves",
      "x": 0.804374098777771,
      "y": 0.35147038102149963,
      "searchText": "gravity waves\n# gravity waves\n\n## ripples, swells, and tsunamis\n\ndrop a stone in a pond. ripples spread outward in concentric circles. stand on a beach and watch the ocean: long, rolling swells march toward shore, steepen, and break. somewhere on the other side of the pacific, a submarine earthquake generates a wave that crosses the entire ocean in hours.\n\nthese are all **gravity waves** \u2014 waves where gravity provides the restoring force. when water is pushed up above its equilibrium level, gravity pulls it back down, and the resulting oscillation propagates outward.\n\nthis chapter develops the mathematical description of these waves, culminating in the shallow-water equations that govern everything from tidal bores to tsunamis.\n\n## the shallow-water equations\n\nwhen the wavelength is much larger than the water depth (think tsunamis in the open ocean, tidal flows in harbors, or flood waves in rivers), we can average the flow over the depth and arrive at the **shallow-water equations**:\n$$\n\\frac{\\partial v_x}{\\partial t} + (v_x \\nabla_x + v_y \\nabla_y)v_x = -g_0 \\nabla_x \\eta + f\\,v_y\n$$\n$$\n\\frac{\\partial v_y}{\\partial t} + (v_x \\nabla_x + v_y \\nabla_y)v_y = -g_0 \\nabla_y \\eta - f\\,v_x\n$$\n$$\n\\frac{\\partial \\eta}{\\partial t} + \\nabla_x(h\\,v_x) + \\nabla_y(h\\,v_y) = 0\n$$\n\nhere $\\eta$ is the surface elevation, $h$ is the water depth, $f$ is the coriolis parameter (earth's rotation matters for large-scale waves), and $v_x$, $v_y$ are the depth-averaged velocities.\n\nthe first two equations are momentum conservation (newton's second law applied to each column of water). the third is mass conservation (the water surface rises where more water flows in than out).\n\n## the wave equation and dispersion\n\nfor small-amplitude waves over constant depth $d$, the shallow-water equations linearize to give a **2d wave equation** for the surface elevation:\n$$\n\\left(\\frac{\\partial^2}{\\partial t^2} + f^2\\right)\\eta - g_0 d \\, \\nabla_h^2 \\eta = 0\n$$\n\nwithout rotation ($f = 0$), this is a standard wave equation. waves propagate at speed:\n$$\nc = \\sqrt{g_0 d}\n$$\n\nthis is the shallow-water wave speed: it depends on depth, not on wavelength. in the deep ocean ($d \\approx 4000$ m), this gives $c \\approx 200$ m/s $\\approx 700$ km/h \u2014 which is why tsunamis cross oceans in hours.\n\nthe wave period for a wave of wavelength $\\lambda$ is:\n$$\n\\tau \\approx \\frac{\\lambda}{\\sqrt{g_0 d}}\n$$\n\nwith rotation ($f \\neq 0$), the minimum wave frequency is $f$ \u2014 waves slower than one cycle per half-day (at mid-latitudes) are deflected by the coriolis force into rotating patterns rather than propagating freely.\n\n## what we just learned\n\ngravity waves are driven by the competition between gravity pulling water back to its equilibrium level and inertia carrying it past. the shallow-water equations describe these waves when the wavelength is much longer than the depth. the wave speed depends on depth, which explains why tsunamis slow down and steepen as they approach shore.\n\n## what's next\n\nwe've now covered the main exact solutions and wave phenomena in fluid mechanics. but what about flows where viscosity absolutely dominates \u2014 where the fluid is so sticky or slow that inertia plays no role? that's creeping flow, and it describes everything from honey to glaciers.\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "home",
      "lessonTitle": "Continuum Mechanics",
      "x": 0.66103595495224,
      "y": 0.38209468126296997,
      "searchText": "continuum mechanics\n# continuum mechanics\n\n## the story so far\n\neverything around you \u2014 the water in your glass, the ground beneath your feet, the glacier carving through a valley in greenland \u2014 is made of atoms. trillions upon trillions of them. but here's the thing: you don't need to track every single atom to understand how a bridge holds up, how honey pours, or why an iceberg floats. you just need to *pretend the world is smooth*.\n\nthat's the big trick of continuum mechanics. we ignore the atoms \u2014 the same way you ignore individual raindrops when you say \"it's raining\" \u2014 and instead treat matter as a continuous, smooth substance. from that single act of deliberate near-sightedness, an entire universe of physics unfolds.\n\nthis course is the story of that unfolding.\n\n## the journey\n\nhere's where we're going, step by step:\n\n**act i \u2014 setting the stage**\n\nwe start by asking: *when can we get away with pretending matter is smooth?* then we learn the mathematical language that lets us talk precisely about pushing, pulling, and deforming.\n\n**act ii \u2014 solids fight back**\n\nwe push on things and watch them push back. stress, strain, elasticity \u2014 this is where you learn why cheese deforms, bridges hold, and earthquakes shake.\n\n**act iii \u2014 everything moves**\n\nthe cauchy equation \u2014 the heartbeat of continuum mechanics \u2014 shows up. it's the same equation whether you're squeezing a rubber ball or stirring honey. we see how solids and fluids are two sides of the same coin.\n\n**act iv \u2014 fluids flow**\n\nwe pour the honey. pressure, buoyancy, ideal flows, then viscous flows. beautiful cases we can solve by hand: channels, pipes, waves.\n\n**act v \u2014 when things get sticky**\n\ncreeping flow. stokes equations. the world slows down and viscosity takes over. we develop the weak formulation and build the bridge to computation.\n\n**act vi \u2014 the computer takes over**\n\nfor everything we can't solve by hand (which is most of the real world), we learn the finite element method \u2014 cutting the world into tiny triangles and stitching the answers together.\n\n## learning trajectory\n\n1. **continuum approximation** \u2014 when and why we can pretend matter is smooth.\n2. **tensor fundamentals** \u2014 the language of pushing and pulling: stress tensors, strain tensors, and how they transform.\n3. **stress and strain** \u2014 traction vectors, principal stresses, mohr's circle, and hooke's law.\n4. **elasticity** \u2014 young's modulus, poisson's ratio, work, energy, vibrations, and the speed of sound.\n5. **python packages** \u2014 our computational toolkit. you'll need it from here on.\n6. **the heartbeat equation** \u2014 conservation laws, the material derivative, and cauchy's equation \u2014 the one equation that rules them all.\n7. **fluids at rest** \u2014 what is a fluid? pressure, buoyancy, and why icebergs float the way they do.\n8. **fluids in motion** \u2014 ideal flows, euler's equations, bernoulli's theorem, and vorticity.\n9. **viscous flow** \u2014 viscosity, the navier-stokes equation, and the reynolds number.\n10. **channels and pipes** \u2014 pressure-driven flow, gravity-driven flow, and laminar pipe flow. beautiful exact solutions.\n11. **gravity waves** \u2014 shallow-water equations and dispersion.\n12. **creeping flow** \u2014 stokes flow: when viscosity dominates and inertia vanishes. like swimming through honey.\n13. **weak stokes formulation** \u2014 recasting the equations in weak form, preparing for computation.\n14. **finite element method** \u2014 weighted residuals, galerkin's method, and how we actually simulate glaciers and airplane wings.\n15. **the grand finale** \u2014 every big idea retold as one story. a glacier, an iceberg, and an earthquake walk into a bar...\n\n## meet the cast\n\nthroughout these notes, you'll run into three recurring characters:\n\n- **rosie the rubber band** \u2014 she stretches, she snaps back, she knows all about elasticity.\n- **harry the honey drop** \u2014 he flows, he creeps, he's impossibly viscous and proud of it.\n- **gladys the glacier** \u2014 she's enormous, she's slow, and she's the real-world test case for everything we learn.\n\nwhen the math gets abstract, these three will keep you grounded.\n\n## why this matters\n\n- understanding stress and strain is essential for engineering, geophysics, and biomechanics.\n- fluid dynamics governs weather, ocean currents, blood flow, and industrial processes.\n- the navier-stokes equations remain one of the most important unsolved problems in mathematics.\n- numerical simulation of continua underpins modern engineering and earth science.\n\n## prerequisites\n\n- vector calculus: gradient, divergence, curl.\n- linear algebra: matrices, eigenvalues, tensor notation.\n- ordinary differential equations.\n- basic thermodynamics and classical mechanics.\n\n## further reading \u2014 for the curious\n\nif you want to feel the math in your bones, here are some recommendations:\n\n- **spencer**, *continuum mechanics* \u2014 clean and compact, a good desk companion.\n- **landau and lifshitz**, *theory of elasticity* and *fluid mechanics* \u2014 deep and beautiful, like a masterclass from old-world physicists.\n- **batchelor**, *an introduction to fluid dynamics* \u2014 the classic. dense but rewarding.\n- **lautrup**, *physics of continuous matter* \u2014 the textbook for this course. practical, physical, and full of real examples.\n- **lamb**, *hydrodynamics* \u2014 it's old but delicious, like a 100-year-old wine. read it to feel the elegance.\n- **feynman**, *the feynman lectures on physics, vol. ii* \u2014 chapters 38-41 on elasticity and fluid flow. because feynman makes everything clearer.\n\n## visual and simulation gallery\n\n[[figure continuum-stress-tensor]]\n\n[[figure continuum-hookes-law]]\n\n[[figure continuum-density-fluctuations]]\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "notebook",
      "lessonTitle": "Course Notebook",
      "x": 0.5426403284072876,
      "y": 0.3139796853065491,
      "searchText": "course notebook\n# course notebook\n\n\n2 mandatory assignments, that doesnt count towards grade.\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "pythonpackages",
      "lessonTitle": "Python Packages \u2014 Let's Play With the Universe",
      "x": 0.6489396095275879,
      "y": 0.33404043316841125,
      "searchText": "python packages \u2014 let's play with the universe\n# python packages \u2014 let's play with the universe\n\n## your computational toolkit\n\nyou've spent the last several chapters building up the theory: tensors, stress, strain, elasticity, fluid dynamics, the finite element method. now it's time to *compute*. the good news: with modern python packages, you can go from \"here's my differential equation\" to \"here's a beautiful simulation of a bending beam\" in about 20 lines of code.\n\nthis chapter introduces the packages you'll use throughout the rest of the course. keep it open as a reference.\n\n## fenics \u2014 the heavy lifter\n\nfenics is a powerful finite element library that lets you solve pdes by writing them in a form that looks almost like the math. here's a taste \u2014 a complete script that computes the deformation of a 1d elastic bar under gravity:\n\n```python\nfrom fenics import *\n\n# create a mesh: 100 elements along a unit interval\nmesh = unitintervalmesh(100)\n\n# define the function space: linear lagrange elements\nv = functionspace(mesh, 'lagrange', 1)\n\n# boundary condition: fixed at x=0\nbc = dirichletbc(v, constant(0), 'near(x[0], 0)')\n\n# define the problem: find u such that\n# integral of (du/dx * dw/dx) dx = integral of f * w dx\nu = trialfunction(v)      # the unknown displacement\nw = testfunction(v)        # the weight/test function\nf = constant(-1.0)        # body force (gravity, pointing down)\n\n# this is the weak form \u2014 compare to your notes!\na = inner(grad(u), grad(w)) * dx   # stiffness term\nl = f * w * dx                      # load term\n\n# solve it\nu_sol = function(v)\nsolve(a == l, u_sol, bc)\n\n# that's it. u_sol now contains the displacement field.\nplot(u_sol)\n```\n\nthat's 15 lines of real code, and you've just solved an elasticity problem. the key fenics functions:\n\n| function | what it does |\n|----------|-------------|\n| `unitintervalmesh(n)` | creates a 1d mesh with $n$ elements |\n| `rectanglemesh(...)` | creates a 2d rectangular mesh |\n| `functionspace(mesh, type, degree)` | defines the approximation space. `'lagrange', 1` = linear polynomials |\n| `vectorfunctionspace(...)` | for vector-valued problems (2d/3d displacements) |\n| `trialfunction(v)` | the unknown function you're solving for |\n| `testfunction(v)` | the weight function in the weak form |\n| `dirichletbc(v, value, boundary)` | boundary conditions: \"fix this value on this boundary\" |\n| `inner(a, b)` | the double-dot product ($a : b$) |\n| `solve(a == l, u, bcs)` | find the coefficients that satisfy the weak form |\n\nfor 2d problems, the syntax extends naturally:\n\n```python\nmesh = rectanglemesh(point(0, 0), point(width, height), nx, ny)\nv = vectorfunctionspace(mesh, 'lagrange', 1)\n\n# boundary condition: fixed bottom edge\ndef bottom(x, on_boundary):\n    return on_boundary and near(x[1], 0)\n\nbc = dirichletbc(v, constant((0, 0)), bottom)\n```\n\nfenics handles the meshing, assembly, and linear algebra behind the scenes. your job is to express the physics in weak form \u2014 and you already know how to do that from the previous chapters.\n\n## sympy \u2014 algebra without tears\n\nsympy does symbolic computation inside python. it's invaluable for tensor calculations where you need exact expressions rather than numbers:\n\n```python\nfrom sympy import symbols, diff, simplify, matrix\n\nx, y, z = symbols('x y z')\n\n# define a velocity field\nvx = x**2 * y\nvy = -x * y**2\n\n# compute the strain rate tensor\neps_xx = diff(vx, x)\neps_yy = diff(vy, y)\neps_xy = (diff(vx, y) + diff(vy, x)) / 2\n\nprint(f\"strain rate tensor:\")\nprint(f\"  eps_xx = {eps_xx}\")\nprint(f\"  eps_yy = {eps_yy}\")\nprint(f\"  eps_xy = {eps_xy}\")\n```\n\nuse `lambdify` to convert symbolic expressions into fast numerical functions when you need to evaluate them on arrays.\n\n## plotly \u2014 interactive 3d visualization\n\nmatplotlib is great for 2d plots, but for 3d stress fields and deformation surfaces, plotly gives you interactive visualizations you can rotate, zoom, and explore:\n\n```python\nimport plotly.graph_objects as go\n\n# plot a 3d surface of a stress field\nfig = go.figure(data=[go.surface(z=stress_field, x=x, y=y)])\nfig.update_layout(title='von mises stress')\nfig.show()\n```\n\n## shapely \u2014 geometric operations\n\nfor setting up domain geometries, computing cross-sections, and geometric preprocessing:\n\n```python\nfrom shapely.geometry import polygon\n\n# define a glacier cross-section\nglacier = polygon([(0, 0), (10, 0), (8, 5), (2, 5)])\nprint(f\"area: {glacier.area}\")\nprint(f\"centroid: {glacier.centroid}\")\n```\n\n## rasterio \u2014 working with real terrain data\n\nfor real-world applications (modeling glacier flow over actual terrain), rasterio reads geotiff and other raster formats used in geographic information systems:\n\n```python\nimport rasterio\nwith rasterio.open('terrain.tif') as src:\n    elevation = src.read(1)  # numpy array of elevation data\n```\n\n## gmsh \u2014 custom meshes\n\nfor domains more complex than rectangles, gmsh generates high-quality finite element meshes. you can define geometries programmatically or through its gui, export the mesh, and import it into fenics. see [gmsh.info](https://gmsh.info/).\n\n## gridap (julia)\n\nif you're curious about fem in julia, gridap offers a similar high-level interface. the syntax and workflow are analogous to fenics, which is reassuring: the *concepts* transfer across languages. see [gridap.github.io](https://gridap.github.io).\n\n## what we just learned\n\nmodern computational tools let you express the physics in code that closely mirrors the math. fenics handles fem problems, sympy handles symbolic tensor calculations, and plotly/rasterio handle visualization and real-world data. these tools will be your companions for the rest of the course.\n\n## what's next\n\nwe've covered solids, fluids, and the computational tools to simulate them. now it's time to bring everything together in the grand finale: a review of every major idea, told through stories and real-world examples.\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "stressandstrain",
      "lessonTitle": "Stress and Strain",
      "x": 0.7345274686813354,
      "y": 0.25679802894592285,
      "searchText": "stress and strain\n# stress and strain\n\n## getting physical \u2014 the rubber band experiment\n\nbefore we write a single equation, try this. take a rubber band and hold it between your fingers. now pull it straight \u2014 you feel it resist, and your fingers are being pulled toward each other. that's **normal stress**: force perpendicular to the surface, pulling the material apart.\n\nnow try twisting the rubber band while holding it taut. feel that sideways tug, the way the material wants to slide past itself? that's **shear stress**: force parallel to the surface.\n\nevery point inside a stressed material experiences some combination of both. the stress tensor is just the bookkeeping system that tracks all of it.\n\n## the stress tensor \u2014 forces on imaginary surfaces\n\nimagine slicing through a stressed material with an imaginary plane. the orientation of that plane is described by its outward normal vector $\\hat{n}$. the **traction vector** \u2014 the force per unit area that the material on one side of the cut exerts on the other \u2014 is:\n$$\nt_i = \\sigma_{ij} \\, n_j\n$$\n\nthis is the stress tensor's job: you give it a direction (the normal to your imaginary cut), and it gives you back the force per unit area on that surface. it's a machine that converts orientations into forces.\n\nthe stress tensor is symmetric ($\\sigma_{ij} = \\sigma_{ji}$) because angular momentum must be conserved \u2014 otherwise every little cube of material would start spontaneously spinning. this symmetry means that in 3d, we have six independent stress components, not nine.\n\n[[simulation stress-strain-sim]]\n\n## principal stresses \u2014 finding the sweet spot\n\nhere's something remarkable: no matter how complicated the stress state, there's always a special orientation where all the shear stresses vanish. in this **principal coordinate system**, the stress tensor is diagonal:\n$$\n\\sigma = \\begin{pmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 \\\\ 0 & 0 & \\sigma_3 \\end{pmatrix}\n$$\n\nthe values $\\sigma_1$, $\\sigma_2$, $\\sigma_3$ are the **principal stresses** \u2014 the eigenvalues of $\\sigma_{ij}$. they tell you the maximum and minimum normal stresses at that point, and the eigenvectors tell you which directions they act in.\n\nfinding principal stresses is just an eigenvalue problem. if you know the stress tensor in any coordinate system, you can always rotate to the principal system.\n\n## the strain tensor \u2014 measuring deformation\n\nwhen you push on a material, it deforms. the **strain tensor** quantifies that deformation. for small displacements $u_i$ from the original position:\n$$\n\\varepsilon_{ij} = \\frac{1}{2}\\left(\\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i}\\right)\n$$\n\nthe entries have clear physical meanings:\n\n- **normal strain** ($\\varepsilon_{ii}$): fractional change in length along axis $i$. positive means stretching, negative means compressing.\n- **shear strain** ($\\varepsilon_{ij}$, $i \\neq j$): change in angle between two originally perpendicular lines. it measures how much the material *skews*.\n\n## hooke's law \u2014 the simplest possible relationship\n\nso stress causes strain, and strain implies stress. what's the relationship between them?\n\nfor a **linear elastic, isotropic** material (meaning it responds the same way in all directions, and the response is proportional to the load), the relationship is beautifully simple:\n$$\n\\sigma_{ij} = \\lambda \\, \\varepsilon_{kk} \\, \\delta_{ij} + 2\\mu \\, \\varepsilon_{ij}\n$$\n\nhere $\\lambda$ and $\\mu$ are the **lame parameters**, material constants that you can look up for steel, rubber, ice, or cheese. equivalently, using young's modulus $e$ and poisson's ratio $\\nu$:\n$$\n\\varepsilon_{ij} = \\frac{1+\\nu}{e}\\,\\sigma_{ij} - \\frac{\\nu}{e}\\,\\sigma_{kk}\\,\\delta_{ij}\n$$\n\nwhat do these constants mean physically?\n\n- **young's modulus** $e$: how stiff is the material? pull on it \u2014 $e$ tells you how much force per unit area you need to stretch it by a given fraction. steel: $e \\approx 200$ gpa. rubber: $e \\approx 0.01$ gpa.\n- **poisson's ratio** $\\nu$: when you stretch something in one direction, how much does it thin in the other directions? for most materials, $\\nu \\approx 0.3$. for incompressible materials (like rubber), $\\nu \\approx 0.5$.\n\n[[simulation stress-strain-curve]]\n\n## mohr's circle \u2014 the world's most useful clock face for stress\n\nyou have a stress state. you want to know: if i cut through the material at some angle, what normal stress and shear stress will i see on that surface? you *could* do the matrix rotation by hand every time. or you could use **mohr's circle**.\n\nhere's how it works. given principal stresses $\\sigma_1 > \\sigma_2 > \\sigma_3$:\n\n1. draw a horizontal axis for normal stress $\\sigma_n$ and a vertical axis for shear stress $\\tau$.\n2. plot the points $(\\sigma_1, 0)$, $(\\sigma_2, 0)$, $(\\sigma_3, 0)$ on the horizontal axis.\n3. draw circles: one connecting $\\sigma_1$ and $\\sigma_3$ (the big outer circle), one connecting $\\sigma_1$ and $\\sigma_2$, and one connecting $\\sigma_2$ and $\\sigma_3$.\n4. every possible stress state on any plane through that point lies *on or between* these three circles.\n\nthink of it as a clock face for stress. the position on the circle tells you the normal and shear stress on a plane at a given angle. as you \"rotate the clock hand\" (change the orientation of your imaginary cut), you sweep around the circle.\n\nthe maximum shear stress jumps out immediately: it's the radius of the biggest circle:\n$$\n\\tau_{\\max} = \\frac{\\sigma_1 - \\sigma_3}{2}\n$$\n\nno eigenvalue calculation needed. just draw the circle, read off the answer. engineers have been doing this on napkins for over a century.\n\n[[simulation mohr-circle]]\n\n## what we just learned\n\nthe stress tensor describes internal forces, the strain tensor describes deformation, and hooke's law connects them linearly for elastic materials. mohr's circle gives us a graphical shortcut to understand stress transformations without grinding through matrix algebra. these tools work for any material that obeys linear elastic"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "tensorfundamentals",
      "lessonTitle": "Tensor Fundamentals",
      "x": 0.7431735396385193,
      "y": 0.26190319657325745,
      "searchText": "tensor fundamentals\n# tensor fundamentals\n\n## the ant on the rubber sheet\n\nimagine you're an ant walking on a rubber sheet. someone is stretching the sheet \u2014 pulling it to the right, squeezing it from above. as you walk, the ground under your feet stretches in one direction and compresses in another. the way the sheet deforms around you depends on *which direction you're facing*. that direction-dependent description of stretching and squeezing \u2014 that's a tensor.\n\na scalar (like temperature) tells you one number at each point. a vector (like velocity) tells you a magnitude and a direction. a tensor tells you something richer: it tells you how a quantity *changes depending on which direction you look*. in continuum mechanics, we need tensors because the forces inside a material aren't just big or small \u2014 they act differently in different directions. a beam might be compressed vertically but stretched horizontally, all at the same point.\n\nnow let's give these ideas precise names so we don't have to wave our hands anymore.\n\n## the cauchy stress tensor \u2014 six little hands\n\nimagine cutting a tiny cube out of the interior of a stressed material. on each face of that cube, the surrounding material is pushing and pulling. it's like having **six little hands** pressing and twisting on every tiny cube inside the material.\n\nthe cauchy stress tensor $\\sigma$ captures all of this. in 3d, it's a $3 \\times 3$ matrix:\n$$\n\\sigma = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\sigma_{13}\\\\\n\\sigma_{21} & \\sigma_{22} & \\sigma_{23}\\\\\n\\sigma_{31} & \\sigma_{32} & \\sigma_{33}\\\\\n\\end{pmatrix}\n$$\n\nwhat do the entries mean?\n\n- **diagonal elements** ($\\sigma_{11}$, $\\sigma_{22}$, $\\sigma_{33}$) are the **normal stresses** \u2014 they push or pull straight into each face of the cube. positive means tension (pulling apart), negative means compression (squeezing together).\n- **off-diagonal elements** ($\\sigma_{12}$, $\\sigma_{13}$, etc.) are the **shear stresses** \u2014 they slide the faces sideways, like rubbing your hands together.\n\nthe columns of $\\sigma$ are the **traction vectors**: the force per unit area on each face. every element has units of pascal ($\\text{n/m}^2$).\n\nfor the tensor to be physically consistent (no spontaneously spinning cubes!), angular momentum conservation requires it to be symmetric: $\\sigma = \\sigma^t$. that means $\\sigma_{12} = \\sigma_{21}$, and so on \u2014 only 6 independent components in 3d, not 9.\n\nin 2d and 1d, the tensor shrinks accordingly:\n$$\n\\sigma_{2d} = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12}\\\\\n\\sigma_{21} & \\sigma_{22}\\\\\n\\end{pmatrix}, \\qquad\n\\sigma_{1d} = \\sigma_{11}\n$$\n\nhere's a key insight: **normal and shear stresses are a matter of perspective**. they depend on which coordinate system you choose, which is arbitrary. there always exists a special basis \u2014 the eigenbasis \u2014 where the stress tensor is purely diagonal and all the shear stresses vanish. the stresses in that basis are called the **principal stresses**, and they represent the purest description of the stress state at that point.\n\n## the stress deviator \u2014 relative to what?\n\nsometimes you don't care about the total stress \u2014 you care about how the stress *deviates* from uniform pressure. think about building a house. the materials were tested at atmospheric pressure. the house will stand at atmospheric pressure. so you want to know: how much *extra* stress does the structure experience beyond the background pressure?\n\nthe **stress deviator** strips away the uniform pressure part:\n$$\ns = \\sigma - p\\,\\mathbf{i} \\qquad \\text{where} \\qquad p = \\frac{1}{3}\\text{tr}(\\sigma)\n$$\n\nwritten out:\n$$\n\\begin{pmatrix}\ns_{11} & s_{12} & s_{13}\\\\\ns_{21} & s_{22} & s_{23}\\\\\ns_{31} & s_{32} & s_{33}\\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sigma_{11} - p & \\sigma_{12} & \\sigma_{13}\\\\\n\\sigma_{21} & \\sigma_{22} - p & \\sigma_{23}\\\\\n\\sigma_{31} & \\sigma_{32} & \\sigma_{33} - p\\\\\n\\end{pmatrix}\n$$\n\nthe deviator is what matters for predicting *shape change* and *failure*. uniform pressure changes volume but doesn't change shape \u2014 it's the deviator that warps, bends, and eventually breaks things.\n\n## invariants \u2014 what doesn't change when you rotate\n\nthe stress tensor looks different in different coordinate systems, but certain quantities remain the same no matter how you rotate your axes. these **invariants** are the truly physical quantities.\n\nfor the cauchy stress tensor, the three invariants are:\n\n$i_1 = \\sigma_1 + \\sigma_2 + \\sigma_3$\n\n$i_2 = \\sigma_1\\sigma_2 + \\sigma_2\\sigma_3 + \\sigma_3\\sigma_1$\n\n$i_3 = \\sigma_1\\sigma_2\\sigma_3$\n\nfor the stress deviator:\n\n$j_1 = s_{kk} = 0$ (by construction \u2014 we removed the pressure)\n\n$j_2 = \\frac{1}{2}\\text{tr}(s^2) = \\frac{1}{2}\\left(\\text{tr}(\\sigma^2) - \\frac{1}{3}\\text{tr}(\\sigma)^2\\right)$\n\n$j_3 = \\det(s) = \\frac{1}{3}\\left(\\text{tr}(\\sigma^3) - \\text{tr}(\\sigma^2)\\text{tr}(\\sigma) + \\frac{2}{9}\\text{tr}(\\sigma)^3\\right)$\n\nthe $j_2$ invariant is especially important because it connects to the **von mises yield criterion**: a practical rule for predicting when a material will permanently deform. for many metals, the ratio of shear yield stress to tensile yield stress is:\n$$\n\\frac{\\sigma_{\\text{shear,yield}}}{\\sigma_{\\text{tensile,yield}}} = \\frac{1}{\\sqrt{3}} \\approx 0.577\n$$\n\nwhen this holds, the material will start to permanently deform when:\n$$\n\\sigma_{\\text{von mises}} = \\sqrt{3 j_2}\n$$\nexceeds the yield strength. this works regardless of your choice of coordinates \u2014 that's the power of invariants.\n\n## the cauchy strain tensor \u2014 measuring deformation\n\nso far we've described the *forces* inside a material. but what about the *deformation* itself? if you push on rosie the rubber band, how much does she actually stretch?\n\nconsider a velocity field describing motion in a continuum:\n$$\n\\mathbf{v}(x,y,z) = \\begin{pmatrix} v_x(x,y,z)\\\\ v_y(x,y,z)\\\\ v_z(x,y,z) \\end{pmatrix}\n$$\n\nthe **cauchy strain tensor** (or strain rate tensor) captures how the material deforms:\n$$\n\\epsilon = \\begin{pmatrix}\n\\frac{\\partial v_x}{\\partial x}"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "viscousflow",
      "lessonTitle": "Viscous Flow",
      "x": 0.8224509358406067,
      "y": 0.41081005334854126,
      "searchText": "viscous flow\n# viscous flow\n\n## the battle in a high-school cafeteria\n\nimagine a high-school cafeteria at lunchtime. a crowd of students is streaming toward the door. their inertia wants to carry them straight ahead \u2014 they've got momentum and they want to keep going. but there's also social friction: people bumping shoulders, friends grabbing friends, the crowd *smearing out* the individual trajectories.\n\nthat's the battle at the heart of viscous flow: **inertia** (the tendency to keep going straight) versus **viscosity** (the tendency to smear everything out). the reynolds number tells you who's winning.\n\nbut we're getting ahead of ourselves. let's start with what viscosity actually is.\n\n## viscosity \u2014 how sticky is your fluid?\n\nviscosity is a fluid's resistance to being sheared. honey has high viscosity (hard to stir). water has low viscosity (easy to stir). air has very low viscosity (you barely notice it).\n\nimagine two parallel plates with fluid between them. you slide the top plate to the right while holding the bottom plate still. the fluid in between develops a velocity gradient \u2014 fast near the top plate, stationary near the bottom. **newton's law of viscosity** says:\n$$\n\\sigma_{xy} = \\eta \\frac{dv_x}{dy}\n$$\n\nthe shear stress is proportional to the velocity gradient. the proportionality constant $\\eta$ is the **dynamic viscosity** (units: pa$\\cdot$s). water: $\\eta \\approx 10^{-3}$ pa$\\cdot$s. honey: $\\eta \\approx 2$\u2013$10$ pa$\\cdot$s. the fluid between the plates is being *sheared*, and viscosity determines how hard it pushes back.\n\na useful variant is the **kinematic viscosity**:\n$$\n\\nu = \\frac{\\eta}{\\rho}\n$$\n\nthis is the viscosity \"per unit density\" \u2014 it's what shows up most often in the equations. for water: $\\nu \\approx 10^{-6}$ m$^2$/s. for air: $\\nu \\approx 1.5 \\times 10^{-5}$ m$^2$/s.\n\n## velocity-driven planar flow \u2014 viscosity as diffusion\n\nthe simplest viscous flow problem: one plate at rest, one plate suddenly set in motion. the velocity profile evolves as:\n$$\n\\frac{\\partial v_x}{\\partial t} = \\nu \\frac{\\partial^2 v_x}{\\partial y^2}\n$$\n\nthis is a **diffusion equation** \u2014 exactly the same form as heat diffusion! viscosity *diffuses* momentum the same way thermal conductivity diffuses heat. the viscous layer spreads outward from the moving plate at a rate $\\sim \\sqrt{\\nu t}$.\n\n## the viscous stress tensor\n\nfor a general incompressible newtonian fluid, the stress tensor has two parts \u2014 pressure and viscous shear:\n$$\n\\sigma_{ij} = -p\\,\\delta_{ij} + \\eta\\left(\\frac{\\partial v_i}{\\partial x_j} + \\frac{\\partial v_j}{\\partial x_i}\\right)\n$$\n\nor in compact notation:\n$$\n\\sigma = -p\\,\\mathbf{i} + 2\\eta\\,\\dot{\\varepsilon}\n$$\n\nwhere $\\dot{\\varepsilon}$ is the strain rate tensor. compare this to the elastic solid: $\\sigma = -p\\,\\mathbf{i} + 2\\mu\\,\\varepsilon$. the *structure* is identical \u2014 the difference is that fluids resist the *rate* of deformation while solids resist the deformation itself.\n\n## the navier-stokes equation \u2014 nature's accounting book for fluid motion\n\nso you want to know how the velocity of a viscous fluid changes over time? here's the accounting book nature uses. plug the viscous stress into cauchy's equation, assume incompressible, isotropic, homogeneous newtonian fluid:\n$$\n\\frac{\\partial \\mathbf{v}}{\\partial t} + (\\mathbf{v} \\cdot \\nabla)\\mathbf{v} = \\mathbf{g} - \\frac{1}{\\rho_0}\\nabla p + \\nu \\nabla^2 \\mathbf{v}, \\qquad \\nabla \\cdot \\mathbf{v} = 0\n$$\n\nevery term has a physical meaning:\n\n| term | what it means |\n|------|--------------|\n| $\\partial \\mathbf{v}/\\partial t$ | local acceleration \u2014 how velocity changes at a fixed point |\n| $(\\mathbf{v} \\cdot \\nabla)\\mathbf{v}$ | advection \u2014 momentum carried by the flow itself |\n| $\\mathbf{g}$ | gravity (or other body forces) |\n| $-\\nabla p / \\rho_0$ | pressure gradient \u2014 fluid flows from high to low pressure |\n| $\\nu \\nabla^2 \\mathbf{v}$ | viscous diffusion \u2014 friction smears out velocity gradients |\n\nthe assumptions behind this equation: the fluid is **incompressible** ($\\nabla \\cdot \\mathbf{v} = 0$), **newtonian** ($\\nu$ is constant), and **isotropic** (same material properties in all directions).\n\nthese equations are among the most studied in all of physics. they can produce laminar flow, turbulence, boundary layers, vortex streets, and chaos \u2014 all from four lines of math.\n\n## the reynolds number \u2014 who wins the cafeteria battle?\n\nback to our cafeteria. the reynolds number quantifies the competition between inertia and viscosity:\n$$\n\\text{re} = \\frac{|\\text{advective term}|}{|\\text{viscous term}|} \\approx \\frac{u^2/l}{\\nu u / l^2} = \\frac{ul}{\\nu}\n$$\n\nwhere $u$ is a characteristic velocity and $l$ is a characteristic length.\n\n- **re $\\ll$ 1**: viscosity dominates. the flow is smooth, predictable, and *creeping*. think of bacteria swimming through mucus, or honey pouring off a spoon. harry the honey drop lives here.\n- **re $\\sim$ 1**: a fair fight. inertia and viscosity are comparable.\n- **re $\\gg$ 1**: inertia dominates. the flow becomes chaotic, turbulent, unpredictable. think of smoke rising from a campfire, or the wake behind a fast boat.\n\nsome examples:\n\n| system | re |\n|--------|----|\n| bacterium swimming | $\\sim 10^{-4}$ |\n| blood flow in capillaries | $\\sim 10^{-3}$ |\n| honey pouring | $\\sim 10^{-1}$ |\n| water in a pipe | $\\sim 10^3$ |\n| airplane wing | $\\sim 10^7$ |\n| ocean currents | $\\sim 10^9$ |\n\n## newtonian vs. non-newtonian \u2014 when the rules change\n\neverything above assumes the fluid is **newtonian**: the stress is linearly proportional to the strain rate. but many fluids don't play by these rules.\n\n- **shear-thinning** (pseudoplastic, $n < 1$): viscosity *decreases* under shear. examples: ketchup, blood, paint. this is why you shake the ketchup bottle \u2014 the shearing makes it flow.\n- **shear-thickening** (dilatant, $n > 1$): viscosity *increases* under shear. examples: cornstarch in water (oobleck), wet sand at the beach.\n\nthe power-law model captures this: $\\sigma = k \\dot{\\gamma}^n$, where $n$ is the flow"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "weakstokes",
      "lessonTitle": "Weak Stokes Formulation",
      "x": 0.7647804021835327,
      "y": 0.4680629074573517,
      "searchText": "weak stokes formulation\n# weak stokes formulation\n\n## why \"weak\"? \u2014 from perfect to practical\n\nthe stokes equation $\\nabla p = \\eta \\nabla^2 \\mathbf{v}$ with $\\nabla \\cdot \\mathbf{v} = 0$ is the **strong form**: it demands that the equation is satisfied at every single point in the domain. that's a tall order when your domain is a glacier valley with jagged rock walls and complex boundary conditions.\n\nthe **weak form** relaxes this demand. instead of requiring the equation to hold pointwise, we require it to hold *on average* \u2014 specifically, when integrated against test functions. this might sound like we're giving something up, but we're actually gaining something enormous: the weak form can be solved numerically using the finite element method.\n\nthink of it this way: the strong form is like requiring a student to know the answer at every point on the exam. the weak form is like requiring the student to get the right average score. the average-score requirement is easier to enforce, and for well-posed problems, it's equivalent to the pointwise requirement.\n\n## the cauchy momentum balance in weak form\n\nstart with the cauchy momentum balance for steady stokes flow:\n$$\n-\\nabla \\cdot \\sigma + \\nabla p = \\mathbf{f}, \\qquad \\nabla \\cdot \\mathbf{v} = 0\n$$\n\nwhere $\\sigma = 2\\eta\\,\\dot{\\varepsilon}$ for a newtonian fluid and $\\mathbf{f}$ is the body force.\n\nto get the weak form, we multiply the momentum equation by a **test function** $\\mathbf{w}$ (a vector function that vanishes on the boundary where we've prescribed velocities), and integrate over the domain $\\omega$:\n$$\n\\int_\\omega 2\\eta\\,\\dot{\\varepsilon}(\\mathbf{v}) : \\dot{\\varepsilon}(\\mathbf{w}) \\, dv - \\int_\\omega p \\, (\\nabla \\cdot \\mathbf{w}) \\, dv = \\int_\\omega \\mathbf{f} \\cdot \\mathbf{w} \\, dv + \\int_{\\gamma_n} \\mathbf{t} \\cdot \\mathbf{w} \\, ds\n$$\n\nsimilarly, the incompressibility constraint is tested against a scalar test function $q$:\n$$\n\\int_\\omega q \\, (\\nabla \\cdot \\mathbf{v}) \\, dv = 0\n$$\n\nnotice what happened: the second derivatives that were in the strong form got distributed (via integration by parts) between $\\mathbf{v}$ and $\\mathbf{w}$. we no longer need $\\mathbf{v}$ to be twice differentiable \u2014 once differentiable is enough. this is the technical payoff of the weak form: weaker smoothness requirements on the solution.\n\n## from weak form to linear system\n\nin the finite element method (next chapter), we'll approximate $\\mathbf{v}$ and $p$ as combinations of basis functions. the weak form then becomes a system of linear equations:\n$$\n\\begin{pmatrix} \\mathbf{k} & \\mathbf{g}^t \\\\ \\mathbf{g} & \\mathbf{0} \\end{pmatrix} \\begin{pmatrix} \\mathbf{u} \\\\ \\mathbf{p} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{f} \\\\ \\mathbf{0} \\end{pmatrix}\n$$\n\nwhere $\\mathbf{k}$ is the viscosity matrix, $\\mathbf{g}$ enforces incompressibility, $\\mathbf{u}$ contains the unknown velocities at mesh nodes, and $\\mathbf{p}$ contains the unknown pressures. this is a **saddle-point system** \u2014 it requires careful choice of finite element spaces to ensure stability (the inf-sup condition).\n\n## what we just learned\n\nthe weak form of the stokes equations replaces pointwise satisfaction with integral satisfaction against test functions. integration by parts lowers the smoothness requirements on the solution and produces a form ready for finite element discretization. the result is a saddle-point linear system coupling velocity and pressure unknowns.\n\n## what's next\n\nwe have the equations in the right form. now we need to learn how to actually *solve* them on a computer. that's the finite element method \u2014 where we cut the domain into tiny pieces and solve for each one.\n"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "bacterial-growth",
      "lessonTitle": "Bacterial Growth Physiology",
      "x": 0.5765603184700012,
      "y": 0.0,
      "searchText": "bacterial growth physiology\n# bacterial growth physiology\n\n## phases of bacterial growth\n\n*e. coli* has been intensively used for research on bacterial growth. monod (1949) showed that there are several phases in bacterial growth, and we focus on the **exponential growth phase**. in the exponential phase, the bacteria reach a quasi-steady state of growth.\n\n## definition of steady-state growth\n\n1. intrinsic parameters of the cell remain constant.\n2. extrinsic parameters increase exponentially with precisely the same doubling time.\n3. growth conditions remain constant.\n\n## bacterial growth law by jacques monod (1949)\n\nhe found growth rate $\\lambda$ depends on concentration of limiting nutrient in a michaelis-menten manner:\n\n$$\n\\lambda = \\lambda_\\mathrm{max} \\frac{s}{k_s + s}\n$$\n\n[[simulation michaelis-menten]]\n\n## frederick c. neidhardt (1999)\n\n> for me, encountering the bacterial growth curve was a transforming experience.\n\n## bacterial biomass is mainly protein\n\nwe now know bacterial growth is governed by a complex metabolic network. however, 55% of the total dry weight of bacteria is protein. we can approximate bacterial growth as mass of protein. the differential equation for this is:\n\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} m = \\lambda \\cdot m\n$$\n\nhere, $m$ is total protein mass and $\\lambda$ is the growth rate. if protein synthesis is rate limiting for growth:\n\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} m = k \\cdot n_r^a\n$$\n\nhere $k$ is the translation rate per ribosome and $n_r^a$ is the total number of actively translating ribosomes. thus we approximate protein production as depending only on the number of ribosomes.\n\nwe can roughly divide protein into three fractions:\n- metabolism/transport: $\\phi_\\mathrm{p} = m_\\mathrm{p}/m$\n- \"extended\" ribosomes (ribosomal protein): $\\phi_\\mathrm{r} = m_\\mathrm{r}/m$\n- other tasks: $\\phi_\\mathrm{q} = m_\\mathrm{q}/m$\n\nhere $\\phi_\\mathrm{x} = m_\\mathrm{x}/m$ is the fraction of protein molecules dedicated to a specific task. we immediately know that $\\phi_\\mathrm{p} + \\phi_\\mathrm{r} + \\phi_\\mathrm{q} = 1$.\n\n## efficient resource allocation\n\nnutrient influx by $\\phi_\\mathrm{p}$ should match nutrient usage by $\\phi_\\mathrm{r}$. but how does the bacterium know when to adjust the balance?\n\nthe molecule called **ppgpp** (guanosine pentaphosphate) is the molecular signal to stop making ribosomes. in the default state, bacteria make a lot of ribosomes because synthesis of the \"extended ribosome\" is regulated mainly by the promoters for the ribosomal rna genes.\n\n## measuring numbers of growth\n\nas an equation describing bacterial growth, we have:\n\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} m = k \\cdot n_r^a\n$$\n"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "differential-equations",
      "lessonTitle": "Differential Equations in a Nutshell",
      "x": 0.6179722547531128,
      "y": 0.02108740620315075,
      "searchText": "differential equations in a nutshell\n# differential equations in a nutshell\n\n## where we are headed\n\nthis is our first lesson, and we need a language for talking about how things change over time. that language is differential equations. by the end of this lesson you will see that one simple idea \u2014 *the rate of change equals what flows in minus what flows out* \u2014 is enough to explain how molecules accumulate, decay, and reach a steady state inside a living cell. everything else in this course builds on top of this.\n\n## the bathtub picture\n\nbefore we write any symbols, think about a bathtub. water flows in from the faucet at a constant rate. water drains out through the plug at a rate that depends on how full the tub is \u2014 the more water, the faster it drains. what happens?\n\nat first the tub fills up quickly because very little is draining out. as the water level rises, the drain gets faster and faster. eventually the inflow and the outflow balance perfectly, and the water level stops changing. that final level is the **steady state**.\n\nthis is exactly how molecules work inside a cell. mrna is produced at some rate by the transcription machinery, and it is degraded at a rate proportional to how much is present. the steady-state concentration is simply the ratio of production to degradation. nature finds this balance automatically.\n\n## creation: molecules appearing at a constant rate\n\nsuppose molecules (say, mrna) are created at a constant rate $k$ (molecules per unit time). after a short time $\\delta t$, the number of molecules goes from $n(t)$ to\n\n$$\nn(t+\\delta t) = n(t) + k \\, \\delta t.\n$$\n\n> *in words: the new count equals the old count plus however many were made during that interval.*\n\nrearranging and taking the limit $\\delta t \\rightarrow 0$ gives us our first differential equation:\n\n$$\n\\frac{\\mathrm{d} n(t)}{\\mathrm{d} t} = k.\n$$\n\n> *this says: the number of molecules increases at a constant rate, no matter how many are already there.*\n\nthe solution is simply $n(t) = n(0) + kt$ \u2014 a straight line. molecules pile up forever. clearly something is missing.\n\n## degradation: molecules falling apart\n\nnow suppose molecules are degraded at a rate $\\gamma$ (per unit time). the number destroyed in a short interval $\\delta t$ is $\\gamma \\, n(t) \\, \\delta t$ \u2014 it depends on how many molecules are present. following the same logic:\n\n$$\n\\frac{\\mathrm{d} n(t)}{\\mathrm{d} t} = -\\gamma \\, n(t).\n$$\n\n> *this says: the more molecules you have, the faster they disappear.*\n\nthe solution is $n(t) = n(0) \\, e^{-\\gamma t}$ \u2014 exponential decay. you can also think of this in terms of the **half-life** $t_{1/2} = \\ln 2 / \\gamma$, the time it takes for half the molecules to be gone. or equivalently, the **time constant** $\\tau = 1/\\gamma$, which is the time for the number to drop to about 37% of its starting value.\n\n## the full picture: creation and degradation together\n\nnow combine both processes. this is the bathtub equation:\n\n$$\n\\frac{\\mathrm{d} n(t)}{\\mathrm{d} t} = k - \\gamma \\, n(t).\n$$\n\n> *in words: the rate of change equals what's being made minus what's falling apart.*\n\nat **steady state** nothing is changing, so we set the left side to zero:\n\n$$\n0 = k - \\gamma \\, n_\\mathrm{ss} \\quad \\longrightarrow \\quad n_\\mathrm{ss} = \\frac{k}{\\gamma}.\n$$\n\n> *the steady-state number of molecules is just the production rate divided by the degradation rate. that's it. this beautifully simple result shows up everywhere in biology.*\n\nif you start with zero molecules, the system climbs toward $n_\\mathrm{ss}$ on a timescale set by $1/\\gamma$ \u2014 fast degradation means the system reaches steady state quickly. if you start above $n_\\mathrm{ss}$, the excess decays away on the same timescale.\n\n## why does nature do it this way?\n\nyou might ask: why does a cell bother degrading its own mrna? it costs energy to make it, so why destroy it? the answer is speed. a cell that only produces molecules (no degradation) can never reduce their levels \u2014 it is stuck. but a cell that both produces and degrades can change its steady state simply by adjusting the production rate. the faster the degradation, the quicker the response. this is why many mrnas in bacteria have half-lives of just a few minutes.\n\n## check your understanding\n\n- if you double the production rate $k$ while keeping $\\gamma$ fixed, what happens to the steady-state level?\n- a certain mrna has a half-life of 3 minutes. what is its degradation rate $\\gamma$?\n- why does faster degradation lead to faster response, even though it seems wasteful?\n\n## big ideas\n\n- **the bathtub equation** $\\dot{n} = k - \\gamma n$ is the foundation of almost every model in this course.\n- **steady state** is where production balances degradation: $n_\\mathrm{ss} = k / \\gamma$.\n- **the degradation rate sets the response time** \u2014 fast turnover means the cell can change its mind quickly.\n"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "feedback-loops",
      "lessonTitle": "Feedback Loops in Biological Systems",
      "x": 0.6811743974685669,
      "y": 0.06226205453276634,
      "searchText": "feedback loops in biological systems\n# feedback loops in biological systems\n\n## where we are headed\n\nlast time you saw how the hill function lets a cell turn gene expression up or down in response to a signal. that is powerful, but it is still a one-way street: a signal comes in, the gene responds. today we close the loop. what happens when a gene product influences its *own* production? this simple act \u2014 feedback \u2014 is what gives cells memory, decision-making ability, and the capacity to oscillate. switches, clocks, and irreversible commitments all emerge from feedback, and you will see exactly why.\n\n## why feedback matters\n\na gene product that influences its own production creates a **feedback loop**, and the sign of that influence determines everything:\n\n- **negative feedback** (the gene product represses itself): promotes homeostasis, speeds up the response, and reduces noise. the system finds a stable set point and sticks to it.\n- **positive feedback** (the gene product activates itself): generates bistability, memory, and irreversible switching. the system can commit to one of two states and stay there.\n\n## negative autoregulation\n\nsuppose a transcription factor $x$ represses its own promoter. using the hill function from last lesson:\n\n$$\n\\frac{dx}{dt} = \\frac{\\beta}{1 + (x/k)^n} - \\gamma x,\n$$\n\nwhere $\\beta$ is the maximal production rate, $k$ is the repression threshold, $n$ is the hill coefficient, and $\\gamma$ is the degradation rate.\n\n> *in words: when $x$ is low, the repression term is small and production runs at nearly full speed. as $x$ builds up, it starts sitting on its own promoter and slowing down its own production. the system self-corrects.*\n\nthis self-correction gives negative autoregulation three beautiful properties:\n\n- **faster response time**: the system overshoots at first (production starts at full blast when $x$ is low), then self-limits. this gets to steady state faster than a gene without feedback.\n- **reduced noise**: if $x$ fluctuates above the mean, the increased repression pushes it back down. if it drops below, repression eases and production speeds up. the feedback acts like a thermostat.\n- **robustness**: the steady-state level is less sensitive to parameter variations. if the degradation rate changes slightly, the feedback compensates.\n\n## finding the steady state graphically\n\nhere is a powerful trick for analyzing any one-dimensional feedback circuit. think of it as drawing two curves on the same piece of paper:\n\n1. draw the **production rate** $f(x) = \\beta/(1 + (x/k)^n)$ as a function of $x$. for negative feedback, this curve starts high and decreases.\n2. draw the **degradation rate** $\\gamma x$ \u2014 a straight line through the origin.\n3. **where they cross is where the system is happy and stays put.** that intersection is the steady state.\n\n> *at the crossing point, production exactly balances degradation \u2014 nothing changes. if $x$ is below the crossing point, production exceeds degradation, so $x$ increases. if $x$ is above, degradation wins, so $x$ decreases. the system is pulled toward the crossing like a ball rolling to the bottom of a valley.*\n\nto check stability: if the production curve is *below* the degradation line to the right of the crossing (and above to the left), the fixed point is stable. if the slopes tell the opposite story, it is unstable.\n\n[[simulation steady-state-regulation]]\n\n## positive feedback and bistability\n\nnow suppose the transcription factor *activates* its own production:\n\n$$\n\\frac{dx}{dt} = \\frac{\\beta (x/k)^n}{1 + (x/k)^n} + \\beta_0 - \\gamma x,\n$$\n\nwhere $\\beta_0$ is a small basal (leak) production rate \u2014 even without feedback, a trickle of $x$ is made.\n\nfor sufficiently cooperative activation ($n \\geq 2$), the production curve can be s-shaped, and it may cross the degradation line at **three points**: two stable (low and high expression) and one unstable in between. this is **bistability**.\n\n> *imagine a light switch with two stable positions \u2014 up and down \u2014 and an unstable balance point in the middle. the cell can sit in the \"low\" state or the \"high\" state, but not in between. a push large enough to cross the unstable middle point flips the switch.*\n\nbistability gives cells **memory**: once a cell commits to the high state, it remains there even after the inducing signal is removed. the positive feedback maintains itself.\n\n## the genetic toggle switch\n\nin a landmark experiment, gardner, cantor, and collins (2000) built a synthetic bistable circuit from two mutually repressing genes:\n\n$$\n\\frac{du}{dt} = \\frac{\\alpha_1}{1 + v^n} - u, \\qquad \\frac{dv}{dt} = \\frac{\\alpha_2}{1 + u^n} - v.\n$$\n\n> *gene $u$ represses gene $v$, and gene $v$ represses gene $u$. it is a molecular arm-wrestling match. one of them wins, and the loser stays repressed.*\n\nthis toggle switch has two stable steady states (high-$u$/low-$v$ and low-$u$/high-$v$) separated by an unstable saddle point. you can flip it between states by hitting it with a transient pulse of inducer \u2014 like flipping a light switch by briefly pushing it.\n\n## oscillations: the repressilator\n\nelowitz and leibler (2000) asked a daring question: can you build a biological clock from scratch? they constructed a synthetic oscillator \u2014 the **repressilator** \u2014 from three genes arranged in a ring of repression: $a$ represses $b$, $b$ represses $c$, $c$ represses $a$.\n\n$$\n\\frac{dm_i}{dt} = \\frac{\\alpha}{1 + p_j^n} + \\alpha_0 - m_i, \\qquad \\frac{dp_i}{dt} = \\beta(m_i - p_i),\n$$\n\nwhere $j$ is the upstream repressor of gene $i$.\n\nthink of it as **three friends who can never all be happy at once**. when $a$ is high, it represses $b$, so $b$ is low. but with $b$ low, $c$ is free to rise. when $c$ gets high enough, it represses $a$, which releases $b$, and the cycle continues. the system chases its own tail forever.\n\n> *here is the deep principle: negative feedback loops arranged in odd-numbered rings can generate oscillations. two mutual repressors give a switch (even number = stable). thre"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "gene-expression-noise",
      "lessonTitle": "Quantifying Noise in Gene Expression",
      "x": 0.5671692490577698,
      "y": 0.07526843249797821,
      "searchText": "quantifying noise in gene expression\n# quantifying noise in gene expression\n\n## where we are headed\n\nin the last two lessons you learned to write differential equations for mrna and protein, and you saw that the system settles into a nice, clean steady state. beautiful. but here is the twist: that steady state is a *lie*. or rather, it is the average of something much messier. inside a real cell, molecules are made and destroyed one at a time by random collisions, and when the numbers are small \u2014 tens or hundreds of molecules, not trillions \u2014 the randomness matters enormously. today we find out just how noisy gene expression really is, and why that noise is not just a nuisance but a tool that cells use to make life-or-death decisions.\n\n## the central dogma, revisited\n\nyou already know the central dogma:\n\n$$\n\\text{dna} \\xrightarrow{\\text{transcription}} \\text{mrna} \\xrightarrow{\\text{translation}} \\text{protein}\n$$\n\ntwo molecular machines drive this process: **rna polymerase** transcribes dna into mrna, and the **ribosome** translates mrna into protein. in our deterministic model, these machines hum along at constant rates. but in reality, every molecular encounter \u2014 polymerase finding the promoter, ribosome latching onto the mrna \u2014 is a random event driven by diffusion. the molecules are doing a drunken walk through the cytoplasm, and sometimes they find their target quickly, sometimes slowly.\n\n## measuring the noise\n\nto see noise in action, biologists fuse a **reporter protein** like gfp (green fluorescent protein) to a gene of interest. the brighter the cell glows, the more protein it has.\n\n- **bulk measurement** (a tube full of billions of cells): you see the population average. smooth. boring.\n- **single-cell measurement** (microscopy or flow cytometry): you see the full distribution. and it is *wide*. genetically identical cells, grown in the same conditions, can have two-fold or even ten-fold differences in protein level.\n\nthis is not measurement error. it is real, biological noise.\n\n## why is gene expression noisy?\n\nthe fundamental reason is **small numbers**. a typical *e. coli* cell has one or two copies of each gene, perhaps a handful of mrna molecules, and maybe a few hundred to a few thousand proteins from each gene. when you are working with numbers this small, random fluctuations are unavoidable \u2014 it is the same reason that flipping a coin 10 times gives a much noisier fraction of heads than flipping it 10,000 times.\n\nthis noise has real biological consequences:\n\n- **competence in *b. subtilis***: noisy expression of the master regulator *comk* causes a small fraction of cells to stochastically flip into a state where they can take up dna from the environment \u2014 a survival strategy under stress.\n- **persister cells**: in a genetically uniform bacterial population, noise creates rare cells with low metabolic activity that survive antibiotic treatment, even without resistance mutations.\n\n## quantifying noise: the coefficient of variation\n\nthe **coefficient of variation** (cv) measures noise as the ratio of the standard deviation to the mean:\n\n$$\n\\eta = \\frac{\\sigma}{\\langle n \\rangle} = \\frac{\\sqrt{\\langle (n - \\langle n \\rangle)^2 \\rangle}}{\\langle n \\rangle},\n$$\n\nwhere $n$ is the protein copy number in a single cell, and the angle brackets denote the average over the population.\n\n> *in words: the cv tells you how wide the distribution is relative to its center. a cv of 0.5 means the typical fluctuation is about half the mean \u2014 that is very noisy.*\n\n## decomposing noise: the elowitz experiment\n\nhere is one of the most clever experiments in modern biology. in 2002, michael elowitz and colleagues asked: where does the noise come from? is it because individual gene copies fire randomly (**intrinsic noise**), or because the whole cell's environment fluctuates \u2014 varying numbers of ribosomes, polymerases, and metabolites from cell to cell (**extrinsic noise**)?\n\nthe trick: put two *different-colored* fluorescent reporters (cfp and yfp) under the control of *identical* promoters in the same cell. if the noise is all extrinsic (upstream fluctuations shared by both genes), the two colors go up and down together \u2014 every cell lands on the diagonal of a cfp-vs-yfp plot. if the noise is intrinsic (independent random firing), the colors fluctuate independently \u2014 cells scatter off the diagonal.\n\nthe math makes this precise. imagine two glasses of water, one blue and one red, and you are trying to figure out why the water levels fluctuate. one glass might be noisy because the faucet drips unpredictably (intrinsic \u2014 each glass has its own random drip). the other source of variation is that some days you leave the tap on longer (extrinsic \u2014 both glasses get more or less water together).\n\n**extrinsic noise** (correlated fluctuations):\n\n$$\n\\eta_{\\mathrm{ext}}^2 = \\frac{\\langle n^{(1)} n^{(2)} \\rangle - \\langle n^{(1)} \\rangle \\langle n^{(2)} \\rangle}{\\langle n^{(1)} \\rangle \\langle n^{(2)} \\rangle}.\n$$\n\n**intrinsic noise** (uncorrelated fluctuations):\n\n$$\n\\eta_{\\mathrm{int}}^2 = \\frac{\\langle (n^{(1)} - n^{(2)})^2 \\rangle}{2\\,\\langle n^{(1)} \\rangle \\langle n^{(2)} \\rangle}.\n$$\n\nand the total noise decomposes exactly:\n\n$$\n\\eta_{\\mathrm{total}}^2 = \\eta_{\\mathrm{int}}^2 + \\eta_{\\mathrm{ext}}^2.\n$$\n\n> *the beauty of this decomposition is that you can measure it directly from the two-color data, without knowing anything about the underlying mechanism.*\n\n[[simulation gene-expression-noise]]\n\n## the fano factor\n\nan alternative way to characterize noise is the **fano factor**:\n\n$$\nf = \\frac{\\text{var}[n]}{\\langle n \\rangle}.\n$$\n\n> *for a poisson process (completely random, independent events), $f = 1$. if $f > 1$, the noise is \"burstier\" than poisson \u2014 transcription happens in bursts, with several mrnas made in quick succession followed by silence. if $f < 1$, the noise is suppressed below poisson, which can happen when negative feedback clamps down on fluctuations.*\n\nthe fano factor is a fingerprint:"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "gene-regulatory-networks",
      "lessonTitle": "Gene Regulatory Networks",
      "x": 0.6664794683456421,
      "y": 0.07360156625509262,
      "searchText": "gene regulatory networks\n# gene regulatory networks\n\n## where we are headed\n\nover the past several lessons you have built up a toolkit: differential equations, the hill function, feedback loops, and signaling. now we step back and look at the big picture. inside every cell, hundreds of genes regulate each other in a vast interconnected **network**. how do we make sense of something so complex? the answer, it turns out, is that these networks are not random tangles \u2014 they are built from a small set of recurring patterns called **motifs**. these are the lego bricks that nature uses over and over, and once you learn to recognize them, you can read a regulatory network the way an engineer reads a circuit diagram.\n\n## types of regulation\n\nevery regulatory interaction falls into one of two categories:\n\n- **positive regulation**: gene a activates gene b, symbolized as $a \\rightarrow b$. more of a means more of b.\n- **negative regulation**: gene a represses gene b, symbolized as $a \\dashv b$. more of a means less of b.\n\nby combining these two simple interaction types, cells build circuits of extraordinary sophistication.\n\n## from small genomes to complex regulation\n\nas genome size increases, the fraction of genes devoted to regulation increases *faster* than the total number of genes. a simple bacterium might dedicate 5% of its genes to regulation; a complex organism dedicates far more. regulation is expensive, but it pays for itself: the more complex the environment, the more a cell needs to coordinate its response.\n\n## networks are built from motifs\n\neven the fruit fly *drosophila* has a regulatory network with thousands of genes and tens of thousands of interactions. staring at the whole thing is hopeless. but here is the key insight from uri alon and others: certain small subpatterns \u2014 **network motifs** \u2014 appear far more often than you would expect by chance. evolution has discovered that these motifs are *useful*, and keeps reusing them.\n\nthe major motifs are:\n\n### positive and negative feedback loops\n\nyou already know these from the feedback lesson:\n- **positive feedback** ($a \\leftrightarrow b$, or $a$ activating itself): creates bistability and memory.\n- **negative feedback** ($a \\rightarrow b \\dashv a$): creates homeostasis and oscillations.\n\n### feed-forward loops\n\na feed-forward loop has three genes: $a$ regulates both $b$ and $c$, and $b$ also regulates $c$. the signal reaches $c$ through two paths \u2014 one direct and one indirect:\n\n$$\na \\rightarrow b \\rightarrow c, \\qquad a \\rightarrow c.\n$$\n\n> *think of it as a boss ($a$) who tells both a manager ($b$) and a worker ($c$) what to do. the worker hears from the boss directly, but also gets instructions through the manager. the delay through the manager creates interesting temporal filtering.*\n\nfeed-forward loops can implement **and logic** (both signals needed to turn on $c$) or **or logic** (either signal suffices), depending on the signs of the interactions:\n\n- **and logic**: the *ara* genes in *e. coli* require both crp and arac as activators. the network $\\mathrm{crp} \\rightarrow \\mathrm{arac} \\rightarrow ara$ and $\\mathrm{crp} \\rightarrow ara$ ensures the genes only turn on when both signals are present.\n- **or logic**: flagellar genes in bacteria can be activated through either of two regulatory paths.\n\n### single input modules\n\na single transcription factor controls an entire group of genes:\n\n$$\na \\rightarrow b, \\quad a \\rightarrow c, \\quad a \\rightarrow d.\n$$\n\n> *one master switch controls a whole program. this is how bacteria coordinate the expression of many genes at once \u2014 for example, turning on all the genes needed for flagellar assembly in response to a single signal.*\n\n## identifying feedback sign: the multiplication rule\n\ngiven a closed loop, you can determine whether it is positive or negative feedback by a simple trick. assign $+1$ to each activation arrow ($\\rightarrow$) and $-1$ to each repression arrow ($\\dashv$), then multiply around the loop. for example:\n\n$$\na \\rightarrow b \\rightarrow c \\dashv a\n$$\n\ngives $(+1)(+1)(-1) = -1$: **negative feedback loop**.\n\n> *this is the same principle we saw with the repressilator: an odd number of repressions in a loop makes it negative (oscillatory), while an even number makes it positive (bistable).*\n\n## simplifying the equations\n\nin practice, transcription and translation are two separate steps, and we have been writing equations for both. but mathematically, when mrna dynamics are much faster than protein dynamics, we can approximate the system with a single equation for protein:\n\n$$\n\\frac{\\mathrm{d}p}{\\mathrm{d}t} = \\frac{\\alpha (p/k)^h}{1+(p/k)^h} - \\gamma_\\mathrm{p} p,\n$$\n\nwhere $\\alpha$ absorbs both the transcription and translation rates.\n\n> *this simplification is not cheating \u2014 it is the same separation of timescales we discussed in the transcription-translation lesson. the mrna adjusts so quickly that it effectively slaves to the protein level.*\n\n## graphical analysis of network motifs\n\nto see the typical behavior of a motif, we can set most parameters to 1 and study the dimensionless equation:\n\n$$\n\\frac{\\mathrm{d}p}{\\mathrm{d}t} = \\frac{p^h}{1+p^h} - \\gamma_\\mathrm{p} p.\n$$\n\nplot $y = p^h/(1+p^h)$ and $y = \\gamma_\\mathrm{p} p$ on the same axes. the intersections are steady states, and the number of intersections tells you whether the system is monostable or bistable.\n\nfor negative regulation, the equation becomes:\n\n$$\n\\frac{\\mathrm{d}p}{\\mathrm{d}t} = \\frac{1}{1+p^h} - \\gamma_\\mathrm{p} p.\n$$\n\n> *with negative regulation the production curve always decreases while the degradation line increases \u2014 they cross exactly once, giving a single stable steady state. with positive regulation the s-shaped production curve can cross the line up to three times, giving bistability.*\n\n[[simulation hill-function]]\n\n## biological examples revisited\n\nnow that you can recognize network motifs, look at the biological systems we have encountered throughout this course:\n\n- **phage lambda"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "home",
      "lessonTitle": "Dynamical Models in Molecular Biology",
      "x": 0.609792947769165,
      "y": 0.12893706560134888,
      "searchText": "dynamical models in molecular biology\n# dynamical models in molecular biology\n\n## why this course exists\n\na single bacterium is not just a bag of chemicals. it is a tiny machine that can sense its environment, remember what happened a minute ago, make decisions, and grow by allocating its limited resources with astonishing precision. understanding *how* it does all of this \u2014 not as a list of pathways to memorize, but as a set of physical principles you can write down, simulate, and predict \u2014 is one of the most exciting things in modern science.\n\nthis course gives you the tools to build that understanding. we start with the simplest possible differential equation, and by the end you will be modeling feedback switches, noisy gene expression, bacterial chemotaxis, and the resource-allocation strategies that let a cell double itself every twenty minutes. along the way, you will see that nature is sneaky, elegant, and surprisingly mathematical.\n\n## what you will learn\n\n- **physics provides the framework**: differential equations, stochastic processes, and steady-state analysis.\n- **biology provides the systems**: genes, proteins, regulatory circuits, and growing cells.\n- **mathematics provides the rigor**: exact solutions, stability analysis, and parameter estimation.\n\nwe focus on simple systems \u2014 often bacterial \u2014 but the principles apply broadly across all of biology.\n\n## key mathematical ideas\n\n- ordinary differential equations for production and degradation kinetics.\n- the hill function as a universal model for cooperative binding.\n- stochastic simulation (gillespie algorithm) for single-cell noise.\n- fixed-point analysis and bifurcation diagrams for feedback circuits.\n- resource allocation models for bacterial growth.\n\n## prerequisites\n\n- basic calculus (derivatives and integrals).\n- introductory probability and statistics.\n- familiarity with basic molecular biology (dna, rna, protein).\n- no advanced mathematics is required; differential equations are introduced from first principles.\n\n## learning trajectory\n\nthe course is organized to build your intuition layer by layer, from molecules to systems:\n\n1. **differential equations** \u2014 the language of change. production, degradation, and the steady state that every living system finds.\n2. **transcription and translation** \u2014 applying those equations to the central dogma: mrna and protein as two coupled timescales.\n3. **gene expression noise** \u2014 now watch what happens when the world is noisy! intrinsic and extrinsic fluctuations, the fano factor, and stochastic simulation.\n4. **mutational analysis** \u2014 the statistics of rare events: how proofreading, mutations, and probability connect to everything we just learned about noise.\n5. **transcriptional regulation** \u2014 the hill function: how cooperative binding turns a gentle dose-response curve into an all-or-nothing switch.\n6. **feedback loops** \u2014 switches, oscillators, and memory. what happens when a gene product talks back to its own promoter.\n7. **signal transduction** \u2014 how cells read their mail: chemotaxis, adaptation, and cell-to-cell communication.\n8. **gene regulatory networks** \u2014 the lego bricks of regulation: motifs, feed-forward loops, and network architecture.\n9. **bacterial growth** \u2014 the grand finale: growth laws, ribosome regulation, ppgpp, and how a single cell turns physics into life.\n\n## recommended reading\n\n- phillips et al., *physical biology of the cell*.\n- alon, *an introduction to systems biology*.\n- weekly scientific articles from the quantitative biology literature.\n\n## cast of characters\n\nyou will meet these molecules and systems throughout the course:\n\n- **laci** \u2014 the classic repressor of the *lac* operon in *e. coli*.\n- **gfp / cfp / yfp** \u2014 fluorescent reporter proteins for measuring gene expression.\n- **chey, chea, cher, cheb** \u2014 the signaling proteins of bacterial chemotaxis.\n- **ci and cro** \u2014 the dueling regulators of phage lambda's life-or-death decision.\n- **ppgpp** \u2014 the \"alarmone\" that tells the cell to stop making ribosomes when nutrients run low.\n- **comk** \u2014 the master regulator of competence in *b. subtilis*, driven by noisy positive feedback.\n\n## visual and simulation gallery\n\n[[simulation binomial-poisson-comparison]]\n"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "mutational-analysis",
      "lessonTitle": "Probability for Mutational Analysis",
      "x": 0.5693705677986145,
      "y": 0.09139478206634521,
      "searchText": "probability for mutational analysis\n# probability for mutational analysis\n\n## where we are headed\n\nlast time we saw that gene expression is noisy \u2014 molecules appear and disappear randomly, and the statistics of those random events (poisson, fano factor) tell us something deep about how cells work. today we zoom in on another source of randomness: **mutations**. dna polymerase is one of the most astonishing proofreaders in nature, making fewer than one error per billion bases copied. how does it achieve that? and when errors *do* slip through, what does the statistics of their appearance look like? the probability distributions we develop here \u2014 binomial and poisson \u2014 are the same ones that underpin everything we said about noise, so this lesson ties directly to what we just learned.\n\n## what causes mutations?\n\nthere are two broad categories:\n\n**spontaneous mutations** arise from chemical mistakes during dna replication. even under perfect conditions, about 5,000 potentially mutagenic events happen per day in a human cell:\n- **depurination**: an adenine or guanine base simply falls off the sugar backbone.\n- **deamination**: cytosine spontaneously converts to uracil.\n\n**induced mutations** are caused by external agents \u2014 uv light, ionizing radiation, or chemical mutagens. these account for roughly 100 additional dna lesions per day in a human cell.\n\n## the fidelity of the central dogma\n\nnot every step in gene expression is equally accurate. the error rates tell a striking story:\n\n- **dna replication**: $\\sim 1$ error per $10^9$ bases \u2014 almost unbelievably precise.\n- **transcription**: $\\sim 1$ error per $10^6$ bases \u2014 good, but a thousand times worse.\n- **translation**: $\\sim 1$ error per $10^4$ bases \u2014 the least accurate step.\n\n> *think about what this means: evolution has invested enormous machinery into keeping dna replication faithful, because errors there are permanent and heritable. errors in transcription and translation are temporary \u2014 the bad mrna or protein will be degraded soon enough.*\n\n## how does dna polymerase proofread so well?\n\nimagine a typist who makes a mistake every few hundred keystrokes. that is roughly the error rate of base-pairing alone \u2014 the free energy difference between a correct and incorrect base pair (4--13 kj/mol) is only enough to give an error rate of about $10^{-2}$. so how does the cell get from $10^{-2}$ down to $10^{-9}$?\n\nthe answer is **two extra layers of proofreading**, each multiplying the fidelity:\n\n**editing by dna polymerase** \u2014 dna polymerase does not just add bases; it also checks the last base it added. if the fit is wrong, the polymerase backs up and removes it. this is like a typist who pauses after every keystroke to check and erase mistakes before moving on.\n\n**strand-directed mismatch repair** \u2014 after replication, dedicated repair enzymes scan the new dna strand for mismatches. they can tell which strand is new (in bacteria, by its methylation pattern) and recruit dna polymerase to fix the error. this is like a second proofreader who reads the whole manuscript after the typist is done.\n\neach layer improves fidelity by roughly a factor of 100--1000, and together they achieve the extraordinary $10^{-9}$ error rate.\n\n## recombination\n\nduring meiosis (in sexually reproducing organisms), chromosomes exchange segments through **crossing-over**. this is not an error \u2014 it is a deliberate reshuffling that generates genetic diversity, the raw material for evolution.\n\n## working with mutants\n\nbacteria are a geneticist's dream: you can grow a billion of them overnight in a single milliliter. to find mutants in this ocean of cells, we use:\n\n- **genetic selection**: design conditions where only mutants survive. for example, grow bacteria on a plate containing an antibiotic \u2014 only resistant mutants form colonies.\n- **genetic screens**: check every colony individually for the phenotype of interest. more labor-intensive, but necessary when there is no way to select directly.\n\n## the statistics of rare events\n\nnow suppose you watch cells divide and count how many carry a new mutation. each cell division is an independent \"trial\" with a tiny probability $p$ of producing a mutant. after $n$ divisions, how many mutants do you expect? this is exactly the setting for the **binomial distribution**.\n\n## the binomial distribution\n\nif each of $n$ independent trials has probability $p$ of success, the probability of getting exactly $k$ successes is:\n\n$$\np_n(k) = \\binom{n}{k} p^k (1-p)^{n-k}.\n$$\n\n> *in words: choose which $k$ trials are the \"successes\" (the binomial coefficient), multiply by the probability of $k$ successes and $n-k$ failures.*\n\nthe mean is $\\mu = np$ and the variance is $\\sigma^2 = np(1-p)$.\n\n## the poisson limit\n\nin biology, $n$ is typically enormous (millions of cell divisions) and $p$ is tiny (one error per billion bases), but the product $m = np$ is a modest number. in this limit, the binomial distribution simplifies beautifully to the **poisson distribution**:\n\n$$\np_m(k) = \\frac{m^k}{k!} e^{-m}.\n$$\n\n> *the poisson distribution has a remarkable property: its mean and variance are both equal to $m$. this is exactly where the fano factor $f = 1$ comes from \u2014 a poisson process has $f = \\text{var}/\\text{mean} = 1$.*\n\nthis connects directly to what we learned about noise: if gene expression events (transcription, translation) are independent poisson events, the fano factor is 1. when we see $f > 1$, something more interesting is going on \u2014 bursts, correlations, or feedback.\n\n[[simulation binomial-distribution]]\n\n[[simulation poisson-distribution]]\n\n## the luria-delbruck experiment: jackpot cultures\n\none of the most beautiful experiments in genetics was performed by salvador luria and max delbruck in 1943. they asked a seemingly simple question: do mutations in bacteria arise *before* or *after* exposure to a selective agent (a virus called phage t1)?\n\nif mutations arise *after* exposure (directed by the selection), you would expect each independen"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "signal-transduction",
      "lessonTitle": "Signal Transduction",
      "x": 0.6451076865196228,
      "y": 0.11369074881076813,
      "searchText": "signal transduction\n# signal transduction\n\n## where we are headed\n\ncells are not just floating in soup \u2014 they are constantly getting mail. chemical signals arrive at the cell surface (food, toxins, hormones, messages from neighbors), and the cell has to read the envelope, decide what to do, and respond. last time we saw how feedback loops create switches and oscillators *inside* the cell. today we zoom out and ask: how does the cell sense what is happening *outside*? the answer \u2014 **signal transduction** \u2014 involves some of the most elegant engineering in all of biology, including a sensory system in bacteria that is sensitive enough to detect a single molecule change per cell volume per micron.\n\n## what is a signal?\n\na signal is anything the cell can detect:\n- **chemical**: food molecules, quorum-sensing signals, hormones, ions, gases.\n- **physical**: pressure, temperature, light.\n\nthe signal binds to a specific **receptor** on the cell surface, triggering a cascade of events inside the cell. this cascade is signal transduction.\n\n## why so many steps?\n\nyou might ask: why not just have the signal directly flip a gene on or off? why the elaborate chain of intermediate molecules? because intermediate steps give the cell superpowers:\n\n- **amplification**: a single receptor event can activate thousands of downstream molecules.\n- **noise filtering**: intermediate steps smooth out random fluctuations.\n- **integration**: the cell can combine information from multiple signals.\n- **memory**: some signaling states persist after the signal is gone.\n- **adaptation**: the cell can reset its sensitivity, responding to *changes* rather than absolute levels.\n\nvery often, these intermediate steps involve a chain of **phosphorylation** events \u2014 one protein adds a phosphate group to the next, passing the signal along like a bucket brigade.\n\n## bacterial chemotaxis: nature's gps\n\nbacterial chemotaxis is one of the best-understood signaling systems in all of biology, and it is breathtaking. *e. coli* can detect the tiniest gradient of food \u2014 a concentration change of just one molecule per cell volume per micron \u2014 and steer toward it. it can do this across five orders of magnitude of background concentration. and it does all of this while being buffeted by brownian motion, with no brain and no map.\n\n### how bacteria swim\n\nsuppose you are a bacterium. you are about 2 micrometers long, and you have several flagella \u2014 helical propellers driven by molecular motors embedded in your membrane.\n\n- when the motors spin **counterclockwise**, the flagella bundle together into a single propeller and you swim in a straight line. this is a **run**.\n- when one or more motors switch to **clockwise**, the bundle flies apart and you tumble randomly, reorienting in a new direction. this is a **tumble**.\n\na run lasts about 1 second, then you tumble and pick a new direction at random. on average, you go nowhere \u2014 it is a random walk. but here is the trick nature discovered: **if things are getting better (more food), you run longer. if things are getting worse, you tumble sooner.** this bias turns a random walk into a directed climb up the food gradient.\n\n### the molecular circuit\n\nthe switch between running and tumbling is controlled by a small signaling network:\n\n- **chea** is a kinase (an enzyme that adds phosphate groups) associated with the receptor. when the receptor detects *less* attractant, chea becomes more active.\n- **chey** receives a phosphate from chea. phosphorylated chey (chey-p) binds to the flagellar motor and makes it switch to clockwise rotation (tumble).\n\n> *so the logic is: less food \u2192 more chea activity \u2192 more chey-p \u2192 more tumbling. more food \u2192 less chea activity \u2192 less chey-p \u2192 longer runs toward the food. simple and beautiful.*\n\n### the problem of saturation\n\nbut wait \u2014 if tumbling frequency depends on the *absolute* concentration of attractant, the system will saturate quickly. at high background concentrations, the receptors are all occupied regardless of the gradient. the bacterium goes blind. if tumbling frequency ignores concentration entirely, the bacterium cannot detect any gradient at all. how does nature solve this?\n\n### adaptation: the receptor's memory\n\nthe answer is **adaptation**, and it is one of the most beautiful ideas in molecular biology. the cell does not respond to the absolute concentration \u2014 it responds to *changes* in concentration. after a step increase in attractant, the tumbling frequency drops immediately (the cell runs), but then gradually returns to its baseline over about a minute. the cell has adapted.\n\nthink of the receptor as a little spring. when attractant binds, the spring relaxes (signal goes down). but then the enzyme **cher** slowly adds methyl groups to the receptor, stiffening the spring back to its original tension. this **methylation** is the receptor's memory \u2014 it records what the \"normal\" level of attractant is and resets the response.\n\nthe enzyme **cheb** (which removes methyl groups) provides the opposing force, and the balance between cher and cheb sets the adaptation level.\n\nthe equation for methylation dynamics is:\n\n$$\n\\frac{\\mathrm{d} [e_m]}{\\mathrm{d} t} = k^r [\\mathrm{cher}] - \\frac{k^b b^{*}(l) \\, e_m}{k_m^b + e_m}.\n$$\n\n> *the first term says cher adds methyl groups at a constant rate. the second term says cheb removes them at a rate that depends on receptor activity $b^*(l)$, which in turn depends on the ligand concentration $l$. the balance of these two processes is what gives the cell its remarkable ability to adapt.*\n\nwith adaptation, the bacterium can be sensitive to tiny changes in concentration at *any* background level \u2014 from nanomolar to millimolar. it is like having a camera that automatically adjusts its exposure, always keeping the image sharp.\n\n## signal transduction in space: morphogen gradients\n\nsignal transduction is not only about temporal sensing. during the development of a fruit fly embryo, **morphogen gradients** establish spatial p"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "transcription-translation",
      "lessonTitle": "Differential Equations for Transcription and Translation",
      "x": 0.6152263879776001,
      "y": 0.03166189044713974,
      "searchText": "differential equations for transcription and translation\n# differential equations for transcription and translation\n\n## where we are headed\n\nlast time we saw how a single molecule type reaches a steady state through the balance of production and degradation. today we apply that same idea to the two-step process at the heart of every cell: dna is transcribed into mrna, and mrna is translated into protein. the beautiful thing is that these two steps operate on very different timescales \u2014 and that difference has profound consequences for how cells respond to change.\n\n## two timescales, one system\n\nimagine tuning a radio in a noisy room. the raw signal from the antenna jitters up and down rapidly \u2014 that is like mrna, which is made and destroyed on a timescale of minutes in a bacterium. but what you actually hear through the speaker is smoothed out, because the speaker (like a capacitor in the circuit) averages over those rapid fluctuations. that smooth output is like the protein level: it changes slowly, following the mrna signal with a delay.\n\nthis two-timescale structure is not a bug \u2014 it is a feature. it means protein levels are naturally buffered against rapid noise in transcription. nature builds its own low-pass filter.\n\n## the equations\n\nfor **transcription**, mrna is produced at rate $k_\\mathrm{m}$ and degraded at rate $\\gamma_\\mathrm{m}$:\n\n$$\n\\frac{\\mathrm{d} n_\\mathrm{m}(t)}{\\mathrm{d} t} = k_\\mathrm{m} - \\gamma_\\mathrm{m} \\, n_\\mathrm{m}(t).\n$$\n\n> *this is exactly the bathtub equation from last time, applied to mrna.*\n\nfor **translation**, protein is produced at a rate proportional to the number of mrna molecules (more mrna means more ribosomes translating) and degraded at rate $\\gamma_\\mathrm{p}$:\n\n$$\n\\frac{\\mathrm{d} n_\\mathrm{p}(t)}{\\mathrm{d} t} = k_\\mathrm{p} \\, n_\\mathrm{m}(t) - \\gamma_\\mathrm{p} \\, n_\\mathrm{p}(t).\n$$\n\n> *notice the key difference: the production of protein depends on how much mrna is present. these two equations are coupled.*\n\nthe parameters:\n- $n_\\mathrm{m}(t)$: number of mrna molecules at time $t$\n- $n_\\mathrm{p}(t)$: number of protein molecules at time $t$\n- $k_\\mathrm{m}$: transcription rate (mrna molecules per unit time)\n- $k_\\mathrm{p}$: translation rate (proteins per mrna per unit time)\n- $\\gamma_\\mathrm{m}$: mrna degradation rate\n- $\\gamma_\\mathrm{p}$: protein degradation rate (often dominated by dilution through cell growth)\n\n## steady state\n\nat steady state, mrna reaches $n_\\mathrm{m}^\\mathrm{ss} = k_\\mathrm{m} / \\gamma_\\mathrm{m}$, just as before. plugging this into the protein equation gives:\n\n$$\nn_\\mathrm{p}^\\mathrm{ss} = \\frac{k_\\mathrm{p} \\, k_\\mathrm{m}}{\\gamma_\\mathrm{p} \\, \\gamma_\\mathrm{m}}.\n$$\n\n> *the steady-state protein level depends on all four parameters: both production rates and both degradation rates. change any one, and the protein level shifts.*\n\n## number of molecules versus concentration\n\nso far we have been counting molecules. but experimentally (and theoretically) it is often more convenient to work with **concentrations** \u2014 number per volume. if the cell has volume $v$, we define $c_\\mathrm{m}(t) = n_\\mathrm{m}(t)/v$ and $c_\\mathrm{p}(t) = n_\\mathrm{p}(t)/v$. the equations keep the same form:\n\n$$\n\\frac{\\mathrm{d} c_\\mathrm{m}(t)}{\\mathrm{d} t} = k_\\mathrm{m} - \\gamma_\\mathrm{m} \\, c_\\mathrm{m}(t),\n$$\n\n$$\n\\frac{\\mathrm{d} c_\\mathrm{p}(t)}{\\mathrm{d} t} = k_\\mathrm{p} \\, c_\\mathrm{m}(t) - \\gamma_\\mathrm{p} \\, c_\\mathrm{p}(t).\n$$\n\n> *the switch from number to concentration is just a rescaling \u2014 the physics does not change. but be careful: when cells grow and divide, the volume changes, and dilution acts like an extra degradation term.*\n\n## why does nature do it this way?\n\nwhy have two separate steps (transcription and translation) instead of going directly from dna to protein? one reason is **amplification**: each mrna molecule can be translated many times, so a single gene can produce thousands of protein copies. another reason is **control**: the cell can regulate gene expression at either step independently \u2014 adjusting how much mrna is made, how fast it is degraded, or how efficiently it is translated. two knobs are better than one.\n\n## check your understanding\n\n- if mrna degradation becomes much faster ($\\gamma_\\mathrm{m} \\gg \\gamma_\\mathrm{p}$), what happens to the protein response time? why?\n- a gene is transcribed 10 times per minute and each mrna is translated 20 times before it is degraded. roughly how many proteins does the cell make from this gene per minute?\n- why is protein degradation in bacteria often dominated by dilution (cell division) rather than active degradation?\n\n## big ideas\n\n- **transcription and translation form a coupled two-step system**, each described by a bathtub equation.\n- **mrna is fast and jittery, protein is slow and smooth** \u2014 the two-timescale structure acts as a natural noise filter.\n- **steady-state protein level** = $(k_\\mathrm{p} \\, k_\\mathrm{m}) / (\\gamma_\\mathrm{p} \\, \\gamma_\\mathrm{m})$ \u2014 a product of all four rate parameters.\n"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "transcriptional-regulation",
      "lessonTitle": "Transcriptional Regulation and the Hill Function",
      "x": 0.668901801109314,
      "y": 0.026557790115475655,
      "searchText": "transcriptional regulation and the hill function\n# transcriptional regulation and the hill function\n\n## where we are headed\n\nso far we have treated the transcription rate as a constant \u2014 the gene is either on or off, and we do not ask what controls it. but in a real cell, genes are regulated. a repressor protein sits on the promoter and blocks transcription; an activator protein recruits rna polymerase and turns it on. today we derive the mathematical function that describes this regulation \u2014 the **hill function** \u2014 and you will see how nature turns a gentle, graded response into a sharp, switch-like one. this is one of the most important functions in all of quantitative biology.\n\n## the committee picture\n\nbefore any math, think about it this way. suppose a gene's promoter is like a conference room, and the gene only turns on when the room is occupied by a transcription factor. if only *one* molecule needs to sit down, then the gene turns on gradually as the concentration of that molecule increases \u2014 some rooms are occupied, some are not, and the fraction increases smoothly.\n\nbut now suppose the promoter requires a *committee*: four transcription factor molecules must all be present at the same time to turn the gene on. at low concentrations, the chance of all four showing up simultaneously is tiny. at high concentrations, it is almost certain. the transition from \"almost never\" to \"almost always\" happens over a narrow range of concentration. you have turned a gentle slope into a switch.\n\nthat is the hill function. the hill coefficient $n$ is the size of the committee.\n\n## deriving the hill function from binding kinetics\n\n### a single transcription factor\n\nconsider a transcription factor (tf) that binds to and represses a promoter. the promoter can be in two states: **free** (gene on) or **occupied** (gene off). the binding rate depends on tf concentration, but the unbinding rate does not:\n\n$$\n\\frac{\\mathrm{d} p_\\mathrm{free}}{\\mathrm{d} t} = - v_\\mathrm{bind} \\, c_\\mathrm{p} \\, p_\\mathrm{free} + k_\\mathrm{unbind} \\, (1 - p_\\mathrm{free}).\n$$\n\n> *in words: free promoters get occupied at a rate proportional to tf concentration, and occupied promoters become free at a constant unbinding rate.*\n\nwe assume binding and unbinding are much faster than transcription itself, so the promoter reaches a quasi-steady state. setting the derivative to zero:\n\n$$\np_\\mathrm{free} = \\frac{1}{1 + c_\\mathrm{p}/k}, \\qquad \\text{where } k = \\frac{k_\\mathrm{unbind}}{v_\\mathrm{bind}}.\n$$\n\n> *$k$ is the **dissociation constant** \u2014 the concentration at which the promoter is occupied half the time. when $c_\\mathrm{p} = k$, you get $p_\\mathrm{free} = 1/2$. below $k$, the gene is mostly on; above $k$, the gene is mostly off.*\n\n### dimers: the committee of two\n\nnow suppose the transcription factor must form a **dimer** (a pair) before it can bind the promoter. the binding rate becomes proportional to $c_\\mathrm{p}^2$ instead of $c_\\mathrm{p}$:\n\n$$\np_\\mathrm{free} = \\frac{1}{1 + (c_\\mathrm{p}/k_2)^2}.\n$$\n\n> *the exponent of 2 makes the transition from \"on\" to \"off\" steeper. the gene does not care about individual molecules \u2014 it waits until the concentration is high enough that pairs are common.*\n\n### the general hill function\n\nfor a complex of $n$ molecules (hill coefficient $n$), the pattern generalizes:\n\n$$\np_\\mathrm{free} = \\frac{1}{1 + (c_\\mathrm{p}/k)^n}.\n$$\n\nplay with this in your head:\n- **$n = 1$**: a gentle, hyperbolic curve. the gene gradually turns off as repressor concentration increases.\n- **$n = 2$**: steeper. the transition sharpens.\n- **$n = 4$**: very steep. the gene is either fully on or fully off, with a narrow transition zone around $c_\\mathrm{p} = k$.\n\n> *look how nature turns a gentle slope into a switch! the higher the hill coefficient, the more \"all-or-nothing\" the response. this is the mathematical heart of biological decision-making.*\n\n[[simulation hill-function]]\n\n## repression\n\nfor a self-repressing gene, the transcription rate depends on the repressor (protein) concentration through the hill function:\n\n$$\n\\frac{\\mathrm{d} c_\\mathrm{m}}{\\mathrm{d} t} = \\frac{\\alpha_\\mathrm{m}}{1 + (c_\\mathrm{p}/k)^n} - \\gamma_\\mathrm{m} \\, c_\\mathrm{m},\n$$\n\n$$\n\\frac{\\mathrm{d} c_\\mathrm{p}}{\\mathrm{d} t} = k_\\mathrm{p} \\, c_\\mathrm{m} - \\gamma_\\mathrm{p} \\, c_\\mathrm{p}.\n$$\n\n> *when protein is scarce ($c_\\mathrm{p} \\ll k$), the hill function is close to 1 and mrna is produced at full rate $\\alpha_\\mathrm{m}$. when protein is abundant ($c_\\mathrm{p} \\gg k$), the hill function drops to nearly zero and transcription is shut off.*\n\nmore generally, if the repressor is a different protein (not the gene's own product), we replace $c_\\mathrm{p}$ with the concentration of the regulating transcription factor $c_\\mathrm{tf}$.\n\n## activation\n\nfor an activator, the gene needs the tf to be *bound* to produce mrna. we use $p_\\mathrm{occupied} = 1 - p_\\mathrm{free}$:\n\n$$\n\\frac{\\mathrm{d} c_\\mathrm{m}}{\\mathrm{d} t} = \\frac{\\alpha_\\mathrm{m} \\, (c_\\mathrm{tf}/k)^n}{1 + (c_\\mathrm{tf}/k)^n} - \\gamma_\\mathrm{m} \\, c_\\mathrm{m}.\n$$\n\n> *now the gene is off when tf is absent, and turns on sharply as tf concentration rises past $k$. the hill coefficient $n$ controls how sharp the switch is, just as before.*\n\n## regulation by small rna\n\nnature has another elegant trick: **small rnas** (srnas) that bind directly to mrna and mark it for rapid degradation. the equations become:\n\n$$\n\\frac{\\mathrm{d} c_\\mathrm{s}}{\\mathrm{d} t} = \\alpha_\\mathrm{s} - \\gamma_\\mathrm{s} \\, c_\\mathrm{s} - \\delta \\, c_\\mathrm{m} \\, c_\\mathrm{s},\n$$\n\n$$\n\\frac{\\mathrm{d} c_\\mathrm{m}}{\\mathrm{d} t} = \\alpha_\\mathrm{m} - \\gamma_\\mathrm{m} \\, c_\\mathrm{m} - \\delta \\, c_\\mathrm{m} \\, c_\\mathrm{s}.\n$$\n\n> *the key is the third term in each equation: $\\delta \\, c_\\mathrm{m} \\, c_\\mathrm{s}$. when srna meets mrna, they annihilate each other (form a complex that is quickly degraded). this mutual destruction creates a threshold-like response \u2014 the mrna is only abundant"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "bayesian-inversion",
      "lessonTitle": "Bayesian Inversion",
      "x": 0.3326464295387268,
      "y": 0.39700302481651306,
      "searchText": "bayesian inversion\n# bayesian inversion\n\nhere's a question that might have been nagging you. in the [regularization lesson](./regularization), we added a penalty term $\\epsilon^2\\|\\mathbf{m}\\|^2$ and said \"this keeps the model from going crazy.\" but *why that particular penalty*? why not something else? and how do we know if $\\epsilon$ is too big or too small?\n\nthe beautiful thing is that there's an answer \u2014 and it comes from probability. every regularization choice has a precise probabilistic meaning. the penalty is a **prior belief** about the model. the data misfit is a **likelihood**. and the regularized solution is the **most probable model** given both your beliefs and your data.\n\nonce you see this, regularization stops feeling like a hack and starts feeling like honest science.\n\n---\n\n## regularization as a gaussian prior\n\nthe tikhonov objective\n\n$$\ne_\\epsilon(\\mathbf{m}) = \\|\\mathbf{d} - \\mathbf{gm}\\|^2 + \\epsilon^2\\|\\mathbf{m}\\|^2\n$$\n\nis equivalent to maximizing the posterior probability\n\n$$\n\\sigma(\\mathbf{m}) = \\rho_m(\\mathbf{m})\\,l(\\mathbf{m}), \\qquad l(\\mathbf{m}) = \\rho_d(g(\\mathbf{m}))\n$$\n\nwhere the prior $\\rho_m$ is a zero-mean gaussian with covariance proportional to $\\epsilon^{-2}\\mathbf{i}$, and the likelihood $l$ assumes gaussian noise on the data.\n\nwhat does that mean in plain language? the prior says: \"i expect the model parameters to be small \u2014 close to zero \u2014 and i'm about $1/\\epsilon$ uncertain about each one.\" large $\\epsilon$ means a tight prior (you're very confident the model is small). small $\\epsilon$ means a loose prior (you're letting the data do the talking).\n\n[[figure gaussian-process]]\n\nlook at this picture carefully. each colored line is a sample from the prior distribution \u2014 a possible model *before* seeing any data. the prior is telling the model: \"i expect you to be smooth, like a gentle hillside, not like the alps on a bad day.\" the spread of the lines shows your uncertainty. when you combine this prior with data, the posterior (shaded region) shrinks \u2014 the data has taught you something.\n\n---\n\n## weighted formulation: data and model covariance\n\nthe basic tikhonov formulation assumes all data points have equal uncertainty and the model is isotropically smooth. in the real world? never.\n\nsome seismometers are more precise than others. some model parameters are better constrained by geology than others. to handle this, introduce **data covariance** $\\mathbf{c}_d$ and **model covariance** $\\mathbf{c}_m$:\n\n$$\n\\mathbf{v}^t\\mathbf{v} = \\mathbf{c}_d^{-1}, \\qquad \\mathbf{w}^t\\mathbf{w} = \\mathbf{c}_m^{-1}.\n$$\n\ntransform to \"whitened\" variables:\n\n$$\n\\bar{\\mathbf{d}} = \\mathbf{vd}, \\qquad \\bar{\\mathbf{m}} = \\mathbf{wm}, \\qquad \\bar{\\mathbf{g}} = \\mathbf{vgw}^{-1}.\n$$\n\nnow solve the standard tikhonov problem in these new variables. the effect: noisy data points get downweighted automatically. model parameters with strong prior constraints are penalized more heavily.\n\nnotice what we just did \u2014 we turned our beliefs about noise and about geology into two simple matrices. that's the whole game of science: turn intuition into numbers, then let the numbers argue with the data.\n\n---\n\n## from point estimates to posterior exploration\n\nso far, we've been finding the *peak* of the posterior \u2014 the single most probable model. that's useful, but it's not the whole story.\n\nthink about it. the earth doesn't care which of ten different fault geometries you pick \u2014 they all wiggle the surface the same way. so the answer isn't a single line on a map; it's a whole cloud of possible lines. that cloud *is* the real answer.\n\nwhen the posterior is complex \u2014 multimodal (several distinct peaks), asymmetric (skewed to one side), or high-dimensional (hundreds of parameters) \u2014 the point estimate misses crucial structure. you need to explore the full distribution.\n\nhow much did the data actually narrow down the possibilities? that's something we can quantify with information theory \u2014 the tools in the [information and entropy lesson](./information-entropy). but to actually *explore* the posterior, we need sampling methods.\n\nthis is exactly what [monte carlo methods](./monte-carlo-methods) give us: instead of finding one best answer, we generate thousands of plausible models, weighted by how well they explain the data. the spread of those models tells you what you know and what you don't.\n\n---\n\n## takeaway\n\nthe bayesian viewpoint unifies regularization, uncertainty quantification, and model comparison into a single framework. regularization is not a trick \u2014 it is the deterministic shadow of a probabilistic prior. and the posterior distribution, not the point estimate, is the honest answer to an inverse problem.\n\n---\n\n## further reading\n\ntarantola's *inverse problem theory* develops the probabilistic viewpoint with real physical intuition. kaipio & somersalo's *statistical and computational inverse problems* is more mathematical but very thorough. stuart's *inverse problems: a bayesian perspective* (acta numerica, 2010) is an excellent survey. but first, make sure the connection between $\\epsilon$ and the prior variance has really sunk in \u2014 that insight will carry you through everything that follows.\n"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "foundations",
      "lessonTitle": "Introduction to Inverse Problems",
      "x": 0.38117796182632446,
      "y": 0.41951483488082886,
      "searchText": "introduction to inverse problems\n# introduction to inverse problems\n\nyou drop a stone into a pond, and ripples spread outward. that's a **forward problem** \u2014 given the cause, predict the effect. easy enough.\n\nnow imagine you're standing at the edge of the pond and all you see are ripples. can you figure out where the stone landed? *that's* an inverse problem. you observe effects and try to work backwards to the cause. and here's the thing: it's almost always harder than you'd expect.\n\n$$\n\\mathbf{d} = g(\\mathbf{m})\n$$\n\n- $\\mathbf{d}$: the data you actually measured (the ripples)\n- $\\mathbf{m}$: the hidden model parameters you want (where the stone fell, how big it was)\n- $g$: the forward model \u2014 the physics that connects cause to effect\n\nin practice, we know $\\mathbf{d}$ and we can evaluate $g$, but computing $g^{-1}$? that's where the trouble begins. it's usually impossible, or worse \u2014 it's possible but wildly unstable.\n\n---\n\n## why inverse problems are hard\n\njacques hadamard figured this out over a century ago. he said a problem is **well-posed** if it satisfies three conditions:\n\n1. a solution **exists**\n2. the solution is **unique**\n3. the solution **depends continuously on the data** (small changes in data produce small changes in the answer)\n\ninverse problems gleefully violate all three. here's what goes wrong:\n\n**no exact solution.** your measurements have noise. the data you recorded doesn't perfectly match any physical model. so there's no model that exactly reproduces what you measured.\n\n**multiple solutions.** several different models can produce nearly identical data. the earth doesn't care which of ten different fault geometries you pick \u2014 they all wiggle the surface the same way.\n\n**instability.** this is the killer. tiny perturbations in data can send your inferred model flying off to absurd values. let's see this happen right now.\n\n---\n\n## let's break something on purpose\n\nhere's the hadamard instability example, and we're going to *feel* it. consider a 2d heat-flow problem where the temperature satisfies laplace's equation. one family of solutions is:\n\n$$\nt(x,y) = \\frac{1}{n^2}\\sin(nx)\\sinh(ny).\n$$\n\nat the boundary $y = 0$, the temperature is $t(x,0) = 0$ \u2014 perfectly calm. the derivative $\\partial t / \\partial y$ at the boundary is $\\frac{1}{n}\\sin(nx)$, which gets *smaller* as $n$ increases. so the boundary data looks more and more innocent.\n\nbut look what happens in the interior. pick $y = 1$ and watch:\n\n| frequency $n$ | boundary signal $\\sim 1/n$ | interior $t(x,1) \\sim \\sinh(n)/n^2$ |\n|:-:|:-:|:-:|\n| $n = 1$ | 1.0 | 1.18 |\n| $n = 10$ | 0.1 | $1.1 \\times 10^{2}$ |\n| $n = 100$ | 0.01 | $\\sim 10^{41}$ |\n\nread that last row again. the boundary signal is *one hundredth* of the original, barely a whisper. but the interior solution has exploded to $10^{41}$. that's not a number \u2014 that's a catastrophe.\n\nthis is what instability looks like. your data says \"everything is fine.\" your reconstruction says \"the interior is on fire.\" a tiny high-frequency perturbation in the measurements \u2014 something you'd dismiss as noise \u2014 gets amplified into a nonsensical answer.\n\nthat queasy feeling in your stomach? good. hold onto it. it's the reason this entire course exists.\n\n---\n\n## canonical examples\n\nthese aren't abstract curiosities. inverse problems show up everywhere that matters.\n\n### medical and geophysical imaging\n\nin mri and spect imaging, you measure electromagnetic or nuclear responses at the surface of the body and try to reconstruct what's happening inside. in seismic tomography, earthquakes send waves through the earth's interior and we record the wiggles at the surface \u2014 then try to figure out what the interior looks like.\n\n[[figure mri-scan]]\n\n[[figure spect-scan]]\n\n[[figure seismic-tomography]]\n\nevery one of these is an inverse problem. every one of them is ill-posed. and every one of them requires the tools we'll build in this course.\n\n### waveform inversion\n\na simplified acoustic wave equation:\n\n$$\n\\frac{1}{\\kappa(x)}\\frac{\\partial^2 p}{\\partial t^2}(x,t) - \\nabla\\cdot\\left(\\frac{1}{\\rho(x)}\\nabla p(x,t)\\right) = s(x,t)\n$$\n\nwe record the pressure $p(x_n, t)$ at sensors and try to infer the material properties $\\kappa(x)$ and $\\rho(x)$. the forward problem is well-posed. the inverse? spectacularly ill-posed.\n\n---\n\n## the road ahead\n\nthe central challenge is clear: inverse problems are ill-posed, and naive inversion will betray you. but there's good news. over the next several lessons, we'll build a toolkit that tames these problems:\n\n- [regularization \u2014 the first rescue](./regularization): how to penalize wildness and stabilize inversion\n- [iterative methods and large-scale tricks](./tikhonov): when the problem is too big for a formula\n- [bayesian inversion](./bayesian-inversion): the probabilistic viewpoint \u2014 turning beliefs and data into honest answers\n- [linear tomography](./linear-tomography): a complete inversion workflow from rays to images\n- [monte carlo methods](./monte-carlo-methods): exploring the full space of plausible models\n- [geophysical inversion examples](./geophysical-inversion): fault and glacier case studies with real uncertainty\n- [information, entropy, and uncertainty](./information-entropy): measuring how much the data actually taught us\n\nby the time we finish, you will be able to look at any inverse problem and say, calmly, \"i know how to tame you.\"\n\n---\n\n## further reading\n\nif you want to go deeper right away, tarantola's *inverse problem theory* is the gold standard \u2014 he thinks the way we do. aster, borchers & thurber's *parameter estimation and inverse problems* is excellent for the linear algebra foundations. but honestly, work through the demos first. the intuition matters more than the theorems at this stage.\n"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "geophysical-inversion",
      "lessonTitle": "Geophysical Inversion Examples",
      "x": 0.4278954863548279,
      "y": 0.3759261667728424,
      "searchText": "geophysical inversion examples\n# geophysical inversion examples\n\nthis is where everything we've built comes together. we've developed regularization, the bayesian framework, and monte carlo sampling. now let's point these tools at real geophysical problems and see what happens.\n\ntwo case studies: a fault beneath the surface, and a glacier hiding its bed. in both cases, the answer isn't a number \u2014 it's a distribution.\n\n---\n\n## example 1: vertical fault inversion\n\n### the problem\n\nan earthquake ruptures along a fault buried beneath the surface. the rupture displaces the ground \u2014 gps stations and satellite radar measure how much the surface moved. the question: what is the geometry of the fault? how deep does it go? how much did it slip?\n\n[[figure vertical-fault-diagram]]\n\n### the forward model\n\nfor a vertical strike-slip fault in an elastic half-space, the surface displacement at a point $x$ depends on:\n\n- **fault depth** $d$: how deep the top of the fault is\n- **fault slip** $s$: how much the two sides moved past each other\n- **fault dip** $\\delta$: the angle of the fault plane\n\nthe displacement is a nonlinear function of these parameters. given the geometry, we can compute what the surface *should* look like \u2014 that's the forward model. but given the surface measurements, multiple fault configurations can produce nearly identical displacements.\n\n### the data\n\nsuppose we measured surface displacements at 20 stations along a profile perpendicular to the fault:\n\n- station 1 (0.5 km from fault): 3.1 cm displacement\n- station 5 (2.5 km): 2.4 cm\n- station 10 (5.0 km): 1.2 cm\n- station 15 (7.5 km): 0.4 cm\n- station 20 (10.0 km): 0.1 cm\n\nthe displacements decay with distance \u2014 but *how* they decay encodes the fault geometry. a shallow fault produces a sharp near-field signal. a deep fault produces a broader, gentler pattern. a steeply dipping fault looks different from a gently dipping one.\n\n### running the mcmc\n\nwe set up a metropolis sampler to explore the posterior over fault parameters. the algorithm:\n\n1. start from an initial guess (say, $d = 5$ km, $s = 1$ m, $\\delta = 90\u00b0$)\n2. propose a small random perturbation to each parameter\n3. compute the forward model for the proposed fault\n4. compare with the data \u2014 if the fit improves, accept. if it worsens, accept with probability proportional to the likelihood ratio.\n5. repeat 50,000 times.\n\n[[simulation vertical-fault-mcmc]]\n\n### what the posterior tells us\n\nwatch the simulation carefully. the chain doesn't settle on a single answer \u2014 it wanders through a cloud of plausible fault configurations. some things are well-determined: the total seismic moment (roughly, depth \u00d7 slip) is tightly constrained because it controls the total amount of surface displacement. but there's a **trade-off**: a shallow fault with large slip can produce similar surface displacements as a deeper fault with smaller slip.\n\nthis shows up as an elongated, tilted cloud in the depth-vs-slip scatter plot. the data alone cannot break this trade-off. you'd need additional information \u2014 maybe insar data from a different viewing angle, or teleseismic waveforms \u2014 to shrink the posterior further.\n\nthe earth doesn't care which of these fault models you pick \u2014 they all explain the surface measurements equally well. the whole cloud of models is the honest answer. a single \"best\" model would hide this fundamental ambiguity.\n\n---\n\n## example 2: glacier bed topography\n\n### the problem\n\na glacier flows down a valley, and we want to know the shape of the bedrock underneath. why? because bed topography controls ice flow, determines how the glacier will respond to climate warming, and governs whether meltwater can drain or gets trapped.\n\nbut the bed is buried under hundreds of meters of ice. we can't see it directly.\n\n[[figure glacier-valley-diagram]]\n\n### what we can measure\n\nwe have two types of data:\n\n- **surface velocity**: ice flows faster where it's thicker (roughly proportional to the fourth power of thickness \u2014 the glen flow law). measured by tracking features in satellite images.\n- **surface elevation**: precisely measured by gps or lidar. the bed elevation equals surface elevation minus ice thickness.\n\nthe forward model connects bed topography to surface observables through the equations of ice flow. it's nonlinear \u2014 the relationship between bed shape and surface velocity involves a power law and depends on the local slope.\n\n### running the mcmc\n\nwe parameterize the bed as a smooth curve (a set of control points with interpolation) and explore the posterior with mcmc:\n\n1. start from a flat-bed initial guess\n2. propose random perturbations to each control point\n3. run the forward model (ice flow simulation) for the proposed bed\n4. compare predicted surface velocity and elevation with observations\n5. accept or reject using the metropolis criterion\n6. after 30,000 iterations, examine the posterior distribution over bed shapes\n\n[[simulation glacier-thickness-mcmc]]\n\n### a family of possible beds\n\nthe result isn't one bed profile \u2014 it's a family of plausible profiles. some key features emerge:\n\n**well-constrained regions:** where the glacier is thin and surface data is dense, the posterior is narrow. we know the bed pretty well there.\n\n**poorly-constrained regions:** near the glacier center, where the ice is thickest, many different bed shapes produce similar surface patterns. the posterior is wide. we're honestly uncertain.\n\n**the overdeepening question:** some sampled bed profiles show a deep trough near the glacier center (an \"overdeepening\"). others don't. the data alone cannot resolve whether this feature exists. this matters hugely for predictions of future glacier retreat \u2014 if there's an overdeepening, warm ocean water can intrude and accelerate melting.\n\nnotice the trade-off between fit quality and physical plausibility: you could create a bed profile with lots of bumps and ridges that fits the data marginally better, but it would be geologically absurd. the prior (smoothness c"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "home",
      "lessonTitle": "Inverse Problems",
      "x": 0.3815043270587921,
      "y": 0.41061514616012573,
      "searchText": "inverse problems\n# inverse problems\n\nyou measure something. you want to know what caused it. that's an inverse problem \u2014 and it's one of the hardest, most beautiful challenges in computational science.\n\nthe catch? the answer is almost never unique, and the obvious approach (just invert the equations) almost always blows up. this module gives you the tools to handle that \u2014 to stabilize inversion, quantify uncertainty, and extract honest answers from noisy, incomplete data.\n\n[[figure mri-scan]]\n\n[[figure spect-scan]]\n\n[[figure seismic-tomography]]\n\n---\n\n## the journey\n\nhere's where we're going. think of it as a subway map \u2014 each stop builds on the last:\n\n```\n \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557     \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557     \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n \u2551  1. why it's \u2551\u2500\u2500\u2500\u2500\u25b6\u2551  2. regularization \u2551\u2500\u2500\u2500\u2500\u25b6\u2551  3. iterative       \u2551\n \u2551    hard      \u2551     \u2551  the first rescue  \u2551     \u2551  methods            \u2551\n \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d     \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d     \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                          \u2502\n       \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557     \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557    \u2502\n       \u2551  5. linear         \u2551\u25c0\u2500\u2500\u2500\u2500\u2551  4. bayesian     \u2551\u25c0\u2500\u2500\u2500\u256f\n       \u2551  tomography        \u2551     \u2551  inversion       \u2551\n       \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d     \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                \u2502\n                \u25bc\n \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557     \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557     \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n \u2551  6. monte carlo  \u2551\u2500\u2500\u2500\u2500\u25b6\u2551  7. geophysical  \u2551\u2500\u2500\u2500\u2500\u25b6\u2551  8. information   \u2551\n \u2551  methods         \u2551     \u2551  case studies    \u2551     \u2551  & entropy        \u2551\n \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d     \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d     \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n```\n\n**ill-posed \u2192 stabilize \u2192 iterate \u2192 probabilistic view \u2192 full workflow \u2192 explore the posterior \u2192 real applications \u2192 how much did we really learn?**\n\n---\n\n## learning path\n\n1. **why it's hard**: [introduction to inverse problems](./foundations) \u2014 hadamard, instability, and why naive inversion fails\n2. **the first rescue**: [regularization \u2014 the first rescue](./regularization) \u2014 penalizing wildness, the l-curve, finding the sweet spot\n3. **scaling up**: [iterative methods and large-scale tricks](./tikhonov) \u2014 when the formula is too expensive and you need to walk toward the answer\n4. **thinking probabilistically**: [bayesian inversion](./bayesian-inversion) \u2014 regularization as prior belief, from point estimates to posteriors\n5. **a complete workflow**: [linear tomography](./linear-tomography) \u2014 from seismic rays to subsurface images\n6. **exploring the posterior**: [monte carlo methods](./monte-carlo-methods) \u2014 sampling when analysis fails\n7. **real applications**: [geophysical inversion examples](./geophysical-inversion) \u2014 faults, glaciers, and why the answer is always a distribution\n8. **the deep question**: [information, entropy, and uncertainty](./information-entropy) \u2014 measuring how much the data actually taught us\n\n---\n\n## what you'll be able to do\n\nby the end of this module, you will be able to:\n\n- formulate any inverse problem as parameter estimation from indirect measurements\n- recognize ill-posedness and choose appropriate regularization\n- set up and solve linear tomographic inversions\n- run mcmc to explore nonlinear posteriors\n- quantify what the data can and cannot resolve\n- look at any inverse problem and say, calmly, \"i know how to tame you\"\n\n---\n\n## interactive simulations\n\neach lesson includes hands-on simulations. drag sliders, watch posteriors breathe, break things on purpose:\n\n- optimization: `steepest-descent`, `tikhonov-regularization`\n- linear inverse problems: `linear-tomography`\n- monte carlo inversion: `monte-carlo-integration`, `sphere-in-cube-mc`, `vertical-fault-mcmc`, `glacier-thickness-mcmc`\n- information theory: `entropy-demo`, `kl-divergence`\n\nshort theory bursts, then interactive exploration. that's the rhythm.\n\n---\n\n## prerequisites\n\n- linear algebra: matrix operations, eigenvalues, least squares\n- calculus: derivatives, integrals, taylor series\n- probability and statistics: distributions, bayes' theorem, expectation\n- programming in python with numpy\n"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "information-entropy",
      "lessonTitle": "Information, Entropy, and Uncertainty",
      "x": 0.3200720250606537,
      "y": 0.3744927644729614,
      "searchText": "information, entropy, and uncertainty\n# information, entropy, and uncertainty\n\nin 1948, claude shannon asked a deceptively simple question: **how much does one message reduce my uncertainty?** he was thinking about telegraph wires and telephone cables, but his answer turned out to be universal. it applies to everything from compression algorithms to dna to \u2014 you guessed it \u2014 inverse problems.\n\nhere's the connection. you start an inverse problem with uncertainty about the model (the prior). you collect data. after inversion, you have less uncertainty (the posterior). shannon's framework lets you *measure* exactly how much the data taught you \u2014 in bits, in nats, in precise mathematical units.\n\nthat's what this lesson is about: quantifying information. and it turns out to be the deepest reason we regularize.\n\n---\n\n## shannon entropy: measuring uncertainty\n\nif $x$ is a random variable taking values with probabilities $p_i$, the **shannon entropy** is:\n\n$$\nh(x) = -\\sum_i p_i \\log p_i.\n$$\n\nwhat does this mean? think of it as the average surprise. if one outcome is nearly certain ($p_1 \\approx 1$), there's almost no surprise when it happens \u2014 entropy is low. if all outcomes are equally likely, every observation is maximally surprising \u2014 entropy is high.\n\n[[figure claude-shannon]]\n\na fair coin: $h = \\log 2 \\approx 0.693$ nats. a loaded coin (99% heads): $h \\approx 0.056$ nats. the loaded coin is boring \u2014 you almost always know what's coming. the fair coin keeps you guessing.\n\n[[simulation entropy-demo]]\n\ndrag the probability slider and watch entropy change. the maximum is at uniform distribution (maximum ignorance). the minimum is at a spike (you know the answer). every inverse problem starts somewhere on this curve \u2014 and data moves you toward the minimum.\n\n---\n\n## kl divergence: the extra surprise\n\nnow suppose you have two distributions: $p$ (the truth) and $q$ (your model). the **kullback-leibler divergence** measures how much *extra* surprise you experience by using the wrong model:\n\n$$\nd_{\\mathrm{kl}}(p \\| q) = \\sum_i p(i) \\log \\frac{p(i)}{q(i)}.\n$$\n\nhere's the intuition. if your model $q$ matches reality $p$ perfectly, there's no extra surprise: $d_{\\mathrm{kl}} = 0$. if your model assigns low probability to events that actually happen often, you're constantly caught off guard \u2014 $d_{\\mathrm{kl}}$ is large.\n\nthree things to remember:\n\n- $d_{\\mathrm{kl}} = 0$ *only* when the distributions match exactly\n- it's always non-negative (you can't be *less* surprised by using the wrong model)\n- it's **asymmetric**: $d_{\\mathrm{kl}}(p \\| q) \\neq d_{\\mathrm{kl}}(q \\| p)$. being wrong about likely events costs more than being wrong about rare ones.\n\n[[simulation kl-divergence]]\n\n---\n\n## why this matters for inverse problems\n\nthese aren't abstract mathematical toys. they connect directly to every concept in this course.\n\n### measuring what the data taught you\n\nremember the [bayesian framework](./bayesian-inversion)? you start with a prior, collect data, and get a posterior. the kl divergence between posterior and prior \u2014\n\n$$\nd_{\\mathrm{kl}}(\\text{posterior} \\| \\text{prior})\n$$\n\n\u2014 measures exactly how much the data changed your beliefs. large value: the data was highly informative, you learned a lot. small value: the data told you nothing you didn't already know. this is the quantitative answer to \"was the experiment worth running?\"\n\n### why we regularize (the deep reason)\n\nhere's something most textbooks don't tell you. regularization is about controlling how much information you extract from the data.\n\nwith no regularization ($\\epsilon \\approx 0$), you're extracting *everything* from the data \u2014 including the noise. the inferred model has low entropy relative to the prior (you've committed to very specific parameter values), but much of that \"information\" is actually noise. you've fooled yourself into thinking you know more than you do.\n\nwith too much regularization ($\\epsilon$ huge), you're ignoring the data entirely. the posterior is basically the prior. entropy stays high \u2014 you haven't learned anything.\n\nthe right regularization extracts the signal and leaves the noise behind. information theory gives you a principled way to find that balance: maximize the genuine information gain while penalizing overfitting.\n\n### model comparison and selection\n\nwhen two different models both fit the data, which is better? residuals alone can't answer this \u2014 a more complex model will always fit better. kl divergence provides a principled comparison: which model's predicted distribution is closest to the observed data distribution? this is the information-theoretic foundation of model selection criteria like aic and bic.\n\n### monte carlo diagnostics\n\nwhen running [mcmc](./monte-carlo-methods), the entropy of the sampled posterior tells you whether the chain has explored the full range of plausible models. if the sampled entropy is much lower than expected, your chain might be stuck in one region. if it matches the theoretical posterior entropy, you can be more confident your samples represent the true posterior.\n\n---\n\n## the big picture\n\nstep back and look at the entire course through this lens:\n\n1. **foundations**: inverse problems are ill-posed \u2014 entropy of the naive solution is all wrong\n2. **regularization**: stabilize by controlling model complexity \u2014 equivalently, by limiting how much \"information\" (real + noise) you extract\n3. **bayesian framework**: the prior sets the entropy floor, the data reduces it\n4. **monte carlo**: explores the posterior, whose entropy tells you what you actually know\n5. **information theory** (here): quantifies all of this precisely\n\nentropy is not a footnote. it's the thread that runs through the entire course.\n\n---\n\n## takeaway\n\ninformation-theoretic tools \u2014 entropy, kl divergence, mutual information \u2014 quantify what the data taught you and what remains unknown. they explain *why* regularization works (it controls information extraction), *how much* the data helped (kl divergence betw"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "linear-tomography",
      "lessonTitle": "Linear Tomography",
      "x": 0.40916377305984497,
      "y": 0.44817107915878296,
      "searchText": "linear tomography\n# linear tomography\n\nhere's a puzzle. you can't see inside the earth. you can't drill deep enough, and even if you could, you'd only see one tiny spot. but earthquakes send seismic waves criss-crossing through the interior, and we have seismometers all over the surface recording when those waves arrive.\n\neach wave takes a path through the earth. if it passes through slow material, it arrives late. if it passes through fast material, it arrives early. so the arrival time carries information about the material it crossed.\n\nnow: given thousands of arrival times, can you reconstruct the velocity structure of the interior?\n\n*yes.* and the method is called **tomography** \u2014 literally, \"drawing by slices.\" it's the same principle behind a ct scan at the hospital, except the patient is the entire planet.\n\n---\n\n## forward model: rays through the earth\n\nfor a ray traveling along path $\\gamma$, the travel-time anomaly (how late it is compared to a reference model) is:\n\n$$\nt_\\gamma = \\int_\\gamma s(u)\\,du,\n$$\n\nwhere $s(u)$ is the slowness anomaly along the path. slow region? the integral is large. fast region? small.\n\nit's like asking a tourist who walked through 50 cities which ones slowed them down. each tourist only knows about the cities they actually visited \u2014 and each answer only lights up a few boxes on the map. but if you have *hundreds* of tourists taking different routes, you can piece together the whole map.\n\n---\n\n## from rays to matrix form\n\ndiscretize the earth into a grid of cells. each cell has an unknown slowness anomaly. each ray passes through some cells and misses others. this turns the continuous integral into a matrix equation:\n\n$$\n\\mathbf{d} = \\mathbf{gm}.\n$$\n\n- **rows** of $\\mathbf{g}$: one per ray (measurement)\n- **columns** of $\\mathbf{g}$: one per grid cell (unknown)\n- **entry** $g_{ij}$: the length of ray $i$ inside cell $j$ (zero if the ray misses that cell)\n- $\\mathbf{m}$: unknown slowness anomalies in each cell\n- $\\mathbf{d}$: measured travel-time anomalies\n\nmost entries of $\\mathbf{g}$ are zero \u2014 each ray only crosses a small fraction of the grid. this sparsity is what makes large-scale tomography computationally feasible.\n\n---\n\n## building the sensitivity matrix\n\nhere's the code that constructs $\\mathbf{g}$ for a simple 2d cross-borehole geometry with diagonal rays:\n\n```python\ndef make_g(n=13):\n    g_right = [np.eye(n, k=1 + i).flatten() for i in range(n - 2)]\n    g_left = [np.flip(np.eye(n, k=-(1 + i)), axis=0).flatten() for i in range(n - 2)]\n    z = np.zeros((1, n**2))\n    g = np.concatenate([z, g_left[::-1], z, z, g_right, z])\n    return g * (2**0.5) * 1000\n```\n\neach row is one ray. the non-zero entries mark which cells that ray passes through. the $\\sqrt{2} \\times 1000$ factor accounts for diagonal path length and unit conversion. stare at this for a moment \u2014 each row is literally the ray's footprint on the grid.\n\n---\n\n## synthetic experiment: test before you trust\n\na reliable inversion workflow always starts with **synthetic data**. you know the answer, so you can check whether your method recovers it.\n\n1. define a \"true\" anomaly model (something with clear features)\n2. compute noiseless data: $\\mathbf{d}_{\\text{true}} = \\mathbf{gm}_{\\text{true}}$\n3. add controlled noise: $\\mathbf{d}_{\\text{obs}} = \\mathbf{d}_{\\text{true}} + \\boldsymbol{\\eta}$\n4. recover $\\hat{\\mathbf{m}}$ and compare with the truth\n\nthis reveals where your setup has resolving power and where it doesn't \u2014 *before* you ever touch real data. skip this step and you're flying blind.\n\n---\n\n## inversion: regularized reconstruction\n\ntomographic systems are typically underdetermined (more unknowns than data) and noisy. so we use regularized inversion:\n\n$$\n\\hat{\\mathbf{m}} = (\\mathbf{g}^t\\mathbf{g} + \\epsilon^2\\mathbf{i})^{-1}\\mathbf{g}^t\\mathbf{d}_{\\text{obs}}.\n$$\n\nthe target is a model that:\n\n- fits the data within its uncertainty\n- stays stable under noise\n- avoids unrealistic spatial oscillations\n\nif you've been following along, this should feel familiar \u2014 it's exactly the tikhonov formula from the [regularization lesson](./regularization), applied to a concrete problem.\n\n[[simulation linear-tomography]]\n\n---\n\n## resolution and coverage\n\nhere's something that trips up beginners: the quality of your tomographic image depends as much on your **acquisition geometry** as on your inversion algorithm.\n\n- dense, cross-cutting rays \u2192 high local resolution, sharp features\n- sparse, one-directional rays \u2192 elongated blobs, poor depth resolution\n\ntwo stress tests reveal this:\n\n**delta-function spike test.** place a single anomaly in one cell. invert. if the recovered image is a sharp spike, that cell is well-resolved. if it smears into a blob, the ray coverage there is poor.\n\n**checkerboard test.** create an alternating pattern of positive and negative anomalies (like a checkerboard). invert. where the pattern recovers cleanly, you have good resolution. where it smears or disappears, you don't. this test is standard practice in seismology \u2014 it immediately shows you the reliable and unreliable regions of your image.\n\nthe smearing pattern tells you exactly what the data can and cannot see. this is why survey design and inversion are inseparable \u2014 you can't fix bad geometry with clever algorithms.\n\n---\n\n## the bridge: linear to nonlinear\n\neverything we've done so far relies on a crucial assumption: the forward model is **linear**. travel time is a linear function of slowness. the matrix $\\mathbf{g}$ doesn't depend on the model.\n\nin the real earth, this is almost never exactly true. wave propagation depends on the velocity structure (rays bend). material properties interact nonlinearly. the forward map $g(\\mathbf{m})$ is a complicated function, not a simple matrix multiplication.\n\nwhen the forward model is nonlinear, we can no longer find the answer with a single matrix inversion \u2014 even a regularized one. we have to stop hunting for one best model and start exploring a *family* of plausible models. t"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "monte-carlo-methods",
      "lessonTitle": "Monte Carlo Methods",
      "x": 0.41655462980270386,
      "y": 0.31860455870628357,
      "searchText": "monte carlo methods\n# monte carlo methods\n\nimagine you're a hiker trying to find the deepest point in a dark valley. you can't see the terrain \u2014 you can only feel the ground under your feet. you take a step. if it goes downhill, great, keep going. if it goes uphill, you *usually* step back \u2014 but sometimes, with a small probability, you go uphill anyway. why? because the downhill direction might lead to a shallow puddle, while the uphill step might take you over a ridge to a much deeper valley beyond.\n\nthat's the essential idea behind markov chain monte carlo. but let's build up to it.\n\n---\n\n## why sampling?\n\nin the [bayesian lesson](./bayesian-inversion), we wrote down the posterior distribution $p(\\mathbf{m} \\mid \\mathbf{d})$. that's the full answer to the inverse problem \u2014 it tells you which models are plausible and which aren't.\n\nbut here's the problem: for anything beyond simple linear-gaussian cases, you can't compute the posterior analytically. it lives in a high-dimensional space, it might have multiple peaks, curved ridges, and heavy tails. you can't integrate it, you can't visualize it directly, and you certainly can't summarize it with a single number.\n\nwhat you *can* do is **sample** from it. generate thousands of models, each drawn from the posterior distribution. the collection of samples *is* the answer \u2014 means, variances, correlations, confidence intervals all come from the samples.\n\n---\n\n## monte carlo integration: the basic idea\n\nthe simplest form of monte carlo is remarkably straightforward. want to compute the integral $\\int f(\\mathbf{x})\\,p(\\mathbf{x})\\,d\\mathbf{x}$? draw $n$ samples from $p$, evaluate $f$ at each one, and average:\n\n$$\n\\int f(\\mathbf{x})\\,p(\\mathbf{x})\\,d\\mathbf{x} \\approx \\frac{1}{n}\\sum_{i=1}^n f(\\mathbf{x}_i).\n$$\n\nthe error shrinks as $o(n^{-1/2})$ \u2014 regardless of dimension. that's the magic. in one dimension, quadrature rules are faster. in ten dimensions, they choke. in a hundred dimensions, monte carlo is the only game in town.\n\n---\n\n## geometric intuition: throwing darts\n\nbefore full inversion, let's build intuition. suppose you want to estimate the volume of a sphere inscribed in a cube. throw random darts at the cube. count how many land inside the sphere. the ratio estimates the volume fraction.\n\n[[simulation sphere-in-cube-mc]]\n\nwatch the estimate converge as you throw more darts. it's noisy at first, then settles down. that $1/\\sqrt{n}$ convergence rate means you need 100 times more samples to get 10 times more accuracy. not great \u2014 but it works in *any* number of dimensions, which is more than you can say for most methods.\n\n[[simulation monte-carlo-integration]]\n\n---\n\n## the problem with rejection sampling\n\nfor simple distributions, you can use **rejection sampling**: propose a candidate from some easy distribution, accept it with probability proportional to the target density:\n\n$$\np_{\\text{accept}} = \\frac{p(\\mathbf{x}_{\\text{cand}})}{m}, \\qquad m \\ge \\max_{\\mathbf{x}} p(\\mathbf{x}).\n$$\n\nwith a proposal density $q$:\n\n$$\np_{\\text{accept}} = \\frac{p(\\mathbf{x}_{\\text{cand}})}{m\\,q(\\mathbf{x}_{\\text{cand}})}.\n$$\n\nthis works fine in low dimensions. but in high dimensions, the acceptance rate plummets. think about it: in 100 dimensions, almost all the volume of the cube is in the corners, far from the target distribution. you'd throw billions of darts and accept almost none.\n\nwe need a smarter strategy.\n\n---\n\n## markov chain monte carlo (mcmc)\n\nhere's the key insight: instead of generating independent samples from a hard distribution, build a **chain** of samples where each step depends on the previous one. design the chain so that it spends more time in high-probability regions \u2014 exactly where the posterior puts its weight.\n\nthe simplest version is the **metropolis algorithm**:\n\n1. start at some model $\\mathbf{m}_{\\text{current}}$\n2. propose a new model: $\\mathbf{m}_{\\text{new}} = \\mathbf{m}_{\\text{current}} + \\text{random perturbation}$\n3. accept the new model with probability:\n\n$$\np_{\\text{accept}} = \\min\\left(1, \\frac{p(\\mathbf{m}_{\\text{new}})}{p(\\mathbf{m}_{\\text{current}})}\\right)\n$$\n\n4. if accepted, move to $\\mathbf{m}_{\\text{new}}$. if rejected, stay put. either way, record the current position.\n5. repeat. many, many times.\n\nthe beautiful thing: if the new model is *more* probable, you always accept. if it's *less* probable, you sometimes accept anyway \u2014 and the probability of accepting is exactly the ratio of the densities. this is what keeps you from getting trapped in local optima. that occasional uphill step? it's what lets the chain explore the full posterior, not just the nearest peak.\n\nafter enough steps, the chain \"forgets\" where it started and its samples are drawn from the true posterior. the histogram of visited models *is* the posterior distribution.\n\n---\n\n## practical diagnostics: is the chain working?\n\nrunning mcmc is easy. knowing whether it *worked* is the hard part. here's what to watch:\n\n**burn-in.** the chain starts wherever you initialized it, which might be far from the posterior. the initial samples are garbage \u2014 discard them. how many? plot the chain's trajectory and look for where it \"settles down.\"\n\n**acceptance rate.** if you accept everything (rate ~100%), your steps are too tiny and you're barely moving. if you accept almost nothing (rate ~1%), your steps are too big and you keep proposing implausible models. for gaussian targets, optimal acceptance is around 23% in high dimensions, 44% in one dimension.\n\n**mixing.** does the chain explore the full posterior, or does it get stuck in one region? multiple chains started from different points should converge to the same distribution.\n\n**autocorrelation.** consecutive samples are correlated (each step is a small perturbation of the last). effective sample size is the total number of samples divided by the autocorrelation time. a chain of 100,000 samples with autocorrelation time 50 gives you only ~2,000 independent samples.\n\n---\n\n## takeaway\n\nmonte carlo metho"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "regularization",
      "lessonTitle": "Regularization \u2014 The First Rescue",
      "x": 0.3117774426937103,
      "y": 0.43289703130722046,
      "searchText": "regularization \u2014 the first rescue\n# regularization \u2014 the first rescue\n\nimagine you're trying to hang a clothesline between two poles, and all you have are noisy measurements of where the line sags. if you force the clothesline to pass through every single measurement \u2014 including the ones corrupted by wind, by your shaky hands, by a bird that landed on the line mid-measurement \u2014 you get a wild, jagged monstrosity that zigzags between the poles. nobody would hang laundry on that.\n\nso instead, you add a little stiffness. you tell the clothesline: \"sure, get close to the measurements, but also, *don't be crazy*. be smooth.\" that compromise between fitting the data and staying sane? that's regularization. and it's the single most important idea in this course.\n\n---\n\n## the setup\n\nwe start with a linear forward problem plus noise:\n\n$$\n\\mathbf{d} = \\mathbf{gm} + \\boldsymbol{\\eta},\n$$\n\nwhere $\\boldsymbol{\\eta}$ is measurement noise. we want to recover $\\mathbf{m}$ from $\\mathbf{d}$.\n\nwhen $\\mathbf{g}$ is well-conditioned, you just solve a least-squares problem and go home. but when $\\mathbf{g}$ is ill-conditioned or rank-deficient \u2014 and in inverse problems, it almost always is \u2014 direct inversion amplifies noise into garbage. you saw this in the [hadamard example](./foundations). the mathematics is trying to fit noise as if it were signal, and the result is catastrophic.\n\n---\n\n## the l-curve: two forces fighting\n\nbefore we write down any formula, let's understand what we're doing geometrically.\n\nsuppose we try many different amounts of regularization \u2014 call it $\\epsilon$ \u2014 ranging from \"barely any\" to \"crushing.\" for each $\\epsilon$, we solve the problem and record two numbers:\n\n1. **data misfit**: how well does the model explain the data? (residual norm $\\|\\mathbf{d} - \\mathbf{gm}\\|$)\n2. **model norm**: how wild is the model? ($\\|\\mathbf{m}\\|$)\n\nplot these against each other in log-log space. you get an l-shaped curve. on one arm, the misfit is small but the model is insane (fitting noise). on the other arm, the model is simple but the misfit is huge (ignoring data). the corner of the l \u2014 where the two forces reach a compromise \u2014 that's your sweet spot.\n\nthis is the **l-curve method**, and it's pure gold. the data is screaming \"fit me!\" and the model norm is whispering \"don't get crazy.\" the corner is where they shake hands.\n\n---\n\n## the tikhonov objective\n\nnow the formula. we minimize:\n\n$$\nj(\\mathbf{m}) = \\|\\mathbf{d} - \\mathbf{gm}\\|^2 + \\epsilon^2\\|\\mathbf{m}\\|^2.\n$$\n\ntwo terms, fighting:\n\n- **first term**: make the model explain the data\n- **second term**: keep the model from going wild\n\nthe parameter $\\epsilon$ is the referee. small $\\epsilon$: the data dominates, noise gets amplified. large $\\epsilon$: the penalty dominates, you get a boring model that ignores the data. just right: you extract the signal and leave the noise behind.\n\nthe closed-form solution is:\n\n$$\n\\hat{\\mathbf{m}} = (\\mathbf{g}^t\\mathbf{g} + \\epsilon^2\\mathbf{i})^{-1}\\mathbf{g}^t\\mathbf{d}.\n$$\n\nthat $\\epsilon^2\\mathbf{i}$ term is doing all the heavy lifting. without it, $\\mathbf{g}^t\\mathbf{g}$ might be nearly singular and the solution explodes. with it, every eigenvalue gets pushed away from zero. the matrix becomes invertible, the solution becomes stable, and you can breathe again.\n\n---\n\n## choosing $\\epsilon$ in practice\n\nhere's the practical recipe:\n\n1. **sweep** $\\epsilon$ over a log-scale range (say, $10^{-4}$ to $10^{2}$)\n2. for each value, solve the regularized problem\n3. **plot** the l-curve (residual norm vs. model norm)\n4. pick the corner \u2014 the simplest model that still explains the data within its uncertainty\n\nother approaches exist: the discrepancy principle (choose $\\epsilon$ so the residual matches the expected noise level), cross-validation, and bayesian model selection. but the l-curve is intuitive, visual, and often your best first move.\n\n[[simulation tikhonov-regularization]]\n\n---\n\n## the drunk, the sober, and the dead\n\nhere's a picture that will stay with you. imagine solving the same inverse problem with three different values of $\\epsilon$:\n\n**$\\epsilon \\approx 0$ (no regularization):** the solution fits every data point, including noise. it's wild, oscillatory, physically absurd \u2014 like a drunk person trying to walk a straight line. this is **overfitting**.\n\n**$\\epsilon$ just right:** the solution captures the real features of the model while staying smooth and physically plausible. sober. clear-eyed. this is the answer you want.\n\n**$\\epsilon$ way too large:** the solution is flat, featureless, boring. it's so afraid of complexity that it sees nothing. dead. this is **underfitting**.\n\ndrunk. sober. dead. your job is to find the sober one.\n\n---\n\n## takeaway\n\nregularization is not an optional tweak or a mathematical convenience. it is the core mechanism that turns an ill-posed problem into a solvable one. without it, noise eats your answer alive. with too much of it, you miss the signal entirely.\n\nthe art is in the balance \u2014 and the l-curve is your guide.\n\nfor the optimization view of regularization, including iterative methods and steepest descent, see [iterative methods and large-scale tricks](./tikhonov). for the probabilistic interpretation \u2014 why regularization is really a prior belief in disguise \u2014 see [bayesian inversion](./bayesian-inversion).\n\n---\n\n## further reading\n\naster, borchers & thurber give an excellent treatment of the l-curve and parameter choice methods. hansen's *rank-deficient and discrete ill-posed problems* is the definitive reference on regularization tools. but for now, play with the simulation above \u2014 drag the $\\epsilon$ slider and watch the solution morph from drunk to sober to dead. that's worth more than any textbook chapter.\n"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "tikhonov",
      "lessonTitle": "Iterative Methods and Large-Scale Tricks",
      "x": 0.34177616238594055,
      "y": 0.4599817395210266,
      "searchText": "iterative methods and large-scale tricks\n# iterative methods and large-scale tricks\n\nthe [previous lesson](./regularization) gave us the tikhonov formula \u2014 a closed-form solution that stabilizes inversion beautifully. one matrix inversion, done.\n\nbut here's the catch. that formula requires building $\\mathbf{g}^t\\mathbf{g} + \\epsilon^2\\mathbf{i}$ and inverting it. if your model has 100 parameters, no problem. if it has a million parameters \u2014 a 3d seismic tomography model, a full-waveform inversion grid, a climate reanalysis \u2014 that matrix has $10^{12}$ entries. you can't even store it, let alone invert it.\n\nso we need a different strategy: instead of solving the problem in one shot, we *walk* toward the answer, one step at a time. that's iterative optimization.\n\n---\n\n## steepest descent\n\nthe simplest idea: at each step, move in the direction that decreases the objective fastest. for the tikhonov objective\n\n$$\nj(\\mathbf{m}) = \\|\\mathbf{d} - \\mathbf{gm}\\|^2 + \\epsilon^2\\|\\mathbf{m}\\|^2,\n$$\n\nthe gradient is\n\n$$\n\\nabla j = -2\\mathbf{g}^t(\\mathbf{d} - \\mathbf{gm}) + 2\\epsilon^2\\mathbf{m}.\n$$\n\nthe update rule:\n\n$$\n\\mathbf{m}_{k+1} = \\mathbf{m}_k - \\alpha_k \\nabla j(\\mathbf{m}_k),\n$$\n\nwhere $\\alpha_k$ is the step size (learning rate).\n\nsounds simple. and it is \u2014 but the devil is in the details.\n\n[[simulation steepest-descent]]\n\nwatch what happens in the simulation above:\n\n- **step size too large:** the iterates overshoot, oscillate wildly, or diverge entirely. the algorithm is trying to sprint down a narrow valley and keeps bouncing off the walls.\n- **step size too small:** convergence is glacially slow. you're tiptoeing toward the answer and might not get there in your lifetime.\n- **just right:** smooth convergence to the regularized solution \u2014 the same answer the closed-form gives, but obtained without ever building the full matrix.\n\nthe beautiful thing is that each iteration only requires *matrix-vector products* $\\mathbf{g}\\mathbf{v}$ and $\\mathbf{g}^t\\mathbf{v}$ \u2014 not the full matrix $\\mathbf{g}^t\\mathbf{g}$. for large sparse systems, this is the difference between feasible and impossible.\n\n---\n\n## beyond steepest descent\n\nsteepest descent works but it can be slow, especially when the problem is ill-conditioned (eigenvalues spanning many orders of magnitude \u2014 exactly the situation in inverse problems). better options:\n\n- **conjugate gradients:** uses information from previous steps to avoid redundant search directions. converges much faster for quadratic objectives.\n- **l-bfgs:** approximates the curvature of the objective using a limited memory of past gradients. the workhorse of large-scale optimization.\n- **truncated newton methods:** solve the newton system approximately using a few cg iterations. excellent for nonlinear problems.\n\nthe common thread: none of these need the full hessian matrix. they all work with matrix-vector products, which means they scale to the problems that matter.\n\n---\n\n## why the penalty term is physics, not math\n\nit's tempting to think of the regularization term $\\epsilon^2\\|\\mathbf{m}\\|^2$ as a mathematical trick \u2014 something we added to make the numerics work. but that misses the point entirely.\n\nthe penalty term encodes **real physical beliefs** about the world:\n\n- **$\\|\\mathbf{m}\\|^2$ (tikhonov):** the model should have bounded energy. don't put structure where you don't need it.\n- **$\\|\\nabla \\mathbf{m}\\|^2$ (smoothness):** neighboring parameters should be similar. the earth doesn't change density by a factor of ten from one meter to the next.\n- **$\\|\\mathbf{m}\\|_1$ (sparsity):** most parameters should be zero. the model is simple, with a few localized features.\n\neach choice tells the inversion something different about what \"reasonable\" looks like. this mirrors how coarse-grid parameterizations work in climate and earth-system models \u2014 the grid resolution itself imposes a smoothness assumption.\n\n[[figure climate-grid]]\n\nnotice what we're really doing: encoding our physical intuition as mathematics. the penalty term is where domain knowledge enters the computation. get it right, and you extract signal. get it wrong, and you hallucinate structure or miss it entirely.\n\n---\n\n## when iterative methods shine\n\nuse the closed-form solution when you can. use iterative methods when you must \u2014 and you must when:\n\n- the model has more than ~$10^4$ parameters\n- $\\mathbf{g}$ is only available as a function (you can compute $\\mathbf{g}\\mathbf{v}$ but never write down $\\mathbf{g}$ explicitly)\n- the forward model is nonlinear (you linearize at each step and solve iteratively)\n- you want to monitor convergence and stop early as an implicit regularization strategy\n\nearly stopping is itself a form of regularization: iterative methods typically fit the large-scale features first and the noise last. stopping before full convergence can give you a better model than running to completion.\n\n---\n\n## takeaway\n\nwhen the tikhonov formula is too expensive to compute directly, iterative optimization gives you the same answer \u2014 or a better one \u2014 using only matrix-vector products. the regularization term isn't just numerical medicine; it's where your physical knowledge of the problem lives.\n\nfor the probabilistic interpretation of these choices \u2014 why the penalty is really a prior \u2014 see [bayesian inversion](./bayesian-inversion).\n\n---\n\n## further reading\n\nnocedal & wright's *numerical optimization* is the standard reference for iterative methods. for the inverse-problems angle, see vogel's *computational methods for inverse problems*. but the best way to understand this is to play with the steepest descent simulation above and watch how the step size and regularization interact.\n"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "assignments-projects",
      "lessonTitle": "Assignments and Project Ideas",
      "x": 0.0919245257973671,
      "y": 0.15320387482643127,
      "searchText": "assignments and project ideas\n# assignments and project ideas\n\n## empirical assignment track\n\nthese assignments are designed to be small and concrete. each one has a clear starting point and a clear success metric. do not panic \u2014 you are not starting from a blank file. each assignment provides the structure; you provide the implementation.\n\n**assignment 1: hedge in expert advice.** implement the hedge (exponential weights) algorithm for a prediction-with-expert-advice problem. generate $n = 8$ experts with fixed (but unknown to the learner) prediction strategies over $t = 10{,}000$ rounds. plot the cumulative loss of your learner vs the cumulative loss of each expert, and shade the regret region.\n\n*success metric:* your cumulative regret curve should grow sublinearly. specifically, the ratio $r_t / t$ should be below 0.02 by $t = 10{,}000$. your plot should clearly show the learner tracking the best expert within a small additive gap.\n\n**assignment 2: ucb1 and exp3 in multi-armed bandits.** implement ucb1 (for stochastic bandits) and exp3 (for adversarial bandits). test ucb1 on a 10-arm bernoulli bandit with known gap structure. test exp3 on an adversarial sequence where the best arm switches every 500 rounds.\n\n*success metric:* ucb1 regret should be $o(\\log t)$ in the stochastic setting \u2014 plot regret vs $\\log t$ and verify approximate linearity. exp3 regret should scale as $o(\\sqrt{kt \\log k})$ \u2014 plot regret vs $\\sqrt{t}$ and verify approximate linearity. include error bars from at least 50 independent runs.\n\n**assignment 3: sarsa and q-learning in a gridworld.** implement both sarsa and q-learning on a cliff-walking gridworld (a grid with a cliff along one edge, reward of -1 per step, and -100 for falling off the cliff). use epsilon-greedy with $\\epsilon = 0.1$. run both algorithms for 500 episodes and plot the per-episode return.\n\n*success metric:* sarsa should learn a safe path that avoids the cliff. q-learning should learn the optimal (shortest) path along the cliff edge. your per-episode return plots should clearly show this behavioral difference. include a visualization of the learned policies (arrows on the grid).\n\n**assignment 4: dqn on a compact control benchmark.** implement dqn with a replay buffer and target network on the cartpole-v1 environment (or a comparable compact benchmark). train for at least 500 episodes.\n\n*success metric:* your agent should consistently achieve the maximum episode length (500 steps) within 300 episodes. plot the learning curve (episode return vs episode number) and include an ablation: run once without the replay buffer and once without the target network to show that both are necessary for stable learning.\n\n## theoretical assignment track\n\nthese assignments ask you to work through the key proofs and arguments by hand.\n\n**assignment 5: regret bound for hedge.** derive the $o(\\sqrt{t \\log n})$ regret bound for the hedge algorithm. start from the potential function $\\phi_t = \\sum_i w_t(i)$. upper-bound $\\log(\\phi_{t+1}/\\phi_1)$ using the multiplicative update, and lower-bound it using the weight of the best expert. combine to get the final bound. clearly state where the choice $\\eta \\asymp \\sqrt{\\log n / t}$ comes from.\n\n**assignment 6: ucb concentration argument.** prove that ucb1 achieves gap-dependent regret $o\\left(\\sum_{a:\\delta_a > 0} \\frac{\\log t}{\\delta_a}\\right)$. use hoeffding's inequality to bound the probability that the confidence interval for an arm fails, and show that suboptimal arms are not pulled too often.\n\n**assignment 7: bellman contraction and fixed-point uniqueness.** prove that the bellman optimality operator $\\mathcal{t}$ is a $\\gamma$-contraction in the supremum norm. use the banach fixed-point theorem to conclude that $v^*$ is unique and that value iteration converges geometrically. compute the number of iterations needed for $\\epsilon$-accuracy.\n\n**assignment 8: on-policy vs off-policy.** formally state the convergence conditions for q-learning (robbins-monro conditions on step sizes, sufficient exploration). explain why sarsa converges to $q^\\pi$ for the behavior policy $\\pi$, while q-learning converges to $q^*$ regardless of the behavior policy. discuss what breaks when you add function approximation (the deadly triad).\n\n## course project\n\nchoose one applied setting and report reproducible experiments. the project should demonstrate that you can take a real problem, formulate it as an online learning or rl problem, implement a solution, and analyze its performance.\n\n**project idea 1: online advertising or recommendation simulator.** build a contextual bandit simulator where contexts represent user profiles and actions represent items to show. compare exp4 against epsilon-greedy and a baseline that ignores context.\n\n*grading metric:* (1) cumulative regret vs the oracle policy that knows the true reward model, plotted over 50,000 rounds. (2) regret per 1,000-round block, showing learning speed. (3) comparison of at least three algorithms. (4) discussion of how context dimensionality affects performance. (5) clean, reproducible code with a single command to regenerate all figures.\n\n**project idea 2: inventory or control mdp.** model a simple inventory management problem as an mdp (ordering decisions, stochastic demand, holding and shortage costs). apply value iteration (with known model) and q-learning (model-free) and compare.\n\n*grading metric:* (1) average cost per step vs the optimal policy, over 10,000 steps. (2) convergence plot showing how q-learning estimates approach the true q-values over time. (3) sensitivity analysis for at least two parameters (e.g., demand variance, discount factor). (4) comparison of model-based vs model-free approaches. (5) clean, reproducible code.\n\n**project idea 3: custom game or robotics-inspired simulator.** design a small game or physical control problem (maze navigation, simple robotic arm, resource allocation) and train an rl agent. if the state space is large, use dqn.\n\n*grading met"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "average-reward-online-rl",
      "lessonTitle": "Online RL in Average-Reward and Discounted Settings",
      "x": 0.10112935304641724,
      "y": 0.2068997025489807,
      "searchText": "online rl in average-reward and discounted settings\n# online rl in average-reward and discounted settings\n\n## what if the game never ends?\n\nevery algorithm we have built so far assumed the game eventually stops. an episode starts, rewards accumulate, the episode ends, and you get a clean total. but what about a server that runs 24/7? a robot that never shuts down? a factory production line that just keeps going? there is no \"end of episode.\" there is no final score to look back on.\n\nwhen the game never ends, the discounted return $\\sum_{t=0}^{\\infty}\\gamma^t r_t$ still makes mathematical sense \u2014 the geometric discount makes far-future rewards negligible. but the discount factor $\\gamma$ becomes an awkward modeling choice. if $\\gamma$ is close to 1, you care about the far future but the problem is nearly ill-conditioned. if $\\gamma$ is small, you are myopic. there is a more natural way to think about never-ending tasks.\n\n## discounted rl: a quick recap\n\nin the discounted setting, you optimize\n\n$$\n\\mathbb{e}\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_t\\right].\n$$\n\nthe discount factor $\\gamma < 1$ ensures this sum is finite. rewards far in the future are worth exponentially less than immediate rewards. all the value functions, bellman equations, and algorithms we have discussed work in this framework.\n\nbut here is the philosophical issue: in a continuing task, there is no reason why tomorrow's reward should be worth less than today's. the discount factor is a mathematical convenience, not a feature of the world. for many continuing tasks, what you really care about is: over the long run, what is my average reward per step?\n\n## average-reward rl: the steady-state view\n\nin the average-reward formulation, you target the **long-run gain**:\n\n$$\ng^\\pi(s)=\\lim_{t\\to\\infty}\\frac{1}{t}\\mathbb{e}\\left[\\sum_{t=1}^{t}r_t \\mid s_1=s\\right].\n$$\n\nthis is the average reward per time step, in the limit. for ergodic mdps (where every state is reachable from every other state under any reasonable policy), the gain $g^\\pi$ does not depend on the starting state \u2014 it is a single number that characterizes the steady-state performance of policy $\\pi$.\n\nthink of it like the average speed of a car on a long road trip. at the start, you accelerate and brake through city traffic. eventually, you hit the highway and settle into a cruising speed. the average speed over the whole trip converges to something close to your cruising speed. the city traffic part is the transient; the highway cruising is the steady state.\n\n## gain and bias: steady state plus transient\n\nthe **gain-bias decomposition** separates value into two pieces. the **gain** $g^\\pi$ captures the steady-state average reward \u2014 how well you do per step, once the system has settled. the **bias** $h^\\pi(s)$ captures the transient advantage \u2014 how much better or worse it is to start in state $s$ compared to starting in steady state.\n\ntogether, they satisfy the bellman-like equation: $g^\\pi + h^\\pi(s) = r^\\pi(s) + \\sum_{s'} p^\\pi(s'|s) h^\\pi(s')$. the gain is like the horizontal line of steady-state performance, and the bias tells you how far above or below that line you are at the beginning, depending on where you start.\n\npicture a graph of cumulative reward over time. after a while, the curve becomes approximately a straight line with slope $g^\\pi$. the bias measures the vertical offset \u2014 some starting states get a head start (positive bias) and others start behind (negative bias), but eventually they all settle into the same slope.\n\n## algorithms for continuing tasks\n\n**relative value iteration** is the average-reward analog of value iteration. instead of discounting, you subtract the gain at each step to keep the values from growing unboundedly.\n\n**differential td** and **differential q-learning** replace the discount factor with an estimate of the average reward. the td error becomes $r - \\bar{r} + v(s') - v(s)$, where $\\bar{r}$ is a running estimate of the average reward per step.\n\n**r-learning** maintains separate estimates of the average reward and the relative action-values, updating both as experience accumulates.\n\n## regret in online rl\n\nwhen you bring the online learning perspective to rl, regret compares your cumulative reward to the optimal policy in the same environment class. you do not know the transition probabilities; you have to learn them by exploring. the regret measures how much reward you sacrifice during the learning phase.\n\nmodern methods attack this with **optimism** or **posterior sampling**. **ucrl-style** algorithms build confidence sets for the transition probabilities and plan optimistically within those sets \u2014 just like ucb1 was optimistic about arm means, ucrl is optimistic about the mdp dynamics. **psrl-style** (posterior sampling rl) algorithms maintain a bayesian posterior over mdps, sample one, solve it, and follow that policy for a while before re-sampling. both achieve near-optimal regret scaling in tabular mdps, closing the loop between the online learning theory from the beginning of the course and the full rl setting.\n\n[[simulation average-reward-vs-discounted]]\n\n---\n\n*we have now covered the full arc: from regret in online learning, through bandits and contextual bandits, to full rl with mdps, td methods, deep function approximation, and continuing tasks. the next and final section gives you assignments and project ideas to cement all of these ideas through hands-on practice.*\n"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "bandits-ucb-exp3",
      "lessonTitle": "Stochastic and Adversarial Bandits: UCB1 and EXP3",
      "x": 0.0058496869169175625,
      "y": 0.18697528541088104,
      "searchText": "stochastic and adversarial bandits: ucb1 and exp3\n# stochastic and adversarial bandits: ucb1 and exp3\n\nyou are standing in front of $k$ slot machines. each one has a lever. you get to pull one lever per round, for $t$ rounds total. the machine you pull spits out a reward (or a loss). the machines you did *not* pull stay silent. your job: collect as much reward as possible.\n\nthis is the multi-armed bandit problem, and it is the place where exploration meets exploitation head-on.\n\n## ucb1: optimism in the face of uncertainty\n\nimagine you have just moved to a new city, and you are trying to find the best restaurant. you try a few places. the thai place was great \u2014 4.5 stars in your personal rating. the italian place was decent \u2014 3.8 stars. but there is a sushi restaurant you have only been to once, and it was okay \u2014 3.5 stars on that one visit.\n\nshould you go back to the thai place? maybe. but here is the thing: you have been to the thai place ten times, so you have a pretty good estimate of how good it is. the sushi place, you visited only once. maybe that one visit was a bad night. maybe the sushi place is actually a 4.8 if you went back. you do not know because you have not given it enough chances.\n\nucb1 handles this dilemma with a beautiful principle: **be optimistic about what you do not know**. for each arm $a$, you compute an upper confidence bound \u2014 your best estimate of that arm's average reward, plus a bonus for how uncertain you are:\n\n$$\na_t \\in \\arg\\max_a \\left[\\widehat{\\mu}_a(t) + \\sqrt{\\frac{2\\log t}{n_a(t)}}\\right].\n$$\n\nhere $\\widehat{\\mu}_a(t)$ is the empirical mean reward of arm $a$ so far, and $n_a(t)$ is how many times you have pulled it. the square root term is the \"optimism bonus\" \u2014 it is big when $n_a(t)$ is small (you have not tried this arm much) and shrinks as you pull the arm more and get a better estimate.\n\nso ucb1 is like the restaurant reviewer who goes back to the good places but *also* keeps trying the underexplored ones, just in case there is a hidden gem. over time, the bonus terms shrink, and you naturally settle on the best arm.\n\nthe regret of ucb1 depends on the gaps between the arms. if arm $a$ has a suboptimality gap $\\delta_a$ (how much worse it is than the best arm), then the gap-dependent regret is $o\\!\\left(\\sum_{a:\\delta_a>0}\\frac{\\log t}{\\delta_a}\\right)$. arms that are almost as good as the best get pulled more before you can distinguish them. arms that are clearly worse get eliminated quickly. the gap-free version, which does not assume you know the gaps, gives $o(\\sqrt{kt\\log t})$.\n\n## exp3: when the world is adversarial\n\nucb1 relies on a critical assumption: the rewards come from fixed distributions. but what if the adversary can change the payoffs each round? what if the slot machines are rigged, and someone is rewriting the payout tables every night?\n\nthat is the adversarial bandit setting, and ucb1 falls apart. you need exp3 \u2014 the exponential-weight algorithm for exploration and exploitation.\n\nexp3 is hedge's cousin, adapted for the bandit world. remember hedge? it worked in the full-information setting because you could see all the losses. in the bandit setting, you only see the loss of the arm you pulled. so you have to *estimate* the losses of the arms you did not pull. here is how:\n\nyou maintain weights $w_t(i)$ on each arm, just like hedge. you build a sampling distribution that mixes exploration (trying everything) with exploitation (favoring high-weight arms). then you pull an arm, observe the loss, and here comes the clever part \u2014 you construct an importance-weighted loss estimate.\n\nyou only see the loss of the arm you pulled, but you pretend you can estimate the loss of every arm. for the arm you did pull, you take the observed loss and divide it by how likely you were to pull that arm. it is like correcting for the fact that you only measured one kid's height but want to know the class average. if you measured a kid you were very likely to measure, the correction is small. if you measured a kid you almost never pick, the correction is large \u2014 that one measurement carries a lot of weight.\n\nthis importance-weighted estimate is unbiased \u2014 on average it gets the right answer \u2014 but it has high variance. the exploration mixing helps control that variance by ensuring you pull every arm with at least some minimum probability.\n\nthe regret of exp3 scales as\n\n$$\nr_t = o(\\sqrt{kt\\log k}).\n$$\n\nthe lower bound for adversarial bandits is $\\omega(\\sqrt{kt})$, so exp3 is near-optimal, off by just a $\\sqrt{\\log k}$ factor. that is a beautifully tight result.\n\n[[simulation multi-armed-bandit]]\n[[simulation bandit-regret-comparison]]\n[[simulation bernoulli-trials]]\n\n---\n\n*we now have two powerful bandit algorithms \u2014 ucb1 for stochastic worlds and exp3 for adversarial ones. but both of them treat every round the same. what if you get a clue before each decision \u2014 a context that tells you something about the current situation? next lesson, we add context to the bandit problem with exp4, and suddenly the problem starts looking like real-world personalization.*\n"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "contextual-bandits-exp4",
      "lessonTitle": "Contextual Bandits and EXP4",
      "x": 0.023991050198674202,
      "y": 0.18223077058792114,
      "searchText": "contextual bandits and exp4\n# contextual bandits and exp4\n\n## the doctor's dilemma\n\na doctor sees a patient walk through the door. the patient has a specific medical chart \u2014 age, blood pressure, lab results, symptoms. the doctor has three possible treatments. she picks one. the patient either gets better or does not. the doctor never finds out what would have happened under the other two treatments for *this specific patient*.\n\nthat is a contextual bandit problem. the medical chart is the **context**. the treatment choice is the **action**. the outcome is the **bandit feedback** \u2014 you only see the result of the action you took. but unlike plain bandits, the context gives you leverage. patients who look similar to this one might tell you something about which treatment works here.\n\nthis is arguably the most practically important setting in the entire course. personalized advertising works this way \u2014 the user's profile is the context, the ad shown is the action, and whether they click is the feedback. adaptive interfaces, recommendation engines, medical decision support, dynamic pricing \u2014 all contextual bandits.\n\n## how exp4 works\n\nexp4 stands for exponential-weight algorithm for exploration and exploitation using expert advice. it is exp3's big sibling, designed for the contextual setting.\n\nyou have a pool of experts (or policies), and each expert has an opinion about what to do in any given context. when context $x_t$ arrives, each expert proposes a distribution over actions \u2014 \"for this patient, i would prescribe treatment a with 70% probability and treatment b with 30%.\" the learner then mixes these expert opinions, weighted by how well each expert has done so far.\n\nhere is the step-by-step: the context $x_t$ arrives. each expert $e$ proposes an action distribution for this context. you form a mixture distribution over experts, weighted by their accumulated performance. you sample an action from the induced distribution over actions. you observe only the loss of the action you chose. you build an importance-weighted loss estimate (just like exp3), and you update the expert weights exponentially.\n\nthe importance weighting is the same trick as before \u2014 you correct for the fact that you only observed one action's loss by dividing by the probability of having chosen that action. this lets you update *all* expert weights, even though you only saw feedback for one action.\n\n## why the regret bound is magical\n\nexp4 achieves regret that scales roughly with $\\sqrt{t \\log |\\mathcal{e}|}$, where $\\mathcal{e}$ is the expert class (up to constants and the bandit feedback penalty).\n\nhere is why that is remarkable. if you have a million experts \u2014 a million possible policies, each one mapping contexts to actions differently \u2014 the $\\log$ term is only about 14. so you are basically paying the bandit price times $\\sqrt{14}$. whether you compete against ten policies or a million, the penalty is gentle. that is the power of exponential weights: the logarithmic dependence on the number of experts means you can afford to have a huge, rich policy class without blowing up the regret.\n\ncompare this to what would happen if you tried each expert one at a time. with a million experts, you would need a million rounds just to try each one once. exp4 finds the best expert in far fewer rounds because it updates all of them in parallel using the importance-weighted feedback trick.\n\n[[simulation contextual-bandit-exp4]]\n\n---\n\n*we have now covered the full spectrum of online learning, from full-information to bandits to contextual bandits. but in all these settings, the world had no memory \u2014 your action today did not change what happens tomorrow. next lesson, everything changes. we enter the world of markov decision processes, where your actions shape the future state of the world. that is where reinforcement learning really begins.*\n"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "deep-rl-dqn",
      "lessonTitle": "Function Approximation and Deep Q-Learning",
      "x": 0.14010661840438843,
      "y": 0.1890638768672943,
      "searchText": "function approximation and deep q-learning\n# function approximation and deep q-learning\n\n## when the lookup table does not fit\n\nimagine you are trying to build a q-table for a spaceship with a million rooms. each room has ten possible actions. that is ten million entries in your table. now imagine the spaceship is actually an atari game with pixels on the screen \u2014 the state space is every possible arrangement of pixels, which is astronomically large. your lookup table would need more entries than there are atoms in the universe. it simply does not fit.\n\nso we do something radical: instead of storing one value for every state-action pair, we build a **function approximator** \u2014 a machine that takes in a state (or a state-action pair) and outputs a value estimate. we *guess* the values using a parameterized model. for simple problems, a linear model works. for complex, high-dimensional problems like game screens or robot sensors, we use a deep neural network. that network takes the raw state as input and outputs q-values for every action.\n\n## the neural network as a value guesser\n\nthe idea is elegant: train a neural network $q(s,a;\\theta)$ to approximate $q^*(s,a)$. you collect experience by interacting with the environment, and you minimize the td error \u2014 the gap between the network's prediction and the bootstrap target $r + \\gamma \\max_{a'} q(s',a';\\theta)$.\n\nbut then something goes wrong. the network starts dreaming and hallucinating. values diverge. training becomes unstable. why? three problems conspire against you.\n\nfirst, the training data is **correlated**. consecutive transitions in an episode are nearly identical \u2014 the game barely changes between one frame and the next. imagine the network trying to learn from the last game it played, but it keeps replaying the exact same five seconds over and over. that is like studying only page 47 of the textbook. it overfits to recent experience and forgets everything else.\n\nsecond, the **targets move**. in supervised learning, the labels are fixed \u2014 a cat is always a cat. but in q-learning, the target depends on the network's own parameters. when you update the weights, the targets change, which changes the loss, which changes the gradients. you are chasing a moving target, and the target moves because you are chasing it.\n\nthird, we are combining function approximation, bootstrapping, and off-policy learning \u2014 the **deadly triad** from last lesson. without careful engineering, the system explodes.\n\n## dqn: engineering away the instability\n\ndqn (deep q-network) was the breakthrough that showed you *can* make deep rl work. it solves the three problems above with two key tricks.\n\n**the replay buffer** is the solution to correlated data. instead of training on transitions in order, you store every transition $(s, a, r, s')$ in a big memory buffer. when it is time to train, you sample a random mini-batch from the buffer. this mixes up the pages \u2014 you might get a transition from episode 1, another from episode 50, another from episode 200, all in the same batch. the samples are decorrelated, and the network sees a diverse diet of experience.\n\nthe replay buffer also solves **catastrophic forgetting** \u2014 the tendency of neural networks to forget old knowledge when trained on new data. without the buffer, the network would learn to play the current level of the game and completely forget how to play the earlier levels. by replaying old transitions, the network retains its accumulated knowledge.\n\n**the target network** is the solution to moving targets. instead of using the current network to compute the bootstrap target, you maintain a *separate* copy of the network \u2014 the target network \u2014 and only update it periodically (say, every 10,000 steps). between updates, the target is fixed, so the optimization is more like supervised learning. you are still bootstrapping, but the target is stable enough for the gradient descent to make progress without chasing itself in circles.\n\nadditional stabilization tricks include **gradient clipping** (preventing the gradients from becoming too large), **robust loss functions** (like huber loss instead of squared error), and **double-q learning** (using two networks to reduce overestimation of q-values, where one network selects the best action and the other evaluates it).\n\n## beyond dqn\n\ndqn opened the floodgates, but it is just the beginning. **policy gradient methods** directly optimize the policy without learning q-values at all \u2014 they adjust the probability of taking each action based on whether the resulting episode was good or bad. **actor-critic methods** combine the two ideas: a \"critic\" estimates values (like dqn), while an \"actor\" adjusts the policy using the critic's guidance. this gives you lower variance than pure policy gradients and more flexibility than pure value-based methods.\n\nfor **continuous control** \u2014 robotics, motor control, anything where the action is a real number rather than a discrete choice \u2014 variants like ddpg and sac extend these ideas to continuous action spaces.\n\n[[simulation dqn-stability]]\n[[simulation activation-functions]]\n[[simulation cartpole-learning-curves]]\n\n---\n\n*we have scaled rl to huge state spaces with neural networks. but we have been assuming the game eventually ends \u2014 each episode terminates and resets. what happens when the game never ends? next lesson, we tackle average-reward rl and the question of continuing tasks, where there is no natural endpoint and you have to think about long-run steady-state performance.*\n"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "feedback-and-settings",
      "lessonTitle": "Forms of Feedback and Problem Settings",
      "x": 0.03876831382513046,
      "y": 0.22253936529159546,
      "searchText": "forms of feedback and problem settings\n# forms of feedback and problem settings\n\nthe kind of feedback you receive after each decision changes *everything* \u2014 the algorithms you can use, the regret you can achieve, and the difficulty of the problem. this lesson is about understanding those different feedback models, because everything else in the course is just different flavors of the same underlying pain.\n\n## full-information feedback\n\nin the full-information setting, you make a decision and then the world reveals *all* the outcomes \u2014 not just for the action you chose, but for every action you *could* have chosen.\n\nat round $t$, you observe the whole loss vector:\n\n$$\n(\\ell_t(1), \\ell_t(2), \\ldots, \\ell_t(k)).\n$$\n\nthink of it this way: you are betting on a horse race. after the race, you find out not only how your horse did, but the finishing times of *every* horse. you know exactly how much you would have won or lost on each one. that is a huge amount of information.\n\nthis is the setting we call **prediction with expert advice**. you have $n$ experts, each one gives you advice, you make a decision, and then you see how *all* of them would have done. it is the friendliest version of the online learning problem, and it is where algorithms like hedge shine. the regret you can achieve here scales as $o(\\sqrt{t \\log n})$ \u2014 the square root is the signature of a no-regret algorithm.\n\n## bandit (partial) feedback\n\nnow the world gets cruel. you pull the slot-machine lever and the machine only tells you how much you lost on *that* pull. the other levers stay silent. you have no idea what would have happened if you had pulled lever 3 instead of lever 7. that silence is the enemy.\n\nformally, you only observe:\n\n$$\n\\ell_t(a_t).\n$$\n\njust one number. out of potentially hundreds or thousands of actions, you get feedback on exactly one. if you want to know anything about the other actions, you have to *try them*, which means sacrificing rounds where you could have been exploiting what you already know. this is the exploration-exploitation dilemma, and it is the heartbeat of the bandit problem.\n\nthe cost of partial feedback shows up directly in the regret bounds. that $o(\\sqrt{t \\log n})$ from the full-information setting blows up to $o(\\sqrt{kt})$ for bandits, where $k$ is the number of arms. the extra $\\sqrt{k}$ factor is the price you pay for flying blind on all the actions you did not try. you have to explicitly spend rounds exploring, and every exploration round is a round where you might be losing more than you had to.\n\n## contextual feedback\n\nsometimes, before you make your decision, the world hands you a clue. a context vector $x_t$ arrives \u2014 think of it as a patient's medical chart, or a user's browsing history, or the current weather conditions \u2014 and you use that clue to pick your action. then you get bandit feedback: you only see the outcome of the action you chose, given that context.\n\nyou choose $a_t$ using $(x_t, \\text{history})$ and receive only $\\ell_t(a_t)$.\n\nthis is the **contextual bandit** setting, and it is arguably the most practical of the three. a doctor sees a patient (context), prescribes a treatment (action), and observes the outcome (feedback). she never finds out what the other treatments would have done for *this* patient. but the context gives her leverage \u2014 patients who look similar to this one might tell her something useful.\n\n## the spectrum of difficulty\n\nthese three settings \u2014 full information, bandit, contextual \u2014 form a spectrum from easy to hard. in the full-information setting, learning is relatively cheap because you see all the outcomes. in the bandit setting, learning is expensive because you see only one outcome and have to explore. in the contextual setting, you have the bandit's pain but the context's help, and the balance between them determines your regret.\n\nthere are additional complications that make the problem harder still. **delayed feedback** means you do not find out the outcome for several rounds. **noisy feedback** means the loss signal is corrupted. **non-stationary environments** mean the world changes beneath your feet \u2014 the best action today might not be the best action tomorrow. **adversarially chosen outcomes** mean someone is actively trying to make you lose.\n\neach of these variants changes the concentration arguments you can use, the variance of your estimators, and the final regret bounds. but they all build on the same foundation: the type of feedback determines the type of algorithm.\n\n---\n\n*we now understand the three feedback models that govern online learning. next lesson, we are going to build our first real algorithm \u2014 follow the leader and its much smarter cousin, hedge \u2014 and see how exponential weights can achieve no-regret guarantees in the full-information setting. that is our first weapon.*\n"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "ftl-and-hedge",
      "lessonTitle": "Follow the Leader and Hedge",
      "x": 0.016590161249041557,
      "y": 0.2591286301612854,
      "searchText": "follow the leader and hedge\n# follow the leader and hedge\n\n## follow the leader: the obvious idea that fails\n\nthe simplest strategy you could imagine is: at each round, look at everything that has happened so far and pick the action that would have been best. formally, ftl chooses\n\n$$\na_t \\in \\arg\\min_{a \\in \\mathcal{a}} \\sum_{s=1}^{t-1}\\ell_s(a).\n$$\n\nin words: you pick the action with the lowest total loss so far. it sounds perfectly reasonable. if expert 2 has been right most often, you follow expert 2.\n\nftl is simple and parameter-free, and when the losses are well-behaved (think strongly convex or stable), it actually works. but here is the problem: against an adversary, ftl can be a disaster. the adversary just keeps flipping the best action back and forth. round 1, expert a is best. round 2, expert b is best. round 3, expert a again. ftl chases back and forth like a dog chasing two squirrels, and its regret grows *linearly* \u2014 which is the worst possible outcome.\n\n[[simulation ftl-instability]]\n\nthe lesson from ftl is profound: you need *stability*. you cannot just pick the current leader \u2014 you need to hedge your bets.\n\n## hedge: the algorithm that hedges\n\nnow we build the algorithm that fixes ftl's problem. imagine you are in a classroom with $n$ students (experts), and each student raises their hand to vote on the answer. at first, you trust everyone equally. but after each round, when you see who was right and who was wrong, you adjust your trust.\n\nhere is the key insight: instead of following one expert, you follow *all of them*, weighted by how much you trust each one. and when an expert is wrong, you do not drop them entirely \u2014 you just reduce their weight. gently. multiplicatively.\n\nsetting: $n$ experts, full-information losses $\\ell_t(i)\\in[0,1]$.\n\nyou start with equal weights: $w_1(i)=1$ for every expert. at each round, you choose a probability distribution over experts based on their weights:\n\n$$\np_t(i)=\\frac{w_t(i)}{\\sum_j w_t(j)}.\n$$\n\nthen you observe all the losses and update:\n\n$$\nw_{t+1}(i)=w_t(i)\\exp(-\\eta \\ell_t(i)).\n$$\n\nthink of each expert as carrying a backpack of confidence. every time an expert is wrong, you punch a hole in their backpack so they get a little lighter. but you do it gently, controlled by the parameter $\\eta$. a large loss means a bigger hole, and over time the bad experts' backpacks deflate while the good experts stay heavy.\n\n## the learning rate: adjusting the gas pedal\n\nwe need a learning rate $\\eta$ that works like adjusting the gas pedal on a long road trip. if you keep the pedal all the way down (large $\\eta$), you react too aggressively to every mistake \u2014 you overshoot the exit. if you barely tap the pedal (tiny $\\eta$), you never get there in time. the sweet spot turns out to be roughly\n\n$$\n\\eta \\asymp \\sqrt{\\frac{\\log n}{t}}.\n$$\n\nthat is the square root of the log of how many experts you have, divided by the total time horizon. with this choice, regret scales as\n\n$$\nr_t = o(\\sqrt{t\\log n}).\n$$\n\nnotice that the dependence on $n$ is only logarithmic. if you have a million experts, $\\log n$ is about 14. so whether you have 10 experts or a million, hedge handles it gracefully. the regret grows with $\\sqrt{t}$, which is sublinear \u2014 your average regret per round goes to zero.\n\n## the potential function: total confidence in the room\n\nthe proof that hedge works revolves around a beautiful idea called the **potential function**. define\n\n$$\n\\phi_t=\\sum_i w_t(i).\n$$\n\nthis is the total weight in the room \u2014 the sum of all the confidence backpacks. at the start, $\\phi_1 = n$ because everyone starts with weight 1.\n\nnow think about what happens each round. when you apply the exponential update, the total weight changes. if most experts did well (small losses), the total weight barely drops. if everyone did badly, it drops a lot. the key is that $\\phi_t$ can never go below the weight of the best expert \u2014 because the best expert's weight is always part of the sum.\n\nby tracking how fast $\\phi_t$ shrinks from above (using the average loss and a second-order correction from the curvature of the exponential) and how slowly it shrinks from below (because the best expert's weight barely changes), you can sandwich the regret between two expressions that both come out to $o(\\sqrt{t \\log n})$.\n\nthe potential function is like measuring the temperature of the room. if the room is getting cold fast, it means everyone is losing. if it is cooling slowly, the good experts are holding up. the gap between the two rates of cooling is the regret.\n\n[[simulation hedge-weights-regret]]\n\n---\n\n*we just built hedge, our first real no-regret algorithm for the full-information setting. but what happens when you only see the outcome of the action you chose? next lesson, we enter the world of bandits \u2014 ucb1 for stochastic settings and exp3 for adversarial ones \u2014 where the silence of the unchosen arms forces us to explore.*\n"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "home",
      "lessonTitle": "Online and Reinforcement Learning",
      "x": 0.0761379525065422,
      "y": 0.23030193150043488,
      "searchText": "online and reinforcement learning\n# online and reinforcement learning\n\n## why are we even here?\n\npicture two weather forecasters. every single morning, forecaster a walks to the microphone and says \"rain.\" every single morning, forecaster b says \"sunny.\" neither of them looks out the window. neither of them checks the data. they just repeat themselves, day after day, for a thousand days.\n\nnow here is the question that launches this entire course: at the end of those thousand days, which forecaster was better? you do not know. you *cannot* know until you look at what actually happened. if it rained 900 out of 1000 days, forecaster a looks like a genius. if the sun shone 900 days, forecaster b wins.\n\nbut what if *you* could do something smarter? what if, each morning, you looked at the track records of both forecasters \u2014 and all their friends \u2014 and weighted your prediction toward whoever had been right most often? you would start off clueless, but day by day your cumulative mistakes would grow *slower* than the best single forecaster's mistakes. the gap between your total loss and the best expert's total loss \u2014 that gap is called **regret**. and making that gap grow as slowly as possible is the entire game of online learning.\n\nthat is the thread running through everything in this module. we start with regret, we learn algorithms that minimize it, we move from simple expert advice to slot machines where you only see one outcome at a time, and then we enter the full world of reinforcement learning where your actions change the world around you. every lesson builds on the last. every idea comes back to this: how do you make good decisions, one at a time, when you do not know what is coming next?\n\n## map of the course\n\nthink of this module as a journey through four territories, each one richer and harder than the last:\n\n**online learning** (lessons 1\u20133): you meet regret, you learn about different kinds of feedback, and you build your first algorithm (hedge) that plays the expert-advice game with provably small regret.\n\n**bandits** (lessons 4\u20135): now the world gets meaner. you only see the outcome of the action you chose \u2014 the other options stay silent. you learn ucb1, exp3, and exp4, algorithms that balance exploration with exploitation.\n\n**full reinforcement learning** (lessons 6\u20138): the world gains memory. your actions change future states. you learn mdps, monte carlo methods, td learning, sarsa, and q-learning \u2014 the core toolkit for sequential decision-making.\n\n**deep rl and continuing tasks** (lessons 9\u201310): state spaces explode. you cannot keep a lookup table anymore, so you bring in neural networks. then you ask: what happens when the game never ends?\n\n```\nonline learning \u2500\u2500\u2192 bandits \u2500\u2500\u2192 full rl \u2500\u2500\u2192 deep rl & continuing tasks\n (lessons 1-3)    (lessons 4-5)  (lessons 6-8)    (lessons 9-10)\n  regret            ucb1          mdps              dqn\n  feedback          exp3          monte carlo       average reward\n  hedge             exp4          td/sarsa/q        online rl\n```\n\n[[figure mdp-agent-environment-loop]]\n\n[[figure orl-cartpole-learning-image]]\n\n## why this topic matters\n\nonline learning and reinforcement learning are not abstract curiosities. they are the mathematics behind any system that must make repeated decisions under uncertainty.\n\nwhen a spam filter decides whether to block an email, it is playing this game \u2014 the adversary is an attacker who adapts. when a doctor chooses between treatments for a patient, she is facing a contextual bandit \u2014 each patient is different, and she only sees the outcome of the treatment she prescribed. when a robot navigates a warehouse, it is solving a markov decision process \u2014 its actions change where it ends up next.\n\nthese ideas power online advertising, recommendation engines, adaptive routing in networks, automated trading, and robotics. wherever you see repeated interaction with an uncertain world, you will find the tools from this course.\n\n## key mathematical ideas\n\nthe mathematics in this course revolves around four big themes. first, **regret minimization** \u2014 you will see how to define and bound the gap between your performance and the best fixed strategy in hindsight, without any statistical assumptions on the data. second, **exponential weights and mirror-style updates** \u2014 the engine behind algorithms like hedge that multiply confidence by performance. third, **exploration-exploitation trade-offs** \u2014 the bandit's dilemma, where you must balance trying new actions against sticking with what has worked. fourth, **bellman operators and fixed-point reasoning** \u2014 the backbone of mdp theory, where the right value is the one that does not change when you apply the update rule.\n\n## prerequisites\n\nyou should be comfortable with basic probability and random variables \u2014 expectations, variances, conditional probabilities. you will need some linear algebra, mostly matrix-vector products and norms. basic optimization ideas (gradients, convexity) will help, and some exposure to machine learning concepts will give you context, though we will build everything we need from scratch.\n\n## recommended reading\n\nwe recommend four books that complement this course beautifully. sutton and barto's *reinforcement learning: an introduction* (2nd edition) is the standard rl textbook and covers the mdp and td material in great depth. cesa-bianchi and lugosi's *prediction, learning, and games* is the bible for online learning and expert advice. lattimore and szepesvari's *bandit algorithms* is the modern reference for everything bandits. and hazan's *introduction to online convex optimization* gives you the optimization perspective that ties it all together.\n\n## learning trajectory\n\nthis module is organized from foundational online learning concepts through bandits to full reinforcement learning:\n\n1. **the notion of regret** \u2014 the central performance metric for online learning, and the hero of our story.\n2. **forms of feedback and problem settings** \u2014 full-informati"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "mdp-and-dp",
      "lessonTitle": "MDPs and Dynamic Programming",
      "x": 0.14374099671840668,
      "y": 0.23813942074775696,
      "searchText": "mdps and dynamic programming\n# mdps and dynamic programming\n\n## the world remembers\n\nup to now, the world had no memory. you pulled a slot-machine lever, you got a payout, and the world reset. your action did not change what happened next. now the world *remembers* what you did yesterday. that changes everything.\n\nif you are a robot navigating a warehouse, and you turn left at the first aisle, you end up in a different part of the building than if you turned right. the state of the world \u2014 your position \u2014 depends on your previous action. and the reward you get (finding the package vs hitting a wall) depends on that state. this is a **markov decision process**: a world where your actions have consequences that ripple forward in time.\n\n## the mdp tuple\n\nan mdp is described by five things:\n\n$$\n\\langle \\mathcal{s}, \\mathcal{a}, p, r, \\gamma \\rangle\n$$\n\n$\\mathcal{s}$ is the set of states \u2014 all the places the world can be. $\\mathcal{a}$ is the set of actions \u2014 all the moves you can make. $p$ is the transition kernel \u2014 for every state and action, it tells you the probability of ending up in each next state. $r$ is the reward function \u2014 what you earn for taking an action in a state. and $\\gamma$ is the discount factor, a number between 0 and 1 that says how much you care about future rewards compared to immediate ones.\n\na **policy** $\\pi$ tells you what to do in each state. it can be deterministic (\"always go left in state 5\") or stochastic (\"go left with probability 0.7, right with probability 0.3\"). the goal is to find the policy that maximizes cumulative discounted reward.\n\n## value functions: scoring a policy\n\nhow good is a policy $\\pi$? we measure it with value functions. the state-value function $v^\\pi(s)$ tells you the expected total discounted reward if you start in state $s$ and follow $\\pi$ forever:\n\n$$\nv^\\pi(s)=\\mathbb{e}^\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_t \\mid s_0=s\\right].\n$$\n\nthe action-value function $q^\\pi(s,a)$ tells you the same thing, but conditioned on taking action $a$ first and then following $\\pi$:\n\n$$\nq^\\pi(s,a)=\\mathbb{e}^\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_t \\mid s_0=s,a_0=a\\right].\n$$\n\nthink of $v^\\pi(s)$ as the \"worth\" of being in state $s$ under policy $\\pi$. a state near the goal with a good policy has high value. a state far from the goal or served by a bad policy has low value.\n\n## the bellman equation: the fixed point where ice meets water\n\nthe bellman equation is the most important equation in reinforcement learning. it says: the value of a state is the immediate reward plus the discounted value of wherever you end up next.\n\n$$\nv^*(s)=\\max_a\\left[r(s,a)+\\gamma\\sum_{s'}p(s'|s,a)v^*(s')\\right].\n$$\n\nread it aloud: the optimal value of state $s$ is the best you can do by choosing the action that maximizes your immediate reward plus $\\gamma$ times the expected value of the next state.\n\nthis equation has a **fixed point** \u2014 a unique solution $v^*$ that does not change when you apply the bellman operator. think of it like the temperature where ice and water are happy together: 0\u00b0c. if you heat the ice, it melts and comes back to 0\u00b0. if you cool the water, it freezes and comes back to 0\u00b0. the fixed point is the equilibrium. the bellman equation works the same way \u2014 if you start with any guess for $v$ and keep applying the operator, you converge to $v^*$. the operator is a **contraction** in the discounted case: every application brings you closer to the fixed point. that guarantees convergence and uniqueness.\n\nfor policy evaluation (not optimizing, just scoring a fixed policy), the bellman equation becomes linear: $v^\\pi = r^\\pi + \\gamma p^\\pi v^\\pi$. this you can solve by matrix inversion in small problems, or by iterating the backup operator until convergence.\n\n## dynamic programming: planning when you know the model\n\nwhen you know the transition probabilities and rewards \u2014 when you have a perfect map of the world \u2014 you can solve the mdp with dynamic programming. these are **planning** methods, not learning methods. you are doing math on the model, not interacting with the world.\n\n**policy evaluation** takes a fixed policy $\\pi$ and computes $v^\\pi$ by iteratively applying the bellman backup. you start with an arbitrary guess and keep updating each state's value based on its successors' values. each sweep brings you closer to the true value.\n\n**policy iteration** alternates between two steps: evaluate the current policy (compute $v^\\pi$), then improve it by acting greedily with respect to $v^\\pi$ (pick the action that looks best given the current values). this creates a new, better policy, and you repeat. policy iteration converges in a finite number of steps because there are only finitely many deterministic policies.\n\n**value iteration** is more direct \u2014 you repeatedly apply the bellman *optimality* operator, which combines evaluation and improvement into one step. it converges to $v^*$ as the number of sweeps grows.\n\nthese methods are elegant and exact, but they require knowing $p$ and $r$. in the real world, you usually do not have that luxury. the next lessons are about what happens when you have to *learn* the values by interacting with the environment.\n\n[[simulation gridworld-mdp]]\n[[simulation dp-convergence]]\n[[simulation mdp-simulation]]\n\n---\n\n*we just learned how to plan in a world with memory \u2014 mdps and dynamic programming give us the tools to compute optimal policies when we know the model. but what if we do not know the model? next lesson, we learn monte carlo methods: play the game until it ends, write down the score, and learn from experience. no model required.*\n"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "monte-carlo-rl",
      "lessonTitle": "Monte Carlo Methods for RL",
      "x": 0.15923301875591278,
      "y": 0.21228134632110596,
      "searchText": "monte carlo methods for rl\n# monte carlo methods for rl\n\n## just play the game and write down the score\n\nmonte carlo is just \"play the game until it ends and write down the score.\" sounds stupid until you realize that is exactly how you learned to ride a bike \u2014 you did not solve differential equations of balance and angular momentum. you got on, you fell off, you got on again, and eventually your brain figured out the pattern from the accumulated experience of many complete attempts.\n\nmonte carlo methods estimate values from **complete episodes**. no transition model is required. you do not need to know the probabilities of anything. you just need to be able to play the game, observe the rewards, and remember what happened.\n\n## the core idea\n\nafter an episode finishes, you look back at every state you visited and compute the **return** \u2014 the total discounted reward from that point forward:\n\n$$\ng_t = \\sum_{k=0}^{t-t-1} \\gamma^k r_{t+k+1}.\n$$\n\nthen you estimate the value of each state by averaging the returns you observed whenever you visited that state:\n\n$$\nv^\\pi(s)\\approx \\frac{1}{n(s)}\\sum_{t:\\,s_t=s} g_t.\n$$\n\nthat is it. play many episodes, average the returns for each state, and the law of large numbers does the rest. as the number of episodes grows, your estimates converge to the true values.\n\n## variants: when to count the visit\n\n**first-visit mc** only uses the return from the *first* time you visit a state in an episode. if you pass through state $s$ three times in one episode, you only record the return from the first visit. this gives you independent samples (across episodes) and clean convergence guarantees.\n\n**every-visit mc** uses the return from *every* visit to state $s$ within an episode. you get more data per episode, but the samples within an episode are correlated. both variants converge to the right answer, but their finite-sample properties differ.\n\n## on-policy vs off-policy\n\n**on-policy mc control** learns about the policy it is actually following. you play episodes using an epsilon-greedy policy (mostly exploit, sometimes explore), compute returns, update action-values, and improve the policy. the exploration keeps you from getting stuck, and the updates keep improving the policy. it is simple and stable.\n\n**off-policy mc** is trickier but more flexible. you collect episodes using one policy (the behavior policy) and use importance sampling to correct the returns so they reflect a *different* policy (the target policy). this is useful when you want to learn about the optimal policy while exploring with a safe, exploratory policy. the importance-sampling correction can have high variance, though, so you need many episodes for the estimates to settle down.\n\n## trade-offs: why mc is both wonderful and frustrating\n\nmonte carlo methods have **low bias**. the return targets are computed from actual rewards, not from estimated values. there is no bootstrapping, no approximation in the target. what you see is what you get.\n\nbut they have **high variance**, especially when episodes are long. a single episode might wander through many states and collect wildly different rewards depending on the random actions taken. averaging over many episodes tames this variance, but it can take a lot of episodes.\n\nmonte carlo is also **naturally episodic**. you have to wait until the episode ends to compute returns. this is fine for games that terminate (chess, go, a round of poker), but it is a problem for continuing tasks that never end (a robot that runs 24/7). for those, you need temporal-difference methods, which is exactly where we are headed next.\n\n[[simulation monte-carlo-convergence]]\n\n---\n\n*monte carlo taught us to learn values from complete episodes, no model required. but waiting until the game ends to learn anything feels wasteful. what if you could update your estimates after every single step? next lesson, we meet td learning, sarsa, and q-learning \u2014 methods that bootstrap from their own estimates and learn as they go.*\n"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "regret",
      "lessonTitle": "The Notion of Regret",
      "x": 0.05191873759031296,
      "y": 0.25348973274230957,
      "searchText": "the notion of regret\n# the notion of regret\n\n## the weather forecaster story\n\nimagine you are a weather forecaster, and every morning you have to announce \"rain\" or \"sunny.\" you have access to five colleagues \u2014 five experts \u2014 who each give you their prediction. you do not know who is good. you do not know the weather patterns. all you can do is listen to them, pick one prediction, and then wait to see what actually happens.\n\nafter a thousand mornings, you look back. one of those five experts \u2014 let us call her expert 3 \u2014 turned out to be right 820 times. you, following your strategy of mixing and matching their advice, were right 790 times. the gap \u2014 those 30 extra mistakes you made compared to expert 3 \u2014 that is your **regret**.\n\nnow here is the beautiful part. you did not know in advance that expert 3 would be the best. you had to figure it out as you went. and yet your regret was only 30 out of 1000 rounds. that is a tiny fraction. your average per-round regret was essentially zero. that is what we are after in this course: strategies that guarantee your regret stays small, no matter what the world throws at you.\n\n## the formal definition\n\nfor losses, we write external regret over horizon $t$ as\n\n$$\nr_t = \\sum_{t=1}^{t} \\ell_t(a_t) - \\min_{a \\in \\mathcal{a}} \\sum_{t=1}^{t} \\ell_t(a).\n$$\n\nthe first sum is your cumulative loss \u2014 every bad call you made. the second term is the cumulative loss of the single best action in hindsight. the difference is regret: how much worse you did than the best fixed strategy, looking back with perfect knowledge.\n\nif you plot two curves \u2014 your loss growing over time and the best expert's loss growing over time \u2014 the shaded area between them is your regret. a good algorithm keeps that shaded area from growing too fast.\n\n[[simulation regret-growth-comparison]]\n\n## sublinear regret: why it means you are winning\n\n**sublinear regret** means $r_t = o(t)$. in plain language: your regret grows, but it grows slower than the number of rounds. if you play for a thousand rounds and your regret is only 30, that is 3% per round. if you play for a million rounds and your regret is only 1000, that is 0.1% per round. the longer you play, the closer your average performance gets to the best expert. if you are only a tiny bit worse each day, over a thousand days you still look like a genius.\n\nthis is why regret replaces the usual \"generalization error\" from classical machine learning. you do not need any i.i.d. assumption. you do not need the data to come from a nice distribution. the losses can be chosen by an adversary who is actively trying to mess you up, and you can *still* guarantee sublinear regret. that is a remarkable guarantee.\n\n## variants of regret\n\nthere are several flavors of regret, and it helps to know which one you are working with.\n\n**expected regret**, $\\mathbb{e}[r_t]$, averages over any randomness in your algorithm. this is the most common version in stochastic settings.\n\n**high-probability regret** gives bounds that hold with probability at least $1-\\delta$. you pay a small price in the bound (usually a log factor in $1/\\delta$), but you get a guarantee that works on any single run, not just on average.\n\n**pseudo-regret** shows up in stochastic bandits. instead of comparing against the best fixed arm's *realized* loss sequence, you compare against its *expected* loss. it is a slightly weaker benchmark but often easier to analyze.\n\n**external regret** vs **internal (swap) regret**: external regret compares you against the best single action. internal regret asks a harder question \u2014 for every action $a$ you played, would you have done better by swapping it with some other action $a'$? no-internal-regret algorithms are more powerful and connect to game-theoretic equilibria.\n\n## adversarial vs stochastic: two different worlds\n\nin **stochastic bandits**, the losses or rewards come from fixed distributions. you are pulling slot-machine levers, and each lever has a true average payout that does not change. the difficulty is that you do not know these averages.\n\nin **adversarial online learning**, the losses can be anything. an adversary picks the loss sequence, possibly after seeing your strategy. no distributional assumptions at all. this is the harder setting, and the guarantees you get are necessarily weaker \u2014 but they are also more robust.\n\nthe minimax lower bounds tell you the price of feedback. with full information (you see every expert's loss after each round), the best possible regret is $\\omega(\\sqrt{t \\log n})$ where $n$ is the number of experts. with bandit feedback (you only see the loss of the arm you pulled), it jumps to $\\omega(\\sqrt{kt})$ where $k$ is the number of arms. that extra $\\sqrt{k}$ is the tax you pay for partial feedback.\n\n## practical examples\n\nthink of a portfolio manager who invests in stocks every day. at the end of the year, she looks back and compares her cumulative return to the single best stock she could have held all year. her regret is the difference. a no-regret strategy guarantees she is not much worse than that best-in-hindsight stock, even though she could not predict the market.\n\nor think of an email system that routes your messages to different folders using a set of rules (experts). after a year of routing, you compare the system's accuracy to the single best routing rule. regret measures the gap, and a good algorithm makes that gap vanishingly small over time.\n\n---\n\n*okay, we just learned what regret is and why it is the right way to measure performance in online learning. next lesson, we are going to ask: what kind of feedback does the learner actually get? the answer \u2014 full information, bandit, or contextual \u2014 determines everything about which algorithms work and how fast regret can shrink.*\n"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement",
      "lessonSlug": "td-sarsa-qlearning",
      "lessonTitle": "Temporal-Difference Learning, SARSA, and Q-Learning",
      "x": 0.1471557319164276,
      "y": 0.17209821939468384,
      "searchText": "temporal-difference learning, sarsa, and q-learning\n# temporal-difference learning, sarsa, and q-learning\n\n## learning without waiting for the end\n\nmonte carlo said: play the whole game, then learn from the final score. td learning says: why wait? after every single step, you can update your estimate using the reward you just received and your current guess about the next state's value. you are learning as you go, one step at a time.\n\nthis is **bootstrapping** \u2014 using your own estimates as part of the learning target. it sounds circular, and in a way it is. but it works, and it works remarkably well.\n\n## td(0): the one-step update\n\nthe simplest td method updates the state-value estimate after every transition:\n\n$$\nv(s_t)\\leftarrow v(s_t)+\\alpha\\left[r_{t+1}+\\gamma v(s_{t+1})-v(s_t)\\right].\n$$\n\nthe bracketed term is the **td error** \u2014 the difference between what you expected ($v(s_t)$) and a better estimate based on what actually happened ($r_{t+1}+\\gamma v(s_{t+1})$). you nudge your estimate in the direction of this error, scaled by the step size $\\alpha$.\n\nthink of it like this: you are walking to a restaurant and you estimated it would take 20 minutes. after 5 minutes, you have walked a quarter of the way and you now estimate 18 minutes total. td learning says: update your original estimate *right now*, based on the partial information. you do not have to arrive at the restaurant to revise your prediction.\n\n## sarsa: on-policy control\n\nsarsa learns the value of state-action pairs, and it learns about the policy it is actually following. the name comes from the five quantities it uses: $s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}$.\n\n$$\nq(s_t,a_t)\\leftarrow q(s_t,a_t)+\\alpha\\left[r_{t+1}+\\gamma q(s_{t+1},a_{t+1})-q(s_t,a_t)\\right].\n$$\n\nyou are in state $s_t$, you take action $a_t$, you get reward $r_{t+1}$, you land in state $s_{t+1}$, and you take action $a_{t+1}$ (according to your current policy, say epsilon-greedy). then you update $q(s_t,a_t)$ toward the reward plus the discounted value of what you *actually did* next.\n\nbecause sarsa uses the action you *actually took* in the next state, it learns the value of the policy you are following, including its exploration. if your epsilon-greedy policy occasionally walks off a cliff (because it explores), sarsa learns that the cliff-edge state is dangerous \u2014 it bakes in the risk of falling.\n\n## q-learning: off-policy control\n\nq-learning looks almost identical to sarsa, with one critical difference:\n\n$$\nq(s_t,a_t)\\leftarrow q(s_t,a_t)+\\alpha\\left[r_{t+1}+\\gamma \\max_{a'}q(s_{t+1},a')-q(s_t,a_t)\\right].\n$$\n\ninstead of using the action you *actually took* in the next state, q-learning uses the *best* action \u2014 the $\\max$. it does not matter what you actually did next; q-learning assumes you will act optimally from the next state onward.\n\nthis makes q-learning **off-policy**: it learns about the optimal policy regardless of the behavior policy generating the data. you can explore wildly (random actions, curious detours) and q-learning still converges to $q^*$, the optimal action-value function, as long as you visit every state-action pair often enough and your step sizes satisfy the right conditions.\n\nthe difference is sharp. imagine a gridworld with a cliff along one edge. sarsa, following epsilon-greedy, learns a *safe* path that stays away from the cliff \u2014 because its own exploration sometimes sends it tumbling off. q-learning learns the *optimal* path right along the cliff edge \u2014 because it evaluates the optimal policy, which never falls. q-learning finds the faster route, but sarsa finds the route that is actually safe given how you are behaving. neither answer is wrong; they are answering different questions.\n\n[[simulation sarsa-vs-qlearning]]\n\n## exploration strategies\n\nyou need exploration to ensure you visit enough state-action pairs, but you want to exploit what you have learned to collect reward. three common approaches:\n\n**epsilon-greedy** is the simplest \u2014 with probability $\\epsilon$, take a random action; otherwise, take the greedy action. easy to implement, easy to tune. but it wastes exploration on clearly bad actions.\n\n**softmax (boltzmann) exploration** assigns probabilities proportional to $\\exp(q(s,a)/\\tau)$, where $\\tau$ is a temperature. high temperature means near-uniform exploration; low temperature means near-greedy. it focuses exploration on actions that look promising rather than wasting pulls on obviously bad ones.\n\n**optimism and ucb-like bonuses** add exploration bonuses to the q-values, similar to how ucb1 works in bandits. this connects the rl exploration problem back to the bandit ideas from earlier in the course.\n\n## the deadly triad: three bad roommates\n\nthere is a stability problem lurking in td methods, and it becomes critical when we scale up. imagine three roommates who are perfectly fine individually but, when you put them together, they burn the house down. the three roommates are:\n\n**function approximation** \u2014 instead of storing one value per state in a table, you use a parameterized function (like a neural network) to generalize across states. perfectly reasonable on its own.\n\n**bootstrapping** \u2014 using your own estimates as targets, as td does. works great in tabular settings.\n\n**off-policy learning** \u2014 learning about one policy from data generated by a different policy. efficient and flexible.\n\neach one alone is fine. any two together are usually manageable. but all three together can cause the value estimates to diverge \u2014 to spiral upward or oscillate wildly, never converging. the parameters chase their own tail: the target changes because the parameters changed, which changes the target again, in a runaway feedback loop.\n\nthis is the **deadly triad**, and it is the central challenge of modern rl. dqn, which we meet next lesson, is essentially an engineering answer to the question: how do we use all three roommates without burning the house down?\n\n[[simulation concentration-bounds]]\n[[simulation stochastic-a"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "atom-field-interaction",
      "lessonTitle": "Atom-Field Interaction",
      "x": 0.8649227619171143,
      "y": 0.8215571641921997,
      "searchText": "atom-field interaction\n# atom-field interaction\n__topic 12 keywords__\n- perturbation theory \n- rabi oscillations\n- jaynes-cummings model\n\n# readings\nch. 4.3-5 \n\n# 4.3 interaction of an atom with a quantized field\n#### dipole approximation\nimagine a situation with electromagnetic field act on one atom.\n\nto begin, hamiltonian of an electron bound to an atom in vacuum is\n$$\n    \\hat{h}_\\mathrm{atom}\n    =\n    \\frac{\\hat{p}^2}{2m}\n    +\n    v(\\vec{r})\n$$\nhere, $v(\\vec{r})$ is coulomb potential.\n\nin presence of external fields, the hamiltonian become,\n$$\n    \\hat{h}\n    =\n    \\frac{1}{2m}\n    \\left(\n    \\hat{p}^2\n    +\n    e \\vec{a}(\\vec{r}, t)\n    \\right)^2\n    +\n    v(\\vec{r})\n$$\n\nwe can further simplify the equation.\nthe spatial dependency of electromagnetic field is $e^{i \\vec{k} \\cdot \\vec{r}}$.\nby assuming typical light wavelength as $\\lambda \\sim 500 \\mathrm{nm}$, \nand $\\vec{r}$ is few angstroms, \nthe magnitude of $\\left| \\vec{k} \\cdot \\vec{r} \\right|$ is smaller than one.\n$$\n    \\left| \\vec{k} \\cdot \\vec{r} \\right|\n    \\ll\n    1\n$$\nthus, we can ignore the spatial dependency of electromagnetic field.\n$$\n    \\hat{h}\n    =\n    \\hat{h}_\\mathrm{atom}\n    -\n    \\hat{\\vec{d}} \\cdot \\hat{\\vec{e}} (t)\n$$\nthis is **dipole approximation**.\n\n#### 2-level emitter in cavity (perturbation theory)\nimagine a 2-level system in cavity. \nwe apply single optical mode.\nelectric field on atom is \n$$\n    \\hat{\\vec{e}}\n    =\n    \\vec{\\mathcal{e}}\n    \\left( \n        \\hat{a} + \\hat{a}^\\dag\n    \\right)\n$$\nwe are going to solve schr\u00f6dinger equation.\n$$\n    i\\hbar\n    \\frac{\\partial}{\\partial t}\n    \\ket{\\psi(t)}\n    =\n    \\hat{h}\n    \\ket{\\psi(t)}\n$$\nwe use *ansatz* solution. \n$$\n\\begin{aligned}\n    \\ket{\\psi(t)}\n    &=\n    c_i(t) \n    \\cdot\n    \\ket{a} \n    \\ket{n}\n    \\cdot\n    e^{-i e_a t/\\hbar}\n    e^{-i n \\omega t}\n    +\n    c_f(t) \n    \\cdot\n    \\ket{b} \n    \\ket{n-1}\n    \\cdot\n    e^{-i e_b t/\\hbar}\n    e^{-i (n-1) \\omega t}\n    \\\\&=\n    c_i(t) \n    \\cdot\n    \\ket{i} \n    \\cdot\n    e^{-i e_a t/\\hbar}\n    e^{-i n \\omega t}\n    +\n    c_f(t) \n    \\cdot\n    \\ket{f} \n    \\cdot\n    e^{-i e_b t/\\hbar}\n    e^{-i (n-1) \\omega t}\n\\end{aligned}\n$$\nhere, \n$c_i(t)$ is complex coefficient, \n$\\ket{i}$ is initial state with $a$ state and $n$ photon, \n$e^{-i e_a t/\\hbar}$ is time evolution of atom, \nand $e^{-i n \\omega t}$ is time evolution of photon.\n\n\n# 4.4 the rabi model\n\n# 4.5 fully quantum-mechanical model; the jaynes-cummings model\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "cavity-qed",
      "lessonTitle": "Cavity QED",
      "x": 0.9078330397605896,
      "y": 0.7839357256889343,
      "searchText": "cavity qed\n# cavity qed\n\n## cavity quantum electrodynamics\n\n**cavity qed** studies the interaction between atoms and photons confined in a high-quality resonator. by trapping the electromagnetic field in a small volume, the atom-photon coupling strength $g$ can be made large relative to the dissipation rates, enabling the observation of fundamentally quantum phenomena.\n\nthe three key rates are:\n- $g$: atom-cavity coupling strength.\n- $\\kappa$: cavity photon loss rate (inverse of cavity lifetime).\n- $\\gamma$: atomic spontaneous emission rate into non-cavity modes.\n\nthe **strong coupling regime** $g \\gg \\kappa, \\gamma$ allows coherent exchange of excitations between atom and field before either decays.\n\n## the purcell effect\n\nwhen an atom is placed in a resonant cavity, its spontaneous emission rate is modified. the **purcell factor** gives the enhancement:\n\n$$\nf_p = \\frac{\\gamma_{\\text{cav}}}{\\gamma_{\\text{free}}} = \\frac{3}{4\\pi^2}\\left(\\frac{\\lambda}{n}\\right)^3 \\frac{q}{v},\n$$\n\nwhere $q$ is the cavity quality factor and $v$ is the mode volume. high-$q$, small-$v$ cavities dramatically enhance emission into the cavity mode.\n\nin the weak coupling regime ($g < \\kappa$), the atom still decays irreversibly but at the enhanced purcell rate $\\gamma_{\\text{cav}} = 4g^2/\\kappa$. in the strong coupling regime, the decay is replaced by reversible rabi oscillations described by the jaynes-cummings model.\n\nconversely, when the cavity is far detuned from the atomic transition, the density of states is suppressed and spontaneous emission is **inhibited**. this was first demonstrated by kleppner (1981) using rydberg atoms between conducting plates.\n\n## experimental platforms\n\n**microwave cavity qed** uses rydberg atoms (with transition frequencies in the ghz range) passing through superconducting microwave cavities. pioneered by haroche and colleagues, these experiments achieve:\n- photon lifetimes of $\\sim 0.1$ seconds in superconducting cavities.\n- strong coupling with $g/2\\pi \\sim 50$ khz.\n- single-atom, single-photon resolution.\n\n**optical cavity qed** uses alkali atoms trapped in high-finesse fabry-perot cavities. pioneered by kimble and colleagues, these experiments work at optical frequencies with:\n- small mode volumes ($\\sim \\lambda^3$).\n- strong coupling with $g/2\\pi \\sim 10{-}100$ mhz.\n- direct single-photon detection.\n\n**circuit qed** replaces atoms with superconducting qubits and cavities with microwave transmission line resonators. the coupling strength is orders of magnitude larger than in natural atoms:\n- $g/2\\pi \\sim 100$ mhz (easily in strong coupling).\n- highly controllable fabrication.\n- the dominant platform for quantum computing (ibm, google).\n\n## cat-state generation\n\na **schrodinger cat state** is a superposition of two macroscopically distinct coherent states:\n\n$$\n|\\text{cat}_\\pm\\rangle = \\mathcal{n}_\\pm(|\\alpha\\rangle \\pm |-\\alpha\\rangle),\n$$\n\nwhere $\\mathcal{n}_\\pm$ is a normalization constant. the even cat $|+\\rangle$ contains only even photon numbers, while the odd cat $|-\\rangle$ contains only odd photon numbers.\n\nin cavity qed, cat states are generated using the **dispersive interaction**. an atom in a superposition $(|e\\rangle + |g\\rangle)/\\sqrt{2}$ interacting dispersively with a coherent state $|\\alpha\\rangle$ creates an entangled state:\n\n$$\n\\frac{1}{\\sqrt{2}}(|e\\rangle|\\alpha e^{i\\chi t}\\rangle + |g\\rangle|\\alpha e^{-i\\chi t}\\rangle).\n$$\n\nat $\\chi t = \\pi/2$, this becomes $\\frac{1}{\\sqrt{2}}(|e\\rangle|i\\alpha\\rangle + |g\\rangle|-i\\alpha\\rangle)$. a subsequent $\\pi/2$ pulse and measurement on the atom projects the cavity into a cat state.\n\nthese experiments were performed by haroche's group and provide direct evidence of quantum superpositions at the mesoscopic scale. the decoherence of cat states (monitored by wigner function tomography) demonstrates the quantum-to-classical transition.\n\n[[simulation wigner-cat-state]]\n\n## decoherence and quantum jumps\n\nreal cavities lose photons at rate $\\kappa$, and atoms decay at rate $\\gamma$. the system dynamics are described by a **master equation**:\n\n$$\n\\frac{d\\hat{\\rho}}{dt} = -\\frac{i}{\\hbar}[\\hat{h}, \\hat{\\rho}] + \\kappa\\mathcal{d}[\\hat{a}]\\hat{\\rho} + \\gamma\\mathcal{d}[\\hat{\\sigma}_-]\\hat{\\rho},\n$$\n\nwhere $\\mathcal{d}[\\hat{o}]\\hat{\\rho} = \\hat{o}\\hat{\\rho}\\hat{o}^\\dagger - \\frac{1}{2}\\{\\hat{o}^\\dagger\\hat{o}, \\hat{\\rho}\\}$ is the lindblad dissipator.\n\nfor cat states, decoherence occurs at a rate $\\gamma_{\\text{decoherence}} = 2\\kappa|\\alpha|^2$, proportional to the \"size\" of the superposition. larger cats decohere faster, consistent with the difficulty of observing quantum effects at macroscopic scales.\n\n**quantum jump** monitoring (measuring the environment) reveals individual photon loss events in real time. between jumps, the system evolves under a non-hermitian effective hamiltonian, and each jump projects the state. this was first observed by nagourney, sauter, and dehmelt (1986) in ion traps.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "coherence-functions",
      "lessonTitle": "Coherence Functions",
      "x": 0.9407525658607483,
      "y": 0.9258388876914978,
      "searchText": "coherence functions\n# coherence functions\n\n## classical coherence\n\n**coherence** describes the ability of light to produce interference. classically, it is quantified by **correlation functions** of the electric field. the first-order correlation function is\n\n$$\ng^{(1)}(\\mathbf{r}_1, t_1; \\mathbf{r}_2, t_2) = \\langle e^*(\\mathbf{r}_1, t_1) e(\\mathbf{r}_2, t_2) \\rangle.\n$$\n\nthe normalized version is the **degree of first-order coherence**:\n\n$$\ng^{(1)}(\\tau) = \\frac{\\langle e^*(t) e(t+\\tau) \\rangle}{\\langle |e(t)|^2 \\rangle}.\n$$\n\nfor a perfectly monochromatic source, $|g^{(1)}(\\tau)| = 1$ for all $\\tau$. for a thermal source with spectral width $\\delta\\nu$, the coherence decays on a timescale $\\tau_c \\sim 1/\\delta\\nu$, called the **coherence time**.\n\n**temporal coherence** measures how well the field correlates with itself at different times at the same point. **spatial coherence** measures correlations between different spatial points at the same time. the **wiener-khintchine theorem** relates $g^{(1)}(\\tau)$ to the power spectral density through a fourier transform.\n\n## quantum coherence functions\n\nin quantum optics, the electric field becomes an operator, and correlation functions involve **normal ordering** (creation operators to the left, annihilation operators to the right). the quantum first-order correlation function is\n\n$$\ng^{(1)}(\\mathbf{r}_1, t_1; \\mathbf{r}_2, t_2) = \\langle \\hat{e}^{(-)}(\\mathbf{r}_1, t_1) \\hat{e}^{(+)}(\\mathbf{r}_2, t_2) \\rangle,\n$$\n\nwhere $\\hat{e}^{(+)}$ and $\\hat{e}^{(-)}$ are the positive and negative frequency parts of the field operator.\n\nfor different quantum states, $g^{(1)}$ behaves differently:\n- **coherent state** $|\\alpha\\rangle$: $|g^{(1)}(\\tau)| = 1$ (perfect first-order coherence, just like a classical field).\n- **number state** $|n\\rangle$: $|g^{(1)}(\\tau)| = 1$ (also perfectly coherent in first order).\n- **thermal state**: $|g^{(1)}(\\tau)|$ decays with $\\tau$, reflecting the broad spectral content.\n\nfirst-order coherence alone cannot distinguish quantum from classical light. the differences emerge at higher orders.\n\n## young's interference with quantum fields\n\nyoung's double-slit experiment illustrates first-order coherence. two pinholes at positions $\\mathbf{r}_1$ and $\\mathbf{r}_2$ sample the field, and the intensity at the observation screen is\n\n$$\ni(\\mathbf{r}) \\propto g^{(1)}(\\mathbf{r}_1, \\mathbf{r}_1) + g^{(1)}(\\mathbf{r}_2, \\mathbf{r}_2) + 2\\operatorname{re}\\left[g^{(1)}(\\mathbf{r}_1, \\mathbf{r}_2) e^{i\\phi}\\right],\n$$\n\nwhere $\\phi$ is the phase difference due to path lengths. the **visibility** of the interference fringes is\n\n$$\n\\mathcal{v} = \\frac{i_{\\max} - i_{\\min}}{i_{\\max} + i_{\\min}} = |g^{(1)}(\\mathbf{r}_1, \\mathbf{r}_2)|.\n$$\n\nremarkably, single photons also produce interference fringes when detected one at a time over many trials. each photon interferes with itself, building up the pattern statistically. this was demonstrated by taylor (1909) and later with more controlled single-photon sources.\n\n## photodetection theory\n\nthe quantum theory of **photodetection** connects correlation functions to measurable quantities. the probability of detecting a photon at position $\\mathbf{r}$ and time $t$ is proportional to\n\n$$\np_1 \\propto \\langle \\hat{e}^{(-)}(\\mathbf{r}, t) \\hat{e}^{(+)}(\\mathbf{r}, t) \\rangle = g^{(1)}(\\mathbf{r}, t; \\mathbf{r}, t).\n$$\n\nthe joint probability of detecting photons at two space-time points involves $g^{(2)}$, the second-order correlation function. normal ordering ensures that these expressions give physically meaningful (non-negative) detection probabilities, consistent with the photoelectric effect.\n\n[[simulation wigner-coherent]]\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "coherent-states-field",
      "lessonTitle": "Coherent States: Field Averages and Photon Statistics",
      "x": 0.8326128721237183,
      "y": 0.9529327154159546,
      "searchText": "coherent states: field averages and photon statistics\n# coherent states: field averages and photon statistics\n\n#### electric field from coherent state viewpoint\nlet's consider the expectation value of electric field operator.\n$$\n    \\hat{e}_x\n    =\n    i \\mathcal{e}_0\n    \\left[\n        \\hat{a}\n        e^{i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n        -\n        \\text{hermitian conjugate}\n    \\right]\n$$\ncoherent state average of electric field is\n$$\n    \\braket{\\alpha | \\hat{e}_x | \\alpha}\n    =\n    i \\mathcal{e}_0 \\alpha\n    e^{i\n    \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n    }\n    +\n    \\text{complex conjugate}\n$$\ncoherent state average of square of electric field is\n$$\n\\begin{aligned}\n    \\braket{\\alpha | \\hat{e}_x^2 | \\alpha}\n    &=\n    - \\mathcal{e}_0^2\n    \\braket{\\alpha |\n    \\left(\n        \\hat{a}\n        e^{i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t \\right)\n        }\n        -\n        \\hat{a}^\\dag\n        e^{-i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n    \\right)^2\n    | \\alpha}\n    \\\\&=\n    - \\mathcal{e}_0^2\n    \\braket{\\alpha |\n    \\left(\n        \\hat{a}^2\n        e^{2i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n        +\n        {\\hat{a}^\\dag }^2\n        e^{-2i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n        -\\hat{a}\\hat{a}^\\dag - \\hat{a}^\\dag\\hat{a}\n    \\right)\n    | \\alpha}\n    \\\\&=\n    - \\mathcal{e}_0^2\n    \\braket{\\alpha |\n    \\left(\n        \\hat{a}^2\n        e^{2i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n        +\n        {\\hat{a}^\\dag }^2\n        e^{-2i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n        - (1 + \\hat{a}^\\dag\\hat{a})\n        - \\hat{a}^\\dag\\hat{a}\n    \\right)\n    | \\alpha}\n\\end{aligned}\n$$\ndetail of calculation is left for readers ;).\nby writing $\\alpha = |\\alpha|e^{i\\theta}$, we can have sine wave which is **very classical**.\n\nfrom these, the variance become\n$$\n    \\left<\n        \\left( \\delta e_x \\right)^2\n    \\right>_\\alpha\n    =\n    \\mathcal{e}_0\n    =\n    \\frac{\\hbar \\omega}{2 \\epsilon_0 v}\n$$\nwhich **does not depend on $\\alpha$!**\nand notice this is **identical to those for a vacuum state!!** (see section 2.2)\n\n#### quadrature operators from coherent state viewpoint\nwe can show that fluctuation of quadrature operator also does not\ndepend on $\\alpha$.\n$$\n    \\hat{x}_1\n    =\n    \\frac{\\hat{a} + \\hat{a}^\\dag}{2}\n$$\n$$\n    \\hat{x}_2\n    =\n    \\frac{\\hat{a} - \\hat{a}^\\dag}{2i}\n$$\n$$\n\\begin{aligned}\n    \\left<\n        \\left(\n            \\delta \\hat{x}_1\n        \\right)^2\n    \\right>_\\alpha\n    &=\n    \\braket{\\alpha|\n            \\hat{x}_1^2\n    |\\alpha}\n    -\n    \\braket{\\alpha|\n            \\hat{x}_1\n            |\\alpha}^2\n    \\\\&=\n    \\braket{\\alpha|\n    \\left(\n        \\frac{\\hat{a} + \\hat{a}^\\dag}{2}\n    \\right)^2\n    |\\alpha}\n    -\n    \\braket{\\alpha|\n        \\frac{\\hat{a} + \\hat{a}^\\dag}{2}\n    |\\alpha}^2\n    \\\\&=\n    \\frac{1}{4}\n    \\braket{\\alpha|\n    \\left(\n        \\hat{a}^2\n        + {\\hat{a}^\\dag}^2\n        + \\hat{a} \\hat{a}^\\dag\n        + \\hat{a}^\\dag \\hat{a}\n    \\right)\n    |\\alpha}\n    -\n    \\frac{1}{4}\n    \\left(\n        \\alpha + \\alpha^*\n    \\right)^2\n    \\\\&=\n    \\frac{1}{4}\n    \\braket{\\alpha|\n    \\left(\n        \\hat{a}^2\n        + {\\hat{a}^\\dag}^2\n        + 1\n        + \\hat{a}^\\dag \\hat{a}\n        + \\hat{a}^\\dag \\hat{a}\n    \\right)\n    |\\alpha}\n    -\n    \\frac{1}{4}\n    \\left(\n        2\\mathrm{re}(\\alpha)\n    \\right)^2\n    \\\\&=\n    \\frac{1}{4}\n    \\left(\n        \\alpha^2\n        + {\\alpha^*}^2\n        + 1\n        + 2\\alpha^* \\alpha\n    \\right)\n    -\n    \\left(\n        \\mathrm{re}(\\alpha)\n    \\right)^2\n    \\\\&=\n    \\frac{1}{4}\n    +\n    \\frac{1}{4}\n    \\left(\n        \\alpha + \\alpha^*\n    \\right)^2\n    -\n    \\left(\n        \\mathrm{re}(\\alpha)\n    \\right)^2\n    \\\\&=\n    \\frac{1}{4}\n\\end{aligned}\n$$\n$$\n\\begin{aligned}\n    \\left<\n        \\left(\n            \\delta \\hat{x}_2\n        \\right)^2\n    \\right>_\\alpha\n    &=\n    \\braket{\\alpha|\n            \\hat{x}_2^2\n    |\\alpha}\n    -\n    \\braket{\\alpha|\n            \\hat{x}_2\n            |\\alpha}^2\n    \\\\&=\n    \\braket{\\alpha|\n    \\left(\n        \\frac{\\hat{a} - \\hat{a}^\\dag}{2i}\n    \\right)^2\n    |\\alpha}\n    -\n    \\braket{\\alpha|\n        \\frac{\\hat{a} - \\hat{a}^\\dag}{2i}\n    |\\alpha}^2\n    \\\\&=\n    -\n    \\frac{1}{4}\n    \\braket{\\alpha|\n    \\left(\n        \\hat{a}^2\n        + {\\hat{a}^\\dag}^2\n        - \\hat{a} \\hat{a}^\\dag\n        - \\hat{a}^\\dag \\hat{a}\n    \\right)\n    |\\alpha}\n    +\n    \\frac{1}{4}\n    \\left(\n        \\alpha - \\alpha^*\n    \\right)^2\n    \\\\&=\n    -\n    \\frac{1}{4}\n    \\braket{\\alpha|\n    \\left(\n        \\hat{a}^2\n        + {\\hat{a}^\\dag}^2\n        - 1\n        - \\hat{a}^\\dag \\hat{a}\n        - \\hat{a}^\\dag \\hat{a}\n    \\right)\n    |\\alpha}\n    +\n    \\frac{1}{4}\n    \\left(\n        2\\mathrm{im}(\\alpha)\n    \\right)^2\n    \\\\&=\n    -\n    \\frac{1}{4}\n    \\left(\n        \\alpha^2\n        + {\\alpha^*}^2\n        - 1\n        - 2\\alpha^* \\alpha\n    \\right)\n    +\n    \\left(\n        \\mathrm{im}(\\alpha)\n    \\right)^2\n    \\\\&=\n    \\frac{1}{4}\n    -\n    \\frac{1}{4}\n    \\left(\n        \\alpha - \\alpha^*\n    \\right)^2\n    +\n    \\left(\n        \\mathrm{im}(\\alpha)\n    \\right)^2\n    \\\\&=\n    \\frac{1}{4}\n\\end{aligned}\n$$\nwe need to use the commutation relation\n$\\left[ \\hat{a}, \\hat{a}^\\dag \\right] = \\hat{a}\\hat{a}^\\dag - \\hat{a}^\\dag\\hat{a}=1$\n\n#### physical meaning of $\\alpha$\nfrom above the $|\\alpha|$ is related to the amplitude of the field.\nthus we can show the relation of photon number and $\\alpha$.\n$$\n\\begin{aligned}\n    \\braket{\\alpha|\\hat{n}|\\alpha}\n    &=\n    \\bar{n}\n    \\\\&=\n    \\braket{\\alpha|\\hat{a}^\\dag \\hat{a}|\\alpha}\n    \\\\&=\n    \\alpha^* \\alpha\n    \\\\&=\n    |\\alpha|^2\n\\end{aligned}\n$$\n$|\\alpha|$ is average photon number of the field.\n\nlet's calculate coherent state average of square of number operator.\n$$\n\\begin{aligned}\n    \\braket{\\alpha|\\hat{n}^2|\\alpha}\n    &=\n    \\braket{\\alpha| \\hat{a}^\\dag \\hat{a} \\hat{a}^\\dag \\hat{a} |\\alpha}\n    \\\\&=\n    \\braket{\\alp"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "coherent-states-properties",
      "lessonTitle": "Coherent States: Displacement, Generation, and Properties",
      "x": 0.869951605796814,
      "y": 0.9936264753341675,
      "searchText": "coherent states: displacement, generation, and properties\n# coherent states: displacement, generation, and properties\n\n# 3.2 displaced vacuum states\nactually, there are three ways to define the coherent state.\n- right eigenstates of the annihilation operator\n- states that minimize the uncertainty relation for the two orthogonal field quadratures (which we didn't do)\n- displaced vacuum state\n\nthis displacing method is closely related to a mechanism for\ngenerating the coherent state from classical currents.\n\nthe displacement operator is defined as\n$$\n    \\hat{d}(\\alpha)\n    =\n    e^{\\alpha \\hat{a}^\\dag - \\alpha^* \\hat{a}}\n$$\nand coherent state given as\n$$\n    \\ket{\\alpha}\n    =\n    \\hat{d}\n    \\ket{0}\n$$\n\n# 3.3 wave packets and time evolution\n$$\n    \\hat{q}\n    \\ket{q}\n    =\n    q\n    \\ket{q}\n$$\n$$\n    \\left| \\psi_\\alpha (q) \\right|\n    =\n    \\left| \\braket{q|\\alpha} \\right|^2\n$$\nthis is gaussian.\nquantum fluctuation is constant with time.\n\n\n# 3.4 generation of coherent states\ncoherent state can be generated by classically oscillating thing.\n\n# 3.5 more on the properties of coherent states\n#### review of number state\n- orthogonal $\\braket{n|n^\\prime} = \\delta_{nn^\\prime}$\n- complete $\\sum_{n=0}^\\infty\\ket{n}\\bra{n} = 1$\n#### properties of coherent state\n##### orthogonality\nlet's check the orthogonality.\n$$\n\\begin{aligned}\n    \\braket{\\beta|\\alpha}\n    &=\n    e^{-\\frac{|\\alpha|^2}{2}}\n    e^{-\\frac{|\\beta|^2}{2}}\n    \\sum_{n=0}^\\infty\n    \\frac\n    { {\\beta^*}^n \\alpha^n }\n    {n!}\n    \\\\&=\n    e^{-\\frac{|\\alpha|^2}{2}}\n    e^{-\\frac{|\\beta|^2}{2}}\n    e^{\n    {\\beta^* \\alpha}\n    }\n    \\\\&=\n    e^{\\frac{1}{2}(\\beta^*\\alpha - \\beta\\alpha^*)}\n    e^{-\\frac{1}{2}|\\beta^2 - \\alpha^2|}\n    \\\\& \\neq\n    0\n\\end{aligned}\n$$\nso coherent states are not orthogonal.\nif $\\left| \\beta - \\alpha \\right|$ is large, they are nearly orthogonal.\n##### completeness\n$$\n\\begin{aligned}\n    \\int \\ket{\\alpha} \\bra{\\alpha} \\mathrm{d}^2 \\alpha\n    &=\n    \\int \\mathrm{d}^2 \\alpha\n    e^{-|\\alpha|^2}\n    \\sum_{n, n^\\prime}\n    \\frac{\\alpha^n {\\alpha^*}^n}{\\sqrt{n!m!}}\n    \\ket{n} \\bra{m}\n    \\\\&=\n    \\sum_{n, m}\n    \\frac{\\ket{n} \\bra{m}}{\\sqrt{n!m!}}\n    \\int \\mathrm{d}^2 \\alpha\n    e^{-|\\alpha|^2}\n    \\alpha^n {\\alpha^*}^n\n    \\\\&=\n    \\sum_{n, m}\n    \\frac{\\ket{n} \\bra{m}}{\\sqrt{n!m!}}\n    \\int \\int \\mathrm{d}r \\mathrm{d}\\theta\n    r\n    e^{-r^2}\n    r^{n+m}\n    e^{i\\theta(n-m)}\n    \\\\&=\n    \\sum_{n}\n    \\frac{\\ket{n} \\bra{n}}{n!}\n    \\int \\mathrm{d}r\n    r^{2n+1}\n    e^{-r^2}\n    2\\pi\n    \\\\&=\n    \\sum_{n}\n    \\ket{n} \\bra{n}\n    \\\\&=\n    \\pi\n\\end{aligned}\n$$\nwe changed the variable $\\alpha=re^{i\\theta}$,\nso integration became $\\mathrm{d}^2\\alpha = r\\mathrm{d}r\\mathrm{d}\\theta$.\nalso, we used the dirac delta function\n$$\n    \\int_0^{2\\pi}\n    \\mathrm{d}\\theta\n    e^{i\\theta (n-m)}\n    =\n    2\\pi\n    \\delta_{nm}\n$$\n\nthus, coherent states is non-orthogonal states and it has a kind of overcomplete property.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "coherent-states",
      "lessonTitle": "Coherent States",
      "x": 0.8602004647254944,
      "y": 1.0,
      "searchText": "coherent states\n# coherent states\n__topic 3 keywords__\n- coherent states\n- displacement operator\n- generation of coherent states\n\n# readings\nch. 3.1-5\n\n# 3.1 eigenstates of the annihilation operator and minimum uncertainty states\n#### definition of coherent state\ncoherent states are defined as eigenstate of annihilation operator.\n$$\n    \\boxed{\n        \\hat{a}\n        \\ket{\\alpha}\n        =\n        \\alpha\n        \\ket{\\alpha}\n    }\n$$\n$$\n    \\boxed{\n        \\hat{a}^\\dag\n        \\bra{\\alpha}\n        =\n        \\alpha^*\n        \\bra{\\alpha}\n    }\n$$\nas you know that $\\hat{a}$ is non-hermitian so the eigenvalue **$\\alpha$ is complex\nnumber**.\nby the way $\\hat{n}=\\hat{a}^\\dag\\hat{a}$ is hermitian.\n\n#### expanding coherent state with number state\nlet's expand coherent state with number state.\nfirst, we multiply identity $\\sum_{n=0}^\\infty \\ket{n}\\bra{n}$ to alpha.\nby this operation, we can project coherent state to number state.\n$$\n\\begin{aligned}\n    \\ket{\\alpha}\n    &=\n    \\sum_{n=0}^\\infty\n    \\ket{n}\n    \\braket{n|\\alpha}\n    \\\\&=\n    \\sum_{n=0}^\\infty\n    c_n\n    \\ket{n}\n\\end{aligned}\n$$\nwe replaced the $\\braket{n|\\alpha}=c_n$.\n\napply operator to $\\ket{\\alpha}$ resuts to\n$$\n\\begin{aligned}\n    \\hat{a}\n    \\ket{\\alpha}\n    &=\n    \\alpha\n    \\ket{\\alpha}\n    \\\\&=\n    \\sum_{n=0}^\\infty\n    \\alpha\n    c_n\n    \\ket{n}\n\\end{aligned}\n$$\nyou know, this is just a definition.\nmultiplying $\\hat{a}$ from left results\n$$\n\\begin{aligned}\n    \\hat{a}\n    \\sum_{n=0}^\\infty\n    c_n\n    \\ket{n}\n    &=\n    \\sum_{n=1}^\\infty\n    \\sqrt{n}\n    c_n\n    \\ket{n-1}\n    \\\\&=\n    \\sum_{n=0}^\\infty\n    \\sqrt{n+1}\n    c_{n+1}\n    \\ket{n}\n\\end{aligned}\n$$\ncoefficient of number state is equal.\n$$\n    \\sqrt{n+1}\n    c_{n+1}\n    =\n    \\alpha\n    c_n\n$$\nthus, we can decide the $c_n$ recursively.\n$$\n\\begin{aligned}\n    c_n\n    &=\n    \\frac\n    {\\alpha}\n    {\\sqrt{n}}\n    c_{n-1}\n    \\\\&=\n    \\frac\n    {\\alpha^2}\n    {\\sqrt{n(n-1)}}\n    c_{n-2}\n    \\\\&=\n    \\cdots\n    \\\\&=\n    \\frac\n    {\\alpha^n}\n    {\\sqrt{n!}}\n    c_{0}\n\\end{aligned}\n$$\nwe almost complete expanding coherent state with number state.\n$$\n    \\ket{\\alpha}\n    =\n    c_0\n    \\sum_{n=0}^\\infty\n    \\frac\n    {\\alpha^n}\n    {\\sqrt{n!}}\n    \\ket{n}\n$$\nwe still need to decide $c_0$. we can do this from normalization condition.\n$$\n\\begin{aligned}\n    1\n    &=\n    \\braket{\\alpha|\\alpha}\n    \\\\&=\n    \\left| c_0 \\right|^2\n    \\sum_{n, n^\\prime}\n    \\frac\n    {\\left| \\alpha \\right|^{2n}}\n    {n!}\n    \\braket{n|n^\\prime}\n    \\left| c_0 \\right|^2\n    \\\\&=\n    \\sum_{n=0}^\\infty\n    \\frac\n    {\\left| \\alpha \\right|^{2n}}\n    {n!}\n    \\\\&=\n    \\left| c_0 \\right|^2\n    e^\n    {\\left| \\alpha \\right|^{2}}\n\\end{aligned}\n$$\nthus,\n$$\n    \\left| c_0 \\right|^2\n    =\n    e^\n    {- \\left| \\alpha \\right|^{2}}\n$$\n$$\n    c_0\n    =\n    e^\n    {- \\left| \\alpha \\right|^{2}/2}\n$$\nfinally coherent coherent state of number state basis is\n$$\n\\boxed{\n    \\ket{\\alpha}\n    =\n    e^\n    {- \\left| \\alpha \\right|^{2}/2}\n    \\sum_{n=0}^\\infty\n    \\frac\n    {\\alpha^n}\n    {\\sqrt{n!}}\n    \\ket{n}\n}\n$$\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "displaced-squeezed-states",
      "lessonTitle": "Displaced Squeezed States",
      "x": 1.0,
      "y": 0.783020555973053,
      "searchText": "displaced squeezed states\n# displaced squeezed states\n\n## displaced squeezed vacuum\n\na **displaced squeezed state** is obtained by first squeezing the vacuum and then displacing it in phase space:\n\n$$\n|\\alpha, \\xi\\rangle = \\hat{d}(\\alpha) \\hat{s}(\\xi) |0\\rangle,\n$$\n\nwhere $\\hat{d}(\\alpha) = \\exp(\\alpha \\hat{a}^\\dagger - \\alpha^* \\hat{a})$ is the displacement operator and $\\hat{s}(\\xi) = \\exp[\\frac{1}{2}(\\xi^* \\hat{a}^2 - \\xi \\hat{a}^{\\dagger 2})]$ is the squeezing operator with $\\xi = re^{i\\theta}$.\n\nthe ordering matters: $\\hat{d}(\\alpha)\\hat{s}(\\xi) \\neq \\hat{s}(\\xi)\\hat{d}(\\alpha)$ in general. the state $|\\alpha, \\xi\\rangle$ is a **minimum uncertainty state** centered at $\\alpha$ in phase space with an elliptical noise distribution whose orientation and eccentricity are determined by $\\xi$.\n\nthe mean values and variances are\n\n$$\n\\langle \\hat{x}_\\phi \\rangle = |\\alpha|\\cos(\\phi - \\phi_\\alpha), \\qquad (\\delta x_\\phi)^2 = \\frac{1}{4}(e^{-2r}\\cos^2\\psi + e^{2r}\\sin^2\\psi),\n$$\n\nwhere $\\psi = \\phi - \\theta/2$ and $\\phi_\\alpha = \\arg(\\alpha)$.\n\n## amplitude and phase squeezed light\n\nthe relative orientation of the squeezing ellipse and the coherent amplitude determines the type of squeezing:\n\n**amplitude squeezed light** has reduced intensity fluctuations. the squeezing axis is aligned with the coherent amplitude, so the radial (amplitude) quadrature has sub-vacuum noise:\n\n$$\n(\\delta n)^2 < \\langle n \\rangle \\quad \\text{(sub-poissonian)}.\n$$\n\nthis is useful for precision intensity measurements and direct detection experiments.\n\n**phase squeezed light** has reduced phase fluctuations. the squeezing axis is perpendicular to the coherent amplitude, compressing the tangential (phase) quadrature:\n\n$$\n\\delta\\phi < \\frac{1}{2\\sqrt{\\langle n \\rangle}} \\quad \\text{(below shot noise)}.\n$$\n\nthis is advantageous for interferometric measurements where phase sensitivity is the limiting factor.\n\nthe wigner function of a displaced squeezed state is a gaussian centered at $\\alpha$ with principal axes tilted by $\\theta/2$:\n\n$$\nw(x, p) = \\frac{1}{\\pi} \\exp\\left[-e^{2r}(x' - x_0')^2 - e^{-2r}(p' - p_0')^2\\right],\n$$\n\nwhere $(x', p')$ are coordinates rotated by the squeezing angle.\n\n[[simulation wigner-squeezed]]\n\n## two-mode squeezing\n\n**two-mode squeezing** correlates two distinct field modes and is the key resource for continuous-variable entanglement. the two-mode squeezing operator is\n\n$$\n\\hat{s}_2(\\xi) = \\exp(\\xi^* \\hat{a}\\hat{b} - \\xi \\hat{a}^\\dagger \\hat{b}^\\dagger),\n$$\n\nwhich creates photon pairs: one in mode $a$ and one in mode $b$. the two-mode squeezed vacuum (tmsv) state is\n\n$$\n|\\text{tmsv}\\rangle = \\frac{1}{\\cosh r} \\sum_{n=0}^{\\infty} (-e^{i\\theta}\\tanh r)^n |n\\rangle_a |n\\rangle_b.\n$$\n\nthe modes are perfectly correlated in photon number ($n_a = n_b$ in every term) but individually each mode is a thermal state with $\\langle n \\rangle = \\sinh^2 r$.\n\n## continuous-variable entanglement\n\nthe tmsv state exhibits **epr-type correlations** in the continuous variables. the sum and difference quadratures are\n\n$$\n\\delta(x_a - x_b)^2 = \\frac{1}{2}e^{-2r}, \\qquad \\delta(p_a + p_b)^2 = \\frac{1}{2}e^{-2r}.\n$$\n\nboth vanish in the limit $r \\to \\infty$, corresponding to the original epr state with perfect correlations. the entanglement is verified by the **duan criterion**: the state is entangled if\n\n$$\n\\delta(x_a - x_b)^2 + \\delta(p_a + p_b)^2 < 1.\n$$\n\nfor a tmsv state, this sum equals $e^{-2r} < 1$ for any $r > 0$, confirming entanglement.\n\n## applications\n\ndisplaced squeezed states and two-mode squeezing are central to:\n\n- **quantum teleportation**: the tmsv provides the shared entangled resource for continuous-variable teleportation of coherent states.\n- **quantum dense coding**: entangled beams allow transmission of classical information at rates exceeding the classical channel capacity.\n- **quantum illumination**: entangled signal-idler pairs improve target detection in noisy environments.\n- **cluster state generation**: large entangled states for measurement-based quantum computation can be deterministically produced using squeezed light and linear optics.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "higher-order-coherence",
      "lessonTitle": "Higher-Order Coherence",
      "x": 0.9442384243011475,
      "y": 0.945340633392334,
      "searchText": "higher-order coherence\n# higher-order coherence\n\n## second-order correlation function\n\nthe **second-order correlation function** measures intensity-intensity correlations and reveals the photon statistics of a light source. it is defined as\n\n$$\ng^{(2)}(\\tau) = \\frac{\\langle \\hat{a}^\\dagger \\hat{a}^\\dagger \\hat{a} \\hat{a} \\rangle}{\\langle \\hat{a}^\\dagger \\hat{a} \\rangle^2} = \\frac{\\langle : \\hat{n}(\\hat{n}-1) : \\rangle}{\\langle \\hat{n} \\rangle^2},\n$$\n\nwhere the colons denote normal ordering. at zero delay, $g^{(2)}(0)$ characterizes the photon number fluctuations:\n\n- **coherent light** (poissonian statistics): $g^{(2)}(0) = 1$.\n- **thermal light** (super-poissonian): $g^{(2)}(0) = 2$.\n- **number state** $|n\\rangle$ (sub-poissonian): $g^{(2)}(0) = 1 - 1/n < 1$.\n- **single photon** $|1\\rangle$: $g^{(2)}(0) = 0$.\n\nthe condition $g^{(2)}(0) < 1$ is a signature of **non-classical light** with no classical analogue. classically, the cauchy-schwarz inequality requires $g^{(2)}(0) \\geq 1$.\n\n## the hanbury brown-twiss experiment\n\nin 1956, hanbury brown and twiss measured intensity correlations of starlight using two detectors. they observed **photon bunching**: photons from a thermal source tend to arrive in pairs, giving $g^{(2)}(0) > g^{(2)}(\\tau)$ for $\\tau > 0$.\n\nthe experimental setup splits the light beam with a beam splitter and sends it to two detectors. a correlator measures the coincidence rate as a function of the time delay $\\tau$ between detections. for thermal light:\n\n$$\ng^{(2)}(\\tau) = 1 + |g^{(1)}(\\tau)|^2.\n$$\n\nat $\\tau = 0$, this gives $g^{(2)}(0) = 2$, meaning photons are twice as likely to arrive together as independently. as $\\tau$ increases beyond the coherence time, $g^{(2)}(\\tau) \\to 1$.\n\nthe hbt result was initially controversial because it seemed to imply that photons \"attract\" each other. the resolution is that bunching arises from the bosonic nature of photons and the statistical properties of thermal states, not from photon-photon interactions.\n\n## photon bunching and antibunching\n\n**photon bunching** ($g^{(2)}(0) > 1$) occurs for thermal and chaotic light sources. it is a classical phenomenon explainable by wave interference of many random emitters.\n\n**photon antibunching** ($g^{(2)}(0) < 1$) has no classical explanation and requires a quantum description. it means photons tend to arrive one at a time, with a suppressed probability of simultaneous detection. antibunching was first observed by kimble, dagenais, and mandel (1977) in the fluorescence of a single atom.\n\nthe key distinction is:\n- bunching: $g^{(2)}(\\tau) < g^{(2)}(0)$ (correlations decrease with delay).\n- antibunching: $g^{(2)}(\\tau) > g^{(2)}(0)$ (correlations increase from a minimum at $\\tau = 0$).\n\n## sub-poissonian statistics\n\nclosely related to antibunching is the concept of **sub-poissonian** photon statistics, where the variance of the photon number is below the poissonian value:\n\n$$\n\\langle (\\delta n)^2 \\rangle < \\langle n \\rangle.\n$$\n\nthe **mandel q parameter** quantifies the deviation:\n\n$$\nq = \\frac{\\langle (\\delta n)^2 \\rangle - \\langle n \\rangle}{\\langle n \\rangle} = \\langle n \\rangle (g^{(2)}(0) - 1).\n$$\n\n$q = 0$ for coherent light, $q > 0$ for super-poissonian (classical) light, and $q < 0$ for sub-poissonian (non-classical) light. number states have $q = -1$, the minimum possible.\n\n## higher-order correlations\n\nthe hierarchy extends to arbitrary order. the $n$-th order correlation function is\n\n$$\ng^{(n)}(\\tau_1, \\ldots, \\tau_{n-1}) = \\frac{\\langle (\\hat{a}^\\dagger)^n \\hat{a}^n \\rangle}{\\langle \\hat{a}^\\dagger \\hat{a} \\rangle^n}.\n$$\n\nfor thermal light, $g^{(n)}(0) = n!$, while for coherent light, $g^{(n)}(0) = 1$ for all $n$. these higher-order functions provide increasingly stringent tests of non-classicality and are relevant for multi-photon experiments and quantum information protocols.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "home",
      "lessonTitle": "Quantum Optics",
      "x": 0.9175193309783936,
      "y": 0.8543413877487183,
      "searchText": "quantum optics\n# quantum optics\n\n## course overview\n\nquantum optics studies the **quantum mechanical aspects of the interaction between light and matter**. unlike classical optics, where electromagnetic fields are treated as continuous waves, quantum optics reveals the discrete photon nature of light and the fundamentally quantum phenomena that arise from field quantization.\n\n- classical optics: continuous electromagnetic waves, intensity, interference.\n- quantum optics: photon concept, field quantization, vacuum fluctuations, non-classical correlations.\n\ncore conceptual framework:\n\n1. quantize the electromagnetic field to obtain photon number (fock) states.\n2. construct coherent states that bridge quantum and classical descriptions.\n3. measure field correlations through photo-detection to reveal quantum coherence.\n4. analyze input/output devices (beam splitters, interferometers) quantum mechanically.\n5. study atom-field interactions, spontaneous emission, and cavity qed.\n6. explore non-classical states: squeezed light, entangled photons, cat states.\n\n## why this topic matters\n\n- quantum communication and quantum key distribution.\n- precision metrology and gravitational wave detection (squeezed light at ligo).\n- quantum computing with photonic platforms and cavity qed.\n- fundamental tests of quantum mechanics (bell inequalities, entanglement).\n- single-photon sources and detectors for quantum networks.\n- laser physics and coherence theory.\n\n## key mathematical ideas\n\n- harmonic oscillator algebra: creation and annihilation operators.\n- coherent states as displaced vacuum and minimum-uncertainty states.\n- quasi-probability distributions: wigner, p, and q functions in phase space.\n- field correlation functions and quantum coherence hierarchy.\n- jaynes-cummings model and dressed states for atom-field coupling.\n- squeezing operators and uncertainty engineering.\n\n## prerequisites\n\n- quantum mechanics (dirac notation, operators, commutators).\n- electrodynamics (maxwell's equations, electromagnetic waves).\n- linear algebra (eigenvalues, hilbert spaces).\n- basic probability and statistics.\n\n## recommended reading\n\n- gerry and knight, *introductory quantum optics*, cambridge university press.\n- walls and milburn, *quantum optics*, springer.\n- scully and zubairy, *quantum optics*, cambridge university press.\n\n## learning trajectory\n\nthis module is organized from field quantization through non-classical light:\n\n1. **quantization of single-mode field** (`quantization-single-mode`) -- maxwell equations, harmonic oscillator analogy, creation/annihilation operators, number states, and vacuum energy.\n2. **quantum fluctuations and quadrature operators** (`quantum-fluctuations`) -- vacuum field fluctuations and the quadrature operator formalism.\n3. **multimode fields** (`multimode-fields`) -- extension to multimode hamiltonians, thermal states, density of states, and planck radiation.\n4. **vacuum fluctuations and observable effects** (`vacuum-fluctuations`) -- spontaneous emission, lamb shift, and casimir effect from zero-point energy.\n5. **coherent states** (`coherent-states`) -- definition as eigenstates of the annihilation operator, fock state expansion, and normalization.\n6. **coherent states: field averages and photon statistics** (`coherent-states-field`) -- electric field expectation values, quadrature variances, poissonian photon statistics.\n7. **coherent states: displacement, generation, and properties** (`coherent-states-properties`) -- displacement operator, wave packets, generation, non-orthogonality, and overcompleteness.\n8. **phase-space pictures** (`phase-space-pictures`) -- coherent and number states on the quadrature complex plane.\n9. **glauber-sudarshan p function** (`p-function`) -- density operator in the coherent state basis, p function calculation, and the optical equivalence theorem.\n10. **wigner function and characteristic functions** (`wigner-function`) -- wigner quasi-probability distribution and its marginals.\n11. **coherence functions** (`coherence-functions`) -- classical and quantum correlation functions, young's interference, and photodetection theory.\n12. **higher-order coherence** (`higher-order-coherence`) -- second-order correlation function, hbt experiment, photon bunching and antibunching, sub-poissonian statistics.\n13. **single photon experiments** (`single-photon-experiments`) -- beam splitter quantum mechanics, hong-ou-mandel effect, and single-photon interference.\n14. **interferometry** (`interferometry`) -- input-output relations, homodyne and heterodyne detection, mach-zehnder and sagnac interferometers, quantum-enhanced phase sensitivity.\n15. **squeezed light** (`squeezed-light`) -- squeezing operator, squeezed vacuum, parametric down-conversion, and applications.\n16. **displaced squeezed states** (`displaced-squeezed-states`) -- displaced squeezed vacuum, amplitude and phase squeezing, two-mode squeezing, and continuous-variable entanglement.\n17. **atom-field interaction** (`atom-field-interaction`) -- dipole approximation, perturbation theory for a two-level atom in a cavity.\n18. **jaynes-cummings model** (`jaynes-cummings`) -- dressed states, rabi oscillations, dispersive regime, and spontaneous emission.\n19. **cavity qed** (`cavity-qed`) -- purcell effect, experimental platforms, cat-state generation, decoherence, and quantum jumps.\n20. **quantum measurements** (`quantum-measurements`) -- measurement operators, povms, photon counting, state tomography, qnd measurements.\n21. **recommended papers** (`papers`) -- key experimental references.\n\n## visual and simulation gallery\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "interferometry",
      "lessonTitle": "Interferometry",
      "x": 0.9726059436798096,
      "y": 0.8599502444267273,
      "searchText": "interferometry\n# interferometry\n\n## input-output relations\n\noptical networks of beam splitters and phase shifters are described by **input-output relations** that map input mode operators to output mode operators. for a network with $m$ modes, the transformation is\n\n$$\n\\hat{a}_{\\text{out}, i} = \\sum_{j=1}^{m} u_{ij} \\hat{a}_{\\text{in}, j},\n$$\n\nwhere $u$ is a unitary matrix. any unitary transformation on $m$ modes can be decomposed into a sequence of beam splitters and phase shifters (reck decomposition). this universality makes linear optics a powerful platform for quantum information processing.\n\n## homodyne detection\n\n**homodyne detection** measures a single quadrature of the electromagnetic field by interfering the signal with a strong local oscillator (lo) at the same frequency. the signal mode $\\hat{a}$ and the lo mode $\\hat{b} \\approx |\\beta|e^{i\\theta}$ are combined on a 50:50 beam splitter, and the photocurrents from two detectors are subtracted:\n\n$$\n\\hat{i}_- \\propto \\hat{a} e^{-i\\theta} + \\hat{a}^\\dagger e^{i\\theta} = \\hat{x}_\\theta,\n$$\n\nwhere $\\hat{x}_\\theta$ is the quadrature operator at phase angle $\\theta$. by varying the lo phase $\\theta$, any quadrature can be measured. this enables full reconstruction of the quantum state via **quantum state tomography**.\n\nfor a coherent state $|\\alpha\\rangle$, homodyne detection yields gaussian noise centered at $2|\\alpha|\\cos(\\theta - \\phi_\\alpha)$ with vacuum-level variance. for squeezed states, the variance is below vacuum for one quadrature.\n\n## heterodyne detection\n\n**heterodyne detection** simultaneously measures both quadratures ($\\hat{x}$ and $\\hat{p}$) by using a local oscillator detuned from the signal frequency. this is equivalent to a joint measurement of non-commuting observables and necessarily adds half a quantum of noise to each quadrature (from the heisenberg uncertainty principle):\n\n$$\n\\delta x \\cdot \\delta p \\geq \\frac{1}{2}.\n$$\n\nheterodyne detection projects onto coherent states and is equivalent to measuring the husimi q-function of the field.\n\n## mach-zehnder interferometer\n\nthe **mach-zehnder interferometer** (mzi) is the workhorse of optical interferometry. two beam splitters enclose two paths with a relative phase shift $\\phi$. the full unitary transformation for a balanced mzi is\n\n$$\nu_{\\text{mzi}} = u_{\\text{bs}} \\cdot u_\\phi \\cdot u_{\\text{bs}} = \\begin{pmatrix} \\cos(\\phi/2) & i\\sin(\\phi/2) \\\\ i\\sin(\\phi/2) & \\cos(\\phi/2) \\end{pmatrix},\n$$\n\nup to global phases. the output intensities oscillate sinusoidally with $\\phi$, allowing precise phase measurements.\n\n## quantum-enhanced interferometry\n\nclassical interferometry with coherent light achieves a phase sensitivity at the **shot-noise limit**:\n\n$$\n\\delta\\phi_{\\text{snl}} = \\frac{1}{\\sqrt{\\bar{n}}},\n$$\n\nwhere $\\bar{n}$ is the mean photon number. this limit arises from the poissonian photon statistics of coherent states.\n\nquantum states can beat this limit. the ultimate bound set by quantum mechanics is the **heisenberg limit**:\n\n$$\n\\delta\\phi_{\\text{hl}} = \\frac{1}{\\bar{n}}.\n$$\n\nstrategies to approach the heisenberg limit include:\n- **squeezed vacuum** injected into the unused port of the mzi reduces the noise in the measured quadrature. this was implemented in ligo to improve gravitational wave sensitivity.\n- **noon states** $|n,0\\rangle + |0,n\\rangle$ achieve heisenberg-limited sensitivity but are fragile and difficult to prepare for large $n$.\n- **twin-fock states** $|n,n\\rangle$ provide robustness against losses compared to noon states.\n\n## sagnac interferometer\n\nthe **sagnac interferometer** uses a common path traversed in opposite directions. rotation of the interferometer introduces a phase shift proportional to the enclosed area and angular velocity (sagnac effect):\n\n$$\n\\delta\\phi = \\frac{8\\pi a \\omega}{c\\lambda},\n$$\n\nwhere $a$ is the enclosed area and $\\omega$ is the rotation rate. fiber-optic gyroscopes and ring laser gyroscopes exploit this principle for navigation and geophysics.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "jaynes-cummings",
      "lessonTitle": "Jaynes-Cummings Model",
      "x": 0.8801456093788147,
      "y": 0.8041127920150757,
      "searchText": "jaynes-cummings model\n# jaynes-cummings model\n\n## the model\n\nthe **jaynes-cummings model** describes the simplest quantum interaction between a single two-level atom and a single mode of the electromagnetic field. the hamiltonian is\n\n$$\n\\hat{h} = \\hbar\\omega_c \\hat{a}^\\dagger \\hat{a} + \\frac{\\hbar\\omega_a}{2}\\hat{\\sigma}_z + \\hbar g(\\hat{a}^\\dagger \\hat{\\sigma}_- + \\hat{a}\\hat{\\sigma}_+),\n$$\n\nwhere $\\omega_c$ is the cavity frequency, $\\omega_a$ is the atomic transition frequency, $g$ is the coupling strength, and $\\hat{\\sigma}_\\pm$ are the atomic raising/lowering operators.\n\nthe interaction term $\\hat{a}^\\dagger \\hat{\\sigma}_-$ describes the atom emitting a photon (going from excited to ground) while creating a cavity photon, and $\\hat{a}\\hat{\\sigma}_+$ describes absorption. this is the **rotating wave approximation** (rwa), valid when $g \\ll \\omega_c, \\omega_a$.\n\n## energy levels and dressed states\n\nthe total excitation number $\\hat{n} = \\hat{a}^\\dagger\\hat{a} + \\hat{\\sigma}_+\\hat{\\sigma}_-$ is conserved, so the hilbert space splits into independent two-dimensional subspaces spanned by $\\{|n, e\\rangle, |n+1, g\\rangle\\}$ for each $n$.\n\nwithin each subspace, diagonalizing gives the **dressed states**:\n\n$$\n|+, n\\rangle = \\cos\\theta_n |n, e\\rangle + \\sin\\theta_n |n+1, g\\rangle,\n$$\n$$\n|-, n\\rangle = -\\sin\\theta_n |n, e\\rangle + \\cos\\theta_n |n+1, g\\rangle,\n$$\n\nwhere $\\tan(2\\theta_n) = 2g\\sqrt{n+1}/\\delta$ and $\\delta = \\omega_a - \\omega_c$ is the detuning. the dressed-state energies are\n\n$$\ne_{\\pm, n} = \\hbar\\omega_c(n + \\tfrac{1}{2}) \\pm \\frac{\\hbar}{2}\\sqrt{\\delta^2 + 4g^2(n+1)}.\n$$\n\nthe splitting $\\hbar\\omega_n = \\hbar\\sqrt{\\delta^2 + 4g^2(n+1)}$ between the dressed states is the **vacuum rabi splitting** when $n = 0$.\n\n## rabi oscillations\n\nif the atom starts in the excited state with $n$ photons, $|\\psi(0)\\rangle = |n, e\\rangle$, the state evolves as\n\n$$\n|\\psi(t)\\rangle = \\cos(\\omega_n t/2)|n, e\\rangle - i\\sin(\\omega_n t/2)|n+1, g\\rangle,\n$$\n\nat resonance ($\\delta = 0$), where $\\omega_n = 2g\\sqrt{n+1}$ is the **$n$-photon rabi frequency**. the atomic inversion oscillates:\n\n$$\n\\langle \\hat{\\sigma}_z(t) \\rangle = \\cos(\\omega_n t).\n$$\n\nthe $\\sqrt{n+1}$ dependence is a purely quantum effect. for a coherent state input $|\\alpha\\rangle$ with $\\bar{n} = |\\alpha|^2$ photons, different fock components oscillate at different frequencies $\\omega_n$, leading to **collapse and revival** of the rabi oscillations. the initial oscillations collapse on a timescale $t_c \\sim 1/(g\\sqrt{\\bar{n}})$ due to dephasing, then revive at $t_r \\sim 2\\pi\\sqrt{\\bar{n}}/g$ when the phases realign.\n\n## dispersive regime\n\nwhen the detuning is large ($|\\delta| \\gg g\\sqrt{n+1}$), the atom and field exchange only virtual excitations. perturbation theory gives an effective hamiltonian\n\n$$\n\\hat{h}_{\\text{disp}} \\approx \\hbar(\\omega_c + \\chi\\hat{\\sigma}_z)\\hat{a}^\\dagger\\hat{a} + \\frac{\\hbar(\\omega_a + \\chi)}{2}\\hat{\\sigma}_z,\n$$\n\nwhere $\\chi = g^2/\\delta$ is the **dispersive shift**. the cavity frequency shifts by $\\pm\\chi$ depending on the atomic state, and the atomic frequency shifts by $\\chi$ per photon (**ac stark shift** or **light shift**).\n\nthis regime enables **quantum non-demolition** (qnd) measurement of photon number: measuring the atomic phase shift reveals $n$ without absorbing photons. it is the foundation of circuit qed readout in superconducting qubits.\n\n## spontaneous emission\n\nin free space, a two-level atom coupled to a continuum of modes decays irreversibly at the **wightman-weisskopf** rate\n\n$$\n\\gamma = \\frac{\\omega_a^3 d^2}{3\\pi\\epsilon_0\\hbar c^3},\n$$\n\nwhere $d$ is the transition dipole moment. the excited-state population decays exponentially: $p_e(t) = e^{-\\gamma t}$.\n\nin a cavity, spontaneous emission is modified by the density of states. when the cavity is resonant, the enhanced rate is $\\gamma_{\\text{cav}} = 4g^2/\\kappa$ (where $\\kappa$ is the cavity decay rate), leading to the **purcell effect** discussed in the cavity qed topic.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "multimode-fields",
      "lessonTitle": "Multimode Fields",
      "x": 0.8208969831466675,
      "y": 0.8430380821228027,
      "searchText": "multimode fields\n# multimode fields\n__topic 2 keywords__\n- multi-mode fields\n- thermal states-density of states\n- planck formula\n- density operators\n- lamb shift\n- casimir forces\n\n# readings\nch. 2.4-6, app. a.\n\n# 2.4 multimode fields\nin the textbook, they use the vector potential. \nwe can do same things in different way.\nconsider the hamiltonian of multi-mode wave.\n$$\n\\begin{aligned}\n    \\hat{h}\n    &=\n    \\sum_{\\vec{k}, s}\n    \\hbar \\omega_k\n    \\left(\n        \\hat{a}^\\dag_{\\vec{k},s}\n        \\hat{a}_{\\vec{k},s}\n        +\n        \\frac{1}{2}\n    \\right)\n    \\\\&=\n    \\sum_j\n    \\hbar \\omega_j\n    \\left(\n        \\hat{n}_j\n        +\n        \\frac{1}{2}\n    \\right)\n\\end{aligned}\n$$\nhere, paramters are follows.\n- $\\omega_k=kc$ is frequency\n- $\\vec{k}$ is the wave vetor\n- $s$ is the polarization index\n\nby using index $j$ we can simply write the eigenstate of this hamiltonian.\nthe eigenstate would be product of number states.\n$$\n    \\ket{\\left\\{n_j\\right\\}}\n    =\n    \\ket{n_1, n_2, \\cdots, n_j, \\cdots}\n$$\nthis state means how many photon in each mode.\n\nwe can also think the annihilation operator of multi-mode state.\n$$\n    \\hat{a}_i\n    \\ket{\\left\\{n_j\\right\\}}\n    =\n    \\sqrt{n_i}\n    \\ket{n_1, n_2, \\cdots, n_i-1, \\cdots}\n$$\n\n# 2.5 thermal fields\n#### thermal light\nso far we consider the zero-temperature box.\nwe can think interaction between photon and thermal wall.\nin thermal equilibrium, the density matrix is\n$$\n    \\hat{\\rho}_\\mathrm{th}\n    =\n    \\frac\n    {e^{ -\\hat{h} / k_\\mathrm{b}t}}\n    {\\operatorname{tr}~e^{- \\hat{h} / k_\\mathrm{b}t}}\n$$\nremember that operator on napier number is symbolic representation of maclaurin \nseries.\n$$\n    e^{ -\\hat{h} / k_\\mathrm{b}t}\n    =\n    1 \n    - \n    \\frac{\\hat{h}}{k_\\mathrm{b}t}\n    +\n    \\frac{1}{2}\n    \\left(\n        \\frac{\\hat{h}}{k_\\mathrm{b}t}\n    \\right)^2\n    \\cdots\n$$\nprobability of occupying $\\ket{n}$ is\n$$\n    p_n \n    =\n    \\braket{\n        n|\n        \\hat{\\rho}_\\mathrm{th}\n        |n\n    }\n$$\nexpectation number of photon is \n$$\n    \\boxed{\n    \\left< n \\right>\n    =\n    \\sum_n\n    n p_n\n    =\n    \\frac\n    {1}\n    {e^{\\hbar \\omega / k_\\mathrm{b}t}  - 1 }\n    }\n$$\nlet's check the order of this number.\nenergy of visible light is \n$$\n    \\hbar \\omega\n    \\sim\n    1\n    \\mathrm{ev}\n$$\nenergy of room temperature is\n$$\n    k_\\mathrm{b}t\n    \\sim\n    \\frac{1}{40}\n    \\mathrm{ev}\n$$\nby putting these number into equation, we see\n$$\n    \\left< \\hat{n} \\right>\n    \\sim\n    0\n$$\nso photon does not care temperature. this is different from quantum computer.\n**temperature does not play a role in quantum optics.**\n\nthermal fluctuation is\n$$\n\\begin{aligned}\n    \\left< \\left( \\delta n \\right)^2 \\right>\n    &=\n    \\left< \\hat{n}^2 \\right>\n    -\n    \\left< \\hat{n} \\right>^2\n    \\\\&=\n    \\bar{n}\n    +\n    \\bar{n}^2\n\\end{aligned}\n$$\nthis is super-poisonian distribution. variance is larger than that of poisonian.\n#### planck's radiation law\nconsider the light in the box with each length $l$ in thermal equilibrium.\nfrom boundary condition,\n$$\n    e^{i k_i x_i}\n    =\n    e^{i k_i (x_i+l)}\n$$\n$$\n    k_i \n    =\n    \\frac{2\\pi}{l} \n    m_i\n$$\nthe number of state is \n$$\n\\begin{aligned}\n    \\delta m\n    &=\n    \\delta m_x\n    \\delta m_y\n    \\delta m_z\n    \\\\&=\n    2\n    \\frac{v}{2\\pi}\n    \\delta k_x\n    \\delta k_y\n    \\delta k_z\n\\end{aligned}\n$$\nhere multiplication of 2 is number of polarization.\n\nwe can think this in spherical coordinate.\n$$\n\\begin{aligned}\n    \\mathrm{d}m\n    &=\n    \\frac{v}{4\\pi^3}\n    \\mathrm{d}^3 k\n    \\\\&=\n    \\frac{v}{4\\pi^3}\n    k^2\n    \\mathrm{d} k\n    \\mathrm{d} \\omega\n    \\\\&=\n    \\frac{v}{4\\pi^3}\n    \\frac{\\omega^2}{c^3}\n    \\mathrm{d} \\omega\n    \\mathrm{d} \\omega\n\\end{aligned}\n$$\ndensity of state is \n$$\n    \\rho(\\omega)\n    =\n    \\frac{\\omega^2}{\\pi^2 c^3}\n$$\ndimension of density of state is \n$\\frac{\\text{\\# of state}}{\\text{frequency} \\cdot \\text{volume}}$\n\nenergy density become\n$$\n\\begin{aligned}\n    \\bar{u}\n    &=\n    \\hbar \\omega \\bar{n} \\rho (\\omega)\n    \\\\&=\n    \\frac\n    {\\hbar\\omega}\n    {e^{\\hbar\\omega/k_\\mathrm{b}t} - 1}\n    \\frac\n    {\\omega^2}\n    {\\pi^2 c^3}\n\\end{aligned}\n$$\n\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "p-function",
      "lessonTitle": "Glauber-Sudarshan P Function",
      "x": 0.788448691368103,
      "y": 0.9842071533203125,
      "searchText": "glauber-sudarshan p function\n# glauber-sudarshan p function\n\n# 3.7 density operators and phase-space probability distributions\n#### $p$ function\nas we learned in topic 3.5, the *completeness* of coherent state is\n$$\n    \\frac{1}{\\pi}\n    \\int \\mathrm{d}^2 \\alpha\n    \\ket{\\alpha}\n    \\bra{\\alpha}\n    =\n    1\n$$\ndensity operator can be represented by\n$$\n\\begin{aligned}\n    \\hat{\\rho}\n    &=\n    \\left(\n        \\frac{1}{\\pi}\n        \\int \\mathrm{d}^2 \\alpha^\\prime\n        \\ket{\\alpha^\\prime}\n        \\bra{\\alpha^\\prime}\n    \\right)\n    \\hat{\\rho}\n    \\left(\n        \\frac{1}{\\pi}\n        \\int \\mathrm{d}^2 \\alpha\n        \\ket{\\alpha}\n        \\bra{\\alpha}\n    \\right)\n    \\\\&=\n    \\int \\mathrm{d}^2 \\alpha\n    \\underbrace{\n        \\frac{1}{\\pi^2}\n        \\int \\mathrm{d}^2 \\alpha^\\prime\n        \\ket{\\alpha^\\prime}\n        \\bra{\\alpha^\\prime}\n        \\hat{\\rho}\n    }\n    \\ket{\\alpha}\n    \\bra{\\alpha}\n\\end{aligned}\n$$\nwe want to replace the under braced part with something useful.\nwe introduce the glauber-sudarshan $p$ function.\n$$\n    \\hat{\\rho}\n    =\n    \\int \\mathrm{d}^2 \\alpha\n    p(\\alpha)\n    \\ket{\\alpha}\n    \\bra{\\alpha}\n$$\n\nwe are curious about the property of this density operator.\nlet's check trace.\n$$\n\\begin{aligned}\n    1\n    &=\n    \\operatorname{tr}\n    \\hat{\\rho}\n    \\\\&=\n    \\operatorname{tr}\n    \\left(\n    \\int \\mathrm{d}^2 \\alpha\n    p(\\alpha^\\prime)\n    \\ket{\\alpha}\n    \\bra{\\alpha}\n    \\sum_i\n    \\ket{n}\n    \\bra{n}\n    \\right)\n    \\\\&=\n    \\int \\mathrm{d}^2 \\alpha\n    \\sum_i\n    p(\\alpha^\\prime)\n    \\braket{n|\\alpha}\n    \\braket{\\alpha|n}\n    \\\\&=\n    \\int \\mathrm{d}^2 \\alpha\n    p(\\alpha^\\prime)\n    \\sum_i\n    \\braket{\\alpha|n}\n    \\braket{n|\\alpha}\n    \\\\&=\n    \\int \\mathrm{d}^2 \\alpha\n    p(\\alpha^\\prime)\n    \\braket{\\alpha|\\alpha}\n    \\\\&=\n    \\int \\mathrm{d}^2 \\alpha\n    p(\\alpha^\\prime)\n\\end{aligned}\n$$\nthus, $p$ function satisfies normalization condition.\n$p$ function is analogous to probability distribution function.\n\n#### calculate $p$ function\nbut how do we calculate $p(\\alpha)$?\nwe need to use fourier transform in the complex plane.\nby using coherent states $\\ket{u}$ and $\\ket{-u}$, the density operator as $p$\nfunction become\n$$\n\\begin{aligned}\n    \\braket{-u|\\hat{\\rho}|u}\n    &=\n    \\int \\mathrm{d}^2 \\alpha\n    p(\\alpha)\n    \\braket{-u|\\alpha}\n    \\braket{\\alpha|u}\n    \\\\&=\n    \\int \\mathrm{d}^2 \\alpha\n    p(\\alpha)\n    \\cdot\n    e^{-\\frac{|u|^2}{2}}\n    e^{-\\frac{|\\alpha|^2}{2}}\n    e^{\n    {-u^* \\alpha}\n    }\n    \\cdot\n    e^{-\\frac{|\\alpha|^2}{2}}\n    e^{-\\frac{|u|^2}{2}}\n    e^{\n    {\\alpha^* u}\n    }\n    \\\\&=\n    e^{-|u|^2}\n    \\int \\mathrm{d}^2 \\alpha\n    p(\\alpha)\n    e^{-|\\alpha|^2}\n    e^{\n    \\alpha^* u\n    -\n    u^* \\alpha\n    }\n\\end{aligned}\n$$\nremember, as we learned in topic 3.5,\nwe have kind of *orthogonal* relation of coherent states.\n$$\n    \\braket{\\beta|\\alpha}\n    =\n    e^{-\\frac{|\\alpha|^2}{2}}\n    e^{-\\frac{|\\beta|^2}{2}}\n    e^{\n    {\\beta^* \\alpha}\n    }\n$$\nnotice that\n$$\n    \\alpha^* u - u^* \\alpha\n    =\n    2i\n    \\left(\n        \\mathrm{im}(\\alpha)\n        \\mathrm{re}(u)\n        -\n        \\mathrm{re}(\\alpha)\n        \\mathrm{im}(u)\n    \\right)\n$$\naha. we have a exponent of pure imaginary number.\nit is a time to do fourier transform.\n\nwe define fourier transforms in the complex plane:\n$$\n    g(u)\n    =\n    \\int \\mathrm{d}^2 \\alpha\n    \\cdot\n    f(\\alpha)\n    e^{\n    \\alpha^* u\n    -\n    u^* \\alpha\n    }\n$$\n$$\n    f(\\alpha)\n    =\n    \\frac{1}{\\pi^2}\n    \\int \\mathrm{d}^2 u\n    \\cdot\n    g(u)\n    e^{\n    u^* \\alpha\n    -\n    \\alpha^* u\n    }\n$$\ncorresponding $f(\\alpha)$ and $g(\\alpha)$ are below.\n$$\n    g(u)\n    =\n    e^{\\left| u \\right|^2}\n    \\braket{-u|\\hat{\\rho}|u}\n$$\n$$\n    f(\\alpha)\n    =\n    e^{-\\left| \\alpha \\right|^2}\n    p(\\alpha)\n$$\nwe need to do some transforms.\n$$\n\\begin{aligned}\n    f(\\alpha)\n    &=\n    e^{-\\left| \\alpha \\right|^2}\n    p(\\alpha)\n    \\\\&=\n    \\frac{1}{\\pi^2}\n    \\int \\mathrm{d}^2 u\n    \\cdot\n    g(u)\n    e^{\n    u^* \\alpha\n    -\n    \\alpha^* u\n    }\n    \\\\&=\n    \\frac{1}{\\pi^2}\n    \\int \\mathrm{d}^2 u\n    \\cdot\n    e^{\\left| u \\right|^2}\n    \\braket{-u|\\hat{\\rho}|u}\n    e^{\n    u^* \\alpha\n    -\n    \\alpha^* u\n    }\n\\end{aligned}\n$$\nfrom this,\n$$\n    p(\\alpha)\n    =\n    \\frac\n    {e^{\\left| \\alpha \\right|^2}}\n    {\\pi^2}\n    \\int \\mathrm{d}^2 u\n    \\cdot\n    e^{\\left| u \\right|^2}\n    \\braket{-u|\\hat{\\rho}|u}\n    e^{\n    u^* \\alpha\n    -\n    \\alpha^* u\n    }\n$$\n\n#### example of $p$ function\nlet's see the $p$ function of pure coherent state $\\ket{\\beta}$.\n$$\n\\begin{aligned}\n    \\braket{-u|\\hat{\\rho}|u}\n    &=\n    \\braket{-u|\\beta}\n    \\braket{\\beta|u}\n    \\\\&=\n    e^{-\\frac{|u|^2}{2}}\n    e^{-\\frac{|\\beta|^2}{2}}\n    e^{\n    {-u^* \\beta}\n    }\n    \\cdot\n    e^{-\\frac{|\\beta|^2}{2}}\n    e^{-\\frac{|u|^2}{2}}\n    e^{\n    {\\beta^* u}\n    }\n    \\\\&=\n    e^{-|\\beta|^2}\n    e^{-|u|^2}\n    e^{\n    - u^* \\beta\n    + \\beta^* u\n    }\n\\end{aligned}\n$$\n$$\n\\begin{aligned}\n    p(\\alpha)\n    &=\n    \\frac\n    {e^{\\left| \\alpha \\right|^2}}\n    {\\pi^2}\n    \\int \\mathrm{d}^2 u\n    \\cdot\n    e^{\\left| u \\right|^2}\n    \\cdot\n    e^{-|\\beta|^2}\n    e^{-|u|^2}\n    e^{\n    - u^* \\beta\n    + \\beta^* u\n    }\n    \\cdot\n    e^{\n    u^* \\alpha\n    -\n    \\alpha^* u\n    }\n    \\\\&=\n    e^{\\left| \\alpha \\right|^2}\n    e^{-\\left| \\beta \\right|^2}\n    \\underbrace{\n        \\frac\n        {1}\n        {\\pi^2}\n        \\int \\mathrm{d}^2 u\n        \\cdot\n        e^{\n        u^*\n        \\left( \\alpha - \\beta \\right)\n        -\n        u\n        \\left( \\alpha^* - \\beta^* \\right)\n        }\n    }\n\\end{aligned}\n$$\nnotice that underbraced part is dirac delta function of complex form.\n$$\n\\begin{aligned}\n    \\delta^2 \\left( \\alpha-\\beta \\right)\n    &=\n    \\delta\n    \\left[\n        \\mathrm{re}\\left(\\alpha\\right)\n        -\n        \\mathrm{re}\\left(\\beta\\right)\n    \\right]\n    \\cdot\n    \\delta\n    \\left[\n        \\mathrm{im}\\left(\\alpha\\right)\n        -\n        \\mathrm{im}\\left(\\beta\\right)\n    \\right]\n    \\\\&=\n    \\frac\n    {1}\n    {\\pi^2}\n    \\int \\mathrm{d}^2 u\n    \\cdot\n  "
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "papers",
      "lessonTitle": "Recommended Papers",
      "x": 0.9201098680496216,
      "y": 0.7469019889831543,
      "searchText": "recommended papers\n# recommended papers\n\n## key experimental papers\n\n- **deleglise et al. (2008)** [nature](https://doi.org/10.1038/nature07288) - reconstruction of non-classical cavity field states with snapshots of their decoherence. demonstrates wigner function tomography of fock states and cat states in a microwave cavity, observing decoherence in real time.\n\n- **kirchmair et al. (2013)** [nature](https://doi.org/10.1038/nature11902) - observation of quantum state collapse and revival due to the single-photon kerr effect. shows the dispersive regime of circuit qed producing cat-state dynamics.\n\n- **vlastakis et al. (2013)** [science](https://doi.org/10.1126/science.1243289) - deterministically encoding quantum information using large cat states in a superconducting cavity. demonstrates cat-code quantum error correction.\n\n- **weinbub et al. (2018)** [applied physics reviews](https://doi.org/10.1063/1.5046663) - comprehensive review of computational approaches to wigner function methods in quantum mechanics and quantum transport.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "phase-space-pictures",
      "lessonTitle": "Phase-Space Pictures",
      "x": 0.8505941033363342,
      "y": 0.9418448805809021,
      "searchText": "phase-space pictures\n# phase-space pictures\n__topic 4 keywords__\n- quasi-probability distributions\n- characteristic functions\n- phase-space picture\n\n# readings\nch. 3.6-8\n\n# 3.6 phase-space pictures of coherent states\n#### coherent state on a complex plane\nrecall quadrature operators are\n$$\n    \\hat{x_1}\n    =\n    \\frac{1}{2}\n    \\left(\n        \\hat{a} + \\hat{a}^\\dag\n    \\right)\n$$\n$$\n    \\hat{x_2}\n    =\n    \\frac{1}{2i}\n    \\left(\n        \\hat{a} - \\hat{a}^\\dag\n    \\right)\n$$\nas we learned in topic 3.1, coherent state averages of quadrature operators are\n$$\n\\begin{aligned}\n    \\braket{\\alpha|\n    \\hat{x_1}\n    |\\alpha}\n    &=\n    \\frac{1}{2}\n    \\braket{\\alpha|\n    \\left(\n        \\hat{a} + \\hat{a}^\\dag\n    \\right)\n    |\\alpha}\n    \\\\&=\n    \\frac{1}{2}\n    \\left(\n        \\alpha + \\alpha^*\n    \\right)\n    \\\\&=\n    \\mathrm{re}(\\alpha)\n\\end{aligned}\n$$\n$$\n\\begin{aligned}\n    \\braket{\\alpha|\n    \\hat{x_2}\n    |\\alpha}\n    &=\n    \\frac{1}{2i}\n    \\braket{\\alpha|\n    \\left(\n        \\hat{a} - \\hat{a}^\\dag\n    \\right)\n    |\\alpha}\n    \\\\&=\n    \\frac{1}{2i}\n    \\left(\n        \\alpha - \\alpha^*\n    \\right)\n    \\\\&=\n    \\mathrm{im}(\\alpha)\n\\end{aligned}\n$$\nthus the coherent state $\\ket{\\alpha}$ is centered at a $\\alpha$ on a complex $\\hat{x}_1$-$\\hat{x}_2$ plane.\n\nremember that in topic 3.1, we saw that that variance of quadrature operators are\n$$\n    \\left<\n        \\left(\n            \\delta \\hat{x}_1\n        \\right)^2\n    \\right>_\\alpha\n    =\n    \\left<\n        \\left(\n            \\delta \\hat{x}_2\n        \\right)^2\n    \\right>_\\alpha\n    =\n    \\frac{1}{4}\n$$\nthus the fluctuation of quadrature operators are both $\\frac{1}{2}$.\non a complex $\\hat{x}_1$-$\\hat{x}_2$ plane, coherent state $\\ket{\\alpha}$ is centered at a $\\alpha$ with fluctuation of radius $\\frac{1}{2}$.\n\nas we learned in topic 3.1, the average number of photon is $\\bar{n}=|\\alpha|^2$.\nthus the distance between coherent state and origin of complex plane is\nsquare root of number of photon.\n\nnotice when we consider $\\alpha = \\left|\\alpha\\right| e^{i\\theta}$,\nthe uncertainty of theta $\\delta \\theta$ become\nmaximum $2\\pi$ when $\\left|\\alpha\\right|=0$ and minimum $0$ when\n$\\left|\\alpha\\right| \\rightarrow \\infty$.\n\n#### number state on a complex plane\nhow about number state $\\ket{n}$?\nlet's calculate!\n\n[[simulation phase-space-states]]\n$$\n\\begin{aligned}\n    \\braket{n|\n    \\hat{x_1}\n    |n}\n    &=\n    \\frac{1}{2}\n    \\braket{n|\n    \\left(\n        \\hat{a} + \\hat{a}^\\dag\n    \\right)\n    |n}\n    \\\\&=\n    \\frac{1}{2}\n    \\left(\n        0 + 0\n    \\right)\n    \\\\&=\n    0\n\\end{aligned}\n$$\n$$\n\\begin{aligned}\n    \\braket{n|\n    \\hat{x_2}\n    |n}\n    &=\n    \\frac{1}{2i}\n    \\braket{n|\n    \\left(\n        \\hat{a} - \\hat{a}^\\dag\n    \\right)\n    |n}\n    \\\\&=\n    \\frac{1}{2i}\n    \\left(\n        0 - 0\n    \\right)\n    \\\\&=\n    0\n\\end{aligned}\n$$\nhmm. number state is centered at origin.\n\nhow about fluctuation?\n$$\n\\begin{aligned}\n    \\left<\n        \\left(\n            \\delta \\hat{x}_1\n        \\right)^2\n    \\right>_n\n    &=\n    \\braket{n|\n            \\hat{x}_1^2\n    |n}\n    -\n    \\braket{n|\n            \\hat{x}_1\n            |n}^2\n    \\\\&=\n    \\braket{n|\n    \\left(\n        \\frac{\\hat{a} + \\hat{a}^\\dag}{2}\n    \\right)^2\n    |n}\n    -\n    0\n    \\\\&=\n    \\frac{1}{4}\n    \\braket{n|\n    \\left(\n        \\hat{a}^2\n        + {\\hat{a}^\\dag}^2\n        + \\hat{a} \\hat{a}^\\dag\n        + \\hat{a}^\\dag \\hat{a}\n    \\right)\n    |n}\n    \\\\&=\n    \\frac{1}{4}\n    \\braket{n|\n    \\left(\n        \\hat{a}^2\n        + {\\hat{a}^\\dag}^2\n        + 1\n        + \\hat{a}^\\dag \\hat{a}\n        + \\hat{a}^\\dag \\hat{a}\n    \\right)\n    |n}\n    \\\\&=\n    \\frac{1}{4}\n    \\left(\n        1\n        +\n        2n\n    \\right)\n    \\\\&=\n    \\frac{1}{2}\n    \\left(\n        \\frac{1}{2} + n\n    \\right)\n\\end{aligned}\n$$\n$$\n\\begin{aligned}\n    \\left<\n        \\left(\n            \\delta \\hat{x}_2\n        \\right)^2\n    \\right>_n\n    &=\n    \\braket{n|\n            \\hat{x}_2^2\n    |n}\n    -\n    \\braket{n|\n            \\hat{x}_2\n            |n}^2\n    \\\\&=\n    \\braket{n|\n    \\left(\n        \\frac{\\hat{a} - \\hat{a}^\\dag}{2i}\n    \\right)^2\n    |n}\n    -\n    0\n    \\\\&=\n    -\n    \\frac{1}{4}\n    \\braket{n|\n    \\left(\n        \\hat{a}^2\n        + {\\hat{a}^\\dag}^2\n        - \\hat{a} \\hat{a}^\\dag\n        - \\hat{a}^\\dag \\hat{a}\n    \\right)\n    |n}\n    \\\\&=\n    -\n    \\frac{1}{4}\n    \\braket{n|\n    \\left(\n        \\hat{a}^2\n        + {\\hat{a}^\\dag}^2\n        - 1\n        - \\hat{a}^\\dag \\hat{a}\n        - \\hat{a}^\\dag \\hat{a}\n    \\right)\n    |n}\n    \\\\&=\n    -\n    \\frac{1}{4}\n    \\left(\n        - 1\n        - 2n\n    \\right)\n    \\\\&=\n    \\frac{1}{2}\n    \\left(\n        \\frac{1}{2} + n\n    \\right)\n\\end{aligned}\n$$\nwhen there is no photon, the fluctuation is $\\frac{1}{2}$.\ntherefore, the number state $\\ket{n=0}$ is centered at origin\nand its uncertainty is circle with radius $\\frac{1}{2}$.\nthis agree with that of coherent state $\\ket{\\alpha=0}$.\n\nwhen there is photon, the radius of fluctuation\n$\\sqrt{\n    \\frac{1}{2}\n    \\left(\n        \\frac{1}{2} + n\n    \\right)\n}$\nbecome larger than that of coherent state.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "quantization-single-mode",
      "lessonTitle": "Quantization of Single Mode Field",
      "x": 0.8358505964279175,
      "y": 0.8619502186775208,
      "searchText": "quantization of single mode field\n# quantization of single mode field\n __topic 1 keywords__\n- maxwell equations\n- single mode fields\n- creation/annihilation operators\n- number state\n- field fluctuations\n\n# readings\nch. 2.1-3\n\n# why do we need to quantize?\n- so far, we learned that electron in high-energy state decays back to ground state with the emission of light.\n- however, according to the schr\u00f6dinger equation $$\\hat{h} \\ket{\\psi_m} = e_m \\ket{\\psi_m}$$, electron is stable in excited state i.e. *electron stays in high energy state*.\n- how do the decay? something goes wrong. why does spontaneous emission happen?\n- the answer is **fluctuations**. \n- the new question is *what kind of fluctuations do they have*?\n\n# can classical electromagnetic describe this fluctuations?\nin classical electromagnetic, energy of electron in electric field $e(t)$ is \n$$ e \\cdot e(t) $$\nthus, when there is no light, there is no fluctuation.\nhowever, it is known that even without light, there is a fluctuation i.e. quantum\nmechanical fluctuation.\nwe need to use quantum mechanics.\n\n# 2.1 quantization of single mode field\n#### motivation\nlet's start our quantization of electromagnetic waves.\nwhat are we gonna quantize? of course, as you know, it is **energy**.\n#### situation\nimagine a three-dimensional box and there are perfectly conducting walls at \n$z=0$ and $z=l$. \nwe consider **single mode field** i.e. we only take care one mode.\nboundary conditions are\n- $\\vec{e}(z=0)=0$\n- $\\vec{e}(z=l)=0$\n#### energy\nelectromagnetic energy is\n$$\n    h \n    = \n    \\frac{1}{2} \n    \\int \\mathrm{d} v\n    \\left[\n        \\epsilon_0 \\left| \\vec{e}(z, t) \\right|^2\n        + \n        \\frac{1}{\\mu_0} \\left| \\vec{b}(z, t) \\right|^2\n    \\right]^2\n$$\n#### electric field\nfrom boundary condition, we can determine the spatial part of electric field.\n$$\n    \\vec{e}(z, t)\n    =\n    a \\cdot q(t) \\cdot \\sin(kz) \\cdot \\vec{e_x}\n$$\nhere, parameters are described as follows.\n- $a$ is a constant and we can decide. \n- $q(t)$ is the time dependent part and we will see why we use $q$ as symbol.\n- $k=\\frac{2\\pi}{\\lambda}=\\frac{\\pi}{l}m$ is a wave vector and $m$ is integer ($m=0, \\pm1, \\pm2, \\cdots$).\n- $\\vec{e_x}$ is an unit vector of $x$-direction (we decide the electric field has component on $x$ direction).\nwe decide $a$ as follows.\n$$\n    a\n    =\n    \\sqrt{\\frac{2\\omega^2}{\\epsilon_0 v}}\n$$\nhere, $\\omega=kc$ is frequency.\n#### magnetic field\nso, we got electric field. how can we get magnetic field? yea, we use **maxwell equations**.\n$$\n    \\vec{\\nabla} \\times \\vec{b}\n    =\n    \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{e}}{\\partial t}\n$$\nto calculate left-hand side term, we can use the table.\n$$\n    \\vec{\\nabla} \\times \\vec{b}\n    =\n    \\begin{vmatrix}\n        \\vec{e_x} & \\vec{e_y} & \\vec{e_z}\\\\\n        \\partial_x & \\partial_y & \\partial_z\\\\\n        b_x & b_y & b_z\\\\\n    \\end{vmatrix}\n$$\nphysicist can typically notice that\nwe need to take care only $y$ direction of magnetic field..\nyou know we set electric field on $x$ direction.\nthus, \n$$\n    \\vec{\\nabla} \\times \\vec{b}\n    =\n    -\\frac{\\partial b_y}{\\partial z} \\vec{e_x}\n$$\ngotcha. right-hand side is easy.\n$$\n    \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{e}}{\\partial t}\n    =\n    a \\cdot \\dot{q}(t) \\cdot \\sin(kz) \\cdot \\vec{e_x}\n$$\nthen, maxwell equation become\n$$\n    -\\frac{\\partial b_y}{\\partial z} \\vec{e_x}\n    =\n    a \\cdot \\dot{q}(t) \\cdot \\sin(kz) \\cdot \\vec{e_x}\n$$\nthus, \n$$\n\\begin{aligned}\n    \\vec{b}(z, t)\n    &=\n    \\frac{mu_0 \\epsilon_0}{k} \\cdot \n    a \\cdot \\dot{q}(t) \\cdot \\cos(kz) \\cdot \\vec{e_y}\n    \\\\&=\n    \\frac{mu_0 \\epsilon_0}{k} \\cdot \n    a \\cdot p(t) \\cdot \\cos(kz) \\cdot \\vec{e_y}\n\\end{aligned}\n$$\nhere, we introduced $p(t)=\\dot{q}(t)$. again we gonna see why we use symbol $p$.\n#### finally, let's calculate energy!\nwe have electric field and magnetic field so we can calculate energy.\n\n[[simulation wigner-number-state]]\n\nhowever, as you can see the difficulty is $\\int \\mathrm{d} v$. \nhow do we process this?\nwe introduce new variable cross section $\\sigma$.\n$$\n    v \n    =\n    \\sigma \\cdot l\n$$\nenergy function become\n$$\n    h \n    = \n    \\frac{1}{2} \n    \\sigma\n    \\int_0^l \\mathrm{d} z\n    \\left[\n        \\epsilon_0 \\left| \\vec{e}(z, t) \\right|^2\n        + \n        \\frac{1}{\\mu_0} \\left| \\vec{b}(z, t) \\right|^2\n    \\right]^2\n$$\nby using the math trick\n$$\n\\begin{aligned}\n    \\int_0^l \\mathrm{d}z \\sin^2(kz) \n    &=\n    \\int_0^l \\mathrm{d}z \\sin^2\\left(\\frac{\\pi}{l}mz\\right) \n    \\\\&=\n    \\frac{l}{2}\n\\end{aligned}\n$$\nwe see the simple form of energy function.\n$$\n    h\n    =\n    \\frac{1}{2} \\omega^2 q^2(t)\n    +\n    \\frac{1}{2} p^2(t)\n$$\nas you notice this energy function is same as that of unit mass classical \nharmonic oscillator.\nand our way of symbolizing make sense.\n#### warning\ncan we say $q$ as position of photon and $p$ as momentum of photon? no, we can't.\nthis is just mathematical formulation and analogy to classical harmonic oscillator.\n#### quantization\nphew. it was a long journey to get energy function. \nlet's go back to quantum world.\nto quantize the energy function, we need to use operators.\nwe introduce three operators $\\hat{h}, \\hat{q}, \\hat{p}$.\n$$\n    \\hat{h}\n    =\n    \\frac{1}{2} \\omega^2 \\hat{q}^2(t)\n    +\n    \\frac{1}{2} \\hat{p}^2(t)\n$$\nto quantize we use commutation relation to operators $\\hat{q}$ and $\\hat{p}$.\n$$\n    \\left[\n        \\hat{q}, \\hat{p}\n    \\right]\n    =\n    i\\hbar\n$$\nnotice that $\\hat{q}$ and $\\hat{p}$ are hermitian i.e. observable.\n#### new operators\nin addition to these operators, we introduce other two non-hermitian operators.\nnamely, **annihilation operator** $\\hat{a}$ \nand **creation operator** $\\hat{a}^\\dag$ .\n$$\n    \\hat{a} \n    =\n    \\frac{1}{2\\hbar\\omega}\n    \\left(\n        \\omega \\hat{q} \n        + \n        i\\hat{p} \n    \\right)\n$$\n$$\n    \\hat{a}^\\dag \n    =\n    \\frac{1}{2\\hbar\\omega}\n    \\left(\n        \\omega \\hat{q} \n        - \n        i\\hat{p} \n    \\right)\n$$\nthese operators have cool commutation relation.\n$$\n    \\left[\n        \\hat{a}, \\ha"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "quantum-fluctuations",
      "lessonTitle": "Quantum Fluctuations and Quadrature Operators",
      "x": 0.8167304396629333,
      "y": 0.9099560379981995,
      "searchText": "quantum fluctuations and quadrature operators\n# quantum fluctuations and quadrature operators\n\n# 2.2 quantum fluctuations\ntime average of electric field is\n$$\n\\begin{aligned}\n    \\left<\n        \\hat{\\vec{e}}\n    \\right>\n    &=\n    \\braket{n|\\hat{\\vec{e}}|n}\n    \\\\&=\n    \\sqrt{\n        \\frac{\\hbar \\omega}{\\epsilon_0 v}\n    }\n    \\sin (kz)\n    \\braket{n|\\left(\\hat{a}+\\hat{a}^\\dag\\right)|n}\n    \\\\&=\n    0\n\\end{aligned}\n$$\ni see. how about variance?\n$$\n\\begin{aligned}\n    \\left<\n        \\hat{\\vec{e}}^2\n    \\right>\n    &=\n    \\braket{n|\\hat{\\vec{e}}^2|n}\n    \\\\&=\n    \\frac{\\hbar \\omega}{\\epsilon_0 v}\n    \\sin^2 (kz)\n    \\braket{\n        n|\n        \\left(\n        \\hat{a}^2 + {\\hat{a}^\\dag}^2 + \\hat{a}\\hat{a}^\\dag + \\hat{a}^\\dag\\hat{a}\n        \\right)\n        |n\n    }\n    \\\\&=\n    \\frac{\\hbar \\omega}{\\epsilon_0 v}\n    \\sin^2 (kz)\n    \\left(\n        n+\\frac{1}{2}\n    \\right)\n\\end{aligned}\n$$\nwow! even in the vacuum state ($n=0$), electric field fluctuates!!\n\n# 2.3 quadrature operators\nsimilar to $\\hat{q}$ and $\\hat{p}$ operators, we introduce quadrature operators\n$\\hat{x_1}$ and $\\hat{x_2}$..\n$$\n    \\hat{x_1}\n    =\n    \\frac{\\hat{a} + \\hat{a}^\\dag}{2}\n    \\propto\n    \\hat{q}(t)\n$$\n$$\n    \\hat{x_2}\n    =\n    -i\n    \\frac{\\hat{a} - \\hat{a}^\\dag}{2}\n    \\propto\n    \\hat{p}(t)\n$$\n$$\n    \\left[\n        \\hat{x_1}, \\hat{x_2}\n    \\right]\n    =\n    \\frac{1}{2}\n$$\nnotice that quadrature operators are observable and homodyne technique is used for\nthat.\nand these operators does not commute. this is because heisenberg's uncertainty.\n\nwe can rewrite electric field with quadrature operators.\n$$\n\\begin{aligned}\n    \\hat{e_x}\n    &=\n    \\mathcal{e}_0\n    \\left[\n        \\hat{a}e^{-i\\omega t}\n        +\n        \\hat{a}^\\dag e^{i\\omega t}\n    \\right]\n    \\sin (kz)\n    \\\\&=\n    \\mathcal{e}_0\n    \\sin (kz)\n    \\left[\n        \\left(\n            \\hat{a}\n            +\n            \\hat{a}^\\dag\n        \\right)\n        \\cos \\omega t\n        -\n        i\n        \\left(\n            \\hat{a}\n            -\n            \\hat{a}^\\dag\n        \\right)\n        \\sin \\omega t\n    \\right]\n    \\\\&=\n    2\n    \\mathcal{e}_0\n    \\sin (kz)\n    \\left[\n        \\hat{x_1}\n        \\cos \\omega t\n        +\n        \\hat{x_2}\n        \\sin \\omega t\n    \\right]\n\\end{aligned}\n$$\nimportant staff of quadrature operators is their uncertainty.\n$$\n    \\left<\n        \\left(\n            \\delta \\hat{x}_1\n        \\right)^2\n    \\right>\n    \\left<\n        \\left(\n            \\delta \\hat{x}_2\n        \\right)^2\n    \\right>\n    \\ge\n    \\frac{1}{4}\n    \\left|\n        \\left<\n            \\left[\n                \\hat{x}_1, \\hat{x}_2\n            \\right]\n        \\right>\n    \\right|^2\n    \\ge\n    \\frac{1}{16}\n$$\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "quantum-measurements",
      "lessonTitle": "Quantum Measurements",
      "x": 0.9384478330612183,
      "y": 0.8318339586257935,
      "searchText": "quantum measurements\n# quantum measurements\n\n## measurement in quantum mechanics\n\nin quantum optics, measurement plays a fundamental role: the act of detection irreversibly alters the quantum state of light. the measurement formalism connects the abstract quantum state to experimentally observable quantities like photon counts, quadrature values, and correlation functions.\n\na general quantum measurement is described by a set of **measurement operators** $\\{\\hat{m}_m\\}$ satisfying $\\sum_m \\hat{m}_m^\\dagger \\hat{m}_m = \\hat{i}$. the probability of outcome $m$ given state $\\hat{\\rho}$ is\n\n$$\np(m) = \\operatorname{tr}[\\hat{m}_m^\\dagger \\hat{m}_m \\hat{\\rho}],\n$$\n\nand the post-measurement state is $\\hat{\\rho}_m = \\hat{m}_m \\hat{\\rho} \\hat{m}_m^\\dagger / p(m)$.\n\n## projective vs. generalized measurements\n\n**projective (von neumann) measurements** use orthogonal projectors $\\hat{\\pi}_m = |m\\rangle\\langle m|$ satisfying $\\hat{\\pi}_m \\hat{\\pi}_n = \\delta_{mn}\\hat{\\pi}_m$. photon number detection is a projective measurement in the fock basis $\\{|n\\rangle\\}$.\n\n**generalized measurements** (povms) allow non-orthogonal outcomes and describe realistic detectors. a **positive operator-valued measure** consists of positive operators $\\hat{e}_m \\geq 0$ with $\\sum_m \\hat{e}_m = \\hat{i}$, without requiring orthogonality. heterodyne detection is a povm with elements $\\hat{e}_\\alpha = \\frac{1}{\\pi}|\\alpha\\rangle\\langle\\alpha|$ that projects onto coherent states.\n\n## photon counting\n\nideal **photon-number-resolving** (pnr) detectors measure the fock state projectors. for a state $\\hat{\\rho}$, the probability of detecting $n$ photons is $p(n) = \\langle n|\\hat{\\rho}|n\\rangle$.\n\nreal single-photon detectors (avalanche photodiodes, superconducting nanowire detectors) are typically **threshold detectors**: they distinguish \"no click\" from \"one or more clicks\" but cannot resolve the exact photon number. the povm elements are\n\n$$\n\\hat{e}_0 = |0\\rangle\\langle 0|, \\qquad \\hat{e}_{\\text{click}} = \\hat{i} - |0\\rangle\\langle 0|,\n$$\n\nwith detector efficiency $\\eta < 1$ modeled as a beam splitter loss before an ideal detector.\n\ntransition-edge sensors and superconducting nanowire arrays can resolve photon numbers up to $\\sim 10{-}20$, enabling direct measurement of photon statistics and wigner function negativity.\n\n## quantum state tomography\n\n**quantum state tomography** reconstructs the full density matrix $\\hat{\\rho}$ from a set of measurements on many identically prepared copies. for optical fields, this is achieved by:\n\n1. **homodyne tomography**: measure the quadrature $\\hat{x}_\\theta$ for many phase angles $\\theta$. the marginal distributions $p(x_\\theta)$ are projections of the wigner function. the state is reconstructed via the inverse radon transform (same mathematics as medical ct scanning).\n\n2. **maximum likelihood estimation**: numerically find the density matrix that maximizes the likelihood of the observed data, subject to the constraints that $\\hat{\\rho}$ is positive semidefinite with unit trace.\n\nthe wigner function provides a complete phase-space representation:\n\n$$\nw(x, p) = \\frac{1}{\\pi\\hbar}\\int_{-\\infty}^{\\infty} \\langle x + y|\\hat{\\rho}|x - y\\rangle e^{-2ipy/\\hbar} dy.\n$$\n\nnegative values of $w$ indicate non-classical states (e.g., fock states, cat states).\n\n## quantum non-demolition measurements\n\na **quantum non-demolition** (qnd) measurement extracts information about an observable without disturbing it. for photon number, the key requirement is $[\\hat{n}, \\hat{h}_{\\text{int}}] = 0$: the interaction used for measurement commutes with the measured observable.\n\nin cavity qed, the dispersive interaction shifts the atomic phase by an amount proportional to $n$ without absorbing photons. by sending probe atoms through the cavity and measuring their phase shift, one can determine $n$ repeatedly and observe quantum jumps as individual photons are lost.\n\nthis was demonstrated by haroche's group, who tracked the photon number in a microwave cavity decaying from $n = 7$ to $n = 0$ one photon at a time, directly observing the quantum trajectory of the field state.\n\n## back-action and the uncertainty principle\n\nevery measurement has **back-action**: gaining information about one observable increases uncertainty in conjugate observables. for homodyne detection of $\\hat{x}$, the back-action appears as increased noise in $\\hat{p}$.\n\nthe **standard quantum limit** (sql) for monitoring the position of a free mass is\n\n$$\n\\delta x_{\\text{sql}} = \\sqrt{\\frac{\\hbar t}{2m}},\n$$\n\narising from the trade-off between measurement imprecision and radiation-pressure back-action. beating the sql requires correlating the measurement and back-action noise, achievable with squeezed light or back-action evasion techniques.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "single-photon-experiments",
      "lessonTitle": "Single Photon Experiments",
      "x": 0.9771173000335693,
      "y": 0.8796205520629883,
      "searchText": "single photon experiments\n# single photon experiments\n\n## the beam splitter\n\na **beam splitter** is the fundamental optical element for single-photon experiments. it partially reflects and partially transmits incoming light. quantum mechanically, a lossless beam splitter with reflectivity $r$ and transmissivity $t = 1 - r$ transforms the input mode operators as\n\n$$\n\\begin{pmatrix} \\hat{a}_{\\text{out}} \\\\ \\hat{b}_{\\text{out}} \\end{pmatrix}\n=\n\\begin{pmatrix} t & r \\\\ r' & t' \\end{pmatrix}\n\\begin{pmatrix} \\hat{a}_{\\text{in}} \\\\ \\hat{b}_{\\text{in}} \\end{pmatrix},\n$$\n\nwhere $|t|^2 + |r|^2 = 1$ and the matrix must be unitary to preserve the commutation relations $[\\hat{a}, \\hat{a}^\\dagger] = 1$. for a symmetric beam splitter: $t = t' = \\cos\\theta$ and $r = -r'^* = i\\sin\\theta$.\n\n## single photon at a beam splitter\n\nwhen a single photon $|1\\rangle_a |0\\rangle_b$ enters one port of a 50:50 beam splitter, the output state is\n\n$$\n|1\\rangle_a |0\\rangle_b \\xrightarrow{\\text{bs}} \\frac{1}{\\sqrt{2}}\\left(|1\\rangle_c |0\\rangle_d + i|0\\rangle_c |1\\rangle_d\\right).\n$$\n\nthe photon exits through one port or the other but is never split: a single detector click occurs in output $c$ or output $d$, never both simultaneously. this was demonstrated by grangier, roger, and aspect (1986), confirming the particle nature of single photons.\n\nthe joint detection probability at both outputs is $p_{cd} = 0$ for a true single photon, while for a classical wave it would be $p_{cd} > 0$. the **anticorrelation parameter** $\\alpha = p_{cd}/(p_c p_d)$ equals zero for a single photon, providing a clean test of the quantum nature of light.\n\n## coherent state at a beam splitter\n\nfor a coherent state input $|\\alpha\\rangle_a |0\\rangle_b$, the output is a product of coherent states:\n\n$$\n|\\alpha\\rangle_a |0\\rangle_b \\xrightarrow{\\text{bs}} |t\\alpha\\rangle_c |r\\alpha\\rangle_d.\n$$\n\nunlike the single-photon case, the outputs are uncorrelated and each port independently contains a coherent state. this factorization property is unique to coherent states and reflects their classical nature.\n\na 50:50 beam splitter splits a coherent state $|\\alpha\\rangle$ into two coherent states $|\\alpha/\\sqrt{2}\\rangle$, with the total mean photon number preserved: $|\\alpha/\\sqrt{2}|^2 + |\\alpha/\\sqrt{2}|^2 = |\\alpha|^2$.\n\n## two-photon interference: hong-ou-mandel effect\n\nwhen **two indistinguishable single photons** enter a 50:50 beam splitter from different ports, they always exit together through the same port:\n\n$$\n|1\\rangle_a |1\\rangle_b \\xrightarrow{\\text{bs}} \\frac{1}{\\sqrt{2}}\\left(|2\\rangle_c |0\\rangle_d - |0\\rangle_c |2\\rangle_d\\right).\n$$\n\nthe coincidence rate at the two outputs drops to zero, known as the **hong-ou-mandel (hom) dip**. this two-photon quantum interference effect has no classical analogue and was first demonstrated by hong, ou, and mandel (1987).\n\nthe dip visibility depends on the indistinguishability of the photons. if the photons differ in frequency, polarization, or arrival time, the dip becomes shallower. the hom effect is the basis for **bell-state measurements** and linear-optical quantum computing schemes.\n\n## mach-zehnder interferometer\n\na **mach-zehnder interferometer** consists of two beam splitters with a phase shift $\\phi$ in one arm. for a single-photon input, the detection probabilities at the two outputs are\n\n$$\np_c = \\cos^2(\\phi/2), \\qquad p_d = \\sin^2(\\phi/2).\n$$\n\nthis demonstrates single-photon interference: the photon interferes with itself as it traverses both paths simultaneously in superposition. the which-path information determines whether interference is observed, illustrating the complementarity principle.\n\nfor $n$-photon states or squeezed light inputs, the interferometer can achieve phase sensitivity beyond the **shot-noise limit** $\\delta\\phi \\sim 1/\\sqrt{n}$, approaching the **heisenberg limit** $\\delta\\phi \\sim 1/n$.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "squeezed-light",
      "lessonTitle": "Squeezed Light",
      "x": 0.9916220903396606,
      "y": 0.7918331027030945,
      "searchText": "squeezed light\n# squeezed light\n\n## quadrature operators\n\nthe electromagnetic field quadratures are defined as\n\n$$\n\\hat{x} = \\frac{1}{2}(\\hat{a} + \\hat{a}^\\dagger), \\qquad \\hat{p} = \\frac{1}{2i}(\\hat{a} - \\hat{a}^\\dagger),\n$$\n\nsatisfying $[\\hat{x}, \\hat{p}] = i/2$. the heisenberg uncertainty relation gives $\\delta x \\cdot \\delta p \\geq 1/4$. for vacuum and coherent states, the uncertainties are equal: $\\delta x = \\delta p = 1/2$ (minimum uncertainty states with symmetric noise).\n\n**squeezed states** redistribute the quantum noise between the two quadratures. one quadrature has reduced fluctuations below the vacuum level at the expense of increased fluctuations in the conjugate quadrature, while still satisfying the uncertainty relation.\n\n## the squeezing operator\n\nthe single-mode **squeezing operator** is\n\n$$\n\\hat{s}(\\xi) = \\exp\\left[\\frac{1}{2}(\\xi^* \\hat{a}^2 - \\xi \\hat{a}^{\\dagger 2})\\right],\n$$\n\nwhere $\\xi = r e^{i\\theta}$ is the complex squeezing parameter. the magnitude $r$ determines the degree of squeezing. the squeezed vacuum state is $|\\xi\\rangle = \\hat{s}(\\xi)|0\\rangle$.\n\nunder squeezing, the quadrature variances become\n\n$$\n(\\delta x_\\theta)^2 = \\frac{1}{4}e^{-2r}, \\qquad (\\delta x_{\\theta+\\pi/2})^2 = \\frac{1}{4}e^{+2r}.\n$$\n\nthe noise reduction is characterized in decibels: squeezing of $r$ corresponds to $-10\\log_{10}(e^{-2r}) \\approx 8.69 r$ db. current experiments achieve over 15 db of squeezing.\n\n## photon statistics of squeezed vacuum\n\nthe squeezed vacuum contains only **even photon numbers**:\n\n$$\n|\\xi\\rangle = \\frac{1}{\\sqrt{\\cosh r}} \\sum_{n=0}^{\\infty} \\frac{(-e^{i\\theta} \\tanh r)^n \\sqrt{(2n)!}}{2^n n!} |2n\\rangle.\n$$\n\nthe mean photon number is $\\langle n \\rangle = \\sinh^2 r$, and the photon number distribution is super-poissonian despite the sub-vacuum noise in one quadrature. the even-photon-number signature is a distinctive feature observable in photon-number-resolving measurements.\n\n## generation by parametric down-conversion\n\nthe primary method for generating squeezed light is **optical parametric down-conversion** (pdc). a nonlinear crystal with $\\chi^{(2)}$ nonlinearity is pumped by a strong laser at frequency $\\omega_p$. the pump photons are converted into pairs of photons (signal and idler) satisfying energy and momentum conservation:\n\n$$\n\\omega_p = \\omega_s + \\omega_i, \\qquad \\mathbf{k}_p = \\mathbf{k}_s + \\mathbf{k}_i.\n$$\n\nwhen the signal and idler are in the same mode (**degenerate** pdc), the output is a squeezed vacuum state. the hamiltonian describing this process is\n\n$$\n\\hat{h}_{\\text{pdc}} = i\\hbar\\kappa(\\hat{a}^{\\dagger 2} - \\hat{a}^2),\n$$\n\nwhere $\\kappa$ is proportional to the pump amplitude and the nonlinear coefficient. this is exactly the squeezing hamiltonian.\n\nin an **optical parametric oscillator** (opo), the nonlinear crystal is placed inside a cavity. below threshold, the opo produces continuous-wave squeezed light with narrow bandwidth determined by the cavity linewidth. above threshold, it oscillates and produces a coherent output.\n\n## applications of squeezed light\n\nsqueezed light has practical applications in precision measurement:\n\n- **gravitational wave detection**: ligo and virgo inject squeezed vacuum into the interferometer's dark port, reducing shot noise and improving sensitivity. frequency-dependent squeezing (using filter cavities) optimizes noise reduction across the detection bandwidth.\n\n- **quantum key distribution**: squeezed states enable continuous-variable quantum cryptography protocols where security is guaranteed by the uncertainty principle.\n\n- **spectroscopy**: sub-shot-noise measurements improve the sensitivity of absorption and phase-shift spectroscopy.\n\n- **quantum computing**: squeezed states are resources for measurement-based quantum computation in the continuous-variable regime, where large cluster states can be deterministically generated.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "vacuum-fluctuations",
      "lessonTitle": "Vacuum Fluctuations and Observable Effects",
      "x": 0.8228498697280884,
      "y": 0.7791076302528381,
      "searchText": "vacuum fluctuations and observable effects\n# vacuum fluctuations and observable effects\n\n# 2.6 vacuum fluctuations and the zero-point energy\nvacuum energy and fluctuations actually give rise to observable effects such as:\n- spontaneous emission\n- lamb shift\n- casimir effect\n#### lamb shift\nthe lamb shift is a discrepancy between experiment and the dirac relativistic\ntheory of the hydrogen atom.\n- the theory predicts that the $2^2s_{1/2}$ and $2^2p_{1/2}$ levels should be degenerate.\n- optical experiment suggest that these states were not degenerate.\n\nthis discrepancy explained by bethe. here we use the welton's intuitive\ninterpretation.\n\nfirst the potential energy of electron of hydrogen atom is\n$$\n    v(r)\n    =\n    -\n    \\frac{e^2}{r}\n    +\n    v_\\mathrm{vac}\n$$\nwe added the small term $v_\\mathrm{vac}$ to include vacuum energy.\nsmall displacement of potential energy is\n$$\n    \\delta v\n    =\n    \\delta \\vec{r}\n    \\cdot\n    \\vec{\\nabla} v\n    +\n    \\sum_{i=1}^3\n    \\frac{1}{2}\n    \\left(\n        \\delta x_i\n    \\right)^2\n    \\frac\n    {\\partial^2 v}\n    {\\partial x_i^2}\n$$\nwe assume that fluctuation is uniform in all direction in sufficiently long time.\nso we set\n$$\n    \\left< \\delta \\vec{r} \\right> = 0\n$$\nalso, we assume that the displacement is same in all direction.\n$$\n    \\left<\n        \\left(\n            \\delta x_i\n        \\right)^2\n    \\right>\n    =\n    \\frac{1}{3}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n$$\nso the time-average potential energy displacement is\n$$\n\\begin{aligned}\n    \\left<\n        \\delta v\n    \\right>\n    &=\n    \\sum_{i=1}^3\n    \\frac{1}{2}\n    \\left<\n        \\left(\n            \\delta x_i\n        \\right)^2\n    \\right>\n    \\frac\n    {\\partial^2 v}\n    {\\partial x_i^2}\n    \\\\&=\n    \\sum_{i=1}^3\n    \\frac{1}{2}\n    \\frac{1}{3}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    \\frac\n    {\\partial^2 v}\n    {\\partial x_i^2}\n    \\\\&=\n    \\frac{1}{6}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    \\sum_{i=1}^3\n    \\frac\n    {\\partial^2 v}\n    {\\partial x_i^2}\n    \\\\&=\n    \\frac{1}{6}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    \\nabla^2 v\n    \\\\&=\n    \\frac{1}{6}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    4 \\pi e^2 \\delta(\\vec{r})\n\\end{aligned}\n$$\nhere, $\\delta(\\vec{r})$ is dirac delta function.\n\nwe can calculate quantum mechanical energy shift.\n$$\n\\begin{aligned}\n    \\delta e\n    &=\n    \\braket{nl|\\delta v|nl}\n    \\\\&=\n    \\frac{4\\pi e^2}{6}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    \\int\n    \\psi^*_{nl}(\\vec{r})\n    \\delta(\\vec{r})\n    \\psi_{nl}(\\vec{r})\n    \\mathrm{d}\\vec{r}\n    \\\\&=\n    \\frac{2\\pi e^2}{3}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    \\left|\n        \\psi_{nl}(\\vec{r}=0)\n    \\right|^2\n\\end{aligned}\n$$\n\nwe need to calculate\n$\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n$.\nto obtain this, we assume that the important field frequencies exceed the atomic\nresonance frequencies.\nthe displacement $\\delta r_\\nu$ induced with frequency between $\\nu$ and\n$\\nu+\\mathrm{d}\\nu$ is determined by\n$$\n    m\n    \\frac\n    {\\mathrm{d}^2 \\delta r_\\nu}\n    {\\mathrm{d}t^2}\n    =\n    e e_\\mathrm{vac} e^{2\\pi i \\nu t}\n$$\ntotal vacuum energy is\n$$\n\\begin{aligned}\n    \\text{total vacuum energy}\n    &=\n    \\underbrace{\n    \\frac\n    {8\\pi \\nu^2}\n    {c^3}\n    }_\\text{density of state}\n    \\cdot\n    \\overbrace{\n    v\n    }^\\text{volume}\n    \\cdot\n    \\overbrace{\n    \\frac{1}{2}\n    h \\nu\n    }^\\text{vacuum energy}\n    \\\\&=\n    \\frac{1}{8\\pi}\n    \\int\n    \\left(\n        e_{\\mathrm{vac}, \\nu}^2\n        +\n        \\overbrace{\n            \\cancel{\n                b_{\\mathrm{vac}, \\nu}^2\n            }\n        }^{e_\\nu \\gg b_\\nu}\n    \\right)\n    \\mathrm{d}v\n\\end{aligned}\n$$\nthus energy difference become\n$$\n    \\delta e\n    =\n    \\chi\n    \\left|\n        \\psi_{nl}(\\vec{r}=0)\n    \\right|^2\n    \\int_0^\\infty\n    \\mathrm{d} \\nu\n    \\frac{1}{\\nu}\n    \\longrightarrow\n    \\infty\n$$\n\n#### casimir effect\nfrom the vacuum electric field, we can show that two conducting plane attract\neach other.\nconsider the box of dimension $l\\times l \\times d$.\nthe total vacuum energy is\n$$\n    e_0(d)\n    =\n    \\sum_{lmn}\n    2\n    \\frac{1}{2}\n    \\hbar \\omega_{lmn}\n$$\ndue to two independent polarizations, we multiply two.\nhere $\\omega$ can be calculated from periodic boundary conditions.\n$$\n    \\omega_{lmn}\n    =\n    \\pi c\n    \\sqrt{\n        \\frac{l^2}{l^2}\n        +\n        \\frac{m^2}{l^2}\n        +\n        \\frac{n^2}{d^2}\n    }\n$$\nwe will conduct several approximations listed below:\n- calculate $e_0(d)$. we are interested in $l \\gg d$, so we can replace the sums of $l$ and $m$ by integrals.\n- calculate $e(\\infty)$. we assume that $d$ is arbitrarily large, so we can replace the sum by integral.\n- calculate $u(d)=e_0(d)-e_0(\\infty)$, which is energy required to bring the plates from infinity to a distance $d$.\n- to transform $u(d)$ further, we need to introduce polar coordinates in the $x$-$y$ plane.\n- to estimate the sum and integral, we use euler-maclaurin formulae. we keep the terms until third order.\n\nfrom these intensive calculations, we can show\n$$\n    u(d)\n    =\n    -\n    \\frac\n    {\\pi^2 \\hbar c}\n    {720}\n    \\frac\n    {l^2}\n    {d^3}\n$$\nwhich means there is an attractive force (casimir force) between two plates.\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "wigner-function",
      "lessonTitle": "Wigner Function and Characteristic Functions",
      "x": 0.7664602994918823,
      "y": 0.9798140525817871,
      "searchText": "wigner function and characteristic functions\n# wigner function and characteristic functions\n\n#### wigner function\nactually there are three type of quasi-probability distributions.\n- glauber-sudarshan $p$ function\n- husimi $q$ function\n- wigner $w$ function\n\n\nlet's see the wigner function.\n$$\n    w \\left( q, p \\right)\n    =\n    \\frac{1}{2 \\pi \\hbar}\n    \\int_{-\\infty}^\\infty\n    \\mathrm{d} x\n    \\cdot\n    \\braket{\n        q + \\frac{1}{2}x|\n        \\hat{\\rho}\n        |q - \\frac{1}{2}x\n    }\n    e^{-i \\frac{p}{\\hbar} x}\n$$\nhere, $\\ket{q \\pm \\frac{1}{2}x}$ are the eigenkets of the position operator.\n\nin the case of the state in question is a pure state\n$\\hat{\\rho} = \\ket{\\psi} \\bra{\\psi}$,\n$$\n\\begin{aligned}\n    w \\left( q, p \\right)\n    &=\n    \\frac{1}{2 \\pi \\hbar}\n    \\int_{-\\infty}^\\infty\n    \\mathrm{d} x\n    \\cdot\n    \\braket{q + \\frac{1}{2}x | \\psi}\n    \\braket{\\psi | q - \\frac{1}{2}x}\n    e^{-i \\frac{p}{\\hbar} x}\n    \\\\&=\n    \\frac{1}{2 \\pi \\hbar}\n    \\int_{-\\infty}^\\infty\n    \\mathrm{d} x\n    \\cdot\n    \\psi^* \\left(q - \\frac{1}{2}x \\right)\n    \\psi \\left(q + \\frac{1}{2}x \\right)\n    e^{-i \\frac{p}{\\hbar} x}\n\\end{aligned}\n$$\nwhere $\\psi \\left(q + \\frac{1}{2}x \\right) = \\braket{q + \\frac{1}{2}x | \\psi} $.\n\nintegrating wigner function over momentum, we see\n$$\n\\begin{aligned}\n    \\int_{-\\infty}^\\infty\n    \\mathrm{d} p\n    \\cdot\n    w \\left( q, p \\right)\n    &=\n    \\frac{1}{2 \\pi \\hbar}\n    \\int_{-\\infty}^\\infty\n    \\int_{-\\infty}^\\infty\n    \\mathrm{d} x\n    \\mathrm{d} p\n    \\cdot\n    \\psi^* \\left(q - \\frac{1}{2}x \\right)\n    \\psi \\left(q + \\frac{1}{2}x \\right)\n    e^{-i \\frac{p}{\\hbar} x}\n    \\\\&=\n    \\int_{-\\infty}^\\infty\n    \\mathrm{d} x\n    \\cdot\n    \\psi^* \\left(q - \\frac{1}{2}x \\right)\n    \\psi \\left(q + \\frac{1}{2}x \\right)\n    \\cdot\n    \\int_{-\\infty}^\\infty\n    \\mathrm{d} p\n    \\cdot\n    \\frac{1}{2 \\pi \\hbar}\n    e^{-i \\frac{p}{\\hbar} x}\n    \\cdot\n    \\\\&=\n    \\int_{-\\infty}^\\infty\n    \\mathrm{d} x\n    \\cdot\n    \\psi^* \\left(q - \\frac{1}{2}x \\right)\n    \\psi \\left(q + \\frac{1}{2}x \\right)\n    \\cdot\n    \\delta (x)\n    \\cdot\n    \\\\&=\n    \\left| \\psi (q) \\right|^2\n\\end{aligned}\n$$\n\n# 3.8 characteristic functions\n"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "bounding_errors",
      "lessonTitle": "Bounding Errors",
      "x": 0.3496299982070923,
      "y": 0.6147481203079224,
      "searchText": "bounding errors\n# bounding errors\n*why your computer is a liar and how to catch it in the act*\n\n## the first surprise\n\nsuppose you try to compute $\\sqrt{2}$ on a calculator. you punch in 2, hit the square root button, and get 1.41421356... that looks pretty good. but here's the thing: your calculator is lying to you. not maliciously \u2014 it's doing its best with only a finite number of digits. the true $\\sqrt{2}$ goes on forever, and the calculator has to stop somewhere. every number it gives you is a tiny lie.\n\nthe whole game of scientific computing starts right here: **understanding how much your computer is lying, and making sure the lies stay small enough that your answers are still useful.**\n\n## sources of error and error definitions\n\n**sources of approximation** include:\n- modelling (simplifications of the physical system)\n- empirical measurements\n- previous computations\n- truncation/discretization\n- rounding\n\n**absolute error** and **relative error** are different in the obvious manner, i.e., abs. error\n= approx. value - true value, and rel. error = abs. error / true value.\n\n**data error and computational error**: the hats indicate an approximation;\n\n$$\n\\begin{align*}\n\\text{total error} &= \\hat{f}(\\hat{x})-f(x)\\\\\n&= \\left(\\hat{f}(\\hat{x})-f(\\hat{x})\\right) &+&\\left(f(\\hat{x})-f(x)\\right)\\\\\n&= \\text{computational error} &+& \\text{propagated data error}\\\\\ne_\\text{tot} &= e_\\text{comp} &+& e_\\text{data}\n\\end{align*}\n$$\n\n*in plain english: the total error is the sum of (a) mistakes the algorithm makes and (b) mistakes already baked into the input data.*\n\nhere $\\hat{x}$ is the approximate input, $\\hat{f}$ is the approximate function, $f$ is the true function, and $x$ is the true input.\n\n> **you might be wondering...** \"can these two errors cancel each other out?\" yes, sometimes they do! but you can't count on it. that's like hoping two wrongs make a right \u2014 occasionally true, but a terrible strategy.\n\n## truncation error vs rounding error\ncomputational error can be split into truncation error $e_\\text{trunc}$ and rounding error $e_\\text{round}$.\n\n**truncation error** stems from:\n- simplifications of the physical model (frictionless, etc.)\n- finite basis sets\n- truncations of infinite series (replacing derivatives with finite differences)\n\n**rounding error** contains everything that comes from working on a finite computer:\n- accumulated rounding error (from finite arithmetic)\n\n**forward vs. backward error**\n\nforward error is the error in the output, backward error is the error in the input.\n\n*think of it this way: forward error asks \"how wrong is my answer?\" backward error asks \"what slightly different question did i actually solve perfectly?\"*\n\n\n## sensitivity, conditioning, and floating point\n**sensitivity and conditioning**\n\ncondition number: $\\text{cond}(f) \\equiv \\frac{|\\delta y / y|}{|\\delta x / x|} = \\frac{|x \\delta y|}{|y \\delta x|}$\n\nin the limit $\\delta x \\to 0$, this becomes $\\text{cond}(f) = \\left|\\frac{x f'(x)}{f(x)}\\right|$, which measures how sensitive the relative output is to relative changes in input.\n\n*this says: the condition number is how much the problem itself amplifies relative errors. a condition number of 100 means a 1% input error could become a 100% output error. yikes.*\n\n> **you might be wondering...** \"is a bad condition number the computer's fault?\" no! conditioning is a property of the *problem*, not the algorithm. even a perfect computer with infinite precision would struggle with an ill-conditioned problem. it's like trying to balance a pencil on its tip \u2014 the physics makes it hard, not your fingers.\n\n**stability and accuracy**\n\n- fixed points have each bit correspond to a specific scale.\n- floating point (32 bit) has: 1 sign bit (0=positive, 1=negative), 8 exponent bits, and 23 mantissa bits. machine epsilon is $\\epsilon \\approx 2^{-23} \\approx 1.2 \\times 10^{-7}$ for single precision.\n\noverflow and underflow refer to the largest and smallest numbers that can be contained in a floating point representation.\n\n\n## example: finite difference error trade-off\n\nhere's the beautiful part \u2014 watch what happens when two types of error fight each other.\n\ncomputational error of first order finite difference: $$ f'(x) \\approx \\frac{f(x+h) - f(x)}{h} \\equiv \\hat{f'}(x) $$\n\n*this says: estimate the slope by looking at two nearby points and computing rise over run.*\n\ntaylor expand:\n$$ f(x+h) = f(x) + h f'(x) + \\frac{h^2}{2} f''(\\theta), \\qquad \\lvert \\theta - x \\rvert \\leq h $$\n$$ \\frac{f(x+h) - f(x)}{h} = f'(x) + \\frac{h}{2} f''(\\theta) $$\n$$ \\hat{f'}(x) - f'(x) = \\frac{h}{2} f''(\\theta) $$\n$$ m \\equiv \\sup_{|\\theta - x| \\leq h} \\lvert f''(\\theta) \\rvert $$\n$$ e_\\text{trunc} = |\\hat{f'}(x) - f'(x)| \\leq \\frac{m}{2} h \\quad \\sim o(h) $$\n\n*truncation error shrinks as h gets smaller \u2014 make the step tiny, get a better derivative.*\n\nwhat about rounding error? when we compute $f(x+h)$ and $f(x)$ in floating point, each has a relative error bounded by machine epsilon $\\epsilon$. the subtraction $f(x+h) - f(x)$ can lose significant digits (cancellation), and dividing by $h$ amplifies this. the result is:\n$$ e_\\text{round} \\leq \\frac{2\\epsilon}{h} \\quad \\sim o\\left(\\frac{1}{h}\\right) $$\n\n*rounding error grows as h gets smaller \u2014 the very thing that helps truncation error makes rounding error worse!*\n\nthe factor of 2 arises because we subtract two quantities, each with rounding error up to $\\epsilon |f|$.\n\nif you decrease $h$, you decrease truncation error but increase rounding error:\n\n$$ e_\\text{comp} = \\frac{m}{2} h + \\frac{2\\epsilon}{h} $$\n\n*two forces pulling in opposite directions. there's a sweet spot in the middle.*\n\nwhat value of $h$ minimizes it? differentiate and set to zero:\n\n$$ 0 = \\frac{de_\\text{comp}}{dh} = \\frac{m}{2} - \\frac{2\\epsilon}{h^2} $$\n$$ h^2 = \\frac{4\\epsilon}{m} $$\nsince $h$ is a step size (positive by definition):\n$$ h_\\text{optimal} = 2 \\sqrt{\\frac{\\epsilon}{m}} $$\n(note that $\\epsilon$ here is a bound on the relative rounding error.)\n\n> **feynman chal"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "eigenAlgorithms",
      "lessonTitle": "Eigenvalue Algorithms",
      "x": 0.44789016246795654,
      "y": 0.7011600136756897,
      "searchText": "eigenvalue algorithms\n# eigenvalue algorithms\n\n---\n\n## qr algorithm\n\nthe **qr algorithm** is the standard method for computing **all eigenvalues** of a matrix.\n\n### basic qr iteration\n\n$$\n\\begin{aligned}\na_0 &= a \\\\\n\\text{for } k &= 0, 1, 2, \\dots: \\\\\na_k &= q_k r_k \\quad \\text{(qr factorization)} \\\\\na_{k+1} &= r_k q_k\n\\end{aligned}\n$$\n\nthe matrices $a_k$ converge to (quasi-)upper triangular form, with eigenvalues on the diagonal!\n\n[[simulation qr-algorithm-animation]]\n\n### why it works\n\neach iteration is a similarity transformation:\n\n$$\na_{k+1} = r_k q_k = q_k^t q_k r_k q_k = q_k^t a_k q_k\n$$\n\nsimilar matrices have the same eigenvalues, and upper triangular matrices have eigenvalues on the diagonal.\n\n```python\ndef qr_algorithm(a, max_iter=1000, tol=1e-12):\n    \"\"\"\n    compute all eigenvalues using qr iteration.\n    \"\"\"\n    n = len(a)\n    a_k = a.copy()\n\n    for i in range(max_iter):\n        q, r = np.linalg.qr(a_k)\n        a_k = r @ q\n\n        # check convergence (subdiagonal elements small)\n        off_diag = np.sum(np.abs(np.tril(a_k, -1)))\n        if off_diag < tol:\n            break\n\n    eigenvalues = np.diag(a_k)\n    return eigenvalues, a_k, i + 1\n```\n\n### practical improvements\n\n- **hessenberg reduction**: transform to upper hessenberg form first (saves computation)\n- **shifts**: use wilkinson or francis shifts for faster convergence\n- **implicit qr**: avoid explicit qr factorization for efficiency\n\n[[figure qr-convergence-plot]]\n\n---\n\n## gershgorin circle theorem\n\na quick way to **bound eigenvalues** without computing them.\n\n### theorem\n\nevery eigenvalue of $a$ lies within at least one **gershgorin disc**:\n\n$$\nd_i = \\left\\{ z \\in \\mathbb{c} : |z - a_{ii}| \\leq r_i \\right\\}\n$$\n\nwhere the radius is:\n\n$$\nr_i = \\sum_{j \\neq i} |a_{ij}|\n$$\n\n[[simulation gershgorin-circles]]\n\n### implementation\n\n```python\ndef gershgorin(a):\n    \"\"\"\n    compute gershgorin disc centers and radii.\n\n    every eigenvalue lies within at least one disc.\n    \"\"\"\n    n = len(a)\n    centers = np.array([a[i, i] for i in range(n)])\n    radii = np.array([np.sum(np.abs(a[i, :])) - np.abs(a[i, i])\n                      for i in range(n)])\n    return centers, radii\n```\n\n[[figure gershgorin-example]]\n\n---\n\n## functions of matrices\n\nfor **non-defective** $a = t\\lambda t^{-1}$ (i.e., $a$ has a complete eigenbasis), we can define functions of matrices:\n\n$$\nf(a) = t f(\\lambda) t^{-1} = t \\begin{pmatrix} f(\\lambda_1) & & \\\\ & \\ddots & \\\\ & & f(\\lambda_n) \\end{pmatrix} t^{-1}\n$$\n\n### applications\n\n- **matrix exponential**: $e^{at}$ solves $\\frac{dx}{dt} = ax$\n- **matrix logarithm**: used in lie groups\n- **matrix square root**: in covariance analysis\n\n$$\np(a) = \\sum_{k=0}^{d} c_k a^k = t \\left(\\sum_{k=0}^{d} c_k \\lambda^k\\right) t^{-1} = t p(\\lambda) t^{-1}\n$$\n\nfor defective matrices, the jordan normal form extends this framework, but the eigendecomposition $t\\lambda t^{-1}$ no longer exists. in practice, most matrices encountered in physical applications (hermitian operators, normal matrices) are non-defective.\n\nmatrix functions connect eigenvalue theory to dynamical systems (via $e^{at}$), quantum mechanics (operator functions of the hamiltonian), and numerical methods for odes (matrix exponential integrators).\n\n[[simulation matrix-exponential]]\n\n---\n\n## algorithm comparison\n\n| algorithm | finds | convergence | complexity/iter | best for |\n|-----------|-------|-------------|-----------------|----------|\n| power method | dominant $\\lambda$ | linear | $o(n^2)$ | single largest eigenvalue |\n| inverse iteration | $\\lambda$ near shift | linear | $o(n^3)$ | single eigenvalue with estimate |\n| rayleigh quotient | single $\\lambda$ | cubic | $o(n^3)$ | fast convergence |\n| qr algorithm | all $\\lambda$ | quadratic | $o(n^3)$ | all eigenvalues |\n| lanczos | $k$ largest/smallest | superlinear | $o(kn)$ | large sparse matrices |\n\n[[figure algorithm-comparison]]\n\n---\n\n## summary\n\n1. **eigenvalues** characterize how linear transformations scale vectors\n2. **power method** finds the dominant eigenvalue with linear convergence\n3. **inverse iteration** finds eigenvalues near a given shift\n4. **rayleigh quotient iteration** achieves cubic convergence for hermitian matrices\n5. **qr algorithm** computes all eigenvalues and is the industry standard\n6. **gershgorin circles** provide quick eigenvalue bounds without computation\n\n---\n\n## references\n\n1. trefethen & bau, *numerical linear algebra*, chapters 24-28\n2. golub & van loan, *matrix computations*, chapter 7\n3. demmel, *applied numerical linear algebra*, chapter 5\n4. 3blue1brown: [eigenvalues and eigenvectors](https://www.youtube.com/watch?v=pfdu9ovae-g)\n"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "eigenSystems",
      "lessonTitle": "Eigenvalue Problems",
      "x": 0.44882848858833313,
      "y": 0.7005106210708618,
      "searchText": "eigenvalue problems\n# eigenvalue problems\n\nany stable physical/chemical system can be described with an eigensystem. wavefunctions in quantum mechanics are a typical example.\n\n$$\nax = \\lambda x\n$$\n\nwhere $a$ is the operator (mapping $\\mathbb{c}^n \\rightarrow \\mathbb{c}^n$), $x$ an eigenvector, and $\\lambda$ an eigenvalue. the question is: **how do we obtain the eigenvalues and eigenvectors?**\n\n[[simulation eigen-transformation]]\n\n## why eigenvalues matter\n\neigenvalues appear throughout physics and engineering:\n\n- **quantum mechanics**: energy levels are eigenvalues of the hamiltonian\n- **vibrations**: natural frequencies of structures\n- **stability analysis**: system behavior near equilibrium points\n- **principal component analysis**: dimensionality reduction in data science\n- **google pagerank**: largest eigenvector of the web graph\n\n[[figure eigen-applications]]\n\n---\n\n## mathematical foundations\n\n### the characteristic polynomial\n\nfor any fixed $\\lambda$, we have a linear system:\n\n$$\n(a-\\lambda i)x = 0\n$$\n\nthis has **non-trivial** solutions ($x\\neq0$) if and only if:\n\n$$\n\\det(a-\\lambda i) = 0\n$$\n\n[[simulation characteristic-polynomial]]\n\n### eigenspaces and multiplicity\n\neigenvectors form subspaces called **eigenspaces**:\n\n$$\ne_\\lambda = \\{x\\in \\mathbb{c}^n \\mid ax=\\lambda x\\}\n$$\n\nthe characteristic polynomial $p(\\lambda) = \\det(a-\\lambda i)$ is of degree $n$, and by the **fundamental theorem of algebra**, has exactly $n$ complex roots (counting multiplicity).\n\n**two types of multiplicity:**\n\n1. **algebraic multiplicity**: how many times $\\lambda_i$ appears in the characteristic polynomial\n2. **geometric multiplicity**: dimension of the eigenspace $e_\\lambda$\n\n[[figure multiplicity-diagram]]\n\n**key inequality**: geometric multiplicity $\\leq$ algebraic multiplicity\n\n### defective vs non-defective matrices\n\n- **non-defective**: $\\sum_{\\lambda\\in sp(a)} \\dim(e_\\lambda) = n$ \u2014 we have a complete eigenbasis\n- **defective**: $\\sum_{\\lambda\\in sp(a)} \\dim(e_\\lambda) < n$ \u2014 not enough eigenvectors\n\n**non-defectiveness is guaranteed if:**\n- $a$ is **normal**: $a^h a = aa^h$\n- all eigenvalues are distinct\n- $a$ is **hermitian**: $a=a^h$ (best case \u2014 guarantees orthonormal eigenbasis)\n\nfor hermitian matrices: $a = u \\lambda u^h$\n\n[[simulation hermitian-demo]]\n\n---\n\n## the power method\n\nlet $a$ be non-defective with $a=t\\lambda t^{-1}$. order the eigenvalues by magnitude:\n\n$$\n|\\lambda_1| \\geq |\\lambda_2| \\geq \\dots \\geq |\\lambda_n|\n$$\n\n### derivation\n\nfor any $x \\in \\mathbb{c}^n$, expressed in the eigenbasis:\n\n$$\nx = \\tilde{x}_1 t_1 + \\tilde{x}_2 t_2 + \\dots + \\tilde{x}_n t_n\n$$\n\napplying $a$ repeatedly:\n\n$$\na^k x = \\tilde{x}_1 \\lambda_1^k t_1 + \\tilde{x}_2 \\lambda_2^k t_2 + \\dots + \\tilde{x}_n \\lambda_n^k t_n\n$$\n\n$$\n= \\lambda_1^k \\left(\\tilde{x}_1 t_1 + \\tilde{x}_2 \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k t_2 + \\dots + \\tilde{x}_n \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k t_n\\right)\n$$\n\nsince $|\\lambda_1|$ is strictly the largest, the ratios approach zero:\n\n$$\n\\lim_{k\\to\\infty}\\frac{a^k x}{\\|a^k x\\|} = t_1, \\quad \\text{if } \\tilde{x}_1 \\neq 0 \\text{ and } |\\lambda_2| < |\\lambda_1|\n$$\n\n**important**: this requires the dominant eigenvalue to be strictly larger in magnitude than the second largest. if $|\\lambda_1| = |\\lambda_2|$, the method fails to converge. convergence rate is $o(|\\lambda_2/\\lambda_1|^k)$, so a small spectral gap means slow convergence.\n\n[[simulation power-method-animation]]\n\n### power method algorithm\n\n```python\ndef power_iterate(a, x0, tol=1e-10, max_iter=1000):\n    \"\"\"\n    find the dominant eigenvalue and eigenvector.\n\n    parameters:\n        a: square matrix\n        x0: initial guess vector\n        tol: convergence tolerance\n        max_iter: maximum iterations\n\n    returns:\n        eigenvalue, eigenvector, iterations\n    \"\"\"\n    x = x0 / np.linalg.norm(x0)\n\n    for i in range(max_iter):\n        y = a @ x\n        x_new = y / np.linalg.norm(y)\n\n        # rayleigh quotient for eigenvalue estimate\n        # for hermitian a this is real; for general a, use x_new.conj() @ a @ x_new\n        eigenvalue = x_new.conj() @ a @ x_new\n\n        if np.linalg.norm(x_new - x) < tol or np.linalg.norm(x_new + x) < tol:\n            return eigenvalue, x_new, i + 1\n\n        x = x_new\n\n    return eigenvalue, x, max_iter\n```\n\n### limitations\n\n- only finds the **dominant eigenvalue** (largest magnitude)\n- fails if the dominant eigenvalue is not unique\n- slow convergence when $|\\lambda_1| \\approx |\\lambda_2|$ (small spectral gap)\n\n[[figure convergence-comparison]]\n\n---\n\n## inverse iteration\n\nto find an eigenvalue **closest to a shift** $\\sigma$, apply the power method to $(a - \\sigma i)^{-1}$:\n\nthe eigenvalues of $(a - \\sigma i)^{-1}$ are $(\\lambda_i - \\sigma)^{-1}$, so the one closest to $\\sigma$ becomes dominant.\n\n```python\ndef inverse_iterate(a, sigma, x0, tol=1e-10, max_iter=1000):\n    \"\"\"\n    find eigenvalue closest to sigma.\n    \"\"\"\n    n = len(a)\n    x = x0 / np.linalg.norm(x0)\n    a_shifted = a - sigma * np.eye(n)\n\n    for i in range(max_iter):\n        # solve (a - sigma*i) y = x\n        y = np.linalg.solve(a_shifted, x)\n        x_new = y / np.linalg.norm(y)\n\n        eigenvalue = x_new @ a @ x_new\n\n        if np.linalg.norm(x_new - x) < tol or np.linalg.norm(x_new + x) < tol:\n            return eigenvalue, x_new, i + 1\n\n        x = x_new\n\n    return eigenvalue, x, max_iter\n```\n\n[[simulation inverse-iteration]]\n\n---\n\n## rayleigh quotient iteration\n\nthe **rayleigh quotient** provides an eigenvalue estimate:\n\n$$\n\\lambda_r(x) = \\frac{x^t a x}{x^t x}\n$$\n\nrayleigh quotient iteration updates the shift at each step, achieving **cubic convergence** for hermitian matrices!\n\n```python\ndef rayleigh_quotient_iteration(a, x0, tol=1e-12, max_iter=100):\n    \"\"\"\n    fast cubic convergence for hermitian matrices.\n    \"\"\"\n    n = len(a)\n    x = x0 / np.linalg.norm(x0)\n    eigenvalue = x @ a @ x\n\n    for i in range(max_iter):\n        a_shifted = a - eigenvalue * np.eye(n)\n\n        try:\n            y = np.linalg"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "fft",
      "lessonTitle": "Fast Fourier Transform",
      "x": 0.4946443438529968,
      "y": 0.49143925309181213,
      "searchText": "fast fourier transform\n# fast fourier transform\n\n## the discrete fourier transform\n\nthe **discrete fourier transform** (dft) converts a sequence of $n$ samples in the time domain into a sequence of $n$ complex coefficients in the frequency domain:\n\n$$\nx_k = \\sum_{n=0}^{n-1} x_n \\, e^{-2\\pi i \\, kn/n}, \\qquad k = 0, 1, \\ldots, n-1.\n$$\n\nthe inverse transform recovers the original signal:\n\n$$\nx_n = \\frac{1}{n} \\sum_{k=0}^{n-1} x_k \\, e^{2\\pi i \\, kn/n}.\n$$\n\neach coefficient $x_k$ represents the amplitude and phase of a sinusoidal component at frequency $k/n$ cycles per sample. the **power spectrum** $|x_k|^2$ reveals the dominant frequencies in the signal.\n\n## the fft algorithm\n\ncomputing the dft directly requires $o(n^2)$ operations. the **fast fourier transform** (cooley-tukey, 1965) reduces this to $o(n \\log n)$ by exploiting the symmetry and periodicity of the complex exponentials.\n\nthe key idea for radix-2 fft (when $n$ is a power of 2):\n\n$$\nx_k = \\underbrace{\\sum_{m=0}^{n/2-1} x_{2m} \\, e^{-2\\pi i \\, k(2m)/n}}_{\\text{even-indexed dft}} + e^{-2\\pi i \\, k/n} \\underbrace{\\sum_{m=0}^{n/2-1} x_{2m+1} \\, e^{-2\\pi i \\, k(2m)/n}}_{\\text{odd-indexed dft}}.\n$$\n\nthis splits one $n$-point dft into two $n/2$-point dfts plus $o(n)$ multiplications. applying this recursively yields the $o(n \\log n)$ complexity.\n\n**practical impact**: for $n = 10^6$, the fft is roughly $50{,}000$ times faster than the direct dft.\n\n## applications in signal processing\n\nthe dft and fft are ubiquitous in scientific computing:\n\n- **spectral analysis**: identify periodic components in time series (e.g., tidal data, heart rhythms, seismic signals).\n- **filtering**: multiply the spectrum by a transfer function to remove noise or isolate frequency bands.\n- **interpolation and zero-padding**: increasing $n$ by appending zeros refines the frequency resolution.\n- **image processing**: the 2d dft decomposes images into spatial frequencies for compression (jpeg) and enhancement.\n\n## the convolution theorem\n\none of the most powerful properties of the dft is the **convolution theorem**:\n\n$$\n\\mathcal{f}\\{f * g\\} = \\mathcal{f}\\{f\\} \\cdot \\mathcal{f}\\{g\\},\n$$\n\nwhere $*$ denotes convolution and $\\cdot$ denotes pointwise multiplication. this means that convolution in the time domain becomes multiplication in the frequency domain.\n\n**practical consequence**: convolving two sequences of length $n$ via the fft costs $o(n \\log n)$, compared to $o(n^2)$ for direct convolution. this speedup is exploited in:\n\n- polynomial multiplication.\n- cross-correlation and matched filtering.\n- solving pdes with periodic boundary conditions (spectral methods).\n\n## aliasing and the nyquist frequency\n\nthe dft assumes the signal is periodic with period $n$. if the signal contains frequencies above the **nyquist frequency** $f_{\\text{nyq}} = f_s / 2$ (where $f_s$ is the sampling rate), those components are **aliased** into lower frequencies.\n\nthe **sampling theorem** (shannon-nyquist) states that a bandlimited signal can be perfectly reconstructed from its samples if and only if the sampling rate exceeds twice the highest frequency present.\n\n## windowing\n\nreal signals are finite in duration. truncation introduces spectral **leakage**, spreading energy from a true frequency into neighboring bins. **window functions** (hann, hamming, blackman) taper the signal at the edges to reduce leakage at the cost of slightly reduced frequency resolution.\n"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "home",
      "lessonTitle": "Scientific Computing",
      "x": 0.5012792348861694,
      "y": 0.4231264293193817,
      "searchText": "scientific computing\n# scientific computing\n\n## course overview\n\nscientific computing develops the **numerical methods** needed to solve problems in biology, physics, nanoscience, and chemistry that have no closed-form solution. the emphasis is on deriving algorithms, programming them, and understanding their error behavior.\n\n- accuracy: how close is the numerical answer to the true solution?\n- efficiency: how does the computational cost scale with problem size?\n- robustness: does the method work reliably across a range of inputs?\n- stability: do small perturbations in input produce small perturbations in output?\n\n## why this topic matters\n\n- most differential equations arising in science cannot be solved analytically.\n- linear systems with thousands of unknowns appear in finite-element modeling, data fitting, and network analysis.\n- optimization underlies machine learning, inverse problems, and experimental design.\n- understanding numerical error is essential for trusting computational results.\n\n## why this order? a mountain trail map\n\nthink of this course as a hike up a mountain. each stop builds on the last, and the view keeps getting better.\n\n```\n                                        ___\n                                       /   \\    11. pdes\n                                      / pde  \\  \"painting the whole sky\"\n                                     /________\\\n                                    /          \\    10. odes\n                                   / time march \\   \"marching through time\"\n                                  /______________\\\n                                 /                \\    09. fft\n                                /   wave music     \\   \"hearing the hidden notes\"\n                               /____________________\\\n                              /                      \\    07-08. eigenvalues\n                             / secret personalities   \\   \"the dna of matrices\"\n                            /__________________________\\\n                           /                            \\    06. optimization\n                          /      smart hiking            \\   \"finding the best trail\"\n                         /________________________________\\\n                        /                                  \\    04-05. nonlinear\n                       /     nonlinear wilderness           \\   \"where the wild things are\"\n                      /______________________________________\\\n                     /                                        \\    03. least squares\n                    /          messy data                       \\   \"making peace with noise\"\n                   /____________________________________________\\\n                  /                                              \\    02. linear equations\n                 /            exact linear                        \\   \"the tools that always work\"\n                /__________________________________________________|\n               /                                                    \\    01. bounding errors\n    start --> /          floating-point lies                          \\   \"your computer is a liar\"\n             /____________________________________________________________\\\n```\n\nyou start at base camp learning that your computer lies to you (floating-point errors). then you climb through the world of linear equations where we actually have perfect tools. higher up, the data gets messy (least squares) and the equations get wild (nonlinear). optimization teaches you to hike smart. eigenvalues reveal the secret personalities hidden inside matrices. the fft lets you hear the music in data. ode solvers teach you to march through time. and at the summit, you can paint the whole sky with pde simulations of heat, waves, and fluid flow.\n\n## learning trajectory\n\n| # | lesson | subtitle | time |\n|---|--------|----------|------|\n| 01 | **bounding errors** | *why your computer is a liar and how to catch it in the act* | 45 min + 20 min coding |\n| 02 | **linear equations** | *the one family of problems we can solve perfectly (almost)* | 60 min + 30 min coding |\n| 03 | **linear least squares** | *when the data doesn't fit, find the closest thing that does* | 50 min + 25 min coding |\n| 04 | **nonlinear equations** | *welcome to the jungle where anything is possible* | 55 min + 25 min coding |\n| 05 | **nonlinear systems** | *newton goes to higher dimensions and brings friends* | 50 min + 25 min coding |\n| 06 | **optimization** | *finding the best answer when \"best\" is all you've got* | 50 min + 30 min coding |\n| 07-08 | **eigenvalue problems & algorithms** | *discovering the secret personalities hiding inside matrices* | 70 min + 30 min coding |\n| 09 | **fast fourier transform** | *the machine that tells you which notes are in a song* | 40 min + 20 min coding |\n| 10 | **initial value problems** | *teaching the computer to march through time without tripping* | 55 min + 25 min coding |\n| 11 | **partial differential equations** | *painting the whole sky: heat, waves, and everything in between* | 60 min + 30 min coding |\n\n## key mathematical ideas\n\n- matrix factorizations (lu, qr, svd) and their role in solving linear systems.\n- iterative methods for nonlinear equations (newton-raphson, fixed-point iteration).\n- numerical integration of odes (euler, runge-kutta) and stability theory.\n- finite-difference discretization of pdes.\n- the discrete fourier transform and the fft algorithm.\n- condition numbers and the propagation of rounding errors.\n\n## prerequisites\n\n- programming in python with numpy.\n- linear algebra: matrix operations, eigenvalues, vector spaces.\n- calculus: derivatives, integrals, taylor series.\n\n## recommended reading\n\n- heath, *scientific computing: an introductory survey*.\n- trefethen and bau, *numerical linear algebra*.\n- press et al., *numerical recipes*.\n\n### further reading (only if you're curious)\n\n- 3blue1brown: [essence of linear algebra](https://www.youtube.com/playlist?list=plzhqobowtqdpd3m"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "initialValueProblems",
      "lessonTitle": "Initial Value Problems",
      "x": 0.43804189562797546,
      "y": 0.5674622654914856,
      "searchText": "initial value problems\n# initial value problems\n\n## introduction\n\nan **initial value problem** (ivp) asks us to find $y(t)$ satisfying an ordinary differential equation $y' = f(t, y)$ with a given initial condition $y(t_0) = y_0$. most ode systems arising in physics and engineering cannot be solved analytically, so we rely on numerical methods that advance the solution step by step through discrete time increments.\n\nthe key challenges are **accuracy** (how close the numerical solution tracks the true one), **stability** (whether errors grow or decay), and **efficiency** (how much computation is needed for a given accuracy). these three concerns drive the design of all ivp solvers.\n\n## euler's method\n\nthe simplest approach discretizes the derivative directly. given a step size $h$, **forward euler** updates the solution as\n\n$$\ny_{n+1} = y_n + h f(t_n, y_n).\n$$\n\nthis is a first-order method: the local truncation error per step is $o(h^2)$, giving a global error of $o(h)$. while easy to implement, euler's method requires very small step sizes for acceptable accuracy and is unstable for stiff problems.\n\n**backward euler** replaces the right-hand side with the function evaluated at the new time:\n\n$$\ny_{n+1} = y_n + h f(t_{n+1}, y_{n+1}).\n$$\n\nthis is an **implicit** method since $y_{n+1}$ appears on both sides and generally requires solving a nonlinear equation at each step. the payoff is greatly improved stability for stiff systems.\n\n## runge-kutta methods\n\n**runge-kutta methods** achieve higher accuracy by evaluating $f$ at intermediate points within each step. the classical fourth-order method (rk4) computes\n\n$$\n\\begin{aligned}\nk_1 &= f(t_n, y_n), \\\\\nk_2 &= f(t_n + h/2, \\; y_n + h k_1/2), \\\\\nk_3 &= f(t_n + h/2, \\; y_n + h k_2/2), \\\\\nk_4 &= f(t_n + h, \\; y_n + h k_3),\n\\end{aligned}\n$$\n\nand then advances with\n\n$$\ny_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4).\n$$\n\nrk4 has a local truncation error of $o(h^5)$ and a global error of $o(h^4)$, offering an excellent balance of accuracy and simplicity. it is the workhorse method for non-stiff problems.\n\na general $s$-stage runge-kutta method is defined by a **butcher tableau** specifying the coefficients $a_{ij}$, $b_i$, and $c_i$. the method is explicit if $a_{ij} = 0$ for $j \\geq i$ and implicit otherwise.\n\n## multistep methods\n\nrather than using multiple evaluations within a single step, **multistep methods** use information from several previous steps. an $s$-step **adams-bashforth** method (explicit) takes the form\n\n$$\ny_{n+1} = y_n + h \\sum_{j=0}^{s-1} \\beta_j f(t_{n-j}, y_{n-j}).\n$$\n\nthe two-step version ($s = 2$) is\n\n$$\ny_{n+1} = y_n + \\frac{h}{2}\\bigl(3f_n - f_{n-1}\\bigr).\n$$\n\n**adams-moulton** methods are the implicit counterparts and include $f_{n+1}$ on the right-hand side. in practice, a **predictor-corrector** scheme uses adams-bashforth to predict and adams-moulton to correct, combining the efficiency of explicit methods with the improved stability of implicit ones.\n\n## stability analysis\n\nto study stability, we apply numerical methods to the **test equation** $y' = \\lambda y$ where $\\lambda \\in \\mathbb{c}$. the **stability region** of a method is the set of values $h\\lambda$ for which the numerical solution does not grow without bound.\n\nfor forward euler, the stability region is the disk $|1 + h\\lambda| \\leq 1$ in the complex plane. for backward euler, the stability region is the complement of $|1 - h\\lambda| < 1$, which includes the entire left half-plane. a method is **a-stable** if its stability region contains the entire left half-plane $\\operatorname{re}(h\\lambda) \\leq 0$.\n\nexplicit methods have bounded stability regions, so step sizes must satisfy $|h\\lambda| < c$ for some constant $c$. for stiff problems (where eigenvalues of the jacobian span many orders of magnitude), this restriction makes explicit methods impractical.\n\n## stiffness\n\na problem is **stiff** when it contains both fast-decaying and slow-varying components. the fast components force explicit methods to use tiny step sizes even when the solution is smooth.\n\na concrete example: in chemical reaction kinetics, a fast reaction (e.g., radical recombination with rate $k_1 = 10^9$) reaches equilibrium in nanoseconds, while a slow reaction (e.g., product formation with rate $k_2 = 1$) evolves over seconds. an explicit method must resolve the fast timescale ($h < 1/k_1 \\sim 10^{-9}$) even long after the fast mode has decayed, wasting billions of steps tracking a nearly-constant component.\n\nthis connects back to conditioning: the jacobian of a stiff system has eigenvalues spanning many orders of magnitude, giving it a large condition number. the ratio $|\\lambda_\\text{max}/\\lambda_\\text{min}|$ of the jacobian eigenvalues is a measure of stiffness and plays a role analogous to $\\text{cond}(a)$ in linear systems.\n\nother classic examples include circuit simulations (fast capacitive transients vs. slow resistive dynamics) and discretized parabolic pdes (where spatial refinement introduces increasingly stiff eigenvalues).\n\nfor stiff problems, **implicit methods** (backward euler, implicit runge-kutta, bdf methods) are essential. the **backward differentiation formulas** (bdf) of order $s$ are\n\n$$\n\\sum_{k=0}^{s} \\alpha_k y_{n+1-k} = h \\beta_0 f(t_{n+1}, y_{n+1}).\n$$\n\nbdf methods up to order 5 are a-stable or nearly so and form the basis of production codes like lsoda and sundials.\n\n## adaptive step size control\n\nin practice, a fixed step size is wasteful: the solution may be smooth in some regions and rapidly varying in others. **adaptive methods** estimate the local error and adjust $h$ to maintain a user-specified tolerance.\n\na common approach uses an **embedded runge-kutta pair** where two methods of different orders share the same function evaluations. the **dormand-prince** method (used by matlab's `ode45` and scipy's `solve_ivp`) pairs a fourth-order and fifth-order method. the difference between the two solutions estimates the local error:\n\n$$\n\\text{err} \\approx |y"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "linear_equations",
      "lessonTitle": "Linear Equations",
      "x": 0.3808703124523163,
      "y": 0.6774951219558716,
      "searchText": "linear equations\n# linear equations\n*the one family of problems we can solve perfectly (almost)*\n\n> *remember the condition number from lesson 01? watch how it shows up again here \u2014 this time deciding whether your linear system's answer is trustworthy or garbage.*\n\n## why linear systems matter\nlinear systems are special: they're something we can solve really well, and they cover an enormous amount of problems. even when you've got a non-linear system, you can isolate the linearity and solve that, and treat the non-linearity in another way.\n\n## from abstract linear systems to matrices\n#### going from abstract linear systems to matrices\n\na **linear function** is anything that behaves in the way we call linear. they exist in vector spaces: a mapping $f: x \\rightarrow y$ (abstract, eg: wave functions) is linear if $f(ax + bx') = af(x) + bf(x')$.\n\n*this says: linearity means \"scaling and adding inputs does the same thing as scaling and adding outputs.\" it's the nicest property a function can have.*\n\nlinear functions and matrices are the same thing (in finite dimensions, once bases are chosen)!\n* let's say you have a basis for $x$, eg: $\\{e_{1}, e_2, \\dots e_n\\}$, then any vector $x \\in x$ can be written uniquely as $x = a_1e_1 + \\dots + a_n e_n$\n* let $\\{f_{1}, f_2, \\dots f_n\\}$ form a basis for $y$, any vector $y \\in y$ can be written as $y = b_1f_1 + \\dots + b_m f_m$\n* $$f(e_j)=b_{1j}f_1 + \\dots + b_{mj}f_m$$\n\twe can write it because of linearity, but you'll notice that it's just matrix multiplication.\n* $$ f(x) = f(a_1e_1 + \\dots + a_ne_n) = a_1f(e_1) + \\dots + a_nf(e_n) $$\n\t$$ = \\sum_{j=1}^n a_j \\sum_{i=1}^m b_{ij} f_i $$\n\t$$ = \\sum_{i=1}^m f_i \\left(\\sum_{j=1}^n a_j b_{ij} \\right) $$\n\tthe thing on the right is nothing more than a matrix product. think of the total thing as:  representation of $f$ in $e,f$ basis $*$ (coordinates of $x$ in $e$ basis) = coordinates of $y$ in $f$ basis\n\nwe've shown how an abstract linear problem can be represented with matrices.\n\nwe're missing one thing: how do you find $b_{ij}$, the components of the matrix?\n$$b_{ij} = \\left< f_i \\vert f e_j \\right>$$\nfor example, $\\int f_i(\\alpha)(fe_j)(\\alpha) d\\alpha$, if $y$ is a space of functions $f(\\alpha)$\n\n**punch line:** the actions of $f: x\\rightarrow y$ are exactly the same as $f: \\mathbb{c}^m \\rightarrow \\mathbb{c}^n$.\n\n#### how to solve linear systems of equations\n\nfind all $x \\in x$ such that $f(x) = y$ (some known $y\\in y$)\n1. pick basis sets $\\{e_1, \\dots, e_n\\}$ for $x$, $\\{f_1, \\dots, f_m\\}$ for $y$\n2. write down matrix $f$ in $e,f$ basis and rhs $y$ in $f$ basis\n3. solve matrix equation $fx=y$, where $f$ and $y$ are known\n4. dot result $x$ with $e$ basis to get the result in the problem domain $x=x_1e_1+\\dots+x_ne_n$\n\n## existence of solutions\n#### can we solve $f(x)=y$?\nyes, when solutions exist!\n\ntoday, we'll use the case where $m=n$, i.e, matrix $\\underline{f}$ is square.\n\nif you apply $f$ to all the vectors in the source space, you get a subset (not proper) of $y$, which is called the _image_. $im(f) = \\{f(x) \\vert x \\in x\\}$.\n\nin the non-abstract space, it's called the _span_: $span(\\underline{f}) = \\{\\underline{f} \\,\\underline{x} \\vert \\underline{x} \\in \\mathbb{c}\\}$\n\nthree cases when $n=m$\n\n* **$f$ is non-singular**: (if one of them holds, all of them hold: they're equivalent)\n* $im(f) = y$ (dimension of source space $x$ = dimension of target space $y$)\n* $span(\\underline{f} = \\mathbb{c}^n)$\n* $rank(\\underline{f}) = n$\n* $det(\\underline{f}) \\neq 0$\n* $\\underline{f}\\,\\underline{x} = 0 \\leftrightarrow \\underline{x}=0$ (trivial kernel)\n* $\\underline{f}$ is invertible (deterministically find unique solution)\n* **$f$ is singular**:\n* $im(f) \\not\\subseteq y$\n* $span(\\underline{f} \\not\\subseteq \\mathbb{c}^n)$\n* $rank(\\underline{f}) < n$\n* $det(\\underline{f}) = 0$\n* there is a non-trivial subspace $ker(\\underline{f})$ such that $\\underline{f}(\\underline{x})=0$ for $\\underline{x}\\in ker(\\underline{f})$\n\nsingular $f$ splits into two sub-cases:\n1. $y \\not\\in im(f) \\implies$ there are no solutions\n2. $y \\in im(f) \\implies$ infinitely many solutions (because we can add something from the kernel and get another solution).\n\nthat was the mathematical part: now we're going to look at a case where we have exact solutions: when are the solutions stable, and when do small perturbations cause it to blow up?\n\n## condition number interactive demo\n\n<conditionnumberdemo />\n\n## sensitivity of a linear system of equations\n\nthe more orthogonal the matrix is, the lower the condition number. it's ~1 for orthogonal, but as they get closer to one another, i.e, they get closer to being linearly dependent on one another, condition number increases. the webpage has a calculation of the exact condition number.\n\n## interactive gaussian elimination and lu decomposition\n\n<gaussianelimdemo />\n\n<ludecompdemo />\n\n## how to build the algorithms from scratch\n\nconstruct algorithms that transform $b=ax$ into $x$ using a modest number of operations that are linear, invertible, and simple to compute.\n\n**fundamental property:** if $m$ is invertible, then $max = mb$ has the same solutions as $ax=b$\n\n*this says: multiplying both sides of the equation by the same invertible matrix doesn't change the answer. it's like weighing something on a different scale \u2014 the object doesn't change.*\n\nwhat we know:\n1. our algorithm must have the same effect as multiplying by the inverse. we avoid explicitly computing $a^{-1}$ and multiplying because inverses amplify rounding errors, especially for ill-conditioned matrices: $a \\rightarrowtail i; i \\rightarrowtail a; b \\rightarrowtail x$\n2. each step must be linear, invertible\n3. we first want $a = lu$ (lower and upper triangular matrices)\n4. every step, we want to take an $n\\times n$ matrix, and reduce the leftmost column to be zero below the first element. then, we just recursively continue for an $(n-1)\\times(n-1)$ matrix\n5. row scaling (it's linear and invertible)\n6. row addition and subtraction is also linear "
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "linearLeastSquares",
      "lessonTitle": "Linear Least Squares",
      "x": 0.37758129835128784,
      "y": 0.6946328282356262,
      "searchText": "linear least squares\n# linear least squares\n*when the data doesn't fit, find the closest thing that does*\n\n> *remember the condition number from lesson 01? here's where it bites us \u2014 the normal equations square it, and householder reflections save us.*\n\n## overdetermined systems: m > n\n\nlarge number of equations, small number of unknowns. in general, there are _no exact solutions_.\n\nall the places you can reach are driven by $im(a)$, the image. there's no $x$ that you can feed in that results in you ending up outside the image.\n\n### how would a least squares approximate solution have to look?\n\n**write** $\\mathbb{c}^m = im(a) \\oplus im(a)^\\perp$\nany vector space with a subspace can be broken down into the subspace and a space that's orthogonal to the subspace.\n**means** $b \\in \\mathbb{c}^m$ can be uniquely written $b = \\tilde{b} + b^\\perp$, with $\\tilde{b} \\in im(a), b^\\perp \\in im(a)^\\perp$\n**where**\n$$im(a)^\\perp = \\{x\\in \\mathbb{c}^m \\mid x^t x' = 0 \\;\\forall\\; x' \\in im(a)\\}$$\n\n$$=\\{x\\in \\mathbb{c}^m \\mid a^t_i x=0 \\text{ for } 1 \\leq i \\leq m \\}$$\n\nwatch this trick \u2014 pythagoras makes the whole thing click:\n\n$$||r||^2 = ||b-ax||^2$$\n$$= ||\\tilde{b} + b^\\perp - ax||^2 = ||(\\tilde{b}-ax)+b^\\perp||^2$$\nsince $(\\tilde{b}-ax) \\in im(a)$ and $b^\\perp \\in im(a)^\\perp$, these two components are orthogonal. by pythagoras (valid in $l^2$ norm because orthogonal vectors satisfy $\\|u+v\\|^2 = \\|u\\|^2 + \\|v\\|^2$):\n$$ = ||\\tilde{b}-ax||^2 + || b^\\perp ||^2$$\n\n*this says: the total error splits into a part we can control (by choosing x) and a part we can't (the component of b perpendicular to the image). minimize the first, accept the second.*\n\nthe first term is in the image, the second is in the perpendicular subspace. we can minimize the first term to zero (by choosing $ax = \\tilde{b}$), and the second term is a constant independent of $x$.\n\n**so:** the residual of least squares is perpendicular to $im(a)$.\n\n## the least squares problem\nwith given data and a desired function, determine the parameters of the function to minimize the distance to data points.\n\n\n## least squares solution\n- **always exists:** the solution to $ax=\\tilde{b}$ is the least squares solution.\n$\\tilde{b}$ is the projection on the image space, and $b^\\perp$\nthe projection on the orthogonal component.\n- **unique if rank(a) = n:** if the rank is less than $n$ (columns are linearly dependent), there are\ninfinitely many solutions: you can find one and add the kernel space\nto it to get them all.\n- **normal equations**:\n$a^t_i r=0 \\rightarrow a^t r=0 \\implies a^t(b-ax)=a^tb-a^tax=0$.\nthe fact that the residual is orthogonal to the image means that\nfor all the rows of $a$, the dot product with the residual has to be 0.\n- **$a^ta$ is a small square matrix**\n- the only problem is that this has a large condition number: $\\text{cond}(a^ta) = \\text{cond}(a)^2$\n\n*this says: forming the normal equations squares the condition number. if your matrix was a bit ill-conditioned (say cond = 1000), the normal equations make it horrifically ill-conditioned (cond = 1,000,000). that's like photocopying a blurry photo \u2014 every generation makes it worse.*\n\nwe're almost at our goal, but not there yet. we've found an efficient solution (the normal equations) which are great for mathematical calculations, but we can't use them because a small error will ruin us because of the squared condition number.\n\n### how do we save our significant digits (hopefully without too much work?)\n\nnote that $im(a)^\\perp = ker(a^t) \\implies \\mathbb{c}^m = im(a) \\oplus ker(a^t)$.\n\n### recap\nwe decomposed the target space $\\mathbb{c}^m$ into the image of $a$ and the kernel of $a^t$, two orthogonal subspaces.\n\n$\\text{cond}(a^ta) = \\text{cond}(a)^2$, but $\\text{cond}(q^ta) = \\text{cond}(a)$.\n\nwe just need to construct the effect of multiplying our matrix by $q^t$. in general, you never want to calculate $q$ because it can be really large (million x million).\n\n> **you might be wondering...** \"why does multiplying by $q^t$ not mess up the condition number, but multiplying by $a^t$ does?\" because $q$ is orthogonal \u2014 it's a pure rotation/reflection that doesn't stretch anything. multiplying by $q^t$ is like turning your head to look at the problem from a different angle. the problem doesn't change, just your viewpoint. multiplying by $a^t$ is like squishing the problem through a funnel.\n\n## building a least squares algorithm from scratch\n\nnote: norm $\\left(\\,||x||\\,\\right)$ always refers to the euclidean norm $\\left(\\,||x||_2\\,\\right)$ when we're talking about least squares.\n\n**goal:** construct the effect of $q^t$ such that $q^t a = r$, where $r$ is a matrix with the bottom part being 0 and the upper part being upper triangular.\n\n### the householder reflection: bouncing light off a mirror\n\nhere's the beautiful part. imagine you're holding a flashlight (that's your column vector $a$) and you want to redirect all the light so it points straight along the first axis $e_1$ (that's the target direction). how do you do it? **place a mirror at exactly the right angle between the current direction and the target direction, and bounce the light off it.** that's exactly what a householder reflection does \u2014 it's a mirror that zeros out everything below the diagonal in one bounce.\n\n**building blocks:** we want to perform _unitary_ operations: i.e., do things that don't change the length of vectors: operations like rotation, reflection (not translations, because although translations don't change lengths, they change the norm).\n- 2d: rotations and reflections\n- 3d: rotations, reflections and inversions (like reflection through a point)\n- higher dimensions: lots more (symmetries of n-spheres), permutations (everywhere)\n\n**what do we want to build?** similar to lu, we want to take a column and eliminate everything below the diagonal. construct a reflection operation $h$ (for householder), which is a reflection. the norm of the entire column must be the same though, and so the top element (the diago"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "nonlinearEquations",
      "lessonTitle": "Nonlinear Equations",
      "x": 0.37792089581489563,
      "y": 0.6099429130554199,
      "searchText": "nonlinear equations\n# nonlinear equations\n*welcome to the jungle where anything is possible*\n\n> *we just mastered linear systems where everything was guaranteed. now those guarantees vanish. welcome to the real world.*\n\n# root finding in one dimension\n## notation:\n\n* $f(x) = 0$: 1 dimensional stuff. $f:\\mathbb{r} \\to \\mathbb{r}$\n* $f(x) = 0$: higher dimensional stuff, $f$ is a matrix an $x$ a vector. $f:\\mathbb{r}^n\\to\\mathbb{r}^m$\n\ncomplex numbers are complicated, so we'll only work with reals.\n\n\n## introduction\n\nemergent macroscopic behaviour comes out of high dimensionality of linear systems. for example, you don't figure out the aerodynamics of a plane by using the schrodinger equation for every atom. you can make a simpler theory based on the emergent behaviour, which could be non-linear despite the underlying rules being linear. you could also just make up a non-linear problem, like in economics.\n\n**linear systems:**\n- we know exactly how many solutions exist (by looking at the matrix's rank)\n- we have methods to find exact solutions (if they exist) or approximate solutions (if they don't exist)\n- we can find the full solution space of the problem by adding the kernel\n- we can routinely solve for billions of dimensions: it's very efficient\n\n**non-linear systems:**\n- no idea how many (if any) solutions. all we can hope for is rules of thumb, heuristics, and we can look for something that works as much as possible and fails rarely. we won't get great results in a finite number of steps; sometimes it gets closer and sometimes it doesn't.\n- no fail-proof solvers\n- no way of knowing if we've found all the solutions\n- even 1 dimensional solutions can take ages\n\n## how many solutions?\n\nglobally, *anything is possible*. $e^{-x} = 0$ has no solutions, but $(e^{-x}-\\delta) = 0$ (where $\\delta$ is a small positive number) does. $\\sin(x) = 0$ has countably infinite solutions. even a simple-looking polynomial like $x^5 - x = 0$ has multiple roots that are hard to predict without analysis.\n\nlocally, we can sometimes work it out. in 1d, we can look at where $f(x)$ changes sign, and we can assume that there's a root in between (intermediate value theorem, assuming the function is continuous). in particular, there are $>1$ roots in the region, and there are an odd number of roots.\n\n## general algorithm construction scheme\n\nhere's a powerful recipe for inventing algorithms:\n1. find invariant that guarantees existence of a solution in the search space\n2. design operation that preserves 1. and shrinks search space\n\n*this says: first make sure you're looking in the right place, then systematically make that place smaller. it's a tried and true scheme \u2014 euclid did it thousands of years ago, and we'll do it now.*\n\nlet's use this to build an equation solver on a bracket. (a *bracket* is an interval in which $f(x)$ changes sign).\n\n## bisection method\n\n**step 1:** establish the invariant: $a<b$ and $\\text{sign}(f(a)) \\neq \\text{sign}(f(b))$\n\n*the function crosses zero somewhere between a and b. we're sure of it.*\n\n**step 2:** cut the interval in half: set $m = \\frac{a+b}{2}$ and evaluate $s_m = \\text{sign}(f(m))$\n\n**step 3:** keep the half that still brackets the root:\n- if $s_m == s_a$: the root is in the right half, so $a=m$\n- if $s_m == s_b$: the root is in the left half, so $b=m$\n- if $s_m == 0$: we've found the root\n\nnote: you should not use $m = \\frac{a+b}{2}$ because of floating point error: use  $m = a + \\frac{b-a}{2}$\n\n```python\ndef bisection(f, a, b, tolerance):\n    n_steps = int(np.ceil(np.log2((b - a) / tolerance)))\n    s_a = np.sign(f(a))\n    for i in range(n_steps):\n        m = a + (b - a) / 2      # midpoint without overflow\n        s_m = np.sign(f(m))\n        if s_m == s_a:\n            a = m                 # root is in right half\n        else:\n            b = m                 # root is in left half\n    return m\n```\n\n*bisection gains exactly one bit of accuracy per step. slow but sure \u2014 the tortoise of root-finding.*\n\n## conditionings\n\nthe conditioning for evaluating $f(x)$ is approximately $\\left| \\frac{x f'(x)}{f(x)} \\right|$ (from taylor expansion), and the absolute error is $|f'(x)|$. when evaluating $f^{-1}$, the condition number is $\\approx \\left|  \\frac{f(x)}{x f'(x)} \\right|$ and the absolute error is $\\left|\\frac{1}{f'(x)} \\right|$.\n\n*this says: if a function has a steep slope near the root, the root is easy to find (well-conditioned). if the function barely grazes zero (shallow crossing), the root is hard to pin down.*\n\nas you get a high sensitivity in the inverse, you get a low sensitivity in the inversion and vice-versa. the function doesn't need to have an inverse in order to find the inverse in a local region.\n\nwe're trying to look for $f(x)=0$: when we're close to zero, we never use the relative accuracy but always the absolute one.\n\n## convergence\n\n$e_{k}$ (the error at the $k^{\\text{th}}$ step) $= x_k - x^*$\n\n$$e^k_{rel} = \\frac{e_k}{x^*}  = \\frac{x_k - x^*}{x^*}$$\n\nwe need to look at the number of significant bits, because it's exact unlike significant decimal digits.\n\n$$\n\\begin{align*}\n\\text{bits/step} &= -\\log_2(e^{k+1}_{rel}) - \\left(-\\log_2(e^k_{rel}) \\right)   \\\\\n& = \\log_2\\left(\\frac{\\frac{x_k - x^*}{x^*}}{\\frac{x_{k+1} - x^*}{x^*}} \\right) \\\\\n& = \\log_2\\left(\\frac{\\vert x_k - x^*\\vert}{\\vert x_{k+1} - x^*\\vert} \\right)   \\\\\n& = \\log_2\\left(\\frac{\\vert e_k\\vert}{\\vert e_{k+1}\\vert}\\right) \t\t\t\t\\\\\n& = -\\log_2\\left(\\frac{\\vert e_{k+1}\\vert}{\\vert e_k\\vert}\\right)\n\\end{align*}\n$$\n\nif $\\lim_{k\\to 0} \\frac{\\vert e_{k+1}\\vert}{\\vert e_k\\vert^r} = c$, and $0 \\leq c \\lt 1$, method converges with rate r=1 $\\implies$ linear, r=2 $\\implies$ quadratic, etc\n\n**how fast is fast?** think of convergence rates like this:\n- **linear (r=1):** like earning simple interest \u2014 you gain a fixed number of correct digits each step. bisection does this: one bit per step, steady and reliable.\n- **quadratic (r=2):** like compound interest on steroids \u2014 the number of correct digits *doubles* each "
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "nonlinearSystems",
      "lessonTitle": "Nonlinear Systems",
      "x": 0.3718872666358948,
      "y": 0.5557011961936951,
      "searchText": "nonlinear systems\n# nonlinear systems\n*newton goes to higher dimensions and brings friends*\n\n> *everything we learned about newton's method in 1d carries over \u2014 but now the derivative is a jacobian matrix, and we solve a linear system at every step. remember lesson 02? you'll need it here.*\n\n## nonlinear solvers\n\n## fixed point iteration (recap)\n\nrecall from the previous page: we can transform $f(x)=0$ to $g(x)=x$ and iterate $x_{k+1} = g(x_k)$. the key results were:\n\n* if $|g'(x^*)| < 1$, $g$ is a contraction near $x^*$ and we get **linear convergence**: $|e_{k+1}| \\leq c^k |e_0|$\n* if $g'(x^*) = 0$, we get **quadratic convergence**: $|e_{k+1}| \\leq \\bar{c}^k |e_0|^{2^k}$\n\nthis all generalizes directly to higher dimensions, replacing $|g'|$ with $\\vert g' \\vert$ (the jacobian norm).\n\n\n## newton's method\n\na quadratically convergent fixed point iteration solver. take a point, take the tangent of the curve at that point, your new point is the tangent's x-intercept. intuition: find zero for linear approximation, set as next $x$.\n\ntaylor expand:\n$$ f(x_{k+1}) = f(x_k) + f'(x_k) (x_{k+1}-x_k) + \\mathcal{o}(\\vert x_{k+1}-x_k\\vert^2)$$\n$$ \\approx f(x_k) + f'(x_k) (x_{k+1}-x_k)$$\n$$ 0 \\approx f(x_k) + f'(x_k) \\delta x_{k+1}$$\n$$ f'(x_k) \\delta x_{k+1} = f(x_k)$$\n\n*this says: at each step, pretend the world is linear (use the tangent), solve the linear problem to find the step, then take that step.*\n\nremember the above line, cause we'll use it in higher dimensions as it's solving linear systems\n$$ \\delta x_k = -\\frac{f(x_k)}{f'(x_k)}  $$\nthis works in 1d, but is not general because we're dividing by a matrix (the jacobian) and not a number\n$$ \\implies g(x_k) = x_k + \\delta x_k = x_k - \\frac{f(x_k)}{f'(x_k)}$$\n\nif $f(x^*) = 0$,\n$$g(x^*) = x^* - \\frac{f(x)}{f'(x)} = x^*$$\nalso,\n$$ g'(x) = \\frac{d}{dx}\\left(x - \\frac{f(x)}{f'(x)}\\right)$$\n$$ 1 - \\frac{f'(x)f'(x) + f(x)f''(x)}{f'(x)^2} $$\n$$  = - \\frac{f(x)f''(x)}{f'(x)^2}$$\n$$ \\to 0 \\quad \\text{when}\\quad f(x)=0$$\n\n## quasi-newton/secant methods\n\nthese are methods that try to replicate the wonderful properties of newton's method, but without having to evaluate the derivative. in higher dimensions, you don't want to be evaluating the derivative cause it's a massive matrix.\n\n**method**: use secant (finite difference) instead of tangent (derivative).\n\n$$f'(x) \\approx \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}$$\n\n*this says: instead of knowing the exact slope, estimate it from the last two points. you lose a bit of convergence speed but save a huge amount of computation.*\n\nin 1d, we have the following equation for $\\delta x$:\n$$\\delta x_{k+1} = -\\frac{f(x_k)}{\\hat{f}'(x_k)}$$\nwhere\n$$\\hat{f}'(x_k) = \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}$$\nfor higher dimensions, we want to write it like\n$$ \\hat{f}'(x_k) \\delta x_{k+1} = - f(x_k)$$\ncause we'll solve a linear system instead\n\n## interactive visualizations\n\n### newton's method in 1d: cobweb diagram\n\nexplore newton's method interactively. select functions with known roots, adjust initial guess `x\u2080`, and step through iterations. the cobweb diagram visualizes the fixed-point iteration `x = g(x)` where `g(x) = x - f(x)/f'(x)`.\n\n```tsx\nimport newton1d from '@/components/visualization/nonlinear-equations/newton1d';\n&lt;newton1d /&gt;\n```\n\n<newton1d />\n\nobserve quadratic convergence near the root and potential overshoots or divergences.\n\n### 2d nonlinear optimization: contour descent and basins of attraction\n\nthe himmelblau function has four minima:\n- \u2248 (3.0, 2.0)\n- \u2248 (-2.8, 3.1)\n- \u2248 (-3.8, -3.3)\n- \u2248 (3.6, -1.8)\n\ncompare gradient descent (gd) and newton's method. toggle basins to see attraction regions from initial points grid. newton's method has smaller, more precise basins due to curvature info.\n\n```tsx\nimport himmelblau2d from '@/components/visualization/nonlinear-equations/himmelblau2d';\n&lt;himmelblau2d /&gt;\n```\n\n<himmelblau2d />\n\n**research insights & gaps filled:**\n- **basin attractors**: visualizes fractal-like boundaries in practice, sensitive to method.\n- **contour descent**: paths show gd zigzagging (rosenbrock-like valley), newton direct.\n- **interactivity**: sliders for params/init, animation reveals dynamics.\n- **gaps**: few web interactives compare newton/secant(quasi) in 2d basins; this adds newton vs gd proxy for secant ideas.\n\nfor secant methods like broyden/bfgs, paths approximate newton without exact derivs/hessians.\n\n## going to higher dimensions\n\nimagine you're a blind mountain climber feeling the slope under your boots. in one dimension, you only feel \"uphill\" or \"downhill.\" but in $n$ dimensions, the slope has a direction \u2014 it's a vector (the gradient), and the rate of change along any particular direction is the directional derivative. the jacobian matrix collects all these directional sensitivities into one object.\n\nnotation: $f: \\mathbb{r}\\to\\mathbb{r}$, $f: \\mathbb{r}^n\\to\\mathbb{r}^n$\n\n1d taylor expansion:\n$$f(x_{k+1}) = f(x_k + \\delta x_{k+1})$$\n$$f(x_{k+1}) = f(x_k) + f'(x_k)\\delta x_{k+1} + \\mathcal{o}(|\\delta x_{k+1}|^2)$$\n\nn-dimensional case:\n$$f(x_{k+1}) = f(x_k + \\delta x_{k+1})$$\n$$f(x_{k+1}) = f(x_k) + f'(x_k)\\delta x_{k+1} + \\mathcal{o}(|\\delta x_{k+1}|^2)$$\n\n*here $f'(x_k)$ is the jacobian \u2014 an $n \\times n$ matrix of all partial derivatives. each row tells you how one output component changes with all the inputs.*\n\nto go to the next step, we don't divide by $f'(x)$ like we do in the 1d case, but instead we solve the linear system\n$$f'(x_k) \\delta x_{k+1} = - f(x)$$\nwhere we know the first and last terms and want to find $\\delta x_{k+1}$\n\n*this is the key connection: each newton step in n dimensions requires solving a linear system (lesson 02). the jacobian plays the role of the coefficient matrix, and $-f(x_k)$ is the right-hand side.*\n\nin the secant method, we just use $\\hat{f}'(x)$ instead. this raises an extra problem: we need to find a good $\\hat{f}'$\n\n> **you might be wondering...** \"how expensive is computing the jacobian?\" for $n$ unknowns, the jacobian has $n^2$ "
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "optimization",
      "lessonTitle": "Optimization Methods",
      "x": 0.35175031423568726,
      "y": 0.532940149307251,
      "searchText": "optimization methods\n# optimization methods\n*finding the best answer when \"best\" is all you've got*\n\n> *the quasi-newton ideas from lesson 05 (broyden) show up again here as bfgs \u2014 the same trick of building up curvature knowledge step by step, but now for finding minima instead of zeros.*\n\n## non-linear optimization\n\n### the party in the dark\n\nimagine you're at a party in a pitch-dark room and you're trying to find the coldest spot (next to the air conditioning). you can feel the temperature where you're standing, and maybe you can feel which direction is cooler. that's gradient descent \u2014 just follow the chill.\n\nbut what if the room has multiple cold spots? you might get stuck in a corner that's cool but not the *coolest*. that's a local minimum. now imagine the room is full of people, all searching independently, and they shout to each other when they find somewhere cold. that's metaheuristic optimization \u2014 collective exploration beats individual greedy search.\n\nthe big question is always: **how much do you know about the landscape, and how much can you afford to explore?**\n\n## when to use what:\n\n### questions that'll help decide on what to use\n1. how slow are function evaluations (and gradients)?\n2. how big is your space?\n3. how ugly is your energy landscape? convex, or many minima?\n\n### rules of thumb:\n1. fast function evaluations:\n   1.  you can just take a linspace, evaluate the function at every point, and either use the minimum or feed that to a newton raphson method.  both pretty and ugly energy landscapes works, it just changes the size of the linspace.\n   2.  for medium dimensions (up to 100), you want to use bfgs if the energy landscape is simple. if it's complicated, you want to use bfgs + exploration, as you need some way to escape local minima to find a global minima.\n   3.  for high dimensions (up to 1m), use conjugated gradients. takes longer to converge than bfgs, but you don't have to represent the high dimensional hessian matrix and so it works up to millions of dimensions. for a convoluted energy landscape, you want to use exploration as well.\n\n2. slow function evaluations\n   1. low to medium dimensions: if the energy landscape is simple, bfgs. even if you have a complicated energy landscape, your search area is small enough that you can use bfgs with exploration.\n   2. high dimensions: simple energy landscape, use conjugate gradients. when it's expensive to evaluate the function, and you're in a high-dimensional complex landscape, the search space is too big for you to get anywhere. here you need to think, and tailor your solution to fit your problem. generally, you can try to use some sort of symmetry or structure of your problem and then use a metaheuristic to guide your solutions\n\n> **you might be wondering...** \"why can't i just use gradient descent for everything?\" because gradient descent is like always walking downhill in the steepest direction \u2014 it zigzags in narrow valleys and takes forever to converge. bfgs and conjugate gradients are smarter: they learn the shape of the valley and take much better steps.\n\n## bfgs (broyden-fletcher-goldfarb-shanno)\n\nbfgs is a quasi-newton method for minimizing $f:\\mathbb{r}^n\\to\\mathbb{r}$. newton's method for optimization uses the hessian $h$ to find the step:\n\n$$h_k \\delta x_k = -\\nabla f(x_k)$$\n\n*this says: the hessian tells you the curvature of the landscape, and newton uses it to jump directly to the bottom of the local bowl. but computing the full hessian is expensive.*\n\ncomputing the full hessian is expensive ($o(n^2)$ storage, $o(n^3)$ to solve). bfgs builds an approximation $b_k \\approx h_k^{-1}$ using only gradient information, similar to how broyden's method approximates the jacobian.\n\n**bfgs update rule:** given step $s_k = x_{k+1} - x_k$ and gradient change $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$:\n\n$$b_{k+1} = \\left(i - \\frac{s_k y_k^t}{y_k^t s_k}\\right) b_k \\left(i - \\frac{y_k s_k^t}{y_k^t s_k}\\right) + \\frac{s_k s_k^t}{y_k^t s_k}$$\n\n*this says: update your curvature estimate using the step you just took and how much the gradient changed. each step teaches you a bit more about the shape of the landscape.*\n\nthis is a rank-2 update (two outer products), so each step costs $o(n^2)$ instead of $o(n^3)$.\n\n**algorithm \u2014 a numbered story:**\n1. **start ignorant:** set $b_0 = i$ (pretend the landscape is a simple bowl)\n2. **feel the slope:** compute search direction $\\delta x_k = -b_k \\nabla f(x_k)$\n3. **walk carefully:** line search to find step size $\\alpha_k$ along $\\delta x_k$ (wolfe conditions)\n4. **take the step:** update $x_{k+1} = x_k + \\alpha_k \\delta x_k$\n5. **learn from the step:** update $b_{k+1}$ using the formula above\n6. **repeat** until $\\vert \\nabla f \\vert < \\text{tol}$\n\nbfgs achieves **superlinear convergence** near a minimum and works well up to medium dimensions (~100). beyond that, storing the $n\\times n$ matrix $b_k$ becomes prohibitive.\n\n**l-bfgs** (limited-memory bfgs) avoids storing the full matrix by keeping only the last $m$ pairs $(s_k, y_k)$ and reconstructing the matrix-vector product implicitly. this reduces storage from $o(n^2)$ to $o(mn)$ and works up to millions of dimensions, though it converges somewhat slower than full bfgs.\n\n## conjugate gradient method\n\nfor high-dimensional problems where even $o(n^2)$ storage is too much, the conjugate gradient (cg) method uses only $o(n)$ storage by maintaining just a search direction vector.\n\n**key idea:** instead of steepest descent (which zigzags), choose search directions that are _conjugate_ with respect to the hessian: $d_i^t h d_j = 0$ for $i \\neq j$. this guarantees that progress made in one direction is not undone by later steps.\n\n*think of it like this: if steepest descent is a drunk stumbling downhill (zigzagging back and forth across a valley), conjugate gradients is a sober hiker who remembers where they've already been and never backtracks.*\n\n**nonlinear cg (fletcher-reeves):**\n1. **start steep:** set initial direction $d_0 = -"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "partialDifferentialEquations",
      "lessonTitle": "Partial Differential Equations",
      "x": 0.4539386034011841,
      "y": 0.5630889534950256,
      "searchText": "partial differential equations\n# partial differential equations\n\n## classification\n\npartial differential equations (pdes) are classified by the nature of their highest-order terms. a second-order linear pde in two variables has the general form\n\n$$\na u_{xx} + 2b u_{xy} + c u_{yy} + \\text{lower order terms} = 0.\n$$\n\nthe **discriminant** $b^2 - ac$ determines the type:\n\n- **elliptic** ($b^2 - ac < 0$): e.g., laplace equation $\\nabla^2 u = 0$. describes equilibrium states.\n- **parabolic** ($b^2 - ac = 0$): e.g., heat equation $u_t = \\alpha \\nabla^2 u$. describes diffusion processes.\n- **hyperbolic** ($b^2 - ac > 0$): e.g., wave equation $u_{tt} = c^2 \\nabla^2 u$. describes wave propagation.\n\neach type requires different numerical strategies and boundary conditions. elliptic problems need boundary values on the entire domain boundary. parabolic and hyperbolic problems need initial conditions plus boundary conditions.\n\n## finite difference methods\n\nthe fundamental idea is to replace continuous derivatives with **discrete approximations** on a grid. for a uniform grid with spacing $\\delta x$, the standard finite difference formulas are\n\n$$\n\\frac{\\partial u}{\\partial x} \\approx \\frac{u_{i+1} - u_{i-1}}{2\\delta x} \\quad \\text{(central, } o(\\delta x^2)\\text{)},\n$$\n\n$$\n\\frac{\\partial^2 u}{\\partial x^2} \\approx \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\delta x^2} \\quad \\text{(} o(\\delta x^2)\\text{)}.\n$$\n\nthese replace the pde at each interior grid point with an algebraic equation, producing a system of equations that can be solved by linear algebra techniques.\n\n## the heat equation\n\nthe one-dimensional heat equation $u_t = \\alpha u_{xx}$ is the prototypical parabolic pde. the **ftcs scheme** (forward time, central space) discretizes as\n\n$$\n\\frac{u_i^{n+1} - u_i^n}{\\delta t} = \\alpha \\frac{u_{i+1}^n - 2u_i^n + u_{i-1}^n}{\\delta x^2},\n$$\n\ngiving the explicit update\n\n$$\nu_i^{n+1} = u_i^n + r(u_{i+1}^n - 2u_i^n + u_{i-1}^n),\n$$\n\nwhere $r = \\alpha \\delta t / \\delta x^2$. this scheme is **conditionally stable**: the solution blows up unless $r \\leq 1/2$ (the **cfl condition** for this problem).\n\nthe **implicit (backward euler)** scheme evaluates spatial derivatives at the new time level:\n\n$$\nu_i^{n+1} - r(u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}) = u_i^n.\n$$\n\nthis is unconditionally stable but requires solving a tridiagonal linear system at each time step. the **crank-nicolson** scheme averages the explicit and implicit forms, achieving $o(\\delta t^2, \\delta x^2)$ accuracy while remaining unconditionally stable.\n\n## the wave equation\n\nthe one-dimensional wave equation $u_{tt} = c^2 u_{xx}$ is discretized with central differences in both time and space:\n\n$$\n\\frac{u_i^{n+1} - 2u_i^n + u_i^{n-1}}{\\delta t^2} = c^2 \\frac{u_{i+1}^n - 2u_i^n + u_{i-1}^n}{\\delta x^2}.\n$$\n\nthis leapfrog scheme is second-order accurate and stable when the **courant number** satisfies $c = c\\delta t / \\delta x \\leq 1$. the courant condition ensures that the numerical domain of dependence contains the physical domain of dependence.\n\n## elliptic problems\n\nfor laplace's equation $\\nabla^2 u = 0$ on a 2d grid, the five-point stencil gives\n\n$$\nu_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j} = 0.\n$$\n\nthis produces a large sparse linear system $a\\mathbf{u} = \\mathbf{b}$ where boundary values enter through the right-hand side. direct solvers (lu decomposition) work for moderate grids, but **iterative methods** are preferred for large problems:\n\n- **jacobi iteration**: update each point from its neighbors' previous values.\n- **gauss-seidel**: use updated values as soon as available (faster convergence).\n- **successive over-relaxation (sor)**: accelerates gauss-seidel with a relaxation parameter $\\omega$.\n\nfor poisson's equation $\\nabla^2 u = f$, the same stencil applies with $f_{i,j}$ on the right-hand side.\n\n## boundary conditions\n\nthe three standard types are:\n\n- **dirichlet**: $u = g$ on $\\partial\\omega$. the boundary values are known and directly substituted.\n- **neumann**: $\\partial u / \\partial n = g$ on $\\partial\\omega$. discretized using one-sided or ghost-point differences.\n- **robin (mixed)**: $\\alpha u + \\beta \\partial u / \\partial n = g$. combines the above.\n\nghost points outside the domain are a clean way to implement neumann conditions: introduce a fictitious point $u_{-1}$ and use the centered difference $u_1 - u_{-1} = 2\\delta x \\cdot g$ to eliminate it.\n\n## spectral methods\n\nfor problems with smooth solutions and periodic boundary conditions, **spectral methods** represent the solution in a basis of trigonometric functions. the spatial derivative of $u(x) = \\sum_k \\hat{u}_k e^{ikx}$ is computed exactly in fourier space:\n\n$$\n\\frac{\\partial u}{\\partial x} \\longleftrightarrow ik\\hat{u}_k.\n$$\n\nthe **fast fourier transform** (fft) makes this approach computationally efficient with $o(n\\log n)$ operations. spectral methods achieve exponential convergence for smooth solutions, far outperforming finite differences.\n\nfor non-periodic domains, **chebyshev spectral methods** use chebyshev polynomials as the basis, with clustering of grid points near boundaries to handle boundary layers.\n\n[[simulation reaction-diffusion]]\n\n## method of lines\n\nthe **method of lines** (mol) semi-discretizes the pde by replacing spatial derivatives with finite differences while leaving time continuous. this converts the pde into a system of odes:\n\n$$\n\\frac{d\\mathbf{u}}{dt} = \\mathbf{f}(\\mathbf{u}),\n$$\n\nwhich can then be integrated with any ode solver (euler, rk4, bdf). this approach cleanly separates spatial and temporal discretization, allowing established ode solvers to handle time integration including adaptive step size control.\n\n## stability and convergence\n\nthe **lax equivalence theorem** states that for a consistent finite difference scheme applied to a well-posed linear pde, **stability is equivalent to convergence**. this means we only need to verify stability (via von neumann analysis or matrix methods) to guarantee that the numerical solutio"
    }
  ]
}