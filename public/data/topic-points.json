{
  "meta": {
    "generatedAt": "2026-02-21T21:43:09.503429+00:00",
    "embeddingModel": "text-embedding-3-small",
    "projection": "tsne-2d",
    "inputCount": 106,
    "perplexity": 17
  },
  "topics": [
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing"
    }
  ],
  "points": [
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "analysis-and-theory",
      "lessonTitle": "Analysis and Theory of Deep Learning",
      "x": 0.8429493308067322,
      "y": 0.7673097848892212,
      "searchText": "analysis and theory of deep learning\n# analysis and theory of deep learning\n\n## the puzzle of generalization\n\nhere is the central mystery of deep learning. classical learning theory says: if your model has more parameters than training examples, it will memorize the data and fail on new examples. this is the fundamental bias-variance tradeoff, and it has been the bedrock of statistics for a century. then along comes a neural network with 100 million parameters, trained on 50,000 images, and it *generalizes beautifully*. it has more than enough capacity to memorize every training example \u2014 and it does memorize them \u2014 yet it still performs well on data it has never seen. why?\n\nunderstanding this contradiction is one of the deepest open problems in machine learning.\n\n## generalization bounds\n\n**pac-bayes bounds** provide some of the tightest generalization guarantees for neural networks. the idea is to measure how much the trained model differs from what you expected before seeing the data. for a posterior distribution $q$ over hypotheses and a prior $p$:\n\n$$\n\\mathbb{e}_{h \\sim q}[\\mathcal{l}(h)] \\leq \\mathbb{e}_{h \\sim q}[\\hat{\\mathcal{l}}(h)] + \\sqrt{\\frac{d_{\\text{kl}}(q \\| p) + \\ln(n/\\delta)}{2n}},\n$$\n\nwhere $\\hat{\\mathcal{l}}$ is the empirical loss, $n$ is the number of training examples, and $\\delta$ is the failure probability. the bound favors models that are both accurate on training data and close to the prior \u2014 models that did not have to change much to fit the data.\n\n**rademacher complexity** takes a different approach, measuring the ability of a function class to fit random noise:\n\n$$\n\\mathcal{r}_n(\\mathcal{f}) = \\mathbb{e}\\left[\\sup_{f \\in \\mathcal{f}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_i f(x_i)\\right],\n$$\n\nwhere $\\sigma_i \\in \\{-1, +1\\}$ are random signs. if your model class can easily fit random labels, it has high rademacher complexity and weak generalization guarantees. lower rademacher complexity implies better generalization.\n\n## double descent\n\nfor a hundred years statistics told us: more parameters than data points means guaranteed overfitting. then deep learning came along and shattered that rule. the **double descent** curve is the field's most surprising plot: test error goes down, then **up** (classic overfitting), then **down again** as you keep adding parameters past the interpolation threshold \u2014 the point where the model can exactly fit the training data.\n\nthe second descent is where modern deep learning lives. it challenges the classical bias-variance tradeoff and suggests that overparameterization acts as an implicit regularizer. among all the functions that perfectly fit the training data, gradient descent finds a particularly smooth one. the phenomenon has been observed in three settings:\n\n* model-wise double descent: varying the number of parameters.\n* epoch-wise double descent: varying training time.\n* sample-wise double descent: varying the number of training examples.\n\n[[simulation adl-double-descent]]\n\n## the neural tangent kernel (ntk)\n\nin the **infinite-width limit**, something remarkable happens: a neural network trained with gradient descent behaves like a simple, well-understood algorithm \u2014 kernel regression with a fixed kernel called the **neural tangent kernel**:\n\n$$\n\\theta(\\mathbf{x}, \\mathbf{x}') = \\left\\langle \\nabla_\\theta f(\\mathbf{x}; \\theta_0), \\, \\nabla_\\theta f(\\mathbf{x}'; \\theta_0) \\right\\rangle,\n$$\n\nwhere $\\theta_0$ are the initial parameters. in this infinite-width regime, the network becomes a lazy student who never changes its mind about the shape of the world. the ntk remains approximately constant during training (**lazy training**), meaning the network adjusts its weights linearly without fundamentally reorganizing its internal representations:\n\n* training dynamics become linear, and convergence to a global minimum is guaranteed.\n* the trained network is equivalent to kernel ridge regression with the ntk.\n\n**why this matters and why it is not the whole story**: the ntk theory is elegant and provides convergence guarantees, but it describes an idealization. finite-width networks exhibit **feature learning**, where the internal representations themselves evolve during training. this feature learning \u2014 the network discovering new ways to see the data \u2014 is believed to be essential for the practical success of deep learning. the lazy regime explains convergence; the rich regime explains performance.\n\n## loss landscapes\n\nthe loss function of a deep network defines a surface in a space with millions of dimensions. what does this surface look like? the answer turns out to be far more forgiving than you might expect.\n\n**in 10,000 dimensions, almost every critical point is a saddle point, not a local minimum.** a local minimum requires the hessian to have all positive eigenvalues \u2014 the loss must curve upward in *every* direction simultaneously. in high dimensions, this is astronomically unlikely. at a random critical point, almost all eigenvalues have a mix of signs, meaning at least one direction curves downward (a negative eigenvalue). this makes the point a saddle, not a minimum. this is why deep networks can be trained at all: gradient descent is almost never truly stuck, because there is almost always a direction that leads further downhill.\n\nkey properties of the loss landscape:\n* **local minima vs saddle points**: most critical points are saddles. the loss at local minima (when they exist) tends to be close to the global minimum.\n* **mode connectivity**: different solutions found by sgd are often connected by paths of nearly constant loss (**linear mode connectivity**). the valleys are not isolated; they form a connected web.\n* **sharpness and generalization**: flatter minima tend to generalize better than sharp minima. a sharp minimum means tiny perturbations to the parameters cause large changes in the loss \u2014 that fragility usually means the solution does not transfer well to new data. this motivates techniques like sharpness-aware"
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "ann",
      "lessonTitle": "Artificial Neural Networks",
      "x": 0.917339563369751,
      "y": 0.7791572213172913,
      "searchText": "artificial neural networks\n# artificial neural networks\n\n## the simplest possible learner\n\nbefore we build anything complicated, let us train the simplest thing that can learn. a single artificial neuron takes a handful of numbers as input, multiplies each one by a weight, adds them up, and then asks: \"is the total big enough?\" that is all a neuron does. it is a tiny voting machine: each input casts a weighted vote, and the neuron makes a decision based on the tally.\n\nmathematically, the neuron computes a weighted sum of inputs plus a bias, then applies a nonlinear **activation function**:\n\n$$\ny = \\sigma\\!\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = \\sigma(\\mathbf{w}^t \\mathbf{x} + b).\n$$\n\nthe weights $w_i$ determine how much each input contributes, the bias $b$ shifts the decision boundary, and $\\sigma$ introduces nonlinearity. this abstraction is loosely inspired by biological neurons: dendrites receive signals, the cell body integrates them, and the axon fires when a threshold is exceeded.\n\n## the perceptron\n\nthe simplest neural network is a single neuron called the **perceptron**. for binary classification, the perceptron computes:\n\n$$\n\\hat{y} = \\begin{cases} 1 & \\text{if } \\mathbf{w}^t \\mathbf{x} + b > 0, \\\\ 0 & \\text{otherwise}. \\end{cases}\n$$\n\nthis defines a linear decision boundary (a hyperplane) in the input space. when the perceptron gets a prediction wrong, it adjusts its weights in the direction of the mistake:\n\n$$\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\, (y - \\hat{y}) \\, \\mathbf{x},\n$$\n\nwhere $\\eta$ is the learning rate. the perceptron converges for linearly separable data but cannot solve nonlinear problems like xor. this limitation motivated the development of multilayer networks.\n\n## your first training loop\n\nbefore we go any further, let us actually train something. here is the simplest possible neural network on the mnist handwritten digit dataset. we load the data, build a model, and train it in under 20 lines. run this, watch the loss go down, and then we will explain every piece:\n\n```python\ntrain_data = datasets.mnist(root='data', train=true, download=true,\n                            transform=transforms.totensor())\ntrain_loader = torch.utils.data.dataloader(train_data, batch_size=100, shuffle=true)\n\nmodel = nn.sequential(nn.flatten(), nn.linear(784, 10))\ncriterion = nn.crossentropyloss()\noptimizer = torch.optim.adam(model.parameters(), lr=0.001)\n\nfor epoch in range(5):\n    for batch_x, batch_y in train_loader:\n        optimizer.zero_grad()\n        output = model(batch_x)\n        loss = criterion(output, batch_y)\n        loss.backward()\n        optimizer.step()\n    print(f\"epoch {epoch+1}, loss: {loss.item():.4f}\")\n```\n<!--code-toggle-->\n```pseudocode\ntrain_data = load_dataset(\"mnist\", split=\"train\")\napply_transform(train_data, to_tensor)\ntrain_loader = create_dataloader(train_data, batch_size=100, shuffle=true)\n\nmodel = sequential(flatten, linear(784, 10))\ncriterion = cross_entropy_loss()\noptimizer = adam(model.parameters(), lr=0.001)\n\nfor epoch = 1 to 5:\n    for each (batch_x, batch_y) in train_loader:\n        zero_gradients(optimizer)\n        output = model(batch_x)\n        loss = criterion(output, batch_y)\n        backward(loss)\n        update_parameters(optimizer)\n    print(\"epoch\", epoch, \"loss:\", loss)\n```\n\nthat single linear layer already gets about 92% accuracy on digit recognition. but it can only draw straight decision boundaries. to do better, we need nonlinearity and depth.\n\n## activation functions\n\nwhy do we need the activation function $\\sigma$ at all? without it, stacking linear layers would still produce a linear function \u2014 no matter how many layers you add, the composition of linear functions is still linear. the activation function is what gives the network the ability to bend and curve its decision boundaries.\n\ncommon choices include:\n\n* **relu**: $f(x) = \\max(0, x)$. dead simple: if the input is positive, pass it through; if negative, output zero. fast and effective, but neurons can \"die\" (output zero for all inputs) if the bias drifts too negative.\n* **sigmoid**: $f(x) = 1 / (1 + e^{-x})$. squashes everything into $(0, 1)$, useful for output layers in binary classification. but it has a fatal flaw for deep networks: the derivative $f'(x) = f(x)(1 - f(x))$ is at most $0.25$, so gradients shrink exponentially through many layers. this is the **vanishing gradient problem**.\n* **tanh**: $f(x) = \\tanh(x)$. outputs in $(-1, 1)$, zero-centered. better gradient flow than sigmoid but still saturates for large $|x|$.\n* **swish**: $f(x) = x \\cdot \\sigma(x)$. smooth and non-monotonic. used in efficientnet and modern architectures; often outperforms relu in deep networks.\n\nrelu and its variants (leaky relu, gelu) dominate modern practice because they maintain strong gradients even in deep networks.\n\n[[simulation adl-activation-functions]]\n\n## what if we didn't have activation functions?\n\nwithout nonlinear activations, a 100-layer network would compute exactly the same thing as a single-layer network. the entire stack of matrix multiplications would collapse into one giant matrix multiplication. you could never learn anything more complex than a straight line through the data. activations are the ingredient that turns a linear calculator into a universal function approximator.\n\n## multilayer perceptrons\n\na **multilayer perceptron** (mlp) stacks multiple layers of neurons. the **universal approximation theorem** says something remarkable: a feedforward network with a single hidden layer containing sufficiently many neurons can approximate any continuous function on a compact set to arbitrary accuracy. the network does not need to be deep \u2014 it just needs to be wide enough.\n\nso why bother with depth? in practice, **deeper networks** (more layers, fewer neurons per layer) are vastly more efficient:\n* depth enables hierarchical feature extraction: early layers detect simple patterns, later layers compose them into complex concepts.\n* certain functions require expo"
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "cnn",
      "lessonTitle": "Convolutional Neural Networks",
      "x": 0.9510725736618042,
      "y": 0.8307138085365295,
      "searchText": "convolutional neural networks\n# convolutional neural networks\n\n## seeing edges by hand\n\nbefore any formulas, let us do a convolution by hand. take a tiny 5x5 image \u2014 say, a white square on a black background. now take a small 3x3 grid of numbers called a **sobel filter** (it looks like $[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]$). place this little grid on the top-left corner of the image and multiply each overlapping pair of numbers, then add them all up. that gives you one number. slide the grid one pixel to the right and repeat. keep sliding until you have covered the whole image. what you get is a new image where bright spots mark vertical edges \u2014 places where the intensity changes sharply from left to right. that sliding-and-multiplying operation is a **convolution**, and the little grid is a **kernel**.\n\nthe key insight: a neural network can *learn* what kernels to use. instead of hand-designing a sobel filter, the network discovers \u2014 through backpropagation \u2014 exactly which patterns to look for.\n\n## why convolutions?\n\nfully connected networks treat each pixel independently, ignoring the spatial structure of images. **convolutional neural networks** (cnns) exploit three key properties:\n\n* **translation equivariance**: a pattern detected in one part of the image can be recognized elsewhere without learning separate weights for each position. an edge is an edge, whether it appears in the top-left corner or the bottom-right.\n* **parameter sharing**: the same kernel weights are applied at every spatial location, dramatically reducing the parameter count.\n* **locality**: each output depends only on a small region of the input, capturing local patterns before combining them into global features.\n\nthese inductive biases make cnns far more efficient than mlps for image tasks. a fully connected layer connecting two 28x28 feature maps would require $784^2 \\approx 600{,}000$ parameters; a 3x3 convolutional layer needs only 9.\n\n## the convolution operation\n\nimagine you are sliding a little magnifying glass over the picture and asking, at every spot, \"how much does this tiny 3x3 window look like the pattern i am searching for?\" that sliding multiplication is convolution. formally, a 2d convolution slides a kernel $k$ of size $k \\times k$ across an input feature map $i$, computing at each position:\n\n$$\n(i * k)[i, j] = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} i[i + m, j + n] \\cdot k[m, n].\n$$\n\nkey parameters that control the sliding:\n* **stride**: the step size when sliding the kernel. stride 2 halves the spatial dimensions.\n* **padding**: adding zeros around the border. \"same\" padding preserves spatial dimensions; \"valid\" padding shrinks them.\n* **dilation**: inserting gaps between kernel elements to increase the receptive field without adding parameters.\n\n[[simulation adl-convolution-demo]]\n\n## kernel types\n\ndifferent kernels detect different things:\n* **averaging**: a uniform kernel that blurs the image by averaging nearby pixels.\n* **gaussian blur**: a weighted average with gaussian falloff, producing smoother blurring.\n* **edge detection**: kernels like sobel or laplacian that highlight intensity changes \u2014 exactly what we did by hand above.\n* **dilated (atrous) convolution**: expands the receptive field without increasing parameters or pooling.\n\nin a cnn, the network learns its own kernels through training. early kernels tend to look like edge and texture detectors; deeper kernels respond to increasingly complex patterns.\n\n## feature hierarchies\n\nevolution spent millions of years wiring v1 to it in the ventral visual stream. a modern cnn rediscovers the same strategy in a few hours of training:\n\n* **early layers** learn edges and color gradients \u2014 much like v1.\n* **middle layers** combine these into textures, object parts, and shapes.\n* **deep layers** recognize whole objects and scenes \u2014 much like it cortex.\n\nthat is why transfer learning works: the lowest layers are basically universal visual primitives. swap out the final classification head and the same edge detectors, texture filters, and part detectors serve a completely different task.\n\n[[simulation adl-filter-evolution]]\n\n## what if we didn't have convolutions?\n\nwithout convolutions, you would need a fully connected layer for every pixel-to-pixel connection. a single layer processing a modest 224x224 rgb image would need $224 \\times 224 \\times 3 \\times 224 \\times 224 \\times 3 \\approx 23$ billion parameters \u2014 for *one* layer. and the network would have to learn separately that an edge in the top-left is the same concept as an edge in the bottom-right. convolutions give you parameter sharing and spatial awareness for free.\n\n## pooling layers\n\n**pooling** reduces spatial dimensions and provides a degree of translation invariance:\n\n* **max pooling**: takes the maximum value in each window. preserves the strongest activation \u2014 the most prominent feature in each region.\n* **average pooling**: takes the mean value. smoother but may lose sharp features.\n* **global average pooling** (gap): averages each entire feature map to a single number. replaces fully connected layers at the end of modern architectures, reducing parameters and overfitting.\n\na 2x2 max pool with stride 2 halves both spatial dimensions, reducing the feature map size by 4x while keeping the strongest signals.\n\n## cnn model\n\n```python\nclass convolutionalnetwork(nn.module):\n    def __init__(self, im_shape=(1, 28, 28), n_classes=10):\n        super().__init__()\n        self.conv1 = nn.conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.linear(in_features=8*7*7, out_features=32)\n        self.fc2 = nn.linear(in_features=32, out_features=10)\n\n    def forward(self, x):\n        x = f.relu(self.conv1(x))        # (1,28,28) -> (4,28,28)\n        x = f.max_pool2d(x, 2, 2)        # (4,28,28) -> (4,14,14)\n        x = f.relu(self.conv2(x))         # (4,14,14) -> ("
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "design-and-optimization",
      "lessonTitle": "Design and Optimization of Deep Learning",
      "x": 0.8221092820167542,
      "y": 0.8014453649520874,
      "searchText": "design and optimization of deep learning\n# design and optimization of deep learning\n\n## the optimization problem\n\ntraining a deep network means finding the parameter values that make the loss as small as possible. imagine standing on a vast mountain range in the dark, and all you have is a flashlight that shows you the slope under your feet. you take a step downhill, check the slope again, and repeat. that is gradient descent. the question is: how big should each step be, and should you remember which direction you have been going?\n\nformally, we minimize a loss function $\\mathcal{l}(\\theta)$ over parameters $\\theta$. the choice of optimizer profoundly affects both how fast you get to a good solution and how good that solution ultimately is.\n\n## stochastic gradient descent\n\n**stochastic gradient descent** (sgd) updates parameters using a mini-batch gradient estimate:\n\n$$\n\\theta_{t+1} = \\theta_t - \\eta \\, \\hat{g}_t, \\qquad \\hat{g}_t = \\frac{1}{|b|} \\sum_{i \\in b} \\nabla_\\theta \\mathcal{l}_i(\\theta_t).\n$$\n\nwhy use a mini-batch instead of the full dataset? two reasons: it is faster (you update after seeing 100 examples instead of 60,000), and the noise in the mini-batch gradient actually helps escape shallow local minima \u2014 like shaking a ball on a bumpy surface helps it roll into deeper valleys.\n\n**sgd with momentum** accumulates a velocity term that smooths oscillations:\n\n$$\nv_{t+1} = \\mu v_t + \\hat{g}_t, \\qquad \\theta_{t+1} = \\theta_t - \\eta v_{t+1}.\n$$\n\nmomentum helps the optimizer build speed in consistent gradient directions while dampening oscillations in noisy directions. think of a heavy ball rolling downhill: it picks up speed in the consistent downhill direction and resists being deflected by small bumps. a typical value is $\\mu = 0.9$.\n\n[[simulation adl-optimizer-trajectories]]\n\n## adaptive learning rate methods\n\nthe history of optimizers follows a clean arc: early methods like adagrad gave each parameter its own learning rate (parameters with large gradients take smaller steps, sparse parameters take larger ones), and rmsprop fixed adagrad's tendency to shrink learning rates to zero by using an exponential moving average. **adam** combined the best of both worlds \u2014 momentum for direction plus adaptive rates for scale \u2014 and became the default optimizer for most of deep learning.\n\nadam tracks a running mean $m_t$ and a running variance $v_t$ of the gradients:\n\n$$\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t, \\qquad v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2,\n$$\n\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\qquad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}, \\qquad \\theta_{t+1} = \\theta_t - \\frac{\\eta \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}.\n$$\n\ndefault hyperparameters: $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$. the bias correction terms ($1 - \\beta^t$) compensate for the zero-initialization of the moment estimates during early training.\n\nthere was one remaining flaw: in standard adam, $l_2$ regularization gets scaled by the adaptive learning rate, weakening weight decay for parameters with large gradients. **adamw** fixes this by decoupling weight decay from the gradient update, applying it directly to the parameters. this simple change measurably improves generalization and makes adamw the go-to optimizer in modern practice.\n\nother notable variants include lamb (layer-wise adaptive rates for large-batch training) and adafactor (memory-efficient factorized second moments).\n\n## which optimizer to choose\n\na practical decision framework:\n\n* **default choice**: adamw with weight decay 0.01-0.1. works well across most architectures and datasets.\n* **vision (cnns)**: sgd with momentum 0.9 + cosine schedule often matches or beats adam with proper tuning.\n* **transformers / nlp**: adamw is strongly preferred. sgd converges too slowly for attention-based models.\n* **large batch training**: lamb or lars for scaling to very large batch sizes (>8k).\n* **memory constrained**: adafactor reduces optimizer state memory by ~50%.\n\n```python\n# adamw with weight decay\noptimizer = torch.optim.adamw(model.parameters(), lr=3e-4, weight_decay=0.01)\n\n# sgd with momentum for vision\noptimizer = torch.optim.sgd(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n```\n<!--code-toggle-->\n```pseudocode\n// adamw with weight decay\noptimizer = adamw(model.parameters(), lr=3e-4, weight_decay=0.01)\n\n// sgd with momentum for vision\noptimizer = sgd(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n```\n\n## learning rate schedules\n\nthe learning rate is arguably the single most important hyperparameter. too high and training diverges; too low and it crawls. the best strategy is to change it during training:\n\n* **step decay**: reduce $\\eta$ by a factor at specified epochs. simple but requires manual tuning of milestones.\n* **cosine annealing**: $\\eta_t = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min})(1 + \\cos(\\pi t / t))$. smooth decay that naturally reaches a minimum at the end of training.\n* **warmup**: linearly increase $\\eta$ from zero over the first few thousand steps to stabilize early training, especially important for large batch sizes and transformers. without warmup, the initial random gradients can push parameters far from their initialization before the model has learned anything useful.\n* **one-cycle policy**: warmup then cosine decay; often yields faster convergence.\n\n[[simulation adl-lr-schedule-comparison]]\n\n```python\n# cosine annealing with warmup\nscheduler = torch.optim.lr_scheduler.cosineannealinglr(optimizer, t_max=100, eta_min=1e-6)\n\n# one-cycle policy\nscheduler = torch.optim.lr_scheduler.onecyclelr(optimizer, max_lr=0.01, total_steps=1000)\n```\n<!--code-toggle-->\n```pseudocode\n// cosine annealing\nscheduler = cosine_annealing(optimizer, t_max=100, eta_min=1e-6)\n\n// one-cycle policy\nscheduler = one_cycle(optimizer, max_lr=0.01, total_steps=1000)\n```\n\n## regularization techniques\n\ndeep networks have an enormous capacity to memorize training data. regularization techniques figh"
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "gan",
      "lessonTitle": "Generative Adversarial Networks",
      "x": 0.9867522716522217,
      "y": 0.6219885945320129,
      "searchText": "generative adversarial networks\n# generative adversarial networks\n\n## the adversarial framework\n\npicture a boxing ring with two fighters. in one corner stands the **forger** \u2014 a network that takes random noise and tries to paint a convincing fake image. in the other corner stands the **detective** \u2014 a network that looks at images and tries to figure out which ones are real and which are forgeries. they train simultaneously, each getting better in response to the other. the forger studies why the detective caught him and paints more convincingly next time. the detective studies the forger's latest tricks and sharpens her eye. round after round, the forgeries become more and more indistinguishable from reality.\n\nthat is a **generative adversarial network** (gan). the **generator** $g$ maps random noise $\\mathbf{z} \\sim p_z(\\mathbf{z})$ to synthetic data $g(\\mathbf{z})$, while the **discriminator** $d$ tries to distinguish real data from generated samples. training proceeds as a minimax game:\n\n$$\n\\min_g \\max_d \\; \\mathbb{e}_{\\mathbf{x} \\sim p_{\\text{data}}}[\\log d(\\mathbf{x})] + \\mathbb{e}_{\\mathbf{z} \\sim p_z}[\\log(1 - d(g(\\mathbf{z})))].\n$$\n\nat the nash equilibrium, the generator produces samples indistinguishable from real data, and the discriminator outputs $d(\\mathbf{x}) = 1/2$ everywhere \u2014 it genuinely cannot tell the difference. in practice, training alternates between updating $d$ (sharpen the detective) and updating $g$ (improve the forger).\n\n## architecture\n\na basic gan for image generation uses:\n\n* **generator**: takes a latent vector $\\mathbf{z} \\in \\mathbb{r}^{d}$ (typically $d = 100$) and maps it through fully connected or transposed convolutional layers to produce an image. batch normalization and relu activations are standard in intermediate layers, with a tanh output.\n\n* **discriminator**: takes an image and outputs a single scalar (real/fake probability) through convolutional layers with leakyrelu activations and a sigmoid output.\n\n**dcgan** (deep convolutional gan) established key architectural guidelines: replace pooling with strided convolutions, use batch normalization in both networks, remove fully connected hidden layers, and use relu in the generator but leakyrelu in the discriminator.\n\n```python\nclass generator(nn.module):\n    def __init__(self, latent_dim=100):\n        super().__init__()\n        self.net = nn.sequential(\n            nn.linear(latent_dim, 256),\n            nn.batchnorm1d(256), nn.relu(),\n            nn.linear(256, 512),\n            nn.batchnorm1d(512), nn.relu(),\n            nn.linear(512, 784), nn.tanh()\n        )\n    def forward(self, z):\n        return self.net(z).view(-1, 1, 28, 28)\n```\n<!--code-toggle-->\n```pseudocode\nclass generator:\n    init(latent_dim=100):\n        net = sequential(\n            linear(latent_dim, 256), batch_norm(256), relu,\n            linear(256, 512), batch_norm(512), relu,\n            linear(512, 784), tanh\n        )\n    forward(z):\n        return reshape(net(z), shape=(-1, 1, 28, 28))\n```\n\n## training challenges\n\ngan training is notoriously unstable. the two-player game introduces failure modes that single-loss training never encounters:\n\n**mode collapse** happens when the forger discovers that one particular type of forgery reliably fools the detective, and stops trying anything else. the generator produces only a few distinct outputs, ignoring the diversity of the training distribution. it is like a forger who can perfectly copy one painting but cannot paint anything else.\n\n**vanishing gradients** happen when the detective becomes too good too fast. if $d(g(\\mathbf{z})) \\approx 0$ for all generated samples, the gradient of $\\log(1 - d(g(\\mathbf{z})))$ vanishes, and the generator gets no useful learning signal. the forger has been so thoroughly defeated that she cannot even tell *how* to improve. a practical fix is to train $g$ to maximize $\\log d(g(\\mathbf{z}))$ instead.\n\n**training instability** means the two-player game may not converge at all. the losses oscillate rather than decrease, with the generator and discriminator taking turns dominating.\n\n[[simulation adl-gan-training-dynamics]]\n\n## improved loss functions\n\n**wasserstein gan** (wgan) addresses a limitation of the original saturating loss, which corresponds to minimizing the jensen-shannon divergence in the optimal-discriminator limit. (the non-saturating variant used in practice \u2014 training $g$ to maximize $\\log d(g(\\mathbf{z}))$ \u2014 has a different interpretation.) the js divergence saturates to a constant $\\log 2$ when the real and generated distributions have disjoint supports, which starves the generator of gradient signal. wgan replaces it with the earth mover (wasserstein-1) distance \u2014 a measure of how much \"work\" is needed to transform one distribution into another. the discriminator (called a \"critic\") outputs an unbounded score:\n\n$$\n\\min_g \\max_{d \\in \\mathcal{d}} \\; \\mathbb{e}_{\\mathbf{x}}[d(\\mathbf{x})] - \\mathbb{e}_{\\mathbf{z}}[d(g(\\mathbf{z}))],\n$$\n\nwhere $\\mathcal{d}$ is the set of 1-lipschitz functions. the lipschitz constraint is enforced via **gradient penalty** (wgan-gp):\n\n$$\n\\lambda \\, \\mathbb{e}_{\\hat{\\mathbf{x}}}[(\\|\\nabla_{\\hat{\\mathbf{x}}} d(\\hat{\\mathbf{x}})\\|_2 - 1)^2],\n$$\n\nwhere $\\hat{\\mathbf{x}}$ is interpolated between real and generated samples. wgan provides more meaningful loss curves (the loss actually correlates with sample quality) and significantly more stable training.\n\n## conditional gans\n\n**conditional gans** (cgan) give the generator and discriminator additional information $\\mathbf{y}$ (e.g., class labels):\n\n$$\n\\min_g \\max_d \\; \\mathbb{e}_{\\mathbf{x}}[\\log d(\\mathbf{x}, \\mathbf{y})] + \\mathbb{e}_{\\mathbf{z}}[\\log(1 - d(g(\\mathbf{z}, \\mathbf{y}), \\mathbf{y}))].\n$$\n\nthis enables controlled generation: producing images of a specific digit, translating between image domains (pix2pix), or generating data with desired physical properties.\n\n## vae vs gan: two philosophies of generation\n\nvaes and gans represent fundamentally d"
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "transformers",
      "lessonTitle": "Transformers and Attention Mechanisms",
      "x": 1.0,
      "y": 0.7061157822608948,
      "searchText": "transformers and attention mechanisms\n# transformers and attention mechanisms\n\n## why attention?\n\nimagine reading a 500-word paragraph and trying to connect word 1 to word 499. an rnn has to pass that detail through a chain of 498 intermediate steps \u2014 like the world's worst game of telephone. by the time the signal arrives it has been transformed, attenuated, and mixed with everything in between. and because each step depends on the previous one, you cannot even parallelize the computation.\n\nattention tears up the telephone game: every word gets a direct hotline to every other word and simply asks, \"how relevant are you to me right now?\" that single idea is why transformers replaced everything else.\n\n## the attention mechanism\n\nhere is the core intuition. you are at a loud party trying to hear your friend across the room. your brain does not process every sound equally \u2014 it automatically turns up the volume on your friend's voice and turns down the background chatter. that is attention: a learned mechanism for focusing on the relevant parts of the input while ignoring the rest.\n\nin a neural network, attention works through three roles: **queries**, **keys**, and **values**. think of it as a database lookup. the **query** is \"what am i looking for?\" the **key** is \"what do i have to offer?\" the **value** is \"here is the actual information.\" attention computes a soft match between each query and all keys, then returns a weighted combination of the corresponding values.\n\nformally, given queries $q$, keys $k$, and values $v$, **scaled dot-product attention** computes:\n\n$$\n\\text{attention}(q, k, v) = \\text{softmax}\\!\\left(\\frac{qk^t}{\\sqrt{d_k}}\\right) v,\n$$\n\nwhere $d_k$ is the dimension of the keys. the scaling factor $\\sqrt{d_k}$ prevents the dot products from growing too large, which would push the softmax into saturated regions with tiny gradients.\n\nthe attention weights $\\text{softmax}(qk^t / \\sqrt{d_k})$ form a matrix where each row sums to one. you can picture it as every word in a sentence voting on which other words matter most to it. the word \"it\" might vote heavily for \"the cat\" when trying to determine what \"it\" refers to.\n\n```python\ndef scaled_dot_product_attention(q, k, v, mask=none):\n    d_k = q.size(-1)\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not none:\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n    weights = f.softmax(scores, dim=-1)\n    return torch.matmul(weights, v)\n```\n<!--code-toggle-->\n```pseudocode\nfunction scaled_dot_product_attention(q, k, v, mask=none):\n    d_k = dimension(q, -1)\n    scores = matmul(q, transpose(k)) / sqrt(d_k)\n    if mask is not none:\n        scores = fill_where(scores, mask == 0, -inf)\n    weights = softmax(scores, dim=-1)\n    return matmul(weights, v)\n```\n\n## multi-head attention\n\na single attention function can only capture one type of relationship at a time. but language is rich: words relate to each other syntactically (subject-verb), semantically (synonyms), positionally (nearby words), and logically (cause-effect). **multi-head attention** runs several attention functions in parallel, each looking for a different type of pattern:\n\n$$\n\\text{multihead}(q, k, v) = \\text{concat}(\\text{head}_1, \\ldots, \\text{head}_h) w^o,\n$$\n\nwhere each head computes attention on a different linear projection:\n\n$$\n\\text{head}_i = \\text{attention}(q w_i^q, k w_i^k, v w_i^v).\n$$\n\none head might learn to attend to adjacent words. another might focus on long-range syntactic dependencies. another might track coreference. the network learns which types of relationships are useful.\n\n## the transformer architecture\n\nthe **transformer** replaces recurrence entirely with attention. it is the architecture that changed everything \u2014 enabling modern language models, vision transformers, and protein structure prediction.\n\n**encoder block** (repeated $n$ times):\n1. multi-head self-attention.\n2. add & layer normalization.\n3. position-wise feedforward network.\n4. add & layer normalization.\n\n**decoder block** (repeated $n$ times):\n1. masked multi-head self-attention (causal mask prevents attending to future tokens \u2014 you cannot cheat by looking at the answer).\n2. multi-head cross-attention (attends to encoder output).\n3. position-wise feedforward network.\n4. add & layer normalization at each step.\n\n**positional encoding** injects sequence order information since attention is permutation-invariant. without it, the transformer would treat \"dog bites man\" and \"man bites dog\" identically. the standard approach uses sine and cosine functions of different frequencies:\n\n$$\n\\text{pe}_{(pos, 2i)} = \\sin(pos / 10000^{2i/d}), \\qquad \\text{pe}_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d}).\n$$\n\n[[simulation adl-attention-heatmap]]\n\n## what if we didn't have attention?\n\nwithout attention, a recurrent network must pass information from word 1 to word 100 through 99 sequential steps. by the time the signal arrives, it has been transformed, attenuated, and mixed with everything in between. long-range dependencies become almost impossible to learn. attention gives every word a direct phone line to every other word \u2014 no intermediaries, no signal loss.\n\n## bert: bidirectional encoder representations\n\n**bert** uses only the encoder stack with two pre-training objectives:\n\n* **masked language modeling** (mlm): randomly mask 15% of input tokens and predict them from context. the network must understand language deeply enough to fill in the blanks.\n* **next sentence prediction** (nsp): predict whether two sentences are consecutive.\n\nbert's bidirectional attention allows each token to attend to both left and right context, unlike autoregressive models that can only look backward. fine-tuning bert on downstream tasks (classification, ner, qa) achieved state-of-the-art results across nlp benchmarks.\n\n## gpt: generative pre-trained transformer\n\n**gpt** uses only the decoder stack with causal (left-to-right) attention. pre-training uses autoregressive langu"
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "unet",
      "lessonTitle": "U-Net Architecture",
      "x": 0.988591194152832,
      "y": 0.8323686122894287,
      "searchText": "u-net architecture\n# u-net architecture\n\n## the pixel-level challenge\n\nsuppose you are a doctor looking at a brain scan, and you need to outline exactly where a tumor is \u2014 not just \"there is a tumor in this image,\" but the precise boundary of every single pixel that belongs to the tumor. this is **image segmentation**: assigning a class label to every pixel in an image, producing a segmentation mask with the same spatial dimensions as the input.\n\nthe challenge is that you need two things simultaneously. you need the **big picture** \u2014 enough context to know that this blob is a tumor and not normal tissue. and you need **fine detail** \u2014 the exact pixel boundaries where the tumor ends and healthy tissue begins. how do you build a network that sees both the forest and the trees at the same time? you build a u-shaped pipe.\n\napplications span many domains:\n* **medical imaging**: organ and tumor segmentation in ct/mri scans.\n* **autonomous driving**: identifying roads, pedestrians, vehicles, and obstacles.\n* **satellite imagery**: land use classification, building detection, deforestation monitoring.\n* **microscopy**: cell counting and boundary detection in biological research.\n\n## encoder-decoder architecture\n\nsegmentation requires both **global context** (what object is this?) and **fine spatial detail** (where exactly are its boundaries?). this creates a fundamental tension:\n\n* **downsampling** (pooling, strided convolutions) builds a large receptive field and captures high-level semantics, but it loses spatial resolution. each pooling layer throws away positional information.\n* **upsampling** (transposed convolutions, bilinear interpolation) recovers spatial resolution, but the fine details that were lost during downsampling are gone.\n\nan **encoder-decoder** architecture addresses this by first compressing the input through an encoder (like a classification cnn) and then expanding it through a decoder that progressively restores spatial dimensions. but there is a problem: by the time the information reaches the bottleneck, the fine details have been squeezed out.\n\n## skip connections: the key insight\n\nthe innovation of **u-net** is **skip connections** that directly connect encoder layers to their corresponding decoder layers at each resolution level. instead of forcing all information through the narrow bottleneck, skip connections give the decoder a shortcut to the fine-grained details from the encoder.\n\nat each level, the encoder feature maps are concatenated with the decoder feature maps. the decoder gets both the high-level \"what\" from the bottleneck and the low-level \"where\" from the encoder \u2014 and it learns to combine them.\n\n## what if we didn't have skip connections?\n\nwithout skip connections, the decoder would have to reconstruct fine spatial details from only the compressed bottleneck representation. it is like trying to describe the exact shape of a cloud to a friend over the phone, and then asking them to draw it. by the time the description passes through the bottleneck of language, the fine details are lost. skip connections are like also sending your friend a photograph \u2014 they get both your verbal description (global context) and the actual visual details.\n\n## the u-shape\n\nthe u-net gets its name from its symmetric u-shaped architecture. a typical configuration for 128x128 input:\n\n**encoder (contracting path)**:\n1. 128x128 x 1 channel $\\to$ 128x128 x 64 (two 3x3 convolutions)\n2. pool $\\to$ 64x64 x 64 $\\to$ 64x64 x 128 (two 3x3 convolutions)\n3. pool $\\to$ 32x32 x 128 $\\to$ 32x32 x 256 (two 3x3 convolutions)\n4. pool $\\to$ 16x16 x 256 $\\to$ 16x16 x 512 (bottleneck)\n\n**decoder (expanding path)**:\n5. upsample $\\to$ 32x32 x 512 $\\to$ concat with encoder level 3 $\\to$ 32x32 x 256\n6. upsample $\\to$ 64x64 x 256 $\\to$ concat with encoder level 2 $\\to$ 64x64 x 128\n7. upsample $\\to$ 128x128 x 128 $\\to$ concat with encoder level 1 $\\to$ 128x128 x 64\n8. final 1x1 convolution $\\to$ 128x128 x $c$ (number of classes)\n\nthe left side of the u compresses, the bottom is the bottleneck, and the right side expands back to full resolution. the horizontal connections across the u are the skip connections.\n\n[[simulation adl-unet-architecture]]\n\n## u-net model\n\n```python\nclass unet(nn.module):\n    def __init__(self, in_ch=1, out_ch=10):\n        super().__init__()\n        # encoder\n        self.enc1 = self._block(in_ch, 16)\n        self.enc2 = self._block(16, 32)\n        self.enc3 = self._block(32, 64)\n        self.bottleneck = self._block(64, 128)\n        self.pool = nn.maxpool2d(2, 2)\n        # decoder\n        self.up3 = nn.convtranspose2d(128, 64, 2, stride=2)\n        self.dec3 = self._block(128, 64)   # 64+64 from skip\n        self.up2 = nn.convtranspose2d(64, 32, 2, stride=2)\n        self.dec2 = self._block(64, 32)    # 32+32 from skip\n        self.up1 = nn.convtranspose2d(32, 16, 2, stride=2)\n        self.dec1 = self._block(32, 16)    # 16+16 from skip\n        self.out_conv = nn.conv2d(16, out_ch, 1)\n\n    def _block(self, in_c, out_c):\n        return nn.sequential(\n            nn.conv2d(in_c, out_c, 3, padding=1), nn.relu(),\n            nn.conv2d(out_c, out_c, 3, padding=1), nn.relu()\n        )\n\n    def forward(self, x):\n        # encoder\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool(e1))\n        e3 = self.enc3(self.pool(e2))\n        b = self.bottleneck(self.pool(e3))\n        # decoder with skip connections\n        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n        return self.out_conv(d1)\n```\n<!--code-toggle-->\n```pseudocode\nclass unet:\n    init(in_ch=1, out_ch=10):\n        // encoder blocks: each has two 3x3 convolutions + relu\n        enc1 = conv_block(in_ch, 16)\n        enc2 = conv_block(16, 32)\n        enc3 = conv_block(32, 64)\n        bottleneck = conv_block(64, 128)\n        pool = max_pool_2d(size=2)\n        // decoder: upsample + conv blocks\n       "
    },
    {
      "topicId": "advanced-deep-learning",
      "topicTitle": "Advanced Deep Learning",
      "routeSlug": "advanced-deep-learning",
      "lessonSlug": "vae",
      "lessonTitle": "Variational Autoencoders",
      "x": 0.7328320145606995,
      "y": 0.7037922739982605,
      "searchText": "variational autoencoders\n# variational autoencoders\n\n## standard autoencoders\n\nan **autoencoder** compresses input $\\mathbf{x}$ to a low-dimensional latent representation $\\mathbf{z}$ through an encoder $f_\\phi$, then reconstructs the input through a decoder $g_\\theta$:\n\n$$\n\\mathbf{z} = f_\\phi(\\mathbf{x}), \\qquad \\hat{\\mathbf{x}} = g_\\theta(\\mathbf{z}).\n$$\n\ntraining minimizes reconstruction loss: $\\mathcal{l} = \\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|^2$.\n\nthe problem: the latent space of a standard autoencoder is a mess. it is **discontinuous and unstructured** \u2014 nearby points in latent space may decode to wildly different outputs, and large regions of the latent space correspond to nothing meaningful at all. if you pick a random point in latent space and try to decode it, you will likely get garbage. the autoencoder learned to compress and decompress, but it did not learn a *map* of all possible outputs.\n\n## the vae idea\n\na **variational autoencoder** (vae) fixes this by changing one thing: instead of mapping each input to a single point in latent space, the encoder outputs the parameters of a probability distribution \u2014 a little cloud of uncertainty around where the input *should* be in latent space:\n\n$$\nq_\\phi(\\mathbf{z} | \\mathbf{x}) = \\mathcal{n}(\\mathbf{z}; \\boldsymbol{\\mu}_\\phi(\\mathbf{x}), \\text{diag}(\\boldsymbol{\\sigma}^2_\\phi(\\mathbf{x}))).\n$$\n\nthe decoder then reconstructs from a sample $\\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})$. by forcing these little clouds to stay close to a standard normal prior $p(\\mathbf{z}) = \\mathcal{n}(\\mathbf{0}, \\mathbf{i})$, the vae ensures that the latent space is smooth and well-organized. every region of latent space decodes to something meaningful, and walking smoothly through latent space produces smooth transformations in the output.\n\n[[simulation adl-pca-demo]]\n\n## elbo derivation\n\nwhy does the vae loss function look the way it does? we want to maximize the marginal log-likelihood $\\log p_\\theta(\\mathbf{x})$ \u2014 the probability that our model assigns to the data we actually observe. but computing this directly requires integrating over all possible latent codes $\\mathbf{z}$, which is intractable.\n\nthe trick is to derive a tractable lower bound. starting from the log-marginal likelihood and introducing an approximate posterior $q_\\phi(\\mathbf{z}|\\mathbf{x})$:\n\n$$\n\\log p_\\theta(\\mathbf{x}) = \\log \\int p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z}) \\, d\\mathbf{z}.\n$$\n\napplying jensen's inequality (or equivalently, decomposing the kl divergence):\n\n$$\n\\log p_\\theta(\\mathbf{x}) = \\underbrace{\\mathbb{e}_{q_\\phi}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - d_{\\text{kl}}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}))}_{\\text{elbo}} + d_{\\text{kl}}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p_\\theta(\\mathbf{z}|\\mathbf{x})).\n$$\n\nsince the last kl term is non-negative, the **evidence lower bound** (elbo) is a lower bound on the log-likelihood. maximizing the elbo simultaneously:\n1. maximizes the expected reconstruction quality (first term).\n2. minimizes the gap between the approximate and true posterior (last kl term vanishes when $q_\\phi = p_\\theta(\\mathbf{z}|\\mathbf{x})$).\n\nthe loss function is therefore:\n\n$$\n\\mathcal{l}_{\\text{vae}} = -\\mathbb{e}_{q_\\phi}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] + d_{\\text{kl}}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z})).\n$$\n\nthe first term says \"reconstruct well.\" the second term says \"do not stray too far from the prior.\" together they produce a model that generates well and has a smooth latent space.\n\n## the reparameterization trick\n\nthe reparameterization trick is the cleverest accounting hack in deep learning. here is the problem: the encoder outputs a distribution, and the decoder needs a sample from that distribution. but backprop can flow through addition and multiplication \u2014 not through \"sample from $\\mathcal{n}(\\mu, \\sigma)$.\" the gradient of \"pick a random number\" with respect to the distribution parameters is undefined.\n\nthe trick: we move the randomness **outside** the computation graph that depends on the network parameters. instead of sampling directly from the learned distribution, we sample a \"frozen\" piece of noise $\\boldsymbol{\\epsilon}$ from a fixed standard normal, and then *deterministically* transform it using the learned parameters:\n\n$$\n\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon}, \\qquad \\boldsymbol{\\epsilon} \\sim \\mathcal{n}(\\mathbf{0}, \\mathbf{i}).\n$$\n\nthe sampler is now a fixed random-number generator, not a stochastic node that blocks gradients. all the randomness lives in $\\boldsymbol{\\epsilon}$, which does not depend on any learnable parameters. the gradient flows cleanly through $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$, and we can train the whole system end-to-end with standard backpropagation. we rewrote the sampling step so backprop never sees the random node.\n\n## kl divergence term\n\nfor gaussian encoder and standard normal prior, the kl divergence has a closed-form solution:\n\n$$\nd_{\\text{kl}}(q_\\phi \\| p) = -\\frac{1}{2} \\sum_{j=1}^{d} \\left(1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right).\n$$\n\nwhat is this term actually doing? it is the universe telling the network \"do not be too sure of yourself.\" without the kl term, the encoder would collapse each input to a single point (zero variance) at some arbitrary location \u2014 perfectly memorizing each training example but creating a latent space full of gaps. the kl penalty forces each encoding to be a spread-out cloud that overlaps with the prior, filling the latent space with meaning.\n\nthink of the kl term as a parking-lot attendant who forces every car (encoding) to leave a little space around it. without the attendant, every car parks exactly on top of a training example and the lot becomes useless for new arrivals.\n\n## what if we didn't have the kl term?\n\nwithout the kl regularizer, the vae degenerates into a regular autoencoder. the encoder would learn to place each training example at a unique, precise point i"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "dimensionality-reduction",
      "lessonTitle": "Dimensionality Reduction",
      "x": 0.8012635707855225,
      "y": 0.6693682074546814,
      "searchText": "dimensionality reduction\n# dimensionality reduction\n\nhere is a problem that comes up everywhere in science: you have data with hundreds or thousands of features, but the interesting structure lives on a much lower-dimensional surface. dimensionality reduction finds that surface. it lets you compress data without losing the signal, strip away noise, and \u2014 perhaps most importantly \u2014 see patterns that are invisible in the raw high-dimensional space.\n\n## pca: finding the directions that matter\n\nsuppose you photograph a 3d object from many angles. each photo has millions of pixels, but the photos are not independent \u2014 they all depict the same object, just rotated. the \"real\" information is only three-dimensional (the rotation angles). principal component analysis does exactly this kind of detective work: it looks at your data and asks, \"which directions carry the most variation, and which directions are mostly noise?\"\n\ntechnically, pca computes the covariance matrix of your centered data and finds its eigenvectors. the eigenvector with the largest eigenvalue points in the direction of greatest variance; the second eigenvector points in the direction of greatest remaining variance, perpendicular to the first; and so on.\n\nfor centered data matrix $x$:\n$$\nc=\\frac{1}{n-1}x^\\top x,\\quad z=x v_k\n$$\n\nhere $v_k$ contains the top $k$ eigenvectors of the covariance matrix $c$, and $z$ is your data projected into $k$ dimensions. each column of $v_k$ is a principal component \u2014 a direction in feature space ranked by how much variance it explains.\n\nuse pca when you need a fast, interpretable, linear embedding. it is the right first step for preprocessing before downstream models, for explained-variance diagnostics (\"how many dimensions do we actually need?\"), and as a sanity check before trying fancier methods.\n\n## t-sne: seeing your clusters\n\nt-sne is a nonlinear method designed for one thing: producing beautiful 2d visualizations where similar points cluster together. think of it as placing your data points on a rubber sheet, then stretching and compressing the sheet so that points that were close together in high-dimensional space stay close, while points that were far apart get pushed further away.\n\nthe result is often stunning \u2014 you can *see* clusters that were hidden in 100 dimensions. but there is a loud warning you must internalize: **the distances on a t-sne plot are not meaningful**. two clusters that appear far apart might actually be close in the original space, and the size of clusters can be misleading. use t-sne for visual exploration, never for quantitative distance comparisons.\n\n## umap: t-sne's faster cousin\n\numap achieves similar goals to t-sne \u2014 preserving local neighborhoods to reveal cluster structure \u2014 but it scales better to large datasets and often preserves more of the global organization. if t-sne gives you beautiful clusters but you cannot tell how the clusters relate to each other, umap may give you a more faithful big-picture layout. that said, the same warnings about interpreting distances apply.\n\n## autoencoders: learned compression\n\nimagine a kid who has to describe a painting to a friend over the phone using only ten words. the friend then tries to redraw the painting from those ten words alone. if the redrawing is good, those ten words captured the essence of the painting. the ten-word description is the **latent representation**, and the kid-and-friend pair is an **autoencoder**.\n\nan autoencoder is a neural network trained to reconstruct its input through a narrow bottleneck. the encoder compresses the input to a low-dimensional code; the decoder tries to reconstruct the original from that code.\n\n$$\n\\mathbf{z}=f_\\text{enc}(\\mathbf{x}),\\quad \\hat{\\mathbf{x}}=f_\\text{dec}(\\mathbf{z}),\\quad \\mathcal{l}=\\|\\mathbf{x}-\\hat{\\mathbf{x}}\\|^2\n$$\n\nin words: we encode input $\\mathbf{x}$ to a compressed code $\\mathbf{z}$, decode it back to $\\hat{\\mathbf{x}}$, and minimize the reconstruction error. if the bottleneck has dimension $d$ and reconstruction quality is high, the data's intrinsic dimensionality is at most $d$.\n\nunlike pca, autoencoders can capture curved, nonlinear manifolds. architecture choices (depth, width, activation functions) control the kind of embedding you get. and because they are neural networks, they can be trained end-to-end with a downstream task. for the generative extension (variational autoencoders), see [advanced deep learning \u2014 vaes](/topics/advanced-deep-learning/vae).\n\n## when to use which?\n\nhere is a practical decision guide:\n\n* **you need speed and linear interpretability** \u2014 use pca. it is deterministic, fast, and gives you explained-variance scores to decide how many dimensions to keep.\n* **you need detailed 2d cluster visualizations** \u2014 use t-sne. set perplexity carefully and run it multiple times with different seeds to check that the clusters are stable.\n* **you need a fast nonlinear embedding that scales** \u2014 use umap. it handles large datasets well and often gives clearer global structure than t-sne.\n* **pca underperforms and you need a learnable, nonlinear embedding** \u2014 use autoencoders. especially valuable when you want to train the embedding end-to-end with a downstream model.\n\n[[figure aml-dimred-comparison]]\n\n## interactive simulations\n\n[[simulation aml-pca-correlated-data]]\n\n[[simulation aml-explained-variance]]\n\n[[simulation aml-tsne-umap-comparison]]\n\n## practical warnings\n\n* standardize features before pca \u2014 otherwise the component with the largest scale dominates.\n* t-sne and umap output can vary with random seed and hyperparameters. always run multiple times.\n* do not over-interpret apparent distances between far-apart clusters in t-sne.\n* autoencoder embeddings depend on architecture and training. validate that the bottleneck dimension captures meaningful structure before trusting the embedding.\n\n## big ideas\n\n* high-dimensional data is almost always much lower-dimensional in practice \u2014 the interesting structure lives on a curved sur"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "generative-adversarial-networks",
      "lessonTitle": "Generative Adversarial Networks (Extension)",
      "x": 0.9814938902854919,
      "y": 0.6283043026924133,
      "searchText": "generative adversarial networks (extension)\n# generative adversarial networks (extension)\n\nimagine two kids locked in an arms race. one is a forger who paints fake banknotes. the other is a detective who inspects them. every time the detective catches a fake, the forger studies what gave it away and makes a better fake next time. every time a fake slips past, the detective sharpens their eye. over time, both get astonishingly good \u2014 and the fakes become nearly indistinguishable from the real thing. that is the core idea behind generative adversarial networks.\n\n## the adversarial game\n\na gan trains two neural networks simultaneously:\n\n* a **generator** $g$ takes random noise $z$ and produces a synthetic sample $g(z)$. its goal is to fool the discriminator.\n* a **discriminator** $d$ receives both real samples from the training data and fakes from the generator. its goal is to tell them apart.\n\nthe two play a min-max game:\n\n$$\n\\min_g \\max_d \\; \\mathbb{e}_{x\\sim p_{data}}[\\log d(x)] + \\mathbb{e}_{z\\sim p_z}[\\log(1-d(g(z)))]\n$$\n\nthink of this as a zero-sum tennis match. the discriminator tries to maximize its score (correctly classifying real vs. fake), while the generator tries to minimize it (making fakes that the discriminator calls real). at equilibrium \u2014 if you ever reach it \u2014 the generator produces samples so realistic that the discriminator can do no better than random guessing.\n\nin plain english: the discriminator is rewarded twice \u2014 once for correctly saying \"real\" to real images, and once for correctly saying \"fake\" to generated images. the generator is only rewarded when the discriminator says \"real\" to a fake. its entire job is to make the discriminator's second reward as small as possible \u2014 by becoming indistinguishable from reality.\n\n## mode collapse: when the forger gets lazy\n\nhere is the most infamous failure mode. the forger discovers that painting only happy faces always fools the detective \u2014 so they never learn to paint landscapes, animals, or buildings. the generator has found a narrow pocket of \"safe\" outputs and stubbornly refuses to explore the full diversity of the data distribution.\n\nthis is **mode collapse**: the generator produces high-quality but low-diversity samples, capturing only a few modes of the real distribution. you asked for a model that can generate any face, and you got one that generates the same three faces over and over.\n\n> mode collapse is the gan version of \"the model found one answer that works and stopped exploring.\"\n\n## stabilizing training\n\ngan training is notoriously finicky. the generator and discriminator are locked in a delicate dance, and small imbalances can cascade.\n\n**wasserstein objective** replaces the log-probability game with a smoother distance metric (the earth-mover distance), giving the generator more useful gradients even when the discriminator is very confident.\n\n**gradient penalty** adds a regularization term that prevents the discriminator from becoming too sharp, keeping the gradients informative for the generator.\n\n**spectral normalization** constrains the discriminator's weights to control its lipschitz constant, stabilizing the adversarial dynamics.\n\nthese are engineering solutions to a fundamental tension: the two networks must improve at roughly the same rate, or one overpowers the other and learning stalls.\n\n## where this goes deeper\n\nthis page gives you the intuition for adversarial training. gans are a bridge from classical applied ml to the wild world of deep generative modeling. for architecture details (dcgan, stylegan, conditional gans), training recipes, and the relationship to other generative approaches (vaes, diffusion models), see [advanced deep learning \u2014 generative models](/topics/advanced-deep-learning/gan).\n\n[[figure aml-gan-progression]]\n\n[[simulation aml-gan-forger-arena]]\n\n## big ideas\n\n* a gan is a machine for turning a simple, known distribution (random noise) into a complex, unknown one (real data), with no explicit density to fit \u2014 only a critic that says \"real\" or \"fake.\"\n* mode collapse reveals the core tension in adversarial training: the generator is rewarded for fooling the discriminator, not for being diverse. a clever forger who specializes in one convincing fake will beat an honest forger who tries to cover everything.\n* the wasserstein distance is not a technical detail \u2014 it is a qualitatively different training signal. log-probability games can saturate and give zero gradient; earth-mover distance gives the generator useful information even when it is far from the real distribution.\n* gans are historically important not just for what they produce, but for the idea they introduced: you can define a loss function by training a neural network to evaluate it. that idea reappears in reinforcement learning from human feedback, learned metrics, and many other places.\n\n## what comes next\n\ngans mark the boundary of this module and the entrance to deep generative modeling. the natural next step is the broader landscape of generative approaches: variational autoencoders impose a structured latent space and allow explicit density estimation; diffusion models reverse a noise process and have largely displaced gans for high-quality image synthesis; and normalizing flows learn exact invertible transformations between simple and complex distributions. each of these is a different answer to the same question gans asked: how do you define \"looks like the data\" without writing down the density?\n\nthe adversarial principle itself travels further than image generation. adversarial training for robustness, adversarial examples as a security concern, and discriminator-based reward shaping in reinforcement learning all trace back to the same two-player game you just learned. once you have internalized the forger-and-detective story, you will recognize it operating quietly in places that do not call themselves gans at all.\n\n## check your understanding\n\n* can you explain the gan training loop to a friend using th"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "graph-neural-networks",
      "lessonTitle": "Graph Neural Networks",
      "x": 0.9716835021972656,
      "y": 0.7664070725440979,
      "searchText": "graph neural networks\n# graph neural networks\n\nwhat if your data has friendships?\n\nnot all data fits neatly into a table or a sequence. molecules are atoms connected by bonds. social networks are people connected by relationships. protein structures are amino acids connected in 3d space. in all these cases, the connections carry as much information as the entities themselves. graph neural networks learn on this structure directly, without forcing relational data into a flat grid.\n\n## why graphs\n\nmany systems we care about are fundamentally relational:\n\n* **molecules**: atoms are nodes, bonds are edges. predicting molecular properties requires understanding the connectivity, not just a list of atom types.\n* **social and communication networks**: who talks to whom shapes information flow, influence, and community structure.\n* **recommendation systems**: users and items form a bipartite graph, and similar users connect to similar items.\n* **physical simulations**: particles interact through local forces, forming interaction graphs.\n\na graph $g=(v,e)$ has nodes $v$ (entities) and edges $e$ (relationships). each node can carry a feature vector, and edges can have their own features (bond type, interaction strength, distance).\n\n## message passing: the gossip network\n\nthe central idea in gnns is beautifully simple. every node is gossiping with its neighbors, then updating its own diary based on what it heard.\n\nmore precisely, a single gnn layer does two things:\n\n1. **aggregate**: each node collects messages from its neighbors.\n2. **update**: each node updates its own representation using the collected messages and its current state.\n\n$$\nm_v^{(l+1)}=\\bigoplus_{u\\in\\mathcal{n}(v)}\\phi^{(l)}(h_v^{(l)},h_u^{(l)},e_{uv})\n$$\n$$\nh_v^{(l+1)}=\\psi^{(l)}(h_v^{(l)},m_v^{(l+1)})\n$$\n\nin words: node $v$ gathers messages from all its neighbors $u \\in \\mathcal{n}(v)$ using a message function $\\phi$. the aggregation $\\bigoplus$ (sum, mean, or max) must be permutation-invariant \u2014 it should not matter what order you read the messages. then node $v$ updates its hidden state $h_v$ using an update function $\\psi$ that combines the old state with the aggregated messages.\n\nafter $l$ layers of message passing, each node's representation encodes information from its $l$-hop neighborhood. one layer captures immediate neighbors; two layers capture neighbors-of-neighbors; and so on.\n\n## gcn normalization\n\nthe graph convolutional network (gcn) is one of the simplest and most widely used gnn variants. for adjacency matrix $a$:\n\n$$\n\\tilde{a}=a+i,\\quad\nh^{(l+1)}=\\sigma\\left(\\tilde{d}^{-1/2}\\tilde{a}\\tilde{d}^{-1/2}h^{(l)}w^{(l)}\\right)\n$$\n\nthe key idea: we add self-loops ($a+i$, so each node also receives its own message), then normalize by the degree matrix $\\tilde{d}$ so that high-degree nodes do not dominate simply because they have more neighbors. the result is a weighted average of neighbor features, followed by a linear transformation and nonlinearity \u2014 essentially a localized convolution on the graph.\n\n## a tiny example you can compute by hand\n\nconsider four nodes in a triangle-plus-tail graph: nodes a, b, c form a triangle, and node d connects only to c.\n\ninitial features: $h_a=[1,0]$, $h_b=[0,1]$, $h_c=[1,1]$, $h_d=[0,0]$.\n\nwith a simple mean-aggregation gnn layer (no learned weights, just averaging):\n\n* node a's neighbors are b, c. message: mean of $[0,1]$ and $[1,1]$ = $[0.5, 1.0]$.\n* node b's neighbors are a, c. message: mean of $[1,0]$ and $[1,1]$ = $[1.0, 0.5]$.\n* node c's neighbors are a, b, d. message: mean of $[1,0]$, $[0,1]$, $[0,0]$ = $[0.33, 0.33]$.\n* node d's neighbor is only c. message: $[1,1]$.\n\nafter one layer, each node knows about its immediate neighbors. after two layers, node d would know about a and b through c \u2014 information has propagated across the graph.\n\n## oversmoothing: when everyone sounds the same\n\nthere is a catch with stacking many gnn layers. after enough layers it is like a party where everyone has talked to everyone else ten times. all opinions have blended into one polite average. every node sounds exactly the same \u2014 useful information has been washed away. this is **oversmoothing**: node representations converge to indistinguishable vectors as you add layers, because information has diffused uniformly across the graph. after enough layers it is like every person in a group chat has forwarded every message to everyone else ten times \u2014 eventually everyone is saying exactly the same thing.\n\nin practice, most gnns work best with 2\u20134 layers. beyond that, you need architectural tricks (skip connections, normalization, or attention mechanisms) to preserve node identity.\n\n[[figure aml-gnn-message-steps]]\n\n## interactive simulations\n\n[[simulation aml-graph-message-passing]]\n\n## practical notes\n\n* the aggregation function must be permutation-invariant \u2014 sum, mean, and max are the standard choices.\n* task formulation matters: node-level tasks (classify each node), edge-level tasks (predict whether an edge exists), and graph-level tasks (predict a property of the whole graph) require different readout strategies.\n* for graph-level tasks, you need a pooling step that aggregates all node representations into a single graph vector.\n\n## big ideas\n\n* the central insight of gnns is that structure is information. two molecules with the same atoms but different connectivity are different molecules \u2014 and any model that ignores the connectivity is throwing away the most important part of the data.\n* message passing is a beautifully general operation: it subsumes convolution (which is message passing on a grid), diffusion (which is message passing on a manifold), and social influence (which is message passing on a social graph).\n* permutation invariance is not optional \u2014 it is a hard constraint. the correct answer must not change because you listed node a before node b in your input file.\n* oversmoothing is the depth tax on gnns: each layer you add expands the information horizon but dilutes node identity."
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "loss-optimization",
      "lessonTitle": "Loss Functions and Optimization",
      "x": 0.795028030872345,
      "y": 0.7734745740890503,
      "searchText": "loss functions and optimization\n# loss functions and optimization\n\nimagine a blindfolded hiker in the mountains who can only feel the slope under their feet. every step they take downhill is gradient descent. the steeper the slope, the bigger the step \u2014 until they reach the bottom (a minimum of the loss). everything in machine learning starts here: you pick a number that measures how wrong your predictions are (the loss), and then you adjust your model's parameters to make that number smaller.\n\n## classification losses\n\nwhen the task is to assign labels \u2014 spam or not spam, cat or dog \u2014 we need a loss that punishes wrong labels.\n\n**zero-one loss** is the most natural: it counts the number of mistakes. but it is flat everywhere except at the decision boundary, which means it gives the optimizer no slope to follow. you cannot walk downhill on a plateau.\n\n**hinge loss** fixes this by enforcing a margin. it says: \"not only should you get the label right, you should be confident about it.\" this is the loss behind support vector machines.\n\n**binary cross-entropy** takes a probabilistic view. it says: \"tell me the probability you assign to the correct class, and i will punish you logarithmically for being wrong.\" if you predict 0.99 for the correct class, the penalty is tiny. if you predict 0.01, the penalty is enormous.\n\n$$\n\\mathcal{l}_{bce}=-\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i\\log\\hat{p}_i+(1-y_i)\\log(1-\\hat{p}_i)\\right)\n$$\n\nin words: for each sample, we take the log of the predicted probability for the true class, and average across the dataset. confident correct predictions contribute near-zero loss; confident wrong predictions blow up.\n\n## regression losses\n\nwhen predicting continuous values \u2014 house prices, temperatures, molecular energies \u2014 we need a different kind of penalty.\n\nthink of it this way: suppose you are estimating earthquake damage across a city. **mse (mean squared error)** squares every error before averaging. a house where you are off by $100k gets 100 times the penalty of a house where you are off by $10k. mse hates large errors and will bend over backwards to fix them, even at the cost of making small errors slightly worse.\n\n**mae (mean absolute error)** treats all errors proportionally. off by $10k? penalty of $10k. off by $100k? penalty of $100k. it does not obsess over outliers. but it has a kink at zero, which makes optimization slightly trickier.\n\n**huber loss** gives you the best of both worlds. it behaves like mse for small errors (smooth, easy to optimize) and like mae for large errors (robust to outliers).\n\n[[figure aml-loss-outlier-comparison]]\n\n## optimization dynamics\n\nnow back to our blindfolded hiker. gradient descent updates every parameter by taking a step proportional to the downhill slope:\n\n$$\n\\theta_{t+1}=\\theta_t-\\eta \\nabla_{\\theta}\\mathcal{l}(\\theta_t)\n$$\n\nhere $\\eta$ is the learning rate \u2014 your step size. this single number controls everything. if $\\eta$ is too large, you overshoot the valley and bounce around wildly, possibly diverging. if $\\eta$ is too small, you inch forward painfully slowly and might get stuck in a shallow dip that is not the true minimum.\n\nmodern optimizers improve on vanilla gradient descent. **momentum** remembers past steps and keeps rolling in a consistent direction, like a ball with inertia. **rmsprop** adapts the step size per parameter, taking bigger steps in flat directions and smaller steps in steep ones. **adam** combines both ideas and is the default starting point for most practitioners.\n\n## validation and overfitting\n\nhere is the most important discipline in all of machine learning: always separate your data into three pools.\n\nyour **training set** is where the model learns \u2014 it adjusts parameters by minimizing the loss on these samples. your **validation set** is your mirror \u2014 you check your model's performance here after each round of training, but you never train on it. your **test set** is sacred. you touch it once, at the very end, to get an honest estimate of how the model performs on data it has never influenced.\n\nwatch the training and validation loss curves together. in early training, both go down \u2014 the model is learning real patterns. at some point the training loss keeps falling but the validation loss starts creeping up. that is the moment overfitting begins: the model is memorizing noise in the training data instead of learning generalizable structure. the best parameters correspond to the lowest validation loss.\n\n### k-fold cross-validation\n\nwhen data is limited, holding out a validation set feels wasteful. k-fold cross-validation makes every sample count.\n\nhere is a concrete example. suppose you have five data points: [2, 5, 7, 11, 13]. with $k=5$ (leave-one-out), you train five times. the first time, you hold out 2 and train on [5, 7, 11, 13]. the second time you hold out 5 and train on [2, 7, 11, 13]. and so on. each sample gets exactly one turn as the validator. you average all five validation scores to estimate how well your model generalizes.\n\nin general:\n1. split the data into $k$ equally sized folds.\n2. train on $k-1$ folds, validate on the remaining fold.\n3. rotate the held-out fold and repeat $k$ times.\n4. average the $k$ validation scores to estimate generalization error.\n\ntypical choices are $k=5$ or $k=10$. leave-one-out ($k=n$) gives low bias but high variance and is computationally expensive.\n\n### time-series cross-validation\n\nstandard k-fold is cheating for temporal data \u2014 it lets the model peek into the future when predicting the past. instead, we use chronological splits that respect the arrow of time.\n\n**expanding window**: train on all data up to time $t$, validate on $t+1,\\ldots,t+h$. slide $t$ forward and repeat. your training set grows each round.\n\n**rolling window**: train on a fixed-size window ending at $t$, validate on the next $h$ steps. older data is dropped as the window advances. use this when you believe recent patterns matter more than distant history.\n\nboth approaches "
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "neural-networks-fundamentals",
      "lessonTitle": "Neural Network Fundamentals",
      "x": 0.9159486293792725,
      "y": 0.7621627449989319,
      "searchText": "neural network fundamentals\n# neural network fundamentals\n\nneural networks are parameterized function approximators that learn representations directly from data. instead of hand-engineering features, you give the network raw input and let it discover the patterns. this page introduces the building blocks you need before studying specialized architectures like rnns and gnns.\n\n## the multilayer perceptron\n\na feedforward network (mlp) with $l$ layers computes:\n\n$$\n\\mathbf{h}^{(l)}=\\sigma\\bigl(w^{(l)}\\mathbf{h}^{(l-1)}+\\mathbf{b}^{(l)}\\bigr),\\quad l=1,\\ldots,l\n$$\n\nin words: each layer takes the previous layer's output, applies a linear transformation (multiply by weights $w$, add bias $b$), then passes the result through a nonlinear activation function $\\sigma$. the input is $\\mathbf{h}^{(0)}=\\mathbf{x}$, and the final layer produces predictions \u2014 logits for classification, continuous values for regression.\n\nwhy does stacking layers help? each layer composes a new set of features from the previous ones. the first layer might detect edges; the second, shapes; the third, objects. depth lets the network build increasingly abstract representations.\n\n## activation functions\n\nwithout a nonlinear activation, stacking layers would be pointless \u2014 a chain of linear transformations is just one big linear transformation. the activation function is what gives neural networks their power.\n\n**relu** ($\\max(0,x)$) is the workhorse. it is simple, fast, and its gradient is either 0 or 1, which helps with training. the downside: if a neuron's input is always negative, its output is permanently zero \u2014 a \"dead neuron\" that never recovers.\n\n**leaky relu** ($\\max(\\alpha x, x)$ with small $\\alpha>0$) fixes the dead neuron problem by giving a small slope to negative inputs.\n\n**sigmoid** ($1/(1+e^{-x})$) squashes output to $(0,1)$, making it natural for probability outputs. but for large $|x|$, the gradient nearly vanishes, making deep networks hard to train.\n\n**tanh** ($\\tanh(x)$) is a zero-centered version of sigmoid with the same vanishing gradient issue.\n\n**gelu / silu** are smooth approximations to relu used in modern architectures like transformers. they give slightly better training dynamics in practice.\n\nstart with relu for hidden layers unless you have a specific reason to choose otherwise.\n\n[[figure aml-activation-gallery]]\n\n## backpropagation\n\nhere is the key question: the network has thousands (or millions) of parameters, and the loss is a single number. how do we figure out which parameters to adjust, and by how much?\n\nimagine you are a principal grading a school's exam results. the students did poorly, but you do not know if the problem was the math teacher, the english teacher, or the p.e. coach. so you trace the blame backward: the final grades depend on the teachers' contributions, which depend on the curriculum, which depends on the department heads. you pass the blame backward through the chain until you know exactly how much each person contributed to the failure.\n\nthat is backpropagation. using the chain rule of calculus, we propagate error gradients from the output layer back through every layer to every parameter:\n\n$$\n\\frac{\\partial\\mathcal{l}}{\\partial w^{(1)}}=\\frac{\\partial\\mathcal{l}}{\\partial\\mathbf{h}^{(2)}}\\cdot\\frac{\\partial\\mathbf{h}^{(2)}}{\\partial\\mathbf{h}^{(1)}}\\cdot\\frac{\\partial\\mathbf{h}^{(1)}}{\\partial w^{(1)}}\n$$\n\neach term in this chain tells us: \"how much does a small change in this layer's output affect the next?\" multiplied together, they give the gradient of the loss with respect to the first layer's weights. imagine the loss is angry at the output and starts shouting \"who did this?!\" the blame passes backward through every layer, getting multiplied by how sensitive each layer was. mathematically this is the chain rule in action, but the story is what you will remember when your gradients suddenly disappear at layer 47. that multiplication is why gradients can vanish or explode. modern frameworks (pytorch, jax) compute these gradients automatically, but understanding the chain rule structure still matters for diagnosing training failures \u2014 when gradients explode or vanish, it is usually because one of these terms is too large or too small.\n\n[[figure aml-numeric-forward-backward]]\n\n## regularization\n\nneural networks have many parameters, and given enough capacity they will happily memorize the training data \u2014 including the noise. regularization techniques fight this.\n\n**dropout** randomly zeroes out a fraction of activations during training. this forces the network to learn redundant representations \u2014 no single neuron can become a critical bottleneck. it acts as an implicit ensemble: each training step uses a different random sub-network.\n\n**weight decay** ($l_2$ regularization) adds $\\lambda\\|\\theta\\|_2^2$ to the loss, penalizing large weights. this encourages smoother functions that are less likely to memorize noise.\n\n**batch normalization** normalizes activations within each mini-batch to zero mean and unit variance. this stabilizes training and allows higher learning rates.\n\n**early stopping** monitors validation loss and halts training when it starts increasing. it is the simplest regularization and often the most effective \u2014 your model is telling you it has learned all the real patterns and is now memorizing noise.\n\n## universal approximation\n\na remarkable theorem (the universal approximation theorem) says that a single hidden layer with enough neurons can approximate any continuous function on a compact set. so why use deep networks? because a single wide layer can approximate anything in theory, but it may need an astronomically large number of neurons. depth is a swiss army knife \u2014 deep networks with fewer neurons per layer learn more efficiently, because each layer composes features hierarchically. width is a sledgehammer; depth is elegance.\n\n## architecture landscape\n\nneural networks specialize by changing how layers are connected. here is the family portrait:\n"
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "recurrent-neural-networks",
      "lessonTitle": "Recurrent Neural Networks (Extension)",
      "x": 0.9672080874443054,
      "y": 0.7117074131965637,
      "searchText": "recurrent neural networks (extension)\n# recurrent neural networks (extension)\n\nsome data only makes sense in order. the sentence \"the cat sat on the mat\" means something; \"mat the on sat cat the\" does not. temperature readings, stock prices, sensor streams, musical notes \u2014 whenever the meaning depends on sequence, we need a model that remembers what came before. that is what recurrent neural networks do.\n\n## core recurrence\n\na standard feedforward network processes each input independently, with no memory. an rnn adds a crucial ingredient: a hidden state $h_t$ that carries information from one timestep to the next.\n\nat each timestep $t$:\n$$\nh_t=f(w_{xh}x_t + w_{hh}h_{t-1} + b_h),\\quad\ny_t=g(w_{hy}h_t+b_y)\n$$\n\nin words: the hidden state $h_t$ is a blend of the current input $x_t$ and the previous hidden state $h_{t-1}$, passed through a nonlinearity. the output $y_t$ is then computed from the hidden state. the same weights $w_{xh}$, $w_{hh}$, and $w_{hy}$ are reused at every timestep \u2014 the network \"unrolls\" through time but shares parameters.\n\nthis means an rnn can, in principle, handle sequences of any length. it reads one token at a time, updates its internal state, and carries a compressed summary of everything it has seen so far.\n\n## the vanishing gradient problem\n\nhere is the catch. imagine a game of telephone where a message passes through 50 people. by the time it reaches the last person, the message is garbled beyond recognition. the same thing happens in an rnn: when we backpropagate through many timesteps, the gradients get multiplied by the same weight matrix over and over. if that matrix has eigenvalues less than 1, the gradients shrink exponentially \u2014 they *vanish*. the network forgets early inputs because the error signal from the end of the sequence never makes it back to the beginning.\n\nthe opposite can also happen: if eigenvalues are greater than 1, gradients *explode*, and training becomes unstable. gradient clipping (capping the gradient norm) is a practical fix for explosions, but vanishing gradients require a more fundamental architectural change.\n\n## gated variants: lstm and gru\n\nthe solution is to give the network a notebook \u2014 a persistent memory cell with explicit gates that control what to remember, what to forget, and what to output.\n\n**lstm (long short-term memory)** introduces a cell state $c_t$ that runs alongside the hidden state. three gates control it: the *forget gate* decides what old information to erase, the *input gate* decides what new information to write, and the *output gate* decides what part of the cell state to expose as the hidden state. because the cell state can pass through timesteps with only additive updates (no repeated matrix multiplication), gradients flow much more easily over long sequences.\n\n**gru (gated recurrent unit)** is a simplified variant with two gates instead of three. it merges the cell state and hidden state into one, using a *reset gate* and an *update gate*. grus are faster to train and often perform comparably to lstms. when in doubt, try both.\n\n## a concrete example\n\nconsider predicting the next word. given \"the cat sat on the ___\", a simple rnn can likely predict \"mat\" \u2014 the context is short and recent. but now consider: \"the author, who grew up in paris and studied literature at the sorbonne before moving to new york where she worked as a journalist for twenty years, finally published her ___\". to predict \"book\" or \"novel,\" the model must remember \"author\" and \"published\" across a gap of 30+ words. a vanilla rnn's hidden state would have long forgotten \"author\" by the time it reaches \"her.\" an lstm, with its explicit memory cell, can carry that information across the entire sentence.\n\n[[figure aml-rnn-gradient-flow]]\n\n[[simulation aml-rnn-memory-highway]]\n\n## where this topic goes deeper\n\nthis page gives you the core intuition for sequence modeling. for full architectural details \u2014 bidirectional rnns, attention mechanisms, why attention-based transformers largely replaced recurrent architectures, and practical sequence modeling workflows \u2014 see [advanced deep learning \u2014 sequence models](/topics/advanced-deep-learning/ann).\n\n## practical checklist\n\n* normalize and window your sequence data carefully \u2014 sequence models are sensitive to scale and length.\n* always use chronological validation splits, never random shuffling.\n* start with an lstm or gru. switch to a transformer if sequences are long and you have enough data.\n* track both short-horizon and long-horizon error metrics to understand where your model struggles.\n\n## big ideas\n\n* sequential data has an arrow of time, and that arrow matters. an architecture that ignores order is not just suboptimal \u2014 it is fundamentally misspecified for the problem.\n* the vanishing gradient problem is not a quirk of bad implementation; it is a mathematical inevitability when you multiply the same matrix hundreds of times. lstm gates are a structural solution, not a patch.\n* gating is the key idea: instead of trying to pass all information through a bottleneck, let the network learn *what* to pass. the same idea reappears in attention mechanisms and transformers.\n* chronological validation is non-negotiable for time-series data. random shuffling is data leakage in disguise.\n\n## what comes next\n\nrecurrent networks are powerful but carry a fundamental limitation: they process sequences one step at a time, making parallelization difficult and long-range dependencies hard to maintain even with gating. the transformer architecture, covered in the advanced sequence modeling materials, replaces recurrence entirely with attention \u2014 every token can directly attend to every other token in a single parallel operation. understanding why lstm gates work is the best preparation for understanding why attention works even better.\n\nthe final topic in this module shifts from prediction to creation. generative adversarial networks turn the optimization framework you learned at the start of this module "
    },
    {
      "topicId": "applied-machine-learning",
      "topicTitle": "Applied Machine Learning",
      "routeSlug": "applied-machine-learning",
      "lessonSlug": "trees-ensembles",
      "lessonTitle": "Decision Trees and Ensemble Methods",
      "x": 0.8221017718315125,
      "y": 0.6720403432846069,
      "searchText": "decision trees and ensemble methods\n# decision trees and ensemble methods\n\nhave you ever played twenty questions? you think of an animal, and your friend asks yes-or-no questions to narrow it down. \"does it have fur?\" \"is it bigger than a cat?\" \"does it live in water?\" each question splits the remaining possibilities into two groups. that is exactly what a decision tree does \u2014 except it asks the questions that split the data *best*.\n\n## decision trees\n\na decision tree recursively partitions feature space by choosing, at each node, the question (feature and threshold) that produces the purest child groups. but how do we measure \"pure\"?\n\n**gini impurity** asks: if you picked two random samples from this group, what is the probability they belong to different classes?\n\n$$\ng(s)=1-\\sum_{k=1}^{k}p_k^2\n$$\n\nwhen every sample in a group belongs to the same class, $g=0$ (perfectly pure). when classes are evenly mixed, $g$ is at its maximum. gini is fast to compute and works well in practice.\n\n**entropy** comes from information theory and asks a different question: how surprised are you by a random sample from this group?\n\n$$\nh(s)=-\\sum_{k=1}^{k}p_k\\log p_k\n$$\n\nif every sample is class a, you are never surprised \u2014 entropy is zero. if the group is a 50/50 mix, every sample is maximally surprising \u2014 entropy is at its peak. in practice, gini and entropy give very similar trees. gini is the default in most libraries because it avoids the logarithm.\n\ntrees are wonderfully interpretable \u2014 you can draw them on a whiteboard and explain every prediction. but a single deep tree will memorize the training data, fitting noise along with signal. that is where ensembles come in.\n\n[[figure aml-tree-growth-steps]]\n\n## ensemble methods\n\nthe key insight behind ensembles is that combining many imperfect models can produce something much better than any individual.\n\n**random forests (bagging)** take the \"wisdom of crowds\" approach. imagine 100 people each given a slightly different, incomplete map of a city. individually, each person will get lost sometimes. but if you ask all 100 for directions and go with the majority vote, you almost never get lost. random forests train many trees, each on a random subset of the data and a random subset of features. the trees make independent errors, and averaging cancels those errors out. bagging reduces variance \u2014 the predictions become more stable without significantly increasing bias.\n\n**gradient boosting (adaboost, xgboost, lightgbm)** takes the opposite approach. instead of training many trees in parallel, it trains them sequentially. each new tree focuses specifically on the mistakes the previous trees made. think of it as one very determined hiker who, after every wrong turn, studies exactly where they went wrong and adjusts their strategy. boosting reduces bias \u2014 the model becomes progressively better at capturing complex patterns, though you need to be careful not to overfit by boosting too many rounds.\n\n## bias-variance intuition\n\nhere is the core tradeoff in all of machine learning, and trees make it beautifully concrete.\n\na very shallow tree (say, depth 2) is like a tourist with a simple rule: \"if you are north of the river, go east.\" it is too simple to capture the real layout of the city \u2014 that is **high bias**, underfitting. a very deep tree that memorizes every turn you have ever taken is the opposite problem: it works perfectly on streets you have seen but fails on new ones \u2014 that is **high variance**, overfitting.\n\nbagging (random forests) fights variance by averaging many high-variance trees \u2014 each tree overfits differently, and the errors cancel. boosting fights bias by building a sequence of simple trees that progressively correct each other's mistakes. in practice, gradient boosting with careful tuning (early stopping, regularization) is one of the most powerful methods for structured data.\n\n[[figure aml-xor-ensemble]]\n\n## interactive simulations\n\n[[simulation aml-tree-split-impurity]]\n\n[[simulation aml-tree-growth-steps]]\n\n[[simulation aml-tree-ensemble-xor]]\n\n[[simulation aml-forest-vs-single-tree]]\n\n## model selection notes\n\n* always use cross-validation for small tabular datasets \u2014 a single train/test split is too noisy.\n* key hyperparameters to tune: `max_depth`, `min_samples_leaf`, `n_estimators`, and learning rate (for boosting).\n* when you use predicted probabilities for decisions (not just class labels), run calibration checks \u2014 tree ensembles can produce poorly calibrated probabilities.\n\n## big ideas\n\n* a decision tree is the most interpretable nonlinear model that exists \u2014 you can literally read it out loud and explain every prediction. that interpretability is also its weakness: a single tree deep enough to be accurate will memorize the training data.\n* bagging and boosting are two philosophies for turning weak models into strong ones. bagging parallelizes imperfect estimators and averages away their independent errors. boosting serializes them and forces each new one to focus on what the others got wrong.\n* the bias-variance tradeoff is not abstract \u2014 trees make it concrete. depth controls it directly, and ensemble methods are specific prescriptions for manipulating it.\n* on structured tabular data, gradient-boosted trees still routinely outperform deep neural networks. do not reach for deep learning until you have tried xgboost.\n\n## what comes next\n\ntrees and ensembles are the gold standard for structured, tabular data \u2014 the kind that lives in spreadsheets and databases. but high-dimensional data often has far fewer meaningful dimensions than it appears. before diving into neural networks, it pays to understand how to find and exploit that lower-dimensional structure.\n\ndimensionality reduction \u2014 pca, t-sne, umap, and autoencoders \u2014 gives you the tools to compress, visualize, and preprocess your features. pca on the output of a tree ensemble can reveal which directions in feature space actually matter. and the autoencoder idea \u2014 learning a compre"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "advanced-fitting-calibration",
      "lessonTitle": "Advanced Fitting and Calibration",
      "x": 0.511005699634552,
      "y": 0.6817373633384705,
      "searchText": "advanced fitting and calibration\n# advanced fitting and calibration\n\n## beyond simple fitting\n\nsystematic errors are like ghosts in the machine. you can't see them directly, but they haunt your results. your detector isn't perfectly calibrated. your energy scale drifts. your efficiency varies across the measurement range. and lurking behind your signal of interest is a background that must be modeled before you can claim to have seen anything.\n\nin [the chi-square method](./chi-square-method), you fit models with a handful of parameters and gaussian errors. real experimental data is messier: multiple overlapping signals, correlated parameters, systematic uncertainties from the instrument itself, and competing models that all look plausible. this section builds on that foundation to handle the complications that arise in serious data analysis.\n\n## multi-component models\n\nmany physical measurements involve a **signal plus background** decomposition. you're looking for a small peak sitting on top of a large, slowly-varying background:\n\n$$\nf(x; \\boldsymbol{\\theta}) = s(x; \\boldsymbol{\\theta}_s) + b(x; \\boldsymbol{\\theta}_b),\n$$\n\nwhere $s$ describes the signal of interest and $b$ accounts for background contributions. the total parameter vector $\\boldsymbol{\\theta} = (\\boldsymbol{\\theta}_s, \\boldsymbol{\\theta}_b)$ is estimated simultaneously.\n\nthink of it as trying to hear a conversation in a noisy room. you need to model both the conversation (signal) and the room noise (background) at the same time. if you model the background poorly, it leaks into your signal estimate and distorts your conclusions.\n\nwhen the background shape is known from control measurements or simulation, its parameters may be **constrained** by adding penalty terms to the objective function. this is equivalent to bayesian fitting with informative priors on the background parameters (the connection to [bayesian statistics](./bayesian-statistics) is direct and deliberate).\n\n## profile likelihood\n\nin a model with many parameters, some are interesting (the signal strength, a physical constant) and others are nuisance parameters \u2014 things you must account for but don't actually care about. the nuisance parameters are ghosts you need to catch so they don't contaminate what you're measuring. you want confidence intervals on the interesting parameters that correctly account for uncertainty in the nuisance ones.\n\nthe **profile likelihood** does this by optimizing over the nuisance parameters at every point:\n\n$$\nl_p(\\boldsymbol{\\psi}) = \\max_{\\boldsymbol{\\lambda}} l(\\boldsymbol{\\psi}, \\boldsymbol{\\lambda}).\n$$\n\n[[simulation profile-likelihood]]\n\nthink of it as asking: \"for this particular value of the interesting parameter, what is the *best* the model can do if i freely adjust everything else?\" you trace out a curve of best-case likelihoods, and that curve gives you confidence intervals.\n\na confidence region at level $\\alpha$ is:\n\n$$\n-2 \\ln \\frac{l_p(\\boldsymbol{\\psi})}{l_p(\\hat{\\boldsymbol{\\psi}})} \\leq \\chi^2_{k, \\alpha},\n$$\n\nwhere $k = \\dim(\\boldsymbol{\\psi})$. this correctly propagates parameter correlations into the uncertainty \u2014 something that simply reading off the diagonal of the covariance matrix would miss if parameters are correlated.\n\n## goodness of fit\n\nafter fitting, you must assess whether the model actually describes the data. a beautiful fit to a wrong model is worse than no fit at all.\n\nthe **chi-squared statistic** $\\chi^2 = \\sum_i (y_i - f(x_i; \\hat{\\boldsymbol{\\theta}}))^2 / \\sigma_i^2$ should follow a $\\chi^2$ distribution with $n - p$ degrees of freedom if the model is correct. the **reduced chi-squared** $\\chi^2_\\nu = \\chi^2 / (n - p)$ should be approximately 1. this is the same diagnostic from [the chi-square method](./chi-square-method), now applied to more complex models.\n\nwhat do the numbers mean?\n\n* $\\chi^2_\\nu \\gg 1$: the model doesn't describe the data, or the uncertainties are underestimated. something is wrong.\n* $\\chi^2_\\nu \\approx 1$: the model describes the data within the quoted uncertainties. good.\n* $\\chi^2_\\nu \\ll 1$: the fit is *too* good \u2014 the uncertainties are probably overestimated. suspicious in a different way.\n\nthe **p-value** gives the probability of obtaining a $\\chi^2$ at least as large as observed, assuming the model is correct. small p-values (typically $< 0.05$) suggest the model is inadequate.\n\n## model comparison\n\nwhen multiple models could describe the data, you need principled criteria for deciding which one to use. the temptation is to pick the model with the lowest $\\chi^2$, but a model with more parameters will *always* fit at least as well. you need to penalize complexity.\n\nthe **likelihood ratio test** (from [hypothesis testing](./hypothesis-testing)) compares nested models. the test statistic $\\lambda = -2\\ln(l_0 / l_1)$ follows a $\\chi^2$ distribution with degrees of freedom equal to the difference in number of parameters. it asks: does the extra parameter buy enough improvement to justify its inclusion?\n\nfor non-nested models, **information criteria** balance fit quality against complexity:\n\n$$\n\\text{aic} = -2\\ln l + 2p, \\qquad \\text{bic} = -2\\ln l + p\\ln n,\n$$\n\nwhere $p$ is the number of parameters and $n$ the number of data points. lower values are better. bic penalizes complexity more heavily than aic and tends to favor simpler models, especially for large datasets.\n\n## calibration\n\nall the fitting methods above assume you know the relationship between what your instrument reads and the physical quantity you care about. **calibration** establishes that relationship.\n\na typical calibration procedure:\n\n1. **reference measurements**: measure known standards spanning the range of interest. these are your anchor points.\n2. **fit the calibration curve**: determine $r = g(s; \\boldsymbol{\\theta})$ mapping the true physical signal $s$ to the instrument response $r$.\n3. **invert for unknowns**: given a new measurement $r_{\\text{obs}}$, solve $r_{\\text{obs}} = g(s; \\hat{\\bol"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "anova",
      "lessonTitle": "Analysis of Variance (ANOVA)",
      "x": 0.44961902499198914,
      "y": 0.5852786898612976,
      "searchText": "analysis of variance (anova)\n# analysis of variance (anova)\n\nyour fertilizer worked... or did it? you tested three different fertilizers on three different fields, and the yields look different. but here's the thing \u2014 fields vary naturally. soil depth, sun exposure, drainage \u2014 all of it adds noise. so the real question isn't \"are the numbers different?\" (they always will be). the real question is: \"are the numbers *more* different than you'd expect from noise alone?\"\n\n## the idea behind anova\n\nthe t-test from [hypothesis testing](./hypothesis-testing) compares two groups. but experiments often involve three, four, or a dozen groups. you *could* run t-tests on every pair, but this creates a dangerous multiple-testing problem \u2014 with enough comparisons, you will find \"significant\" differences by sheer chance.\n\n**analysis of variance** solves this with a single test: do *any* of the group means differ? despite its name, anova works by comparing variability *between* groups to variability *within* groups. the key insight: if group means differ substantially, the between-group variance will be large relative to the within-group variance. if all groups come from the same population, the two should be similar.\n\nthink of it as a signal-to-noise ratio. the \"signal\" is how much the group means spread out. the \"noise\" is how much individual measurements scatter within each group. if the signal is big compared to the noise, something real is going on.\n\n## one-way anova\n\nfor $k$ groups with $n_i$ observations each, the model is:\n\n$$\ny_{ij} = \\mu + \\alpha_i + \\varepsilon_{ij}, \\qquad \\varepsilon_{ij} \\sim \\mathcal{n}(0, \\sigma^2),\n$$\n\nwhere $\\mu$ is the grand mean, $\\alpha_i$ is the effect of group $i$, and $\\varepsilon_{ij}$ is random error.\n\nthe total variability decomposes cleanly:\n\n$$\n\\text{ss}_{\\text{total}} = \\text{ss}_{\\text{between}} + \\text{ss}_{\\text{within}}.\n$$\n\nthis decomposition is the heart of anova \u2014 it splits the total variation into the part explained by group differences and the part left over as noise.\n\nthe **f-statistic** is the ratio:\n\n$$\nf = \\frac{\\text{ss}_{\\text{between}} / (k - 1)}{\\text{ss}_{\\text{within}} / (n - k)} = \\frac{\\text{ms}_{\\text{between}}}{\\text{ms}_{\\text{within}}}.\n$$\n\n[[simulation variance-decomposition]]\n\nunder $h_0: \\alpha_1 = \\cdots = \\alpha_k = 0$, this follows an $f(k-1, n-k)$ distribution. a large $f$ means the group differences are large relative to the noise \u2014 evidence that at least one group is different.\n\n**assumptions** (check these before trusting the result):\n\n* independence of observations.\n* normality within each group (check with q-q plots or the shapiro-wilk test).\n* homogeneity of variances (**homoscedasticity**); checked with levene's test or bartlett's test.\n\n## two-way anova\n\nwhat if you're varying two factors simultaneously \u2014 say, fertilizer type *and* watering schedule? two-way anova handles this:\n\n$$\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}.\n$$\n\nthis decomposes variability into three sources:\n\n* main effect of $a$: do levels of factor $a$ differ on average?\n* main effect of $b$: do levels of factor $b$ differ on average?\n* **interaction** $a \\times b$: does the effect of $a$ depend on the level of $b$?\n\nthe interaction term is often the most scientifically interesting finding. a fertilizer might work brilliantly with daily watering but do nothing with weekly watering \u2014 that's an interaction. you might think the two factors work independently \u2014 but actually, they can amplify or cancel each other in ways neither factor alone would reveal.\n\n## factorial designs\n\na **full factorial design** tests all combinations of factor levels. for factors with $a$ and $b$ levels, there are $a \\times b$ treatment combinations. factorial designs are efficient because every observation contributes information about every factor.\n\n**balanced designs** (equal sample sizes per cell) simplify the analysis and make the f-tests exact. when balance is lost (due to missing data or unequal groups), the sums of squares are no longer orthogonal, and you need to choose between type i, ii, or iii sums of squares \u2014 a subtlety that trips up many practitioners.\n\n## post-hoc tests\n\nanova tells you *that* at least one group differs, but not *which* ones. to find the specific differences, you use post-hoc (after-the-fact) comparisons. here's the menu, ordered from most to least commonly used:\n\n* **tukey's hsd**: compares all pairs of group means. the go-to choice for pairwise comparisons. controls the simultaneous confidence level.\n* **bonferroni correction**: divides $\\alpha$ by the number of comparisons. conservative but general-purpose. simple to explain and implement.\n* **scheff\u00e9's method**: allows arbitrary contrasts (not just pairwise). most conservative. use it when you want to test combinations like \"are groups 1 and 2 together different from group 3?\"\n* **dunnett's test**: compares each treatment to a single control group. perfect when you have one reference condition.\n\nall of these control the **family-wise error rate** \u2014 the probability of making *any* false discovery across all comparisons. without this control, the more groups you compare, the more likely you are to find something \"significant\" that isn't real.\n\n## what if the assumptions fail? kruskal-wallis\n\nanova relies on normality and equal variances. but real data is often skewed, heavy-tailed, or heteroscedastic. what then?\n\nthe **kruskal-wallis test** is a non-parametric alternative to one-way anova. instead of comparing means, it compares **ranks**: all observations are ranked together, and the test asks whether the mean ranks differ across groups.\n\n$$\nh = \\frac{12}{n(n+1)} \\sum_{i=1}^{k} n_i (\\bar{r}_i - \\bar{r})^2,\n$$\n\nwhere $\\bar{r}_i$ is the mean rank in group $i$. under $h_0$, $h \\sim \\chi^2(k-1)$ approximately.\n\nbecause it works with ranks rather than raw values, the kruskal-wallis test is robust to outliers, skewness, and non-constant variance. the trade-off is "
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "bayesian-statistics",
      "lessonTitle": "Bayesian Statistics",
      "x": 0.583275556564331,
      "y": 0.6500739455223083,
      "searchText": "bayesian statistics\n# bayesian statistics\n\n## a different way of thinking\n\nthroughout this course, you've used **frequentist** methods: p-values, confidence intervals, maximum likelihood. these treat probability as a long-run frequency \u2014 if you repeat the experiment many times, how often does this outcome occur?\n\n**bayesian statistics** takes a fundamentally different view. probability represents a *degree of belief*. you start with some prior belief about a parameter, observe data, and update that belief. the result is a **posterior distribution** \u2014 a complete description of what you know about the parameter after seeing the data.\n\nneither approach is \"correct\" \u2014 they answer different questions. frequentist methods ask \"how surprising is this data if the hypothesis is true?\" bayesian methods ask \"given the data, what should i believe about the parameter?\" both are useful, and the best practitioners are fluent in both.\n\nyou might think this is a completely new idea. but actually, you've been doing something very close to bayesian reasoning all along. the likelihood function from [probability density functions](./probability-density-functions)? that's the engine of bayesian inference too. the nuisance parameter constraints from [advanced fitting](./advanced-fitting-calibration)? those are priors in disguise. the regularization penalties in [machine learning](./machine-learning-data-analysis)? bayesian priors again. this section makes the connection explicit.\n\n## bayes' theorem\n\nthe mathematical engine of bayesian statistics is **bayes' theorem**. for a hypothesis $h$ and observed data $d$:\n\n$$\np(h|d) = \\frac{p(d|h) \\cdot p(h)}{p(d)}\n$$\n\neach term plays a specific role:\n\n* $p(h|d)$ is the **posterior**: your updated belief about $h$ after seeing the data. this is what you want.\n* $p(d|h)$ is the **likelihood**: the probability of the data given the hypothesis. this is the same likelihood function you've been using since [probability density functions](./probability-density-functions) \u2014 the connection between bayesian and frequentist methods runs deep.\n* $p(h)$ is the **prior**: your belief about $h$ before seeing the data. this is where existing knowledge (or honest ignorance) enters.\n* $p(d)$ is the **evidence** (or marginal likelihood): a normalization constant ensuring the posterior integrates to 1. often the hardest part to compute, but you can frequently sidestep it.\n\nthe formula reads as a recipe: **posterior $\\propto$ likelihood $\\times$ prior**. the data update your belief, with the likelihood acting as the bridge.\n\n## example: is this coin fair?\n\nsuppose someone hands you a coin. you want to know whether it's fair. let $\\theta$ be the probability of heads. let's walk through this step by step \u2014 the way you'd actually think about it.\n\n**prior**: you have no strong reason to think the coin is biased. so you start with a uniform prior: $p(\\theta) = 1$ for $\\theta \\in [0, 1]$. every value is equally plausible before you see any data.\n\n**flip 1**: you flip the coin once and get heads. your likelihood is $p(d|\\theta) = \\theta$. the posterior is $p(\\theta|d) \\propto \\theta \\times 1 = \\theta$. after one head, values of $\\theta$ near 1 look slightly more plausible than values near 0. but the distribution is broad \u2014 one flip doesn't tell you much.\n\n**flips 1-10**: you flip 9 more times and get a total of 7 heads and 3 tails. the likelihood from the binomial distribution is:\n\n$$\np(d|\\theta) = \\binom{10}{7} \\theta^7 (1-\\theta)^3\n$$\n\nthe posterior becomes:\n\n$$\np(\\theta|d) \\propto \\theta^7 (1-\\theta)^3\n$$\n\nthis is a beta(8, 4) distribution. its peak is at $\\theta = 7/10 = 0.7$ \u2014 the same as the maximum likelihood estimate. but the bayesian answer gives you *more*: the full shape of the distribution tells you how uncertain you are. the 95% credible interval runs roughly from 0.40 to 0.93. that's a wide range \u2014 10 flips haven't pinned down $\\theta$ very tightly.\n\n**flips 1-110**: you flip the coin 100 more times and get 70 heads total (out of 110). the posterior tightens dramatically \u2014 more data means more certainty. now the 95% credible interval might run from 0.55 to 0.73. the prior matters less and less as data accumulate.\n\nthis is a reassuring property: with enough data, reasonable people with different priors converge to the same conclusion. the data overwhelm the prior.\n\n> **challenge.** grab a coin. before you flip it, write down your prior belief about $p(\\text{heads})$. now flip it 10 times. after each flip, mentally update your belief: is the coin fair, or is it biased? notice how your confidence grows (or shifts) with each flip. that's bayesian updating in real time \u2014 your brain does this naturally.\n\n[[simulation prior-influence]]\n\n## choosing priors\n\nthe prior is the most controversial part of bayesian statistics. where does it come from? here's the menu:\n\n* **informative priors** encode genuine prior knowledge. if previous experiments have measured a quantity, use that result as your prior. this is one of the great strengths of bayesian methods \u2014 they provide a principled way to combine old and new evidence.\n* **weakly informative priors** gently constrain parameters to physically reasonable ranges without strongly favoring specific values. for example, a half-normal prior on a variance parameter ensures it stays positive without committing to a particular magnitude.\n* **non-informative (flat) priors** attempt to \"let the data speak.\" a uniform prior on $\\theta$ says every value is equally plausible. this sounds objective, but it's not quite: a flat prior on $\\theta$ is *not* flat on $\\theta^2$ or $\\ln\\theta$. the choice of parameterization matters.\n\nin practice, the prior is most important when data are scarce. with abundant data, the likelihood dominates and the prior washes out. if your conclusions depend sensitively on the prior, that's a signal that the data are insufficient to answer the question \u2014 which is itself a useful thing to know.\n\n[[simulation shrinkage-plot]]\n\n## bayes"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "chi-square-method",
      "lessonTitle": "Chi-Square Method",
      "x": 0.4971775710582733,
      "y": 0.6744105815887451,
      "searchText": "chi-square method\n# chi-square method\n\nalex has a model and some data. the model predicts a straight line; the data scatter around it. the question is sharp: *how well does this model actually describe the data?* and what are the best-fit parameters?\n\nyou now have the tools to describe data ([introduction](./introduction-concepts)), model it with distributions ([pdfs](./probability-density-functions)), understand why errors are gaussian ([error propagation](./error-propagation)), and simulate complex scenarios ([simulation](./simulation-fitting)). the next step is to connect models to data quantitatively. this is the domain of the **chi-square method** \u2014 the workhorse of model fitting in the physical sciences.\n\n## linear regression\n\nthe simplest model is a straight line through data:\n\n$$\ny = \\beta_0 + \\beta_1 x + \\epsilon\n$$\n\nwhere $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\epsilon$ is the error term. this is a starting point, but it treats all data points equally. what if some measurements are much more precise than others? you need a way to weight the fit \u2014 to listen more carefully to the reliable measurements and less to the noisy ones.\n\n[[simulation applied-stats-sim-1]]\n\n## the chi-square statistic\n\nremember the likelihood function from [probability density functions](./probability-density-functions)? you learned to find the parameters that make the data most probable. for gaussian errors, that's equivalent to minimizing this:\n\n$$\n\\chi^2(\\theta) = \\sum_i^n \\frac{(y_i - f(x_i,\\theta))^2}{\\sigma_i^2}\n$$\n\nthis is the sum of squared residuals, each divided by the variance of that measurement. precise measurements (small $\\sigma_i$) pull the fit strongly; noisy ones (large $\\sigma_i$) have less influence. the connection to mle is exact: minimizing $\\chi^2$ *is* maximizing the likelihood when errors are gaussian. that's not a coincidence \u2014 it has to be this way. let's see why.\n\nif each measurement $y_i$ is gaussian with mean $f(x_i, \\theta)$ and variance $\\sigma_i^2$, then $-2\\ln\\mathcal{l} = \\chi^2 + \\text{const}$. minimizing one is the same as minimizing the other. the chi-square method and maximum likelihood are two faces of the same coin.\n\nthe number of **degrees of freedom** is:\n\n$$\nn_\\text{dof} = n_\\text{samples} - n_\\text{fit parameters}\n$$\n\n### interpreting the fit\n\nhow do you know if a fit is good? the $\\chi^2$ value should be roughly equal to the number of degrees of freedom. more precisely, you compute the **p-value** \u2014 the probability of getting a $\\chi^2$ at least this large if the model is correct:\n\n$$\n\\text{prob}(\\chi^2 = 65.7, n_\\text{dof}=42) = 0.011\n$$\n\na small p-value (say, below 0.05) suggests the model does not describe the data well. but be careful: it could also mean the error bars are underestimated. alex learned this the hard way \u2014 a \"bad\" fit that turned out to be perfectly fine once the systematic errors were properly accounted for.\n\nif errors are large, different models will all fit similarly well \u2014 the data cannot distinguish between them. with small, precise errors, even slight model deficiencies produce large $\\chi^2$ values. this is a feature, not a bug: better data demands better models.\n\nplot the **residuals** $\\frac{y_i-f(x_i, \\theta)}{\\sigma_i}$ \u2014 these should scatter like a standard normal distribution (mean 0, standard deviation 1) with no visible pattern. trends in the residuals are your early warning system: the model is missing something.\n\n```python\nchi2_prob = stats.chi2.sf(chi2_value, n_dof)\n```\n\nnote: the weighted mean from [introduction and concepts](./introduction-concepts) is actually a special case of chi-squared fitting \u2014 it is just fitting a constant to the data.\n\n### chi-square for binned data\n\nfor large datasets, it is often practical to bin the data into a histogram first:\n\n$$\n\\chi^2 = \\sum_{i\\in \\text{bins}} \\frac{(o_i-e_i)^2}{e_i}\n$$\n\nwhere $o_i$ is the observed count and $e_i$ is the expected count in each bin. empty bins should be excluded (they cause division by zero), at the cost of some loss of resolution.\n\n### why chi-square is powerful\n\n[[simulation applied-stats-sim-5]]\n\nthe power of $\\chi^2$ comes from its geometry. near the minimum, the $\\chi^2$ surface is approximately parabolic \u2014 like a bowl. the curvature of that bowl directly gives you the uncertainties on the fitted parameters. a steep, narrow bowl means the parameter is tightly constrained; a shallow, wide bowl means large uncertainty. this parabolic structure is what makes $\\chi^2$-based confidence intervals so straightforward.\n\n### uncertainties in $x$\n\nso far you've assumed errors only in $y$. if both $x$ and $y$ have uncertainties, the procedure is iterative: fit without $x$ errors first, then fold them in using [error propagation](./error-propagation):\n\n$$\n\\sigma_{y_i}^{\\text{new}} = \\sqrt{\\sigma_{y_i}^2 + \\left( \\frac{\\partial y}{\\partial x}\\bigg|_{x_i} \\sigma_{x_i} \\right)^2}\n$$\n\nrepeat the fit with the updated errors. this converges quickly \u2014 usually one or two iterations suffice.\n\n### reporting errors\n\nwhen you report a result, distinguish between **statistical** uncertainty (from the finite size of your dataset) and **systematic** uncertainty (from imperfect knowledge of the experimental setup). the standard format is:\n\n$$\na = (0.24 \\pm 0.05_\\text{stat} \\pm 0.07_\\text{syst}) \\times 10^4 \\; \\text{kg}\n$$\n\n> **challenge.** explain chi-square fitting to a friend using only the analogy of a target and arrows. each arrow lands somewhere near the bullseye; closer arrows get more \"credit.\" one minute.\n\n## big ideas\n\n* minimizing $\\chi^2$ and maximizing the likelihood are the same thing when errors are gaussian \u2014 two descriptions of one operation, not two different methods.\n* a good fit has $\\chi^2 \\approx n_\\text{dof}$; a reduced $\\chi^2$ much greater than 1 means the model is wrong *or* the error bars are too small \u2014 and you must figure out which.\n* precise measurements don't just give better answers \u2014 they demand better models. when your error bars s"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "design-of-experiments",
      "lessonTitle": "Basic Design of Experiments",
      "x": 0.4412079453468323,
      "y": 0.5628017783164978,
      "searchText": "basic design of experiments\n# basic design of experiments\n\nso far, you've developed tools for analyzing data *after* it's been collected. but here's a secret that experienced researchers know: the quality of any statistical analysis depends critically on how the data was gathered in the first place. a well-designed experiment can answer your question with 50 observations; a poorly designed one might not answer it with 5000.\n\nexperimental design is where you invest thought *before* spending time and money on data collection. the payoff is enormous.\n\n## principles of experimental design\n\nthree foundational principles guide every well-designed experiment:\n\n* **randomization**: randomly assign experimental units to treatments to eliminate systematic bias. without randomization, apparent treatment effects might simply reflect pre-existing differences between groups. **practical tip**: use a random number generator, not convenience or judgment. human \"random\" assignment is notoriously non-random.\n* **replication**: repeat measurements to estimate variability and increase precision. a single measurement tells you nothing about reliability. **practical tip**: determine how many replicates you need *before* starting \u2014 we'll see how in the power analysis section below.\n* **blocking**: group experimental units by a known source of variability to reduce noise. if you know that batches, days, or operators introduce variability, account for it by design rather than hoping it averages out. think of it as neutralizing a nuisance variable before it can contaminate your results.\n\n## completely randomized design (crd)\n\nthe simplest design assigns all experimental units to treatments purely at random. the crd is appropriate when units are homogeneous and no blocking variable is identified.\n\nthe model is identical to one-way [anova](./anova):\n\n$$\ny_{ij} = \\mu + \\tau_i + \\varepsilon_{ij},\n$$\n\nwhere $\\tau_i$ is the treatment effect.\n\n**advantages**: simple to implement and analyze.\n**limitation**: if units vary substantially (different ages, batches, instruments), the within-group variance is inflated and power drops. when you suspect heterogeneity, don't ignore it \u2014 block on it.\n\n## randomized block design (rbd)\n\nwhen a nuisance variable is known (batch, day, subject), blocking removes its effect from the error term:\n\n$$\ny_{ij} = \\mu + \\tau_i + \\beta_j + \\varepsilon_{ij},\n$$\n\nwhere $\\beta_j$ is the block effect. each treatment appears exactly once in each block.\n\n[[simulation randomization-vs-blocking]]\n\n**advantage**: removes block-to-block variability from the error term, increasing the f-statistic for the treatment effect. the signal stays the same but the noise goes down.\n\nthe **relative efficiency** of blocking compares the precision of rbd to crd:\n\n$$\n\\text{re} = \\frac{\\text{ms}_{\\text{blocks}} + (b-1)\\,\\text{ms}_{\\text{error,rbd}}}{b\\,\\text{ms}_{\\text{error,rbd}}},\n$$\n\nwhere $b$ is the number of blocks. values greater than 1 mean blocking was beneficial. in practice, blocking almost always helps \u2014 it rarely hurts and often improves power substantially.\n\n**practical tip**: when in doubt, block. if the blocking variable turns out to be unimportant, you lose very little (just one degree of freedom per block). if it *is* important and you failed to block, you lose much more.\n\n## power and sample size\n\nbefore starting an experiment, you should answer the most practical question of all: *how many observations do i need?*\n\n### the \"how many patients do i really need?\" story\n\nalex is designing a clinical trial. the new drug is expected to lower blood pressure by about 5 mmhg compared to the control. alex knows from past studies that blood pressure measurements have a standard deviation of about 12 mmhg. how many patients per group does alex need?\n\nthe answer depends on four quantities that form a connected system \u2014 changing one affects the others.\n\n**statistical power** is the probability of correctly rejecting $h_0$ when a true effect exists:\n\n$$\n\\text{power} = 1 - \\beta = p(\\text{reject } h_0 \\mid h_1 \\text{ true}).\n$$\n\npower depends on:\n\n* **effect size** ($\\delta$): the magnitude of the difference you want to detect. smaller effects need more data. alex's 5 mmhg is the effect size.\n* **sample size** ($n$): more observations increase power. this is what alex wants to find.\n* **significance level** ($\\alpha$): relaxing $\\alpha$ increases power at the cost of more false positives.\n* **variability** ($\\sigma$): less noise increases power. this is where good experimental design (blocking, precise instruments) pays off. alex's $\\sigma = 12$ mmhg is the noise.\n\nfor a two-sample t-test, the required sample size per group to achieve power $1 - \\beta$ at level $\\alpha$ for detecting a difference $\\delta$ is approximately:\n\n$$\nn \\approx \\frac{2(z_{\\alpha/2} + z_\\beta)^2 \\sigma^2}{\\delta^2}.\n$$\n\nplugging in alex's numbers ($\\delta = 5$, $\\sigma = 12$, $\\alpha = 0.05$, power $= 0.80$): $n \\approx 2(1.96 + 0.84)^2 \\times 144 / 25 \\approx 90$ patients per group. that's a real number alex can take to the funding agency.\n\n**cohen's conventions** for effect size $d = \\delta/\\sigma$: small ($d = 0.2$), medium ($d = 0.5$), large ($d = 0.8$). these are rough benchmarks \u2014 always think about what effect size is scientifically meaningful in your specific context rather than blindly adopting conventions.\n\n### practical tips for power analysis\n\n* always determine sample size *before* starting the experiment. running until you get a significant result is a recipe for false positives.\n* use **pilot studies** to estimate $\\sigma$ when it is unknown. even a small pilot (10-20 observations) gives a rough variance estimate that makes your power calculation much more reliable than guessing.\n* **pre-register** the analysis plan to avoid p-hacking \u2014 the temptation to try many analyses and report only the significant ones.\n* consider **multiple testing corrections** when evaluating many endpoints. the more tests you run, the more likely one "
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "error-propagation",
      "lessonTitle": "Central Limit Theorem and Error Propagation",
      "x": 0.4522853195667267,
      "y": 0.7320162653923035,
      "searchText": "central limit theorem and error propagation\n# central limit theorem and error propagation\n\n## the drunkard's walk to gaussian\n\nimagine a drunkard stumbling home from a bar. each step is random \u2014 left, right, long, short, forward, backward. after one step, the drunkard could be anywhere. after ten steps, the position is hard to predict. but after a *thousand* steps? something remarkable happens: if you repeated this walk many times, the final positions would form a bell curve. every time. no matter how erratic each individual step is.\n\nthat is the **central limit theorem** (clt) in action. take $n$ independent samples from *any* distribution (as long as it has finite variance), compute their mean, and that mean will be approximately gaussian. the more samples you average, the better the approximation. the original distribution can be uniform, exponential, bimodal, or anything else \u2014 the average still converges to a bell curve.\n\n[[simulation applied-stats-sim-3]]\n\nthis is why gaussian error analysis works almost everywhere. your measurement is typically the result of many small, independent influences added together. each influence might follow some complicated distribution, but their sum \u2014 your measurement \u2014 will be gaussian. the clt is the license that lets you use gaussian statistics.\n\nbut there are caveats. for the clt to work well, the contributing distributions should have similar standard deviations. if one source of variation dwarfs all others, the sum looks like that dominant source, not a gaussian. and distributions with very heavy tails (like the cauchy distribution, which has no finite variance) never converge \u2014 no amount of averaging tames them.\n\n**the practical rule**: if each contribution has finite variance and no single contribution dominates, the sum is approximately gaussian. this is the foundation on which the rest of the course builds.\n\n> **challenge.** roll a single die and note the result. now roll five dice and compute the average. do this twenty times. plot your twenty averages on a number line. you'll see them clustering around 3.5 in a roughly bell-shaped pattern \u2014 even though a single die roll is perfectly uniform. that's the clt at work.\n\n## error propagation\n\nnow suppose you've measured some input quantities $x_i$, each with uncertainty $\\sigma(x_i)$, and you compute a derived quantity $y(x_i)$. what is the uncertainty on $y$?\n\nthe intuition is simple: a small wiggle in the input causes a wiggle in the output. the size of that output wiggle depends on how steeply $y$ changes with respect to $x$. if $y$ varies gently, input errors stay small. if $y$ varies steeply, input errors get amplified.\n\nthink of it this way. you measure the radius of a circle with some uncertainty, and you want the area $a = \\pi r^2$. the derivative $da/dr = 2\\pi r$ tells you that a 1% error in $r$ becomes roughly a 2% error in $a$ \u2014 the squaring amplifies it. for the volume of a sphere ($v = \\frac{4}{3}\\pi r^3$), the same 1% error in $r$ becomes a 3% error in $v$. the steeper the function, the bigger the amplification.\n\nformally, for a function of one variable:\n\n$$\n\\sigma(y) = \\frac{\\partial y}{\\partial x}\\sigma(x_i)\n$$\n\nthis works when $y$ is smooth around $x_i$ \u2014 when the slope is roughly constant over the uncertainty range.\n\nfor multiple variables with correlations, the general formula uses the covariance matrix $v_{ij}$ (which we met in [introduction and concepts](./introduction-concepts)):\n\n$$\n\\sigma_y^2 = \\sum_{i,j}^n \\frac{\\partial y}{\\partial x_i} \\frac{\\partial y}{\\partial x_j} v_{ij}\n$$\n\nif there are no correlations between the inputs, only the diagonal terms survive \u2014 each input contributes independently to the total uncertainty. this lets you identify which measurement dominates the error budget and focus your effort on improving *that* one.\n\n### addition\n\n$$\ny = x_1 + x_2 \\implies \\sigma_y^2 = \\sigma_{x_1}^2 + \\sigma_{x_2}^2 + 2v_{x_1, x_2}\n$$\n\nerrors add in quadrature (when uncorrelated). this is why combining independent measurements always improves precision.\n\n### multiplication\n\n$$\ny = x_1 x_2 \\implies \\sigma_y^2 = (x_2\\sigma_{x_1})^2 + (x_1\\sigma_{x_2})^2 + 2x_1 x_2 v_{x_1, x_2}\n$$\n\ndividing by $y^2$ gives the relative uncertainties. here's a beautiful consequence: by engineering *negative* error correlations, you can make errors partially cancel.\n\nalex encounters this trick in the lab. harrison's gridiron pendulum uses two metals with different thermal expansion coefficients \u2014 brass and steel \u2014 arranged so that when temperature rises, the brass rods push the pendulum bob *down* while the steel rods push it *up*. the expansions partially cancel, keeping the pendulum length stable. the key insight: the two metals expand in the same direction physically, but their *contributions to the total length* have opposite signs. harrison exploited the covariance term $v_{x_1, x_2}$ to engineer a pendulum that barely changes length with temperature \u2014 a mechanical implementation of error cancellation, two centuries before anyone wrote down the formula.\n\n### when analytical propagation fails\n\nthe formulas above assume $y(x)$ is smooth and approximately linear over the uncertainty range. when that breaks down \u2014 sharp thresholds, discontinuities, highly nonlinear functions \u2014 you need a different approach: **simulation**.\n\nchoose random inputs $x_i$ from their error distributions, compute $y$ for each draw, and look at the resulting spread in $y$. this monte carlo approach (developed fully in the next section) handles arbitrary functions, non-gaussian inputs, and complex correlations. if $y(x)$ is not smooth, the output distribution may not be gaussian even when the inputs are \u2014 the simulation reveals this automatically.\n\n[[simulation applied-stats-sim-8]]\n\n> **challenge.** explain error propagation to a friend using only the example of measuring a room's area from its length and width. no equations. one minute.\n\n## big ideas\n\n* the central limit theorem is the reason gaussian error an"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "hypothesis-testing",
      "lessonTitle": "Hypothesis Testing and Limits",
      "x": 0.5039589405059814,
      "y": 0.5985611081123352,
      "searchText": "hypothesis testing and limits\n# hypothesis testing and limits\n\n## the logic of hypothesis testing\n\nthink of hypothesis testing as a courtroom trial. the defendant (your null hypothesis) is presumed innocent until proven guilty. you gather evidence (data), and if the evidence is overwhelming enough, you reject the presumption of innocence. notice the asymmetry: you never *prove* innocence \u2014 you either find enough evidence to convict, or you don't.\n\nthe **null hypothesis** $h_0$ is the boring explanation \u2014 nothing interesting is happening, there is no effect, the drug doesn't work. the **alternative hypothesis** $h_1$ says something real is going on. your job is to ask: if $h_0$ were true, how surprising would my data be?\n\nthe answer is the **p-value**. and here is where most people get confused, so let's be precise.\n\nthe p-value is *not* the probability that your hypothesis is true. it's the probability you'd see data this extreme *if your hypothesis were true*. that's a huge difference \u2014 like the difference between \"the suspect is probably guilty\" and \"an innocent person would almost never look this suspicious.\" the first is a statement about the suspect. the second is a statement about innocent people. they sound similar. they are not the same thing.\n\nthink of it this way: a p-value is like the probability of being dealt a royal flush given that you are not cheating \u2014 not the probability that you are cheating.\n\na small p-value means the data would be very surprising under $h_0$, so you reject it. the threshold for \"surprising enough\" is the **significance level** $\\alpha$ (typically 0.05). you can perform **one-tailed** tests (\"is the effect in *this* direction?\") or **two-tailed** tests (\"is there *any* effect at all?\").\n\n## the testing toolkit\n\nwith the logic in place, here's a toolkit of specific tests, each suited to different situations.\n\n### one-sample z test\n\nthe simplest test, but rarely used in practice because it requires knowing the population standard deviation \u2014 which you almost never do. still, it illustrates the general framework clearly:\n\n1. state the null hypothesis: $h_0: \\mu = 100$.\n2. state the alternative: $h_1: \\mu > 100$.\n3. choose a significance level: $\\alpha = 0.05$.\n4. find the rejection region. an area of 0.05 corresponds to $z = 1.645$.\n5. calculate:\n\n$$\nz = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\n$$\n\n6. if $z$ exceeds the critical value, reject $h_0$.\n\nevery test in this section follows this same skeleton. learn it once, and you can apply it everywhere.\n\n### student's t-test\n\nin practice, you estimate the standard deviation from the data itself, which introduces extra uncertainty. the **t-test** accounts for this by using the heavier-tailed $t$-distribution (from [probability density functions](./probability-density-functions)) instead of the gaussian.\n\nthere are two flavors:\n\n* **one-sample t-test**: is this batch of lightbulbs lasting the claimed 1000 hours? compares the mean of a sample to a known value.\n* **two-sample t-test**: do patients on drug a recover faster than patients on drug b? compares the means of two independent groups.\n\nthe test assumes approximately normal data and (for the two-sample version) equal variances. the test statistic is the difference between the sample mean and the hypothesized mean, divided by the standard error, compared against a $t$-distribution with the appropriate degrees of freedom.\n\n[[simulation applied-stats-sim-4]]\n\n### non-parametric tests\n\nwhat if your data is clearly not normal? non-parametric tests make fewer assumptions about the underlying distribution. when the assumptions fail, you stop trusting means and start trusting ranks \u2014 nature doesn't care about your normality test.\n\nthe **kolmogorov-smirnov test** (k-s test) compares a sample with a reference distribution (one-sample) or compares two samples (two-sample). it works by measuring the maximum distance between the cumulative distribution functions.\n\nthe **runs test** checks whether a sequence of two-valued data is random. given a sequence like:\n\n> $+ + + + - - - + + + - - + + + + + + - - - -$\n\nit counts the number of \"runs\" (consecutive sequences of the same value). too few runs suggest clustering; too many suggest alternation. either way, the sequence is not random.\n\n[[simulation residual-pattern]]\n\n## comparing models: the likelihood ratio test\n\nthe tests above compare data to a single hypothesis. but often you want to compare two competing models: does adding an extra parameter significantly improve the fit? this is where the **likelihood ratio test** connects hypothesis testing to the [chi-square framework](./chi-square-method).\n\nthe test statistic is:\n\n$$\nd = -2\\ln\\frac{\\mathcal{l}_\\text{null}}{\\mathcal{l}_\\text{alt}} = -2\\ln(\\mathcal{l}_\\text{null}) + 2\\ln(\\mathcal{l}_\\text{alt})\n$$\n\nunder $h_0$, this follows a $\\chi^2$ distribution with degrees of freedom equal to the difference in the number of parameters. a model with more parameters will *always* fit better (or at least as well), so the test asks: does the improvement justify the added complexity?\n\n## confidence intervals: ranges of belief\n\na p-value gives a binary answer: reject or not. a **confidence interval** gives something richer \u2014 a range of plausible values for the parameter.\n\na 95% confidence interval means: if you repeated the entire experiment many times, 95% of the intervals you compute would contain the true parameter. it is *not* a statement about the probability that the true value lies in this particular interval \u2014 that's the bayesian interpretation, which we'll see in [bayesian statistics](./bayesian-statistics).\n\nthe interval is constructed from the point estimate, the standard error, and the desired confidence level. wider intervals give more confidence but less precision \u2014 there is always a trade-off.\n\nconfidence intervals connect directly to hypothesis testing: if a hypothesized value falls outside the 95% confidence interval, you would reject it at the 5% significance l"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "introduction-concepts",
      "lessonTitle": "Introduction and General Concepts",
      "x": 0.43472206592559814,
      "y": 0.7085909843444824,
      "searchText": "introduction and general concepts\n# introduction and general concepts\n\nyou have a pile of numbers. maybe they're pendulum timings, patient blood pressures, or photon counts from a distant star. before you build any model or run any test, you need to answer two questions: *where* does the data cluster, and *how much does it scatter?*\n\nthese sound simple. they aren't. the \"center\" of a dataset is not a single concept, and neither is \"spread.\" different situations demand different summaries. picking the wrong one can lead you astray before you've even started your analysis.\n\nalex, our experimentalist, has just measured the gravitational acceleration ten times and gotten values between 9.78 and 9.84 m/s$^2$. what should alex report as \"the\" value? and how uncertain should alex be? that's what this section is about.\n\n## measures of central tendency\n\nthe \"center\" of a dataset depends on what question you're asking. here is the menu, ordered from most familiar to most specialized.\n\n### arithmetic mean\n\n[[simulation which-average]]\n\nthe most familiar average. you add up all the values and divide by how many there are:\n\n$$\n\\hat{\\mu} = \\bar{x} = \\langle x \\rangle = \\frac{1}{n}\\sum_i^n x_i\n$$\n\n```python\ndef arithmetic_mean(arr):\n    return np.sum(arr) / len(arr)\n    # or equivalently: np.mean(arr)\n```\n\nthe arithmetic mean is sensitive to extreme values \u2014 a single outlier can drag it far from where most of the data sits. if alex's ten measurements include one wild reading of 10.5 (a bumped table, perhaps), the mean shifts noticeably. that's not always what you want.\n\n### geometric mean\n\nwhen your data are multiplicative in nature (growth rates, ratios, concentrations spanning orders of magnitude), the geometric mean is more appropriate. it is the $n$-th root of the product:\n\n$$\n\\bar{x}_\\text{geo} = \\left( \\prod_i^n x_i\\right)^{1/n} = \\exp\\left(\\frac{1}{n}\\sum_{i}^n\\ln x_i \\right)\n$$\n\nthis is equivalent to taking the arithmetic mean in log-space, which is why it works well for data that are log-normally distributed. if your data span several orders of magnitude, the geometric mean lives where the data actually cluster \u2014 not where the arithmetic mean gets dragged by the largest values.\n\n```python\ndef geometric_mean(arr):\n    # a sum of logs is less prone to\n    # under-/over-flow than a product.\n    return np.exp(np.mean(np.log(arr)))\n```\n\n### median\n\nthe middle value when the data are sorted. unlike the mean, the median is robust to outliers \u2014 even wild values at the extremes do not shift it much. if alex had that one bumped-table reading of 10.5, the median wouldn't flinch.\n\n```python\ndef median(arr):\n    s = np.sort(arr)\n    n = len(s)\n    if n % 2 == 1:\n        return s[n // 2]\n    return (s[n // 2 - 1] + s[n // 2]) / 2\n```\n\n### mode\n\nthe most frequently occurring value. for continuous data, the mode is estimated from a histogram or kernel density estimate. it tells you where the data piles up the most \u2014 useful when your distribution has a clear peak.\n\n### harmonic mean\n\n$$\n\\bar{x}_\\text{harm} = \\left[\\frac{1}{n}\\sum_i^n \\frac{1}{x_i}\\right]^{-1}\n$$\n\nthe harmonic mean is the right choice when you're averaging *rates*. suppose you drive 60 km/h for the first half of a trip and 40 km/h for the second half. the harmonic mean (48 km/h) gives the correct average speed, not the arithmetic mean (50 km/h). the difference matters.\n\n```python\ndef harmonic_mean(x):\n    return (np.sum(x**(-1)) / len(x))**(-1)\n```\n\n### truncated mean\n\na compromise between the sensitivity of the mean and the robustness of the median: compute the arithmetic mean after throwing away the most extreme values on both ends.\n\n```python\ndef truncated_mean(arr, k):\n    arr_sorted = np.sort(arr)\n    return np.mean(arr_sorted[k:-k])\n```\n\n> **challenge.** take ten measurements of anything \u2014 your walking time to class, the temperature outside, the number of words per sentence in a book. compute the mean and the median. are they close? if not, you probably have outliers or a skewed distribution. that's already telling you something about the data.\n\n## measures of spread\n\nknowing the center is only half the story. two datasets can have the same mean but look completely different \u2014 one tightly clustered, the other scattered across a huge range. you need a number that captures *how much the data wanders*.\n\n### standard deviation\n\nstandard deviation measures how much data points typically deviate from the mean. think of it as the \"average distance\" from the center:\n\n$$\n\\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_i^n (x_i - \\mu)^2}\n$$\n\nthis formula assumes you know the true mean $\\mu$. in practice, you use the sample mean $\\bar{x}$, which introduces a subtle bias. here is the surprising thing: the sample mean is always a little closer to the data than the true mean is, so the raw formula systematically underestimates the spread. **bessel's correction** compensates by dividing by $n-1$ instead of $n$, accounting for the lost degree of freedom:\n\n$$\n\\tilde{\\sigma} = \\sqrt{\\frac{1}{n-1}\\sum_i(x_i-\\bar{x})^2}\n$$\n\n### weighted mean\n\nwhen measurements come with different uncertainties, you should trust the precise ones more. the weighted mean does exactly this \u2014 measurements with smaller error bars get larger weights.\n\nimagine each measurement standing on a seesaw. a precise measurement (small $\\sigma$) is a heavy person near the fulcrum \u2014 it has more say in where the balance point lands. the weighted mean is simply where the seesaw balances.\n\n$$\n\\hat{\\mu} = \\frac{\\sum x_i / \\sigma_i^2}{\\sum 1 / \\sigma_i^2}, \\qquad \\hat{\\sigma}_\\mu = \\sqrt{\\frac{1}{\\sum 1/\\sigma_i^2}}\n$$\n\nthe uncertainty on the weighted mean decreases with the number of samples:\n\n$$\n\\hat{\\sigma}_\\mu = \\hat{\\sigma}/\\sqrt{n}.\n$$\n\n[[simulation weighted-mean]]\n\nalex has three measurements of the same length: $10.2 \\pm 0.1$, $10.5 \\pm 0.5$, and $10.1 \\pm 0.2$ cm. the weighted mean pulls toward 10.2, because that measurement is the most precise. this is your first taste of a powerful idea: *let the "
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "longitudinal-data",
      "lessonTitle": "Longitudinal Data and Repeated Measures",
      "x": 0.36466801166534424,
      "y": 0.614208459854126,
      "searchText": "longitudinal data and repeated measures\n# longitudinal data and repeated measures\n\n## what makes longitudinal data special\n\nin the previous section, you handled grouped data where measurements within a cluster are correlated. remember those random effects \u2014 the random intercepts and the icc? here's why they save you in real life.\n\n**longitudinal data** is a specific and important case: the same subjects are measured repeatedly *over time*. suppose you follow 200 patients for five years, measuring their blood pressure every six months. some drop out. some miss visits. now what?\n\nthis creates two fundamental challenges:\n\n* **within-subject correlation**: measurements from the same person are not independent. a patient with high blood pressure in january will probably still have high blood pressure in march.\n* **time structure**: the spacing and ordering of observations carry information. a measurement today is more correlated with yesterday's than with last month's.\n\nignoring these features and applying standard methods treats each measurement as if it came from a different person \u2014 a mistake that produces incorrect standard errors and misleading p-values.\n\npicture one patient's trajectory over time \u2014 their blood pressure goes up, dips, then rises again. that individual curve deviates from the population average in a way that's consistent and personal. the mixed model captures exactly this: the population trend plus each person's individual deviation from it.\n\n## repeated measures anova\n\nthe classical approach extends anova to handle within-subject factors. for a single within-subject factor with $k$ time points:\n\n$$\ny_{ij} = \\mu + \\pi_i + \\tau_j + \\varepsilon_{ij},\n$$\n\nwhere $\\pi_i$ is the subject effect and $\\tau_j$ is the time effect.\n\n**sphericity assumption**: the variances of all pairwise differences between time points must be equal. in plain terms, the change from time 1 to time 2 should be as variable as the change from time 2 to time 3. mauchly's test checks this. when sphericity is violated (which is common):\n\n* **greenhouse-geisser correction**: reduces the degrees of freedom to control type i error. conservative \u2014 it may miss real effects.\n* **huynh-feldt correction**: less conservative alternative.\n\n**limitation**: repeated measures anova requires complete data (no missing time points) and equally spaced measurements. in practice, subjects drop out, miss visits, or show up on irregular schedules. this is where mixed models take over.\n\n## linear mixed models for longitudinal data\n\n**linear mixed models** (lmms) overcome these limitations by modeling each subject's trajectory directly. this builds directly on the mixed model framework from the previous section \u2014 the random intercept model you already know, now with the time dimension added:\n\n$$\ny_{ij} = \\beta_0 + \\beta_1 t_{ij} + u_{0i} + u_{1i} t_{ij} + \\varepsilon_{ij},\n$$\n\nwhere $u_{0i}$ is a **random intercept** (each subject starts at a different level) and $u_{1i}$ is a **random slope** (each subject changes at a different rate).\n\nthe random effects are assumed multivariate normal:\n\n$$\n\\begin{pmatrix} u_{0i} \\\\ u_{1i} \\end{pmatrix} \\sim \\mathcal{n}\\!\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_0^2 & \\rho\\sigma_0\\sigma_1 \\\\ \\rho\\sigma_0\\sigma_1 & \\sigma_1^2 \\end{pmatrix}\\right).\n$$\n\n**advantages over repeated measures anova**:\n\n* handles missing data naturally (uses all available observations, not just complete cases).\n* accommodates unequally spaced time points.\n* models individual trajectories, not just group means.\n* allows complex correlation structures.\n\n[[simulation spaghetti-trajectory]]\n\n## growth curve models\n\nwhen the outcome follows a nonlinear trajectory over time \u2014 growth spurts, learning curves, disease progression \u2014 polynomial or nonlinear growth curves can be fit within the mixed-model framework:\n\n$$\ny_{ij} = \\beta_0 + \\beta_1 t_{ij} + \\beta_2 t_{ij}^2 + u_{0i} + u_{1i} t_{ij} + \\varepsilon_{ij}.\n$$\n\nthe fixed effects $(\\beta_0, \\beta_1, \\beta_2)$ describe the population-average trajectory \u2014 the \"typical\" curve. the random effects $(u_{0i}, u_{1i})$ capture how each individual deviates from that average. some patients improve faster. some start higher but plateau earlier. the model captures all of this.\n\n## autocorrelation\n\nhere's an everyday example. if today is warm, tomorrow is probably warm too. weather doesn't jump randomly from day to day \u2014 it has *memory*. the same is true of many measurement sequences: blood pressure readings, reaction times across trials, stock prices.\n\n**autocorrelation** quantifies how a signal correlates with itself at different time lags:\n\n$$\nr(\\tau) = \\frac{1}{n} \\sum_{t=1}^{n-\\tau} (x_t - \\bar{x})(x_{t+\\tau} - \\bar{x}).\n$$\n\nin longitudinal data, residuals from adjacent time points are often more correlated than residuals from distant ones. the mixed model handles this through the correlation structure of the errors. here's the menu of choices, ordered from simplest to most flexible \u2014 and here's when each one fails:\n\n* **compound symmetry**: constant correlation between all pairs. equivalent to a random intercept. simple, but assumes that measurements 1 day apart are equally correlated as measurements 1 year apart. fails when correlation decays over time.\n* **ar(1)**: correlation decays exponentially with time lag: $\\text{cor}(\\varepsilon_{ij}, \\varepsilon_{ik}) = \\phi^{|j-k|}$. often the most realistic choice for equally-spaced data. fails for irregularly spaced observations.\n* **unstructured**: separate correlation for each pair. the most flexible but parameter-intensive \u2014 only feasible when the number of time points is small (say, fewer than 10). fails by overfitting when time points are many.\n\nmodel selection among correlation structures uses **aic** or **bic** (the information criteria we'll encounter again in [advanced fitting](./advanced-fitting-calibration)).\n\n## handling missing data\n\nsuppose you follow patients for five years. some drop out. som"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "machine-learning-data-analysis",
      "lessonTitle": "Machine Learning and Data Analysis",
      "x": 0.8516070246696472,
      "y": 0.7099292874336243,
      "searchText": "machine learning and data analysis\n# machine learning and data analysis\n\n## the grand finale\n\nmachine learning is just statistics with really good marketing \u2014 and now you know all the tricks inside the black box.\n\nseriously. logistic regression? that's maximum likelihood estimation ([pdfs](./probability-density-functions)) applied to classification. regularization? the frequentist cousin of bayesian priors ([bayesian statistics](./bayesian-statistics)). cross-validation? a practical implementation of the model comparison ideas from [advanced fitting](./advanced-fitting-calibration). the fisher discriminant? [anova](./anova)'s variance decomposition in disguise. everything you've learned \u2014 likelihoods, priors, uncertainty quantification, model selection, error propagation \u2014 is secretly running inside every modern ml algorithm.\n\nthis section makes those connections explicit and introduces the algorithms that have become essential tools in experimental physics and data analysis. ml methods complement traditional techniques for three core tasks: **classification** (separating signal from background), **regression** (predicting continuous quantities), and **clustering** (discovering structure in unlabelled data).\n\nml algorithms fall into two broad categories. **supervised learning** trains on labelled examples and predicts labels for new data. **unsupervised learning** finds structure without labels.\n\n## supervised learning: classification\n\ngiven training data $\\{(\\mathbf{x}_i, y_i)\\}$ where $y_i \\in \\{0, 1\\}$ labels signal vs. background, a classifier learns a decision boundary in feature space. this is the ml version of the [hypothesis testing](./hypothesis-testing) problem: is this event signal or background?\n\n### logistic regression\n\n**logistic regression** models the probability of the positive class as:\n\n$$\np(y=1 | \\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{w}^t \\mathbf{x} - b}},\n$$\n\nwhere $\\mathbf{w}$ and $b$ are learned by maximizing the likelihood \u2014 the same mle principle from [probability density functions](./probability-density-functions), just applied to a different model. despite its simplicity, logistic regression is effective when classes are approximately linearly separable and serves as a useful baseline before trying more complex methods.\n\n### decision trees and ensembles\n\n**decision trees** recursively partition feature space by selecting the feature and threshold that best separates classes at each node. they are interpretable \u2014 you can read off the decision rules \u2014 but prone to overfitting. they memorize noise in the training data.\n\ntwo ensemble strategies fix this:\n\n* **random forests** reduce overfitting by averaging predictions from many trees, each trained on a bootstrap sample with a random subset of features. the averaging smooths out the noise that individual trees memorize.\n* **boosted decision trees** (bdt) build an ensemble sequentially, with each new tree focusing on the examples the previous ones got wrong. gradient boosting (e.g., xgboost) is among the most powerful classifiers for tabular data and is widely used in particle physics for event selection.\n\n## the fisher linear discriminant\n\nthe **fisher discriminant** finds the single linear combination of features that best separates two classes. it connects directly to the variance decomposition from [anova](./anova): just as anova separates between-group from within-group variance, fisher finds the direction that maximizes the ratio of between-class to within-class scatter.\n\nthe discriminant direction is:\n\n$$\n\\mathbf{w} = s^{-1}(\\boldsymbol{\\mu}_0 - \\boldsymbol{\\mu}_1)\n$$\n\nwhere $\\boldsymbol{\\mu}_0$ and $\\boldsymbol{\\mu}_1$ are the class means and $s$ is the **pooled within-class covariance matrix**:\n\n$$\ns = \\frac{1}{n}(s_0 + s_1)\n$$\n\na new observation $\\mathbf{x}$ is classified by projecting onto $\\mathbf{w}$ and comparing to a threshold. this method assumes normally distributed classes with equal covariance matrices \u2014 the same assumptions that underlie anova. when those assumptions are violated, quadratic discriminant analysis or nonlinear classifiers should be used.\n\n## supervised learning: regression\n\nfor predicting continuous targets, the same algorithms adapt. linear regression minimizes squared residuals (the [chi-square method](./chi-square-method) without the weighting by uncertainties). **ridge** and **lasso** regression add penalty terms:\n\n$$\n\\hat{\\mathbf{w}} = \\arg\\min_{\\mathbf{w}} \\sum_i (y_i - \\mathbf{w}^t\\mathbf{x}_i)^2 + \\lambda \\|\\mathbf{w}\\|_p^p.\n$$\n\nthese penalties serve the same purpose as the nuisance parameter constraints in [advanced fitting](./advanced-fitting-calibration) \u2014 they prevent the model from over-adapting to noise:\n\n* **lasso** ($p=1$) performs automatic feature selection by driving irrelevant coefficients to exactly zero. useful when you suspect many features are unimportant.\n* **ridge** ($p=2$) shrinks all coefficients toward zero without eliminating any. better when features are correlated.\n\nthe penalty $\\lambda \\|\\mathbf{w}\\|^2$ is mathematically identical to placing a gaussian prior on $\\mathbf{w}$ and doing map estimation ([bayesian statistics](./bayesian-statistics)). regularization *is* bayesian inference. the \"regularization strength\" $\\lambda$ is the prior's precision. this isn't just an analogy \u2014 it's an exact mathematical equivalence.\n\n## neural networks\n\na feedforward neural network with $l$ layers computes:\n\n$$\n\\mathbf{h}^{(l)} = \\sigma\\bigl(w^{(l)} \\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)}\\bigr), \\quad l = 1, \\ldots, l,\n$$\n\nwhere $\\sigma$ is a nonlinear activation function (relu, sigmoid, or tanh) and $\\mathbf{h}^{(0)} = \\mathbf{x}$. the parameters are optimized by **backpropagation** using gradient descent on a loss function.\n\nneural networks can approximate any continuous function (universal approximation theorem) and excel when feature engineering is difficult \u2014 when you don't know in advance which combinations of inputs matter. the price is that they require larg"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "probability-density-functions",
      "lessonTitle": "Probability Density Functions",
      "x": 0.5597160458564758,
      "y": 0.6338639855384827,
      "searchText": "probability density functions\n# probability density functions\n\nin the previous section you summarized data that was already in hand \u2014 means, spreads, correlations. now you ask a deeper question: what *process* generated that data? if you can describe the underlying mechanism with a mathematical function, you unlock the ability to predict, extrapolate, and quantify how uncertain you really are. that mathematical function is the **probability density function**.\n\n## what is a pdf?\n\na **probability density function** describes a continuous random variable. the pdf itself is not a probability \u2014 it is a *density*. think of it like mass density: the density of steel tells you how heavy a chunk will be, but the density *itself* isn't a mass. you need to integrate over a region to get an actual probability:\n\n$$\np(a \\leq x \\leq b) = \\int_{a}^{b} f(x) \\, dx\n$$\n\nthe **cumulative distribution function** (cdf) accumulates probability from $-\\infty$:\n\n$$\nf_x(x) = \\int_{-\\infty}^{x} f(x') \\, dx'.\n$$\n\nthe cdf is always non-decreasing, starts at 0, and ends at 1. it answers a simple question: what fraction of outcomes fall below $x$?\n\n## common distributions\n\ndifferent physical processes give rise to different distributions. here are the ones you'll encounter most often, organized from discrete counting processes to the continuous workhorse of statistics. each one arises naturally from a specific kind of situation \u2014 learn the situation, and the distribution follows.\n\n### binomial\n\nsuppose you flip a coin $n$ times, each with probability $p$ of landing heads. how many heads should you expect? the binomial distribution answers this:\n\n$$\n\\begin{aligned}\nf(n;n,p) &= \\frac{n!}{n!(n-n)!}p^n(1-p)^{n-n}\\\\\n\\langle f(n;n,p)\\rangle &= np \\\\\n\\sigma^2 &= np(1-p)\n\\end{aligned}\n$$\n\nthis is a **discrete** distribution \u2014 it counts whole events (0 heads, 1 head, 2 heads, ...). any time you have a fixed number of independent yes/no trials, each with the same probability, the binomial is your distribution.\n\n### poisson\n\nnow imagine you have a huge number of trials ($n\\rightarrow \\infty$), each with a tiny probability of success ($p\\rightarrow 0$), but the *expected count* stays finite ($np\\rightarrow\\lambda$). the binomial simplifies to the **poisson** distribution:\n\n$$\nf(n, \\lambda) = \\frac{\\lambda^n}{n!}e^{-\\lambda}\n$$\n\n**example**: a large number of people go into traffic every day ($n\\rightarrow\\infty$), the probability of any one person being killed is tiny ($p\\rightarrow 0$), but some number of fatalities occur every year ($\\lambda\\neq 0$).\n\n[[simulation poisson-to-gaussian]]\n\nthe poisson distribution has a remarkable property: its mean and variance are both equal to $\\lambda$. this means the error on a poisson count is simply the square root of that count \u2014 a fact you'll use constantly when working with histograms. if a histogram bin contains $n$ events, its statistical uncertainty is $\\sqrt{n}$, provided the count is large enough ($n \\gtrsim 5{-}20$) for the gaussian approximation to hold.\n\nthe sum of independent poissons is itself a poisson: $\\lambda = \\lambda_a + \\lambda_b$. and when $\\lambda$ is large ($\\lambda \\gtrsim 20$), the poisson approaches a gaussian. nature keeps converging on that bell curve.\n\n### gaussian\n\nthe **normal distribution** is the central character of statistics:\n\n$$\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right]\n$$\n\nwhy does it appear everywhere? the central limit theorem (covered in the next section) gives the answer: whenever you average many independent contributions, the result tends toward a gaussian regardless of what the individual contributions look like. this is why measurement errors, which arise from the sum of many small disturbances, are almost always gaussian. it is nature's favorite trick.\n\n[[simulation applied-stats-sim-2]]\n\n### student's t-distribution\n\nthe gaussian works beautifully when you have plenty of data. but with small samples, the estimated mean and standard deviation are themselves uncertain, and the gaussian underestimates the tails. you might think the gaussian is always good enough \u2014 but actually, for small samples, the tails matter a lot.\n\nthe **student's t-distribution** accounts for this extra uncertainty by having heavier tails. as the sample size grows, the $t$-distribution converges to the gaussian. for **low statistics** work, always prefer the $t$-distribution.\n\n## maximum likelihood estimation\n\nyou've now met the major distributions. a natural question follows: given observed data, how do you figure out *which* distribution (and which parameter values) generated it?\n\nthink of it as a detective story. the data are the clues left at the scene. the parameters are the culprit. your job is to ask: which culprit makes these clues least surprising?\n\n### the detective's question\n\nsuppose you flip a coin 10 times and get 7 heads. which value of $p$ (the probability of heads) makes this outcome most plausible?\n\nif $p = 0.5$, getting 7 heads is possible but not the most likely result. if $p = 0.7$, it is. if $p = 0.99$, getting *only* 7 heads is actually surprising \u2014 you'd expect more. so the answer is somewhere around 0.7. **maximum likelihood estimation** (mle) formalizes this intuition: find the parameter values that make the observed data as probable as possible.\n\n> **challenge.** grab a coin and flip it 10 times. write down how many heads you got. what value of $p$ would make your result least surprising? if you got 6 heads, the mle is $\\hat{p} = 0.6$. simple \u2014 but that simplicity hides a powerful principle.\n\n[[simulation likelihood-surface]]\n\nmathematically, the **likelihood** is the probability of the data as a function of the parameters $\\theta$:\n\n$$\n\\mathcal{l}(\\theta) = \\prod_i f(x_i; \\theta)\n$$\n\nthis is just the joint probability of all observations, but treated as a function of $\\theta$ rather than of the data. multiplying many small probabilities causes numerical problems, so in practice you maximiz"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "random-effects",
      "lessonTitle": "Random Effects and Mixed Models",
      "x": 0.3681173622608185,
      "y": 0.6072342395782471,
      "searchText": "random effects and mixed models\n# random effects and mixed models\n\n## fixed vs random effects\n\nimagine you're studying how a new teaching method affects math scores. you test it in 10 classrooms across 5 schools. the teaching method is a **fixed effect** \u2014 you chose it deliberately, and your conclusions apply specifically to it. but the classrooms and schools? you didn't pick *those* specific ones because they're special. they're a random sample from a larger population of classrooms and schools. that's a **random effect**.\n\n* **fixed effect**: the specific levels are of direct interest. inference applies only to those levels.\n* **random effect**: the levels are sampled from a population. inference generalizes to the entire population.\n\nthe distinction matters because it changes what you're estimating. with fixed effects, you estimate individual level means. with random effects, you estimate the *variance* across levels \u2014 how much variability exists in the population.\n\nthink of it this way: if you test a drug in several hospitals, the drug effect is fixed (you chose that drug) but the hospital effect is random (each hospital is one sample from a population of hospitals). different hospitals have different patient populations, equipment, and practices \u2014 this creates variability that is not about the drug itself but about the context. you need to account for it without treating each hospital as a separate, deliberate choice.\n\n## a running example: classrooms within schools\n\nlet's follow a concrete story. alex wants to know whether the new teaching method improves math scores. alex collects data from students in classrooms nested within schools. here's the problem: students in the same classroom tend to have more similar scores than students in different classrooms. maybe one teacher is more experienced. maybe one school has more resources. these similarities mean the observations are *not independent*.\n\nif alex ignores this grouping and runs a simple t-test, treating each student as an independent observation, the standard errors will be too small and the p-values will be too optimistic. alex will \"discover\" effects that aren't real. this is **pseudo-replication** \u2014 one of the most common mistakes in applied statistics.\n\nthe solution is a **mixed model**.\n\n## the mixed-effects model\n\na **mixed model** contains both fixed and random effects. the simplest version is a random-intercept model:\n\n$$\ny_{ij} = \\mu + \\beta x_{ij} + u_i + \\varepsilon_{ij},\n$$\n\nwhere $\\beta$ is a fixed effect (the teaching method), $u_i \\sim \\mathcal{n}(0, \\sigma_u^2)$ is a random intercept for group $i$ (each classroom starts at a different baseline level), and $\\varepsilon_{ij} \\sim \\mathcal{n}(0, \\sigma_\\varepsilon^2)$ is the residual error.\n\nthe random effect $u_i$ captures what makes each group different. in alex's study, $u_i$ captures the fact that some classrooms have systematically higher scores \u2014 perhaps due to the teacher, the student demographics, or the time of day the class meets. it's like accounting for family traits in a height study: members of the same family share genetic and environmental factors that make them more similar to each other than to random strangers.\n\n## intraclass correlation coefficient (icc)\n\nhow similar are observations within the same group compared to observations from different groups? the **icc** quantifies this:\n\n$$\n\\text{icc} = \\frac{\\sigma_u^2}{\\sigma_u^2 + \\sigma_\\varepsilon^2}.\n$$\n\n* icc $\\approx 0$: the grouping doesn't matter. observations within a group are no more similar than observations from different groups. you could ignore the structure and use standard methods.\n* icc $\\approx 1$: nearly all variability is between groups. within-group observations are very similar.\n\nthe icc is your diagnostic. when it is substantial (above 0.05-0.10), ignoring the grouping structure will produce incorrect standard errors and misleading p-values. this is where mixed models become necessary, not optional.\n\nin alex's study, suppose the icc is 0.15. that means 15% of the total variability in math scores comes from classroom-level differences. ignoring that would inflate the effective sample size and make the teaching method look significant when it might not be.\n\n## variance components\n\nin a **variance components model**, all factors are random:\n\n$$\ny_{ijk} = \\mu + a_i + b_{j(i)} + \\varepsilon_{ijk},\n$$\n\nwhere $a_i \\sim \\mathcal{n}(0, \\sigma_a^2)$ and $b_{j(i)} \\sim \\mathcal{n}(0, \\sigma_b^2)$ represent nested random effects. the notation $b_{j(i)}$ means the $b$ levels are nested within the $a$ levels \u2014 classrooms within schools, or wells within experimental plates.\n\n### estimating variance components with reml\n\nhow do you estimate $\\sigma_a^2$, $\\sigma_b^2$, and $\\sigma_\\varepsilon^2$? ordinary maximum likelihood (which we used for fitting distributions in [probability density functions](./probability-density-functions)) has a bias problem here. it estimates the fixed effects first and then estimates variances from the residuals, but it doesn't account for the degrees of freedom lost in estimating those fixed effects. the result: ml systematically *underestimates* variance components \u2014 the same issue that bessel's correction fixes for the sample variance (from [introduction and concepts](./introduction-concepts)).\n\n**restricted maximum likelihood** (reml) solves this. it separates the likelihood into two parts: one for the fixed effects and one for the variance components. by estimating variances only from the second part \u2014 which is free of the fixed effects \u2014 reml produces unbiased estimates. it is the standard method for mixed models.\n\nthe practical difference: for large datasets, ml and reml give nearly identical results. for small datasets (where getting the variance right matters most), reml is noticeably better.\n\n## applications to clustered data\n\nmixed models are essential when data have a **hierarchical structure**:\n\n* students nested within classrooms nested within schoo"
    },
    {
      "topicId": "applied-statistics",
      "topicTitle": "Applied Statistics",
      "routeSlug": "applied-statistics",
      "lessonSlug": "simulation-fitting",
      "lessonTitle": "Simulation Methods",
      "x": 0.6064910888671875,
      "y": 0.585825502872467,
      "searchText": "simulation methods\n# simulation methods\n\nwhat if you could run your experiment a million times? not in the real world \u2014 that would take forever and cost a fortune \u2014 but inside a computer, where electrons are cheap and time runs fast.\n\nthe central limit theorem told you that averages converge to a gaussian. error propagation showed you how uncertainties flow through calculations. but both rely on analytical formulas that assume smoothness, linearity, or known distributions. when those assumptions break down \u2014 and in real life, they often do \u2014 you let the computer do the experiment for you.\n\ninstead of deriving a formula, you generate millions of random samples, push them through your calculation, and look at the result. this is **monte carlo simulation** \u2014 named after the famous casino, because it runs on random numbers.\n\n## producing random numbers\n\nevery monte carlo method starts with random numbers drawn from a specific distribution. computers generate **uniform** random numbers natively, so the challenge is converting those into samples from whatever distribution you need. two methods cover almost every case.\n\n## transformation method\n\nthe transformation method is elegant and efficient when it works. the idea: if you can invert the cdf (the cumulative distribution function from [probability density functions](./probability-density-functions)), you can transform uniform random numbers into any distribution you want.\n\nthe recipe is short:\n\n1. verify the pdf is normalized.\n2. compute the cdf.\n3. invert it.\n\n$$\nf(x) = \\int_{-\\infty}^x f(x') \\, dx'\n$$\n\ndraw a uniform random number $p \\in [0,1]$ and compute $x = f^{-1}(p)$. the resulting $x$ follows the target distribution. that's it. the cdf maps probabilities to values, and its inverse maps values back.\n\n### example: exponential distribution\n\nconsider the exponential distribution, which models waiting times between poisson events:\n\n$$\nf(x) = \\lambda \\exp(-\\lambda x), \\quad x \\in [0, \\infty)\n$$\n\nthis is normalized. the cdf is:\n\n$$\nf(x) = 1 - \\exp(-\\lambda x)\n$$\n\ninverting gives:\n\n$$\nf^{-1}(p) = -\\frac{\\ln(1-p)}{\\lambda}\n$$\n\nto sample: draw $p$ uniformly from $[0,1]$ and compute $x = f^{-1}(p)$. each $x$ is a random draw from the exponential distribution. you've just turned uniform noise into structured randomness.\n\n## accept-reject method\n\nthe transformation method requires an invertible cdf, which is not always available. what if your distribution is some complicated function you can evaluate but can't integrate analytically?\n\nthe **accept-reject method** (also called the von neumann method) is more general: it works for any distribution you can evaluate, even if you can't integrate or invert it.\n\nthe idea: draw samples from a simple proposal distribution and keep only those that \"pass\" a random acceptance test. given a target pdf $f(x)$ and a proposal distribution $g(x)$ with $f(x) \\leq m \\cdot g(x)$ for some constant $m$:\n\n1. sample $x$ from $g(x)$.\n2. sample $u$ uniformly from $[0, 1]$.\n3. accept $x$ if $u \\leq f(x) / (m \\cdot g(x))$; otherwise reject and repeat.\n\nthe accepted samples follow the target distribution exactly. the efficiency depends on how tightly the proposal $g$ envelops the target $f$ \u2014 a loose envelope wastes many samples, like casting a wide net when you only want one kind of fish.\n\n[[simulation accept-reject]]\n\n```python\ndef accept_reject_sample(target_pdf, proposal_sample, proposal_pdf, m, num_samples):\n    samples = []\n    while len(samples) < num_samples:\n        x = proposal_sample()\n        u = random.uniform(0, 1)\n        if u <= target_pdf(x) / (m * proposal_pdf(x)):\n            samples.append(x)\n    return samples\n```\n\n[[simulation monte-carlo-integration-stats]]\n\n## why monte carlo scales well\n\nhere is the key advantage of monte carlo, and it's genuinely surprising. the uncertainty on a monte carlo estimate decreases as $1/\\sqrt{n}$, where $n$ is the number of samples. this convergence rate holds *regardless of the dimensionality* of the problem.\n\nby contrast, deterministic numerical integration (like the trapezoidal rule) converges as $1/n^{2/d}$, where $d$ is the number of dimensions. in one dimension, deterministic methods win easily. but as $d$ grows, their convergence collapses while monte carlo stays at $1/\\sqrt{n}$.\n\nthis is why monte carlo is the tool of choice for high-dimensional problems \u2014 integrating over many parameters, propagating errors through complex models, or simulating physical processes with many degrees of freedom. it connects directly to the clt: every monte carlo estimate is an average, and the clt guarantees that average will be approximately gaussian with a calculable uncertainty.\n\nnow that you can fake the universe a million times in a computer, watch what happens when you ask: \"i have a model and some data \u2014 how well does the model fit?\" that's chi-square fitting, and it's where we go next.\n\n> **challenge.** explain the accept-reject method to a friend using only the analogy of throwing darts at a dartboard. one minute. (hint: the dartboard is the proposal distribution, and you only keep darts that land under the target curve.)\n\n## big ideas\n\n* monte carlo simulation is just running a virtual experiment millions of times \u2014 it converts the question \"what is the uncertainty on my derived quantity?\" into the question \"how much do my outputs spread when i wiggle my inputs?\"\n* the transformation method is elegant: if you can invert the cdf, you can turn uniform random numbers into *any* distribution you want, just by changing the scale.\n* monte carlo's $1/\\sqrt{n}$ convergence rate is dimension-independent \u2014 this is why it dominates in high-dimensional problems where deterministic integration becomes impossibly slow.\n* the accept-reject method works for *any* distribution you can evaluate, even if you can't integrate it. generality costs efficiency; tighter proposal distributions waste fewer samples.\n\n## what comes next\n\nyou can now generate random samples from arbitrary distribution"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "agentbased",
      "lessonTitle": "Agent-Based Models and Stochastic Simulation",
      "x": 0.5123627781867981,
      "y": 0.4210359752178192,
      "searchText": "agent-based models and stochastic simulation\n# agent-based models and stochastic simulation\n\nno bird in a flock has a map of the formation. no driver on a highway knows the state of every other car. no neuron in your brain has a blueprint of your thoughts. and yet: the flock moves as one, traffic jams form and dissolve in waves, and your brain somehow thinks.\n\nin all these cases, the amazing thing is the same: **no agent has a global plan, yet order emerges**. simple local rules \u2014 follow your nearest neighbor, slow down if the car ahead is close, fire if enough of your neighbors fire \u2014 produce complex, coordinated global behavior. that is the central insight of agent-based modeling, and it connects directly to everything we have studied: the ising spins following local rules that produce phase transitions, the sandpile grains following local rules that produce power-law avalanches, the network nodes following local rules that produce scale-free structure.\n\nthe power of \"stupid\" local rules producing \"smart\" global behavior is one of the deepest themes in complex physics.\n\n## from equations to agents\n\nin many systems, the relevant actors are discrete individuals, not continuous fields. **agent-based models** (abms) simulate autonomous agents following local rules, and emergent collective behavior arises from their interactions.\n\nabms are particularly powerful when:\n\n* the population is **heterogeneous** (agents differ in attributes or behavior).\n* spatial structure matters (local interactions dominate over global averages).\n* stochasticity at the individual level drives macroscopic phenomena.\n\n## cellular automata: the game of life\n\n**conway's game of life** is the canonical deterministic agent-based model. on a 2d grid, each cell is alive or dead, and its state updates synchronously based on its eight neighbors:\n\n* a live cell **survives** if it has exactly 2 or 3 live neighbors; otherwise it dies (of loneliness or overcrowding).\n* a dead cell **is born** if it has exactly 3 live neighbors (just the right amount of community).\n\n[[simulation game-of-life]]\n\nwatch the simulation and something astonishing happens: from a random initial pattern, you see gliders that translate across the grid, oscillators that pulse with fixed periods, and complex structures that interact, collide, and sometimes produce entirely new structures. the game of life is actually turing-complete \u2014 it can in principle simulate any computation. all from two rules on a grid.\n\ndespite these simple rules, predicting the long-term behavior of a given initial configuration is in general impossible without actually running the simulation. this is the hallmark of complexity: simple rules, unpredictable outcomes.\n\n## stochastic simulation and the gillespie algorithm\n\nwhen reactions involve small numbers of molecules, deterministic rate equations (odes) fail to capture the inherent randomness. the **gillespie algorithm** (stochastic simulation algorithm) provides exact trajectories from the chemical master equation.\n\nat each step:\n\n1. compute all reaction **propensities** $a_i$ (rate $\\times$ number of reactant combinations).\n2. compute the total propensity $a_0 = \\sum_i a_i$.\n3. draw the **waiting time** to the next reaction: $\\delta t \\sim \\text{exp}(a_0)$.\n4. select **which reaction** fires with probability $a_i / a_0$.\n5. update the state and repeat.\n\nthe algorithm generates sample paths that are statistically exact solutions of the master equation. the price is that you simulate one reaction at a time \u2014 but the payoff is that you capture fluctuations that deterministic equations completely miss. in small systems (a few dozen molecules in a cell), these fluctuations can drive qualitatively different behavior: switches, oscillations, extinctions.\n\n## predator-prey dynamics\n\nthe **lotka-volterra model** describes the interaction between predators (foxes) and prey (rabbits). the ode (mean-field) version predicts sustained oscillations:\n\n$$\n\\frac{dr}{dt} = \\alpha r - \\beta r f, \\qquad \\frac{df}{dt} = \\delta r f - \\gamma f.\n$$\n\nbut now put this on a spatial grid as an agent-based model:\n\n* rabbits reproduce with some probability at each step.\n* foxes eat nearby rabbits and reproduce; they die if they go too long without eating.\n* both species move randomly on a spatial grid.\n\nthe agent-based version reveals phenomena invisible to the odes: spatial clustering (predators chase prey in traveling waves), local extinctions (an island of rabbits gets wiped out even though the global population is fine), and stochastic fluctuations that can drive one species to *global* extinction \u2014 something the deterministic equations say is impossible.\n\nthis is a recurring lesson: mean-field equations tell you what happens on average, but agents live in a world of fluctuations, space, and individual histories. the average can be misleading.\n\n## random walks and diffusion\n\n**random walks** are the simplest stochastic models \u2014 and they keep showing up. we saw them in the first-return time analysis of soc. here they are again as the foundation of diffusion.\n\na walker on a lattice takes steps in random directions at each time step. key results for an unbiased random walk in $d$ dimensions:\n\n* mean displacement: $\\langle \\mathbf{r}(t) \\rangle = 0$ (no net drift).\n* mean-squared displacement: $\\langle r^2(t) \\rangle = 2d \\, d \\, t$ (spreads as $\\sqrt{t}$).\n* **recurrence**: the walker returns to the origin with probability 1 in 1d and 2d, but not in 3d and higher (**polya's theorem**). a drunk person will eventually find their way home on a 2d street grid, but a drunk bird in 3d space may wander forever.\n\nthe **langevin equation** provides a continuous-time description:\n\n$$\n\\frac{dx}{dt} = -\\frac{\\partial u}{\\partial x} + \\sqrt{2d} \\, \\xi(t),\n$$\n\nwhere $\\xi(t)$ is gaussian white noise with $\\langle \\xi(t) \\xi(t') \\rangle = \\delta(t - t')$.\n\n## flocking, traffic, and the theme of emergence\n\nlet us tie together the applications that make agent-based models"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "criticalPhenomena",
      "lessonTitle": "Critical Phenomena",
      "x": 0.3039170503616333,
      "y": 0.35028135776519775,
      "searchText": "critical phenomena\n# critical phenomena\n\n## the miracle of forgetting details\n\nhere is one of the most astonishing facts in all of physics. take a magnet near its curie temperature. take a fluid near its liquid-gas critical point. take a binary alloy near its unmixing transition. these systems are made of completely different stuff \u2014 iron atoms with quantum spins, water molecules with hydrogen bonds, copper and zinc atoms with metallic interactions. their microscopic physics could not be more different.\n\nand yet, near their critical points, they all behave *identically*.\n\nthe magnetization of iron near $t_c$ follows the exact same power law as the density difference between liquid and gas water near its critical point. the susceptibility of the magnet diverges with the same exponent as the compressibility of the fluid. the correlation length grows the same way in both systems.\n\nhow is this possible? how can systems that are so different at the microscopic level produce the same macroscopic behavior at the critical point? the answer is that near a critical point, the system develops correlations on *all* length scales. the correlation length $\\xi$ diverges to infinity, and the system \"forgets\" its microscopic details. all that matters is the spatial dimensionality $d$ and the symmetry of the order parameter. everything else washes out.\n\nthis is **universality**, and it is the deepest result in the theory of phase transitions.\n\n## universality\n\nsystems in the same **universality class** share identical critical exponents. what determines the class? just two things:\n\n1. the **spatial dimensionality** $d$.\n2. the **symmetry of the order parameter** (scalar, vector, tensor, etc.).\n\nthe liquid-gas critical point and the 3d ising ferromagnet belong to the same universality class because both have a scalar order parameter in 3 dimensions \u2014 even though one involves molecules bouncing around and the other involves quantum spins on a lattice. nature does not care about the details. she cares about symmetry and dimension.\n\n## critical exponents and scaling laws\n\nnear the critical temperature $t_c$, thermodynamic quantities diverge or vanish as power laws. the **reduced temperature** $t = (t - t_c)/t_c$ measures distance from criticality.\n\nkey critical exponents:\n\n* **order parameter**: $m \\sim |t|^\\beta$ for $t < t_c$.\n* **susceptibility**: $\\chi \\sim |t|^{-\\gamma}$.\n* **heat capacity**: $c \\sim |t|^{-\\alpha}$.\n* **correlation length**: $\\xi \\sim |t|^{-\\nu}$.\n* **correlation function at $t_c$**: $g(r) \\sim r^{-(d-2+\\eta)}$.\n\nthese exponents are not independent. **scaling relations** connect them:\n\n$$\n\\alpha + 2\\beta + \\gamma = 2 \\qquad \\text{(rushbrooke)},\n$$\n\n$$\n\\gamma = \\nu(2 - \\eta) \\qquad \\text{(fisher)},\n$$\n\n$$\n\\nu d = 2 - \\alpha \\qquad \\text{(josephson / hyperscaling)}.\n$$\n\nthese are not just empirical correlations. they follow from the mathematical structure of scale invariance. if you know any two independent exponents, you can compute all the others.\n\n## mean-field theory and its limitations\n\nrecall from the mean-field section: mean-field theory replaces fluctuations with their average, yielding the landau free energy:\n\n$$\nf(m) = a_0 + a_2 t \\, m^2 + a_4 m^4 + \\cdots\n$$\n\nminimizing gives mean-field exponents: $\\beta = 1/2$, $\\gamma = 1$, $\\alpha = 0$, $\\nu = 1/2$.\n\nmean-field theory is exact above the **upper critical dimension** $d_c = 4$ for the ising model. below $d_c$, fluctuations are too strong to ignore, and the actual exponents differ from mean-field predictions \u2014 sometimes dramatically.\n\n| exponent | mean-field | 2d ising | 3d ising |\n|----------|-----------|----------|----------|\n| $\\beta$  | 1/2       | 1/8      | 0.326    |\n| $\\gamma$ | 1         | 7/4      | 1.237    |\n| $\\nu$    | 1/2       | 1        | 0.630    |\n\nlook at those numbers. in 2d, $\\beta$ drops from $1/2$ to $1/8$ \u2014 the magnetization vanishes *much* more slowly than mean-field predicts. the susceptibility diverges almost twice as fast ($\\gamma = 7/4$ vs. $1$). fluctuations matter enormously.\n\n[[simulation phase-transition-ising-1d]]\n\nin this simulation, you can watch how the ising model behaves near its critical point: enormous clusters of aligned spins form and dissolve, the correlation length grows, and the system fluctuates wildly between magnetized and unmagnetized states. this is what criticality looks like.\n\n## the renormalization group\n\nthe **renormalization group** (rg) provides the theoretical framework for understanding universality and scaling. the idea, due to kenneth wilson, is to systematically zoom out:\n\n1. **block spins**: group neighboring spins into blocks and define a new effective spin for each block.\n2. **rescale**: shrink the lattice back to its original size.\n3. **renormalize**: adjust the coupling constants so the partition function is preserved.\n\nthis defines a flow in the space of coupling constants. **fixed points** of the rg flow correspond to scale-invariant systems \u2014 systems that look the same at every scale. these are exactly the critical points.\n\nthe critical exponents are determined by the eigenvalues of the linearized rg transformation at the fixed point. **relevant** directions (which grow under iteration) drive the system away from criticality \u2014 they correspond to temperature and external field. **irrelevant** directions (which shrink) do not affect the critical behavior \u2014 they correspond to microscopic details like lattice structure, interaction range, etc.\n\nthis is why universality works: the irrelevant directions all flow to zero, and only the relevant ones survive. two systems with different microscopic details but the same relevant variables flow to the same fixed point and therefore share the same critical exponents.\n\n## correlation functions\n\nthe **two-point correlation function** measures how fluctuations at two points are statistically related:\n\n$$\ng(\\mathbf{r}) = \\langle s(\\mathbf{0}) \\, s(\\mathbf{r}) \\rangle - \\langle s \\rangle^2.\n$$\n\naway from $t_c$, correlations de"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "econophysics",
      "lessonTitle": "Econophysics",
      "x": 0.5844703316688538,
      "y": 0.4760531485080719,
      "searchText": "econophysics\n# econophysics\n\nremember the sandpile from the self-organized criticality section? you drop grains one at a time, and most of the time nothing happens, but occasionally a massive avalanche rearranges the whole pile. the same mathematics shows up in stock prices.\n\nremember the scale-free networks from the networks section? a few hubs dominate, and the degree distribution follows a power law. the same pattern appears in the distribution of stock returns: small fluctuations are common, but extreme moves (crashes and rallies) are far more frequent than a normal distribution would predict.\n\nremember the ising model from the very beginning? spins interact with their neighbors and collectively decide to align \u2014 a phase transition. traders interact with each other (through news, rumors, herding) and collectively decide to sell \u2014 a market crash.\n\n**econophysics** applies the methods of statistical mechanics and complex systems to financial markets. the central discovery is that markets are not the well-behaved random walks that classical finance assumes. they are complex systems with heavy tails, long-range correlations, and avalanche-like dynamics.\n\n## brownian motion and stock prices\n\nthe simplest model treats log-returns as a random walk. let $s_t$ be the closing price and $x_t = \\log(s_t)$. the variance at lag $\\tau$ is:\n\n$$\n\\text{var}(\\tau) = \\langle (x_{t+\\tau} - x_t)^2 \\rangle.\n$$\n\nfor **geometric brownian motion** (the foundation of the black-scholes model), variance grows linearly with lag:\n\n$$\n\\text{var}(\\tau) \\propto \\tau.\n$$\n\nthis would mean stock returns are uncorrelated, gaussian, and well-behaved. it is a beautiful theory. it is also wrong.\n\n[[simulation stock-variance]]\n\nrun this simulation with real stock data and compare it with the prediction from geometric brownian motion. you will see two major deviations: (1) **heavy tails** \u2014 large moves are far more frequent than a gaussian predicts (the famous \"fat tails\"), and (2) **volatility clustering** \u2014 large changes tend to follow large changes, and calm periods follow calm periods. the market has memory.\n\n## the hurst exponent\n\nthe **hurst exponent** $h$ measures long-range dependence in time series. for the rescaled range $r/s$ of a time series with $n$ data points:\n\n$$\n\\frac{r}{s} \\sim c \\, n^h \\quad \\text{as } n \\to \\infty,\n$$\n\nwhere $c$ is a constant.\n\nthe value of $h$ reveals the nature of correlations:\n\n* $h = 0.5$: uncorrelated random walk (pure brownian motion). each step is independent of the past.\n* $h > 0.5$: **persistent** (trending) behavior. an up move is more likely to be followed by another up move. positive long-range correlations.\n* $h < 0.5$: **anti-persistent** (mean-reverting) behavior. an up move is more likely to be followed by a down move. negative long-range correlations.\n\nfor self-similar time series, $h$ is directly related to the fractal dimension (from the percolation and fractals section): $d = 2 - h$. a brownian motion trace has fractal dimension $1.5$; a persistent time series is smoother ($d < 1.5$); an anti-persistent one is rougher ($d > 1.5$).\n\n[[simulation hurst-exponent]]\n\nestimate the hurst exponent from data in this simulation. log-returns themselves are nearly uncorrelated ($h \\approx 0.5$), but absolute returns $|r|$ show persistent long-range correlations ($h \\approx 0.7$\u2013$0.8$) \u2014 this is the signature of volatility clustering. the returns themselves are unpredictable, but their *magnitude* has long memory.\n\n## the fear-factor model\n\nhere is a simple model that captures the asymmetry in stock returns. let $q$ be the probability of the stock moving up under normal conditions. now introduce a **collective fear event** with probability $p$ that forces all stocks down simultaneously \u2014 think of a market panic, a financial crisis, a sudden loss of confidence.\n\nthe effective probabilities become:\n\n* stock goes up: $(1-p)q$.\n* stock goes down: $p + (1-p)(1-q)$.\n\nfor a neutral random walk (equal probability of up and down):\n\n$$\n(1-p)q = p + (1-p)(1-q) \\implies q = \\frac{1}{2(1-p)}.\n$$\n\nas the fear probability $p$ increases, the required upward probability $q$ must increase to compensate. this creates an asymmetry that mimics the observed **negative skewness** in stock returns: the market rises slowly most of the time (to compensate for the fear premium) but crashes quickly and dramatically when the collective fear event hits.\n\nthe variance of returns includes a term proportional to $p$ that represents **systematic risk** \u2014 risk that cannot be diversified away because it affects everyone simultaneously.\n\n## bet hedging\n\n**bet hedging** is a strategy where an organism (or investor) sacrifices expected performance to reduce variance. it is the mathematical foundation of portfolio diversification.\n\nin a stochastic growth model, consider a population with growth rate $r_t$ drawn from a distribution at each time step. the long-run growth rate is not the arithmetic mean but the **geometric mean**:\n\n$$\n\\bar{r}_{\\text{long-run}} = \\langle \\ln r_t \\rangle = \\langle r_t \\rangle - \\frac{1}{2} \\text{var}(r_t) + \\cdots\n$$\n\nthis **arithmetic-geometric inequality** means that variance *always* reduces long-run growth. an organism (or investor) that reduces its variance \u2014 by hedging its bets across strategies \u2014 can outperform a specialist that maximizes expected growth, especially when the noise is large.\n\n[[simulation bet-hedging]]\n\ntry the simulation: compare a \"specialist\" strategy (high expected return, high variance) with a \"diversified\" strategy (lower expected return, lower variance). run both for many time steps. the specialist might win in the short run, but the diversifier almost always wins in the long run. variance is not just risk \u2014 it is a *drag* on growth.\n\nthe key insight: the optimal strategy depends on the **noise size** relative to the expected return. in a calm, predictable world, specialize. in a noisy, uncertain world, diversify. the mathematics is the same whether you are a bact"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "fractals",
      "lessonTitle": "Fractals",
      "x": 0.35499057173728943,
      "y": 0.4210050702095032,
      "searchText": "fractals\n# fractals\n\nat the percolation threshold, the spanning cluster is not a smooth blob \u2014 it is a tangled, wispy, infinitely detailed object full of holes at every scale. zoom in, and you see the same kind of structure as when you zoom out. this is a **fractal**: a geometric object that exhibits self-similarity across scales.\n\nfractals are not just curiosities \u2014 they appear at every critical point. the correlation structure of a magnet at $t_c$, the branching of a lightning bolt, the coastline of norway \u2014 all are fractal. the mathematics of fractals provides the geometric language for understanding critical phenomena.\n\n## fractal dimension\n\nthe **fractal dimension** $d_f$ characterizes how the mass of an object scales with its linear size:\n\n$$\nm(l) \\sim l^{d_f}.\n$$\n\nfor ordinary solid objects in $d$ dimensions, $d_f = d$ (a square has $d_f = 2$, a cube has $d_f = 3$). for fractals, $d_f$ is typically non-integer \u2014 the object is \"too thin\" to fill its embedding space, but \"too thick\" to be a lower-dimensional object.\n\nexamples:\n\n* koch snowflake: $d_f = \\log 4 / \\log 3 \\approx 1.26$.\n* sierpinski triangle: $d_f = \\log 3 / \\log 2 \\approx 1.58$.\n* percolation cluster at $p_c$ in 2d: $d_f = 91/48 \\approx 1.896$.\n\n[[simulation fractal-dimension]]\n\nplay with this simulation to see how fractal dimension is measured. the object has more \"stuff\" than a line ($d_f > 1$) but less than a plane ($d_f < 2$). it lives in between \u2014 and that fractional dimension is what makes it a fractal.\n\n## the mandelbrot set\n\nthe **mandelbrot set** is defined in the complex plane as the set of values $c$ for which the iteration\n\n$$\nz_{n+1} = z_n^2 + c, \\qquad z_0 = 0,\n$$\n\nremains bounded. the boundary of the mandelbrot set is a fractal with infinite detail at every scale.\n\nthe **escape-time algorithm** colors each point $c$ by the number of iterations needed for $|z_n|$ to exceed a threshold (typically 2), producing the iconic visualizations of the set.\n\n[[simulation mandelbrot-fractal]]\n\nzoom into the boundary of the mandelbrot set. no matter how far you zoom, you keep finding new structure \u2014 spirals, tendrils, miniature copies of the whole set. this infinite self-similarity from a one-line formula ($z \\to z^2 + c$) is one of the most stunning examples of complexity emerging from simplicity.\n\n## box-counting dimension\n\nthe **box-counting method** provides a practical way to estimate the fractal dimension of any set:\n\n1. cover the set with boxes of side length $\\epsilon$.\n2. count the number $n(\\epsilon)$ of boxes needed.\n3. the fractal dimension is $d_f = -\\lim_{\\epsilon \\to 0} \\frac{\\log n(\\epsilon)}{\\log \\epsilon}$.\n\nin practice, $\\log n(\\epsilon)$ is plotted against $\\log(1/\\epsilon)$, and $d_f$ is estimated from the slope of the linear region.\n\n## big ideas\n\n* fractal dimension is non-integer because fractals are \"too thin\" to fill a plane but \"too thick\" to be a line \u2014 they occupy the twilight zone between dimensions.\n* the mandelbrot set is the most complex object you can define with a one-line formula: $z \\to z^2 + c$ produces infinite self-similar detail from pure mathematics.\n* critical clusters are fractals because scale invariance at the critical point means the same kind of structure repeats at every length scale \u2014 the same reason power laws appear in thermodynamic quantities.\n* the box-counting dimension is a practical tool: you can measure the fractal dimension of any real-world object (coastlines, trees, blood vessels) by counting boxes at different scales and fitting a slope.\n\n## what comes next\n\nfractals appear at critical points because you must tune a control parameter \u2014 temperature to $t_c$, or occupation probability to $p_c$ \u2014 to arrive there. but some systems reach the critical point on their own, without anyone turning a knob. [self-organized criticality](selforganizedcriticality) introduces the sandpile model, where adding sand grain by grain drives the pile to the critical slope automatically. once there, the avalanche sizes follow a power law \u2014 and the avalanche shapes are fractal. you have seen all the ingredients; soc brings them together into a single mechanism that may explain why power laws are everywhere in nature.\n\n## check your understanding\n\n1. the koch snowflake is built by repeatedly replacing each line segment with four segments of one-third the length. use this self-similarity to derive the fractal dimension $d_f = \\log 4 / \\log 3$. why does this formula work \u2014 what is the general principle for computing $d_f$ from a self-similar construction rule?\n2. the mandelbrot set boundary has fractal dimension 2 (it is a curve that fills area). yet when you zoom in, you see thin tendrils and spirals, not a solid region. how can a curve have dimension 2?\n3. the percolation cluster at $p_c$ in 2d has fractal dimension $d_f = 91/48 \\approx 1.896$, which is close to but less than 2. what does it mean physically that the spanning cluster almost \u2014 but not quite \u2014 fills the 2d plane?\n\n## challenge\n\nmeasure the fractal dimension of a real-world object using box counting. take a digital image of a coastline, a tree silhouette, or a lightning bolt. convert it to black and white. then, systematically cover it with grids of boxes of side length $\\epsilon = 1, 2, 4, 8, \\ldots$ pixels and count the number of boxes $n(\\epsilon)$ that contain any black pixels. plot $\\log n$ against $\\log(1/\\epsilon)$ and extract the slope. compare your measured fractal dimension to the theoretical values for similar objects (coastlines are typically $d_f \\approx 1.2$\u2013$1.5$, depending on how rugged they are). what are the main sources of error in your measurement?\n"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "meanFieldResults",
      "lessonTitle": "Mean-Field Results",
      "x": 0.2802950441837311,
      "y": 0.3082273006439209,
      "searchText": "mean-field results\n# mean-field results\n\nimagine every spin in our magnet feels an average field from all its neighbors, like a crowd where everyone is trying to face the same way because they see everyone else doing it. we derived the mean-field hamiltonian in the previous section. now let us see what comes out of it.\n\nthe punchline first \u2014 here are the key results:\n\n## mf: z, m, tc, f & critical exponents\n\n$$\n\\begin{align*}\n    z_{\\mathrm{mf}}\n    &=\n    \\exp\n    \\left(\n        * \\beta\\frac{j n z}{2} m^2\n    \\right)\n    \\left[\n    2\\cosh\n    \\left(\n        \\beta j z m + \\beta h\n    \\right)\n    \\right]^n\n    \\\\\n    m\n    &= \\langle s_i \\rangle\n    =\n    \\tanh\n    \\left(\n        \\beta j z m + \\beta h\n    \\right)\n    \\\\\n    t_c &= \\frac{jz}{k_b}\n    \\\\\n    f_{\\mathrm{mf}} &=\n    \\frac{jnz}{2}m^2\n    * \\frac{n}{\\beta} \\ln\n    \\left[\n        2 \\cosh \\left(\\beta jzm + \\beta h \\right)\n    \\right]\n\\end{align*}\n$$\n\nnow let us earn these results.\n\n## mf: z, m, tc, f & critical exponents (derivation)\n\n### mean-field partition function\n\nthe partition function of the ising model is a sum over all $2^n$ spin configurations:\n$$\n\\begin{align*}\n    z\n    &=\n    \\sum_{\\{s_i\\}}\n    e^{-\\beta \\mathcal{h}(\\{s_i\\})}\n    =\n    \\sum_{n}^{2^n}\n    e^{-\\beta e_n}.\n\\end{align*}\n$$\n\nnow we plug in the mean-field hamiltonian. the crucial simplification is that the spins have decoupled \u2014 the sum over all configurations *factorizes*:\n$$\n\\begin{align*}\n    z_{\\mathrm{mf}}\n    &=\n    \\sum_{s_1 = \\pm 1} \\cdots \\sum_{s_n = \\pm 1}\n    \\exp\n    \\left[\n        * \\beta\\frac{j n z}{2} m^2\n        + \\beta \\left( j z m + h \\right) \\sum_i s_i\n    \\right]\n    \\\\&=\n    \\exp\n    \\left(\n        * \\beta\\frac{j n z}{2} m^2\n    \\right)\n    \\prod_{i=1}^n\n    \\left[\n    \\sum_{s_i = \\pm 1}\n    \\exp\n    \\left(\n        \\beta (j z m + h) s_i\n    \\right)\n    \\right]\n    \\\\&=\n    \\exp\n    \\left(\n        * \\beta\\frac{j n z}{2} m^2\n    \\right)\n    \\left[\n    2\\cosh\n    \\left(\n        \\beta j z m + \\beta h\n    \\right)\n    \\right]^n.\n\\end{align*}\n$$\n\neach spin contributes a factor of $2\\cosh(\\beta(jzm + h))$, and since they are all independent in mean-field, these factors multiply. this is why mean-field theory is solvable: the terrible many-body problem has been reduced to $n$ copies of a one-body problem.\n\n### self-consistent equation of magnetization\n\nthe magnetization per spin $m = \\langle s_i \\rangle$ can be calculated directly:\n$$\n\\begin{align*}\n    m\n    &= \\langle s_i \\rangle\n    \\\\&=\n    \\frac{1}{z_{\\mathrm{mf}}}\n    \\sum_{\\{s_i\\}}\n    s_i \\, e^{-\\beta \\mathcal{h}_\\mathrm{mf}}\n    \\\\&=\n    \\frac\n    {\n    \\displaystyle\n    \\sum_{s_i = \\pm 1}\n    s_i\n    \\exp\\left[\\beta (j z m + h) s_i\\right]\n    }\n    {\n    \\displaystyle\n    \\sum_{s_i = \\pm 1}\n    \\exp\\left[\\beta (j z m + h) s_i\\right]\n    }\n    \\\\&=\n    \\frac{2\\sinh(\\beta j z m + \\beta h)}{2\\cosh(\\beta j z m + \\beta h)}\n    \\\\&=\n    \\tanh(\\beta j z m + \\beta h).\n\\end{align*}\n$$\n\nthis is the **self-consistency equation**: $m$ appears on both sides! the magnetization depends on itself through the mean field. with no external field ($h = 0$):\n$$\n    m = \\tanh(\\beta j z m).\n$$\n\nthis equation cannot be solved in closed form, but we can understand it graphically. plot $y = m$ and $y = \\tanh(\\beta jzm)$ and look for intersections:\n\n* at **high temperature** ($\\beta jz < 1$), the $\\tanh$ curve is too shallow \u2014 it only crosses $y = m$ at $m = 0$. the only solution is zero magnetization. the system is disordered.\n* at **low temperature** ($\\beta jz > 1$), the $\\tanh$ curve is steep enough to cross $y = m$ at three points: $m = 0$ and two symmetric nonzero values $\\pm m_0$. the system can be magnetized!\n\nthe transition between these two regimes is the **phase transition**.\n\n### critical temperature of mean-field approximation\n\nthe critical temperature is where the number of solutions changes from one to three. this happens when the slope of $\\tanh(\\beta jzm)$ at $m = 0$ equals the slope of $y = m$ (which is 1).\n\nsince $\\tanh(x) \\approx x$ for small $x$:\n$$\n    \\left.\\frac{\\mathrm{d}}{\\mathrm{d}m} \\tanh(\\beta jzm)\\right|_{m=0}\n    = \\beta jz = 1.\n$$\n\nsolving for temperature:\n$$\n    t_c = \\frac{jz}{k_\\mathrm{b}}.\n$$\n\nthis makes physical sense: stronger coupling $j$ means higher $t_c$ (harder to disorder), and more neighbors $z$ means higher $t_c$ (more peer pressure to stay aligned). however, note that mean-field theory predicts a phase transition in *every* dimension, including 1d \u2014 and we know from the exact solution that there is no phase transition in the 1d ising model. mean-field theory overestimates the tendency to order because it ignores the correlated fluctuations that destroy order in low dimensions.\n\n### free energy of mean-field approximation\n\nfrom the partition function:\n$$\n\\begin{align*}\n    f_\\mathrm{mf}\n    &=\n    * \\frac{1}{\\beta} \\ln z_\\mathrm{mf}\n    \\\\&=\n    \\frac{jnz}{2}m^2\n    * \\frac{n}{\\beta} \\ln\n    \\left[\n        2 \\cosh (\\beta jzm + \\beta h)\n    \\right].\n\\end{align*}\n$$\n\nas a check, we can recover the self-consistency equation by differentiating with respect to $h$:\n$$\n    m = -\\frac{1}{n}\\frac{\\partial f_\\mathrm{mf}}{\\partial h}\n    = \\tanh(\\beta jzm + \\beta h). \\quad \\checkmark\n$$\n\n### critical exponents from landau expansion\n\nto understand the behavior near the transition, we expand the free energy in powers of $m$. introduce the dimensionless temperature $\\theta = t/t_c = 1/(\\beta jz)$ and dimensionless free energy $f_\\mathrm{mf} = f_\\mathrm{mf}/(jzn)$. for $h = 0$ and small $m/\\theta$:\n\nusing $\\cosh(x) \\approx 1 + x^2/2 + x^4/24 + \\cdots$ and $\\ln(1+x) \\approx x - x^2/2 + \\cdots$:\n$$\n\\begin{align*}\n    f_\\mathrm{mf}\n    &=\n    \\frac{1}{2}m^2\\left(1 - \\frac{1}{\\theta}\\right)\n    + \\frac{1}{12}\\frac{m^4}{\\theta^3}\n    * \\theta \\ln 2\n    + \\mathcal{o}(m^6).\n\\end{align*}\n$$\n\nthis is a **landau free energy** \u2014 a polynomial in the order parameter $m$. the coefficient of $m^2$ changes sign at $\\theta = 1$ (i.e., $t = t_c$). that sign change is the phase transition.\n\n### sta"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "metropolisAlgorithm",
      "lessonTitle": "Metropolis Algorithm",
      "x": 0.3705850839614868,
      "y": 0.31267133355140686,
      "searchText": "metropolis algorithm\n# metropolis algorithm\n\nwe just learned that the partition function $z$ is the key to everything. but here is the problem: for an ising model with $n$ spins, there are $2^n$ possible configurations. for a modest $10 \\times 10$ lattice, that is about $10^{30}$ states. you could not sum over them all if you had every computer on earth running until the heat death of the universe.\n\nso what do we do? we cheat \u2014 brilliantly. instead of summing over all states, we let the computer wander through state space, visiting states with the correct boltzmann probabilities. this is the monte carlo method, and the metropolis algorithm is its most famous implementation.\n\n## ising model\n\nbefore we get to the algorithm, let us set up the playground. ernst ising introduced a beautifully simple model of ferromagnetism: put a spin on every site of a lattice, and let each spin point either up ($s_i = +1$) or down ($s_i = -1$). the energy is\n$$\n    \\mathcal{h}\n    =\n    -j \\sum_{\\langle i j \\rangle} s_i s_j - h \\sum_i s_i.\n$$\nhere $\\langle i j \\rangle$ means we sum over nearest-neighbor pairs (each pair counted once), $j > 0$ is the coupling strength that rewards neighboring spins for pointing the same way, and $h$ is an external magnetic field.\n\nthink of it like peer pressure: every spin wants to agree with its neighbors (when $j > 0$). when $j < 0$ the interaction is antiferromagnetic \u2014 spins prefer to alternate. when $j$ varies from pair to pair, you get a spin glass, which is a whole other can of worms.\n\nthe magnetization is simply the average spin:\n$$\n    m = \\frac{1}{n} \\sum_{i=1}^n s_i.\n$$\n\nising himself solved the 1d case exactly (no phase transition \u2014 we will see why in the transfer matrix section). lars onsager solved the 2d case in a celebrated tour de force. the 3d ising model remains unsolved analytically to this day. this is why we need computers.\n\nwe can spot the critical temperature of the 2d ising phase transition by looking for a divergence in the susceptibility $\\chi$, which we approximate as the variance of the magnetization:\n$$\n    \\chi = \\langle m^2 \\rangle - \\langle m \\rangle^2.\n$$\n\n## monte carlo method\n\n*monte carlo is a district of monaco famous for its casinos. according to lore, nicholas metropolis suggested the name \u2014 sampling random numbers to solve physics problems felt a lot like gambling.*\n\nthe idea is to estimate thermal averages without summing over all states. the statistical mechanical average of an observable $a$ is\n$$\n\\begin{align*}\n    \\langle a \\rangle\n    &=\n    \\sum_i a_i p_i\n    \\\\&=\n    \\frac{\\sum_i a_i \\exp(-\\beta e_i)}{\\sum_i \\exp(-\\beta e_i)}.\n\\end{align*}\n$$\n\nif we could generate a sequence of states $\\{s^{(1)}, s^{(2)}, \\ldots\\}$ where state $i$ appears with probability $p_i$, then we could approximate\n$$\n    \\langle a \\rangle\n    \\approx\n    \\frac{1}{m} \\sum_{k=1}^{m} a(s^{(k)}).\n$$\n\nthat is all monte carlo does: replace an impossible sum with a sample average. the metropolis algorithm tells us *how* to generate those samples correctly.\n\n## markov process and master equation\n\nthe key assumption is that the system evolves as a **markov process**: what happens next depends only on the current state, not on the history of how we got here. under this assumption, the probability of being in state $i$ evolves according to the **master equation**:\n$$\n    \\frac{\\mathrm{d}p_i}{\\mathrm{d}t}\n    =\n    \\sum_j \\left( w_{ij}p_j - w_{ji}p_i \\right).\n$$\nhere $w_{ij}$ is the transition rate from state $j$ to state $i$. the first term is probability flowing *into* state $i$; the second is probability flowing *out*.\n\nin steady state ($\\mathrm{d}p_i/\\mathrm{d}t = 0$), the total inflow equals total outflow:\n$$\n    \\sum_j w_{ij}p_j = \\sum_j w_{ji}p_i.\n$$\n\nbut this is not enough. this condition still allows circular flows (probability sloshing around in loops), which would violate the spirit of thermal equilibrium.\n\n## detailed balance\n\nto ensure true equilibrium, we impose a stronger condition called **detailed balance**: the flow between *every pair* of states must individually balance.\n$$\n    w_{ij}p_j = w_{ji}p_i.\n$$\n\nno net current between any two states. no loops. think of it like a bar where every pair of tables must exchange customers at exactly the same rate in both directions \u2014 if two people per minute walk from table a to table b, then two people per minute must walk from b to a. if this holds for every pair, the bar is in equilibrium even though people are still moving around.\n\n## metropolis-hastings algorithm\n\nnow we combine detailed balance with the boltzmann distribution. in equilibrium, $p_i \\propto e^{-\\beta e_i}$, so\n$$\n    \\frac{w_{ij}}{w_{ji}}\n    =\n    \\frac{p_i}{p_j}\n    =\n    \\exp\\left(-\\beta \\delta e_{ij}\\right),\n$$\nwhere $\\delta e_{ij} = e_i - e_j$ is the energy change when transitioning from state $j$ to state $i$.\n\nthe metropolis algorithm turns this into a simple recipe:\n\n1. **propose** a random move (e.g., flip a random spin).\n2. **calculate** the energy difference $\\delta e$.\n3. **if** $\\delta e \\leq 0$ (the move lowers the energy): accept it. always.\n4. **if** $\\delta e > 0$ (the move raises the energy): accept it with probability $e^{-\\beta \\delta e}$. draw a random number $r \\in [0,1]$; if $r < e^{-\\beta \\delta e}$, accept. otherwise reject and keep the old state.\n\nthat is it. this simple accept/reject rule guarantees that after enough steps, the system samples states according to the boltzmann distribution. you do not need to know $z$. you do not need to enumerate all states. you just need to compute energy differences, which are cheap.\n\n## watching the simulation\n\nhere is where things get exciting. run the metropolis algorithm on a 2d ising model and watch the screen.\n\nat **high temperature** the spins flip like mad \u2014 the thermal energy overwhelms the coupling, and you see pure noise, a salt-and-pepper mess of up and down spins. the magnetization bounces around zero.\n\nnow **cool it down slowly**. at first, nothing dramatic \u2014 jus"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "Networks",
      "lessonTitle": "Networks",
      "x": 0.521155059337616,
      "y": 0.4021492898464203,
      "searchText": "networks\n# networks\n\nnew web pages do not link randomly. they link to pages that are already popular \u2014 google, wikipedia, youtube. a new paper does not cite random old papers. it cites the famous ones, the ones everyone else is already citing. a new actor does not get cast with random other actors. they get cast alongside established stars.\n\nthis is the **\"rich get richer\"** effect, and it produces power laws everywhere. the distribution of web links, citation counts, social connections, airport traffic \u2014 they all follow the same pattern: a few hubs with enormous numbers of connections, and a vast majority with very few.\n\nit is the same kind of power law we saw in the sandpile and in percolation clusters. nature loves power laws, and networks are one of her favorite places to put them.\n\n## graph theory basics\n\na **network** (graph) consists of nodes (vertices) connected by edges (links). networks provide a natural language for describing complex systems where relationships between components matter as much as the components themselves.\n\nexamples of real-world networks:\n\n* **social networks**: individuals connected by friendships or interactions.\n* **world wide web**: pages connected by hyperlinks.\n* **biological networks**: proteins connected by interactions, genes by regulatory links.\n* **infrastructure**: power grids, transportation systems, the internet.\n\n## network metrics\n\nkey quantities that characterize a network:\n\n* **degree** $k_i$: the number of edges connected to node $i$. the **degree distribution** $p(k)$ describes the probability that a randomly chosen node has degree $k$. this single function tells you more about a network's structure than almost anything else.\n* **clustering coefficient**: measures the fraction of a node's neighbors that are also connected to each other. high clustering means \"my friends know each other\" \u2014 cliquishness.\n* **betweenness centrality**: the fraction of shortest paths between all pairs of nodes that pass through a given node. high centrality means the node is a bottleneck \u2014 remove it, and many paths break.\n* **connectedness**: how many nodes must be removed to disconnect the network?\n* **modularity**: the strength of division into communities. zero modularity means no meaningful partition.\n\nthe **amplification factor** involves the second moment of the degree distribution:\n\n$$\n\\langle \\mathcal{a} \\rangle = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} - 1.\n$$\n\nthis quantity is crucial: it diverges for scale-free networks with exponent $\\gamma \\leq 3$, with profound consequences for epidemic spreading and network robustness.\n\n## small-world networks\n\nhere is a surprising fact about social networks: any two people on earth are connected by roughly six intermediaries. that is the **\"six degrees of separation\"** phenomenon.\n\n**small-world networks** explain how this is possible by combining two properties:\n\n* **high clustering**: like a regular lattice, neighbors of a node tend to be connected (your friends know each other).\n* **short path lengths**: like a random graph, any two nodes are connected by a short chain (six degrees).\n\nthe watts-strogatz model starts from a regular ring lattice and randomly rewires each edge with probability $p$. the remarkable finding: even a tiny rewiring probability ($p \\sim 0.01$) dramatically reduces the average path length while preserving high clustering. a few long-range shortcuts are enough to make the whole network small-world.\n\n## scale-free networks\n\n**scale-free networks** have a degree distribution that follows a power law:\n\n$$\np(k) \\sim k^{-\\gamma},\n$$\n\ntypically with $2 < \\gamma < 3$. this means a few nodes (hubs) have enormously many connections, while most nodes have few. there is no \"typical\" number of connections \u2014 the distribution is scale-free.\n\nexamples of scale-free networks:\n\n* the world wide web ($\\gamma \\approx 2.1$ for in-degree).\n* protein interaction networks.\n* citation networks.\n* software dependency graphs.\n\n[[simulation scale-free-network]]\n\nwatch the network grow in this simulation. new nodes attach preferentially to well-connected hubs, and the power-law degree distribution emerges naturally. the resulting network looks nothing like a regular grid or a random graph \u2014 it has a few giant hubs surrounded by many weakly connected nodes.\n\n## preferential attachment\n\nthe **barabasi-albert model** explains how scale-free networks arise through **preferential attachment**: new nodes connect preferentially to existing nodes that already have many connections.\n\nalgorithm:\n\n1. start with $m_0$ connected nodes.\n2. add new nodes one at a time, each connecting to $m$ existing nodes.\n3. the probability of connecting to node $i$ is proportional to its degree: $\\pi(k_i) = k_i / \\sum_j k_j$.\n\nthis produces a power-law degree distribution with exponent $\\gamma = 3$. the rich-get-richer mechanism is all you need \u2014 the power law emerges as a mathematical consequence, just as the power law in the sandpile emerges from the self-organized dynamics.\n\n## dynamics on networks\n\nnetworks are not just static structures; processes unfold on them:\n\n* **epidemic spreading** (sir/sis models): on scale-free networks, the epidemic threshold vanishes in the thermodynamic limit. even weakly infectious diseases can spread through the hubs. this is why super-spreaders matter so much.\n* **diffusion and synchronization**: random walks on networks explore hubs quickly; coupled oscillators synchronize more easily on networks with high connectivity.\n* **information cascades**: content spreads through social networks via threshold mechanisms \u2014 one viral post can reach millions through the hub structure.\n\n## robustness and attacks\n\nnetworks respond very differently to random failures versus targeted attacks:\n\n* **random failure**: removing random nodes has little effect on scale-free networks because most nodes have low degree \u2014 you are unlikely to hit a hub by chance.\n* **targeted attack**: removing hubs rapidly fragments the ne"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "percolation",
      "lessonTitle": "Percolation",
      "x": 0.34346646070480347,
      "y": 0.385239839553833,
      "searchText": "percolation\n# percolation\n\nsuppose you are painting a wall with random dots of ink. at first they are isolated islands \u2014 tiny splotches scattered here and there with gaps between them. keep adding dots. some start to merge into small clusters. keep going. more clusters connect, forming larger and larger blobs.\n\nthen, at a critical density, something extraordinary happens: a single cluster suddenly spans from one side of the wall all the way to the other. one more dot of ink, and the whole geometry changes. that sudden appearance of a giant connected cluster is a **phase transition** \u2014 but it is geometric, not energetic. no temperature is involved, no energy is minimized. it is purely about connectivity. beautiful, isn't it?\n\nthis is **percolation**, and it turns out to have the same mathematical structure as the thermal phase transitions we studied earlier \u2014 critical exponents, scaling laws, universality, and all.\n\n## percolation theory\n\nin **site percolation** on a lattice, each site is independently occupied with probability $p$ and empty with probability $1-p$. occupied neighbors form clusters.\n\n* for small $p$, only small isolated clusters exist.\n* at the **percolation threshold** $p_c$, a giant cluster first spans the system.\n* for $p > p_c$, the giant cluster contains a finite fraction of all sites.\n\nthe percolation threshold depends on the lattice geometry:\n\n| lattice | $p_c$ (site) |\n|---------|-------------|\n| square | 0.5927 |\n| triangular | 0.5000 |\n| honeycomb | 0.6962 |\n\n[[simulation percolation]]\n\ntry the simulation: start with $p$ well below the threshold and slowly increase it. watch the clusters grow, merge, and then suddenly \u2014 a single cluster connects the entire system. the transition is sharp and dramatic, just like the magnetic phase transition, even though no energy or temperature is involved.\n\n## critical exponents in percolation\n\nnear the threshold, key quantities follow power laws in $|p - p_c|$, just like thermal phase transitions near $t_c$:\n\n* **order parameter** (fraction in spanning cluster): $p_\\infty \\sim (p - p_c)^\\beta$ for $p > p_c$.\n* **mean cluster size** (excluding the spanning cluster): $\\langle s \\rangle \\sim |p - p_c|^{-\\gamma}$.\n* **correlation length** (typical cluster radius): $\\xi \\sim |p - p_c|^{-\\nu}$.\n\nin 2d: $\\beta = 5/36$, $\\gamma = 43/18$, $\\nu = 4/3$. these exponents are universal within the percolation universality class \u2014 they do not depend on whether you use a square, triangular, or honeycomb lattice. sound familiar? it is the same miracle of universality we saw for thermal phase transitions, just in a different universality class.\n\n## the bethe lattice\n\nthe **bethe lattice** (cayley tree) is an infinite tree where each node has exactly $z$ neighbors. percolation on the bethe lattice is exactly solvable:\n\n$$\np_c = \\frac{1}{z - 1}.\n$$\n\nfor $z = 3$: $p_c = 1/2$. the bethe lattice provides the **mean-field theory** for percolation and gives exact critical exponents $\\beta = 1$, $\\gamma = 1$, $\\nu = 1/2$ \u2014 the same mean-field exponents we saw in the ising model, because mean-field theory always gives the same exponents regardless of the specific system.\n\n[[simulation bethe-lattice]]\n\nin this simulation you can build a bethe lattice and watch how occupied sites form clusters. because every node looks the same (no loops, no boundaries), the math simplifies enormously \u2014 it is the percolation equivalent of mean-field theory.\n\n## big ideas\n\n* percolation is a phase transition without energy or temperature \u2014 the control parameter is simply the density of occupied sites, and the \"order\" is connectivity rather than magnetization.\n* the percolation threshold $p_c$ depends on the lattice geometry, but the critical exponents near $p_c$ do not \u2014 universality again, in a different class from the ising model.\n* the bethe lattice gives the mean-field theory of percolation: it is exactly solvable because it has no loops, and it gives the same mean-field exponents ($\\beta = 1$, $\\gamma = 1$) that appear in every mean-field theory.\n* the spanning cluster at $p_c$ is not a compact blob \u2014 it is fractal, full of holes at every scale, a harbinger of the [fractals](fractals) we encounter next.\n\n## what comes next\n\nthe spanning cluster right at the percolation threshold is not a smooth, space-filling object \u2014 it has holes within holes within holes, self-similar structure all the way down. this is the geometry of a fractal. [fractals](fractals) quantifies this self-similarity through the fractal dimension $d_f$, which is non-integer and tells you exactly how much space a critical cluster fills. the fractal geometry of percolation clusters is not a coincidence: it is the geometric signature of scale invariance at the critical point, the same scale invariance that produces power-law exponents in the thermodynamic quantities.\n\n## check your understanding\n\n1. in site percolation, the \"order parameter\" is $p_\\infty$, the fraction of sites in the spanning cluster. above $p_c$ it is nonzero; below it is zero. why is $p_\\infty$ identically zero below $p_c$ in an infinite system, even though large finite clusters exist?\n2. the percolation threshold for a triangular lattice ($p_c = 0.5$) is exactly solvable by a duality argument. explain qualitatively why triangular and honeycomb lattices are \"dual\" to each other, and why this duality implies $p_c = 0.5$ for the triangular lattice.\n3. the mean cluster size $\\langle s \\rangle$ diverges at $p_c$ from *both* sides \u2014 as $p \\to p_c^-$ and as $p \\to p_c^+$. why does the average cluster size diverge even below the threshold, where the spanning cluster does not yet exist?\n\n## challenge\n\nthe hoshen-kopelman algorithm efficiently labels all clusters in a percolation configuration using a single pass through the lattice. describe how the algorithm works (hint: it uses a union-find data structure). implement it (or trace through a small example by hand) on a $5 \\times 5$ lattice with occupation probability $p = 0.6$. identify the spa"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "phaseTransitions",
      "lessonTitle": "Phase Transitions",
      "x": 0.31521737575531006,
      "y": 0.31750741600990295,
      "searchText": "phase transitions\n# phase transitions\n\npick up a refrigerator magnet. it sticks to your fridge because trillions of tiny atomic magnets inside it have collectively decided to point the same direction. now heat that magnet with a blowtorch. at first nothing much changes. but at a certain temperature \u2014 the curie temperature \u2014 the magnet suddenly stops being a magnet. the atomic spins are still there, still interacting, but the thermal jiggling has overwhelmed their desire to align. above that temperature it is just a lump of iron with no north or south pole. below it, the whole thing points the same way.\n\nthat sudden appearance of a preferred direction is what we call an **order parameter** \u2014 a quantity that is zero in the disordered phase and nonzero in the ordered phase. for a magnet, the order parameter is the magnetization $m$. for a liquid-gas transition, it is the density difference. the order parameter is the signature that tells you something dramatic has happened.\n\n## ising model simulation\n\nwatch the ising model in action. at high temperature, the spins are a random mess \u2014 the magnetization is zero on average. as you cool toward $t_c$, domains of aligned spins start forming and growing. at $t_c$ itself, the fluctuations are enormous: the system cannot decide which way to point. below $t_c$, one direction wins, and the magnetization becomes nonzero. that is the phase transition.\n\n[[simulation ising-model]]\n\nin this simulation, you are watching millions of spins make a collective decision \u2014 no leader, no blueprint, just nearest-neighbor interactions. the fact that global order emerges from purely local rules is one of the deepest surprises in physics.\n\n## mean-field hamiltonian\n\ncan we build a simple theory of this transition? the exact hamiltonian of the ising model couples every spin to its neighbors:\n$$\n    \\mathcal{h}\n    =\n    -j \\sum_{\\langle i j \\rangle} s_i s_j - h \\sum_i s_i.\n$$\n\nthe trouble is the $s_i s_j$ coupling \u2014 it ties every spin to its neighbors, and those neighbors to *their* neighbors, creating a tangled web of correlations.\n\nthe **mean-field approximation** cuts this knot with a beautifully simple idea: instead of feeling the actual fluctuating spins of its neighbors, each spin feels only the *average* field from all of them. imagine being in a crowd where everyone is trying to face the same direction. you do not look at each individual \u2014 you just feel the general pull of the crowd.\n\n## mean-field hamiltonian (derivation)\n\nto make the mean-field idea precise, we decompose each spin into its mean and a fluctuation:\n$$\n    s_i = \\langle s_i \\rangle + \\delta s_i = m + \\delta s_i,\n$$\nwhere $m = \\langle s_i \\rangle$ is the magnetization per spin (all spins are equivalent by symmetry).\n\nthe product of two neighboring spins becomes:\n$$\n\\begin{align*}\n    s_i s_j\n    &=\n    (m + \\delta s_i)(m + \\delta s_j)\n    \\\\&=\n    m^2 + m \\, \\delta s_j + \\delta s_i \\, m + \\delta s_i \\, \\delta s_j\n    \\\\& \\approx\n    m^2 + m(s_j - m) + (s_i - m) m\n    \\\\&=\n    -m^2 + m(s_i + s_j).\n\\end{align*}\n$$\n\nwe dropped the $\\delta s_i \\, \\delta s_j$ term \u2014 this is the mean-field approximation. we are saying that the correlated fluctuations between two spins are small enough to ignore. (this is a good approximation when each spin has many neighbors, and a terrible one in low dimensions \u2014 but we will worry about that later.)\n\nsubstituting into the hamiltonian and carefully handling the nearest-neighbor sums (each spin has $z$ neighbors, and each pair is counted once):\n\n**first term:**\n$$\n    j m^2 \\sum_{\\langle i j \\rangle} = \\frac{j n z}{2} m^2.\n$$\n\n**second term:**\n$$\n    * j \\sum_{\\langle i j \\rangle} m(s_i + s_j) = - j z m \\sum_{i} s_i.\n$$\n\n**mean-field hamiltonian:**\n$$\n\\begin{align*}\n    \\mathcal{h}_\\mathrm{mf}\n    &=\n    \\frac{j n z}{2} m^2\n    * (jzm + h) \\sum_i s_i.\n\\end{align*}\n$$\n\nlook at what happened: the spins have decoupled! each spin $s_i$ now sees an effective field $(jzm + h)$ that depends on the *average* magnetization $m$, not on the actual values of its neighbors. this makes the problem exactly solvable \u2014 each spin is independent, and we just need to find $m$ self-consistently.\n\n## big ideas\n\n* an order parameter is a number that is zero above the transition and nonzero below it \u2014 it is the system's way of \"announcing\" that it has chosen a preferred state.\n* phase transitions are fundamentally about symmetry breaking: the hamiltonian treats \"all up\" and \"all down\" equally, yet the system spontaneously chooses one \u2014 the order parameter selects a direction that the underlying physics does not prefer.\n* mean-field theory tames the many-body problem by replacing all the messy neighbor-neighbor interactions with a single average field \u2014 the \"wisdom of the crowd\" acting on each individual.\n* dropping the fluctuation-fluctuation term $\\delta s_i \\, \\delta s_j$ is the mean-field approximation: it works well when each spin has many neighbors, and fails badly in low dimensions where fluctuations dominate.\n\n## what comes next\n\nwe have derived the mean-field hamiltonian, but the real payoff comes from solving it. [mean-field results](meanfieldresults) works through the partition function, the self-consistency equation for magnetization, and the landau free energy expansion. the punchline is a family of **critical exponents** \u2014 the power laws that describe how the magnetization, susceptibility, and heat capacity behave as you approach $t_c$. these exponents will turn out to be the same for a huge class of systems, hinting at a deep universality that we will explain later.\n\n## check your understanding\n\n1. the order parameter of a magnet is the magnetization $m$, which is zero above $t_c$ and nonzero below. can you think of an order parameter for the liquid-gas phase transition? why does it go to zero at the critical point?\n2. the mean-field approximation drops the term $\\delta s_i \\, \\delta s_j$ (correlated fluctuations). in a 1d chain where each spin has only two neighbors, why would yo"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "selfOrganizedCriticality",
      "lessonTitle": "Self-Organized Criticality",
      "x": 0.4643721580505371,
      "y": 0.39833080768585205,
      "searchText": "self-organized criticality\n# self-organized criticality\n\nyou drop one grain of sand on a sandpile and \u2014 nothing happens. you drop another. nothing. another. still nothing. then you drop one more grain and *whoosh* \u2014 half the pile slides off in a massive avalanche. the next grain? nothing again.\n\nhere is what is remarkable: nobody tuned the slope of that pile. nobody set a parameter to a critical value. the pile did it by itself. through the simple process of adding grains and letting them tumble, the sandpile has driven itself to the exact point where tiny inputs can cause huge outputs. no one turned the knob \u2014 the system found the critical point on its own.\n\nthat is **self-organized criticality** (soc), and it may explain why power laws show up everywhere in nature, from earthquakes to forest fires to the electrical activity of your brain.\n\n## the concept of soc\n\nin everything we have studied so far \u2014 the ising model, percolation \u2014 criticality required **fine-tuning**: we had to set the temperature to exactly $t_c$ or the occupation probability to exactly $p_c$. miss by a little, and the beautiful power laws disappear.\n\nsoc is different. these systems spontaneously evolve toward a critical state without any external tuning. they naturally drive themselves to the edge.\n\nkey signatures of soc:\n\n* power-law distributed event sizes (avalanches of all scales).\n* $1/f$ noise in temporal fluctuations.\n* fractal spatial structure (just like the percolation clusters from the percolation and fractals section).\n* separation of time scales: slow driving (one grain at a time) and fast relaxation (avalanches).\n\n## the sandpile model (btw)\n\nthe **bak-tang-wiesenfeld sandpile model** (1987) is the paradigmatic example of soc.\n\non a 2d grid, each site $i$ has a height $z_i$. sand is added one grain at a time at random sites. when $z_i$ exceeds a threshold $z_c$ (typically 4 for a square lattice), the site **topples**:\n\n$$\nz_i \\to z_i - 4, \\qquad z_j \\to z_j + 1 \\quad \\text{for each neighbor } j.\n$$\n\ntoppling can trigger neighbors to topple in turn, creating an **avalanche** that can cascade across the entire system. grains that topple off the boundary are lost (open boundary conditions).\n\n[[simulation sandpile-model]]\n\nwatch the simulation: drop grains one at a time and see what happens. most of the time, nothing dramatic \u2014 the grain just sits there. but occasionally, a single grain triggers a cascade that rearranges the entire pile. small avalanches are common, large ones are rare, and the distribution follows a power law:\n\n$$\np(s) \\sim s^{-\\tau},\n$$\n\nwith $\\tau \\approx 1.1$ in 2d. the distribution of avalanche areas, durations, and lifetimes also follow power laws. there is no characteristic scale \u2014 avalanches of all sizes occur.\n\nafter a transient, the system reaches a statistically steady state. the average rate of sand input equals the average rate of sand falling off the edges. the sandpile does not need a \"criticality knob.\" it self-organizes to the only slope where avalanches of all sizes are possible \u2014 not too steep, not too shallow. tune the slope steeper and avalanches drain it; tune it shallower and grains accumulate until it steepens again. the pile finds the critical point the way water finds its level.\n\n## the bak-sneppen model\n\nthe **bak-sneppen model** (1993) applies soc to biological evolution. consider $n$ species arranged on a ring, each with a random fitness value $f_i \\in [0, 1]$.\n\nat each time step:\n\n1. find the species with the **lowest fitness**.\n2. replace its fitness and the fitness of its two neighbors with new random values from $[0, 1]$.\n\nthe model self-organizes to a critical state where most fitness values exceed a threshold $f_c \\approx 0.667$ (in 1d). below this threshold, species are unstable and trigger cascading replacements \u2014 the avalanches of the model.\n\n[[simulation bak-sneppen]]\n\nthe analogy to evolution is compelling: the weakest species goes extinct and gets replaced, but this disrupts its neighbors (who depended on it ecologically), potentially triggering a cascade of extinctions and replacements. punctuated equilibrium \u2014 long periods of stasis interrupted by bursts of change \u2014 emerges naturally.\n\n## first-return times of random walks\n\nthe connection between soc and random walks provides theoretical insight. consider a 1d random walk starting at the origin. the **first-return time** $t$ is the number of steps until the walker returns to the origin.\n\nfor an unbiased random walk:\n\n$$\np(t = 2n) \\sim n^{-3/2},\n$$\n\na power law with exponent $-3/2$. this result connects to soc because avalanches in many soc models can be mapped to random-walk first-return problems. the correspondence becomes clear when we map each toppling event to a step in a random walk: an avalanche that returns the pile to its critical slope corresponds exactly to a first-return trajectory, and the exponent $\\tau = 3/2$ follows from the classical result for first-return times. more concretely, each toppling event in the sandpile corresponds to a step away from the origin, and the avalanche ends when the walk first returns \u2014 i.e., when the chain of topplings dies out and the system returns to a stable configuration. the avalanche \"starts\" when the system leaves the critical state and \"ends\" when it returns \u2014 just like a random walk departing from and returning to the origin.\n\n[[simulation random-walk-first-return]]\n\nrun this simulation and watch the random walk wander away from zero and then return. short excursions are common, long ones are rare \u2014 and the distribution of return times follows a power law. no tuning required.\n\n## non-equilibrium steady states\n\nsoc systems are fundamentally different from the equilibrium systems we studied earlier. they are **out of equilibrium**: energy (or sand, or fitness) is continuously injected and dissipated.\n\nthis distinguishes soc from equilibrium critical phenomena:\n\n* no detailed balance.\n* no free energy or partition function.\n* the steady state is main"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "statisticalMechanics",
      "lessonTitle": "Statistical Mechanics",
      "x": 0.32136720418930054,
      "y": 0.24656742811203003,
      "searchText": "statistical mechanics\n# statistical mechanics\n\nsuppose you have a huge box full of air molecules bouncing around. billions upon billions of them, all flying in random directions, colliding, exchanging energy. why is the temperature the same everywhere? why doesn't all the energy suddenly rush to one corner, leaving the other side frozen? that is the miracle we are going to explain.\n\nthe answer is statistics. when you have an enormous number of particles, the laws of probability take over, and they are ruthlessly democratic. the system explores every possible arrangement of energy among its particles, and the most probable arrangement wins by an overwhelming margin. that is the core insight of statistical mechanics: we do not need to track every particle. we just need to count states.\n\n## microcanonical ensemble\n\nthe central assumption of statistical mechanics is the principle of *equal a priori probabilities*: all microstates that share a given total energy $e$ are equally likely, so the probability of any one microstate is $p_i = 1/\\omega$, where $\\omega$ is the total number of microstates at that energy. boltzmann's great insight was that the thermodynamic entropy is simply a measure of this count:\n$$\n    s = k_\\mathrm{b} \\ln \\omega.\n$$\nmore microstates means more entropy, and a system left to itself will find its way to the macrostate with the most microstates, because that is simply the most probable outcome. the key conceptual point is that \"disorder\" wins not because it is preferred, but because disordered macrostates correspond to overwhelmingly more microstates. from this single formula, temperature emerges as a derived quantity, $1/t = \\partial s / \\partial e$, measuring how fast the number of accessible states grows when you add energy. if adding a little energy opens up a huge number of new states, the system is cold; if the count barely changes, the system is hot. for the classic worked example (counting states of an ideal gas via phase-space volume and stirling's approximation to recover the sackur-tetrode equation), see schroeder, *an introduction to thermal physics*, ch. 2, or pathria & beale, *statistical mechanics*, ch. 1.\n\n## canonical ensemble\n\n### temperature of system and reservoir become the same in equilibrium\n\nnow let us zoom in on a small piece of a much larger system. imagine a teacup of water sitting in a room. the teacup is our \"system\" and the room is our \"reservoir\" (or \"heat bath\"). they can exchange energy, but the total energy is fixed:\n$$\n    e_\\mathrm{t} = e_\\mathrm{s} + e_\\mathrm{r}.\n$$\n\nthe number of microstates for the whole arrangement factors into a product:\n$$\n    \\omega(e_\\mathrm{s}, e_\\mathrm{r})\n    = \\omega_\\mathrm{s}(e_\\mathrm{s}) \\, \\omega_\\mathrm{r}(e_\\mathrm{r}),\n$$\nso the total entropy is the sum:\n$$\n    s_\\mathrm{t} = s_\\mathrm{s} + s_\\mathrm{r}.\n$$\n\nthe probability of finding a particular split of energy is\n$$\n    p(e_\\mathrm{s}, e_\\mathrm{r})\n    = \\frac{\\omega_\\mathrm{s}(e_\\mathrm{s}) \\, \\omega_\\mathrm{r}(e_\\mathrm{r})}{\\omega(e_\\mathrm{t})}.\n$$\n\nthe most probable state (thermodynamic equilibrium) maximizes this probability. setting the derivative to zero:\n$$\n\\begin{align*}\n    0 &=\n        \\frac{\\partial \\ln p(e_\\mathrm{s}, e_\\mathrm{r})}{\\partial e_\\mathrm{s}}\n        \\\\&=\n        \\frac{\\partial}{\\partial e_\\mathrm{s}}\n        \\ln \\omega_\\mathrm{s}(e_\\mathrm{s})\n        +\n        \\frac{\\partial}{\\partial e_\\mathrm{r}}\n        \\frac{\\partial e_\\mathrm{r}}{\\partial e_\\mathrm{s}}\n        \\ln \\omega_\\mathrm{r}(e_\\mathrm{r})\n        \\\\&=\n        \\frac{1}{k_\\mathrm{b}}\n        \\frac{\\partial s_\\mathrm{s}}{\\partial e_\\mathrm{s}}\n        -\n        \\frac{1}{k_\\mathrm{b}}\n        \\frac{\\partial s_\\mathrm{r}}{\\partial e_\\mathrm{r}}\n        \\\\&=\n        \\frac{1}{t_\\mathrm{s}}\n        -\n        \\frac{1}{t_\\mathrm{r}}.\n\\end{align*}\n$$\n\nthe conclusion is simple and beautiful: in equilibrium, the system and reservoir have the same temperature.\n$$\n    t_\\mathrm{s} = t_\\mathrm{r}\n$$\n\nthis is not something we assumed. it *followed* from counting states. temperature equality is the most probable outcome, and for large systems it is overwhelmingly probable.\n\n### boltzmann distribution\n\nwhen the system is in a specific microstate $i$ with energy $e_i$, the reservoir has energy $e_\\mathrm{t} - e_i$. since the reservoir is enormous, we can taylor-expand its entropy:\n$$\n\\begin{align*}\n    p_i\n    &\\propto\n        \\omega_\\mathrm{r}(e_\\mathrm{t} - e_i)\n    \\\\&=\n        \\exp\n        \\left[ \\frac{1}{k_\\mathrm{b}} s_r(e_\\mathrm{t} - e_i) \\right]\n    \\\\&\\approx\n        \\exp \\left[ \\frac{1}{k_\\mathrm{b}}\n        \\left(s_\\mathrm{r}(e_\\mathrm{t})\n        * \\left.\n        \\frac{\\mathrm{d}s_\\mathrm{r}}{\\mathrm{d}e}\n        \\right|_{e=e_\\mathrm{t}}e_i\n        \\right)\n        \\right]\n    \\\\&\\propto\n        \\exp \\left(\n        -\\frac{e_i}{k_\\mathrm{b}t}\n        \\right).\n\\end{align*}\n$$\n\nthis is the **boltzmann distribution**: the probability of finding the system in state $i$ falls off exponentially with the energy of that state. high-energy states are exponentially rare. low-energy states are exponentially favored. temperature controls how steep the falloff is.\n\n### partition function, free energy, and thermodynamic observables\n\nimagine you have a huge library of every possible arrangement of energy among the particles in your system. some arrangements have low energy, some high. the partition function is a single number that summarizes the entire library: it is a weighted catalog where each arrangement is counted, but high-energy arrangements are exponentially discounted. once you have this catalog, you can look up any thermodynamic quantity \u2014 energy, entropy, heat capacity \u2014 just by taking derivatives. it is the rosetta stone of statistical mechanics.\n\nthe **partition function** is the normalization constant that makes probabilities add up to one:\n$$\n\\begin{align*}\n    z &= \\sum_i \\exp \\left(-\\frac{e_i}{k_\\mathrm{b}t}\\right)\n        \\\\&= \\sum_i \\exp \\left(-\\beta e_i\\r"
    },
    {
      "topicId": "complex-physics",
      "topicTitle": "Complex Physics",
      "routeSlug": "complex-physics",
      "lessonSlug": "transferMatrix",
      "lessonTitle": "1D Ising Model and Transfer Matrix Method",
      "x": 0.32024112343788147,
      "y": 0.28901374340057373,
      "searchText": "1d ising model and transfer matrix method\n# 1d ising model and transfer matrix method\n\nthink of a one-dimensional chain of spins as a book. each page (spin) can be \"up\" or \"down.\" the energy cost depends only on whether two neighboring pages match or not. the transfer matrix is a clever way of multiplying the probabilities page by page until you reach the end of the book \u2014 and then the eigenvalues pop out and hand you the exact answer.\n\nthis is one of the rare cases in statistical mechanics where we can solve the many-body problem *exactly*. no approximations, no monte carlo, no mean-field hand-waving. just linear algebra.\n\n## hamiltonian\n\nthe hamiltonian of the 1d ising model with periodic boundary conditions ($s_{n+1} = s_1$) is\n$$\n    \\mathcal{h}\n    =\n    -j\\sum_{i=1}^{n} s_i s_{i+1} - h \\sum_{i=1}^{n} s_i.\n$$\n\n## partition function\n\nthe partition function is a sum over all $2^n$ configurations:\n$$\n\\begin{align*}\n    z\n    &=\n    \\sum_{s_1 = \\pm 1} \\cdots \\sum_{s_n = \\pm 1}\n    \\exp\n    \\left(\n        \\beta j\\sum_{i=1}^{n} s_i s_{i+1}\n        + \\beta h \\sum_{i=1}^{n} s_i\n    \\right).\n\\end{align*}\n$$\n\nthe trick is to split the external field term equally between each pair of neighbors:\n$$\n    \\beta h \\sum_i s_i = \\beta h \\sum_i \\frac{s_i + s_{i+1}}{2},\n$$\nso the whole exponent factorizes into a product of identical \"bond\" terms:\n$$\n    z =\n    \\sum_{s_1 = \\pm 1} \\cdots \\sum_{s_n = \\pm 1}\n    \\prod_{i=1}^n\n    \\exp\n    \\left(\n        \\beta j s_i s_{i+1}\n        + \\beta h \\frac{s_i+s_{i+1}}{2}\n    \\right).\n$$\n\neach factor depends only on two neighboring spins. that is a matrix element waiting to happen.\n\n## transfer matrix\n\ndefine the transfer matrix $t$ with elements\n$$\n    t_{s_i, s_{i+1}}\n    =\n    \\exp\n    \\left(\n        \\beta j s_i s_{i+1}\n        + \\beta h \\frac{s_i+s_{i+1}}{2}\n    \\right).\n$$\n\nwritten out explicitly as a $2 \\times 2$ matrix:\n$$\n    t\n    =\n    \\begin{bmatrix}\n    e^{\\beta(j+h)} & e^{-\\beta j} \\\\\n    e^{-\\beta j} & e^{\\beta(j-h)}\n    \\end{bmatrix}.\n$$\n\nnow here is where the magic happens. the partition function involves summing products of these matrix elements over all intermediate spins \u2014 but that is exactly what matrix multiplication does! summing over $s_2$ gives $(t^2)_{s_1, s_3}$, then summing over $s_3$ gives $(t^3)_{s_1, s_4}$, and so on:\n$$\n\\begin{align*}\n    z\n    &=\n    \\sum_{s_1 = \\pm 1} \\cdots \\sum_{s_n = \\pm 1}\n    t_{s_1, s_2} t_{s_2, s_3} \\cdots t_{s_n, s_1}\n    \\\\&=\n    \\sum_{s_1 = \\pm 1}\n    (t^n)_{s_1, s_1}\n    \\\\&=\n    \\mathrm{tr}(t^n).\n\\end{align*}\n$$\n\nthe partition function is the trace of the $n$-th power of the transfer matrix. we have converted the statistical mechanics problem into a linear algebra problem.\n\n## eigenvalues and the thermodynamic limit\n\nsince $t$ is a real symmetric $2 \\times 2$ matrix, it has two real eigenvalues $\\lambda_+ > \\lambda_-$. diagonalizing and using the cyclic property of the trace gives the exact partition function in closed form:\n$$\n    z = \\mathrm{tr}(t^n) = \\lambda_+^n + \\lambda_-^n, \\qquad\n    \\lambda_{\\pm}\n    =\n    e^{\\beta j} \\cosh(\\beta h)\n    \\pm\n    \\sqrt{\n        e^{2\\beta j} \\sinh^2(\\beta h)\n        + e^{-2\\beta j}\n    }.\n$$\nthe entire partition function of $2^n$ configurations reduces to two eigenvalues of a $2 \\times 2$ matrix \u2014 that is the power of the transfer matrix. in the thermodynamic limit ($n \\to \\infty$), $\\lambda_-^n / \\lambda_+^n \\to 0$, and the free energy per spin depends only on the largest eigenvalue:\n$$\n    f = -\\frac{1}{\\beta n} \\ln z = -\\frac{1}{\\beta} \\ln \\lambda_+.\n$$\nthe same structure holds for any 1d nearest-neighbor model: replace the $2 \\times 2$ matrix with a $q \\times q$ transfer matrix (for $q$-state spins or potts models), diagonalize, and the largest eigenvalue determines the thermodynamics. the algebra grows but the logic is identical.\n\n[[simulation transfer-matrix-demo]]\n\n## the punchline: no phase transition in 1d\n\nfor $h = 0$, the eigenvalues simplify to\n$$\n    \\lambda_+ = e^{\\beta j} + e^{-\\beta j} = 2\\cosh(\\beta j), \\qquad\n    \\lambda_- = e^{\\beta j} - e^{-\\beta j} = 2\\sinh(\\beta j).\n$$\n\nboth eigenvalues are positive and analytic (smooth) functions of temperature for all $t > 0$. since the free energy $f = -(1/\\beta)\\ln\\lambda_+$ is an analytic function of $t$, there is *no singularity* at any finite temperature. no singularity means no phase transition.\n\nthis is the exact confirmation of what we suspected: in one dimension, thermal fluctuations always win. a single domain wall (a place where neighboring spins disagree) costs energy $2j$ but increases entropy by $k_\\mathrm{b} \\ln n$ (it can be placed at any of $n$ bonds). for large enough $n$, the entropy always beats the energy, and domain walls proliferate, destroying any long-range order.\n\nmean-field theory predicted a phase transition in 1d \u2014 and that prediction is wrong. this is a concrete example of why mean-field theory fails in low dimensions: it ignores the fluctuations that matter most.\n\n## big ideas\n\n* the transfer matrix converts a statistical mechanics sum over $2^n$ configurations into a linear algebra problem \u2014 the partition function is just $\\mathrm{tr}(t^n)$.\n* in the thermodynamic limit, only the largest eigenvalue $\\lambda_+$ survives: $z \\approx \\lambda_+^n$, so all thermodynamics comes from a single number.\n* a phase transition requires a singularity in the free energy, which requires a singularity in $\\lambda_+$. in 1d, $\\lambda_+$ is always smooth \u2014 so no phase transition exists at any finite temperature, ever.\n* this is not a failure of the model but a deep truth: in 1d, the entropy of domain walls always beats the energy cost of making them, so ordered regions are always destroyed.\n\n## what comes next\n\nthe transfer matrix gave us an exact result, but it was a small-system trick \u2014 it works beautifully in 1d but does not straightforwardly generalize to 2d or 3d. the next natural question is bigger and more mysterious: why do completely different systems \u2014 magnets, fluids, alloys \u2014 behave "
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "channelsandpipes",
      "lessonTitle": "Channels and Pipes",
      "x": 0.21632935106754303,
      "y": 0.822983980178833,
      "searchText": "channels and pipes\n# channels and pipes\n\n## the beauty of exact solutions\n\nmost of the time, the navier-stokes equation is too hard to solve analytically. but for a few special geometries \u2014 flows between parallel plates and through circular pipes \u2014 the equation simplifies enough that we can solve it exactly, by hand. these aren't just textbook exercises. they're the foundations for understanding everything from blood flow in arteries to glaciers sliding down valleys.\n\nthe key simplification: for **steady** ($\\partial/\\partial t = 0$), **fully developed** (the flow doesn't change along the channel), **unidirectional** flow, the nonlinear advection term $(\\mathbf{v} \\cdot \\nabla)\\mathbf{v}$ vanishes. the navier-stokes equation reduces to a balance between the pressure gradient (or gravity) and viscous friction.\n\n## pressure-driven channel flow \u2014 the parabolic profile\n\nimagine two infinite parallel plates separated by a distance $2a$, with fluid pushed through by a pressure gradient $g = -dp/dx$. the no-slip boundary condition (fluid velocity is zero at the walls) and symmetry give:\n$$\nv_x(y) = \\frac{g}{2\\eta}(a^2 - y^2)\n$$\n\nthe velocity profile is a **parabola**: fastest in the center, zero at the walls. this is **poiseuille flow** between parallel plates.\n\nthe shape of this parabola tells you about the fluid. for a newtonian fluid ($n = 1$), it's a perfect parabola. for a **shear-thinning** material ($n < 1$), the profile becomes more \"blunted\" \u2014 flatter in the middle, dropping sharply near the walls. for a **shear-thickening** material ($n > 1$), the profile becomes more \"pointed\" \u2014 sharper in the center.\n\nglaciers are a dramatic example: ice behaves as a shear-thickening fluid ($n > 1$ in glen's flow law), so the velocity profile across a glacier valley is remarkably flat in the middle and drops off steeply near the valley walls. gladys the glacier knows her flow law.\n\n## gravity-driven planar flow \u2014 the waterfall problem\n\nnow tilt the channel. instead of a pressure gradient pushing the fluid, gravity does the work. think of rain flowing down a tilted roof, or a thin film of water on an inclined plane.\n\nfor a uniform film of thickness $a$ on a plane inclined at angle $\\theta$:\n$$\n0 = g_x + \\nu \\frac{\\partial^2 v_x}{\\partial y^2}, \\qquad 0 = g_y - \\frac{1}{\\rho}\\frac{\\partial p}{\\partial y}\n$$\n\nwhere $g_x = g_0 \\sin\\theta$ drives the flow downhill. for a newtonian fluid, the velocity profile is again parabolic:\n$$\nv_x(y) = \\frac{g_0 \\sin\\theta}{2\\nu}(2ay - y^2)\n$$\n\nmaximum velocity is at the free surface ($y = a$), zero at the wall ($y = 0$). by measuring the velocity profile \u2014 for instance, by tracking particles on the surface \u2014 you can estimate the power-law exponent $n$ and learn about the fluid's rheology.\n\n## laminar pipe flow \u2014 the hagen-poiseuille result\n\nthe most practically important exact solution: steady flow through a circular pipe of radius $a$ driven by a pressure gradient $g$. the velocity profile in cylindrical coordinates is:\n$$\nv_z(r) = \\frac{g}{4\\eta}(a^2 - r^2)\n$$\n\nagain parabolic \u2014 fastest on the centerline, zero at the pipe wall. the **volume flow rate** (total discharge) is:\n$$\nq = \\int_0^a v_z(r) \\, 2\\pi r \\, dr = \\frac{\\pi g a^4}{8\\eta}\n$$\n\nthis is the **hagen-poiseuille law**: flow rate scales with the *fourth power* of the pipe radius. double the pipe diameter and you get 16 times the flow. this is why your arteries care so much about even a small amount of plaque buildup \u2014 a 10% reduction in radius cuts flow by over 34%.\n\nthe reynolds number for pipe flow is re $= ud/\\nu$, where $u$ is the mean velocity and $d = 2a$ is the diameter. for re typically below about 2300, the flow is laminar and the poiseuille solution holds. above that, the flow transitions to turbulence and everything gets much more complicated.\n\n[[simulation bernoulli-streamline]]\n\n[[simulation poiseuille-vs-power-law]]\n\n## big ideas\n\n* for steady, fully developed, unidirectional flow, the nonlinear advection term vanishes entirely and navier-stokes reduces to a simple second-order ode \u2014 solvable by hand.\n* both pressure-driven and gravity-driven flows produce parabolic velocity profiles for newtonian fluids. the parabola sharpens (power-law $n > 1$) or blunts ($n < 1$) for non-newtonian materials.\n* the hagen-poiseuille law $q \\propto a^4$ is one of the most consequential scaling laws in biology and engineering: a 10% reduction in artery radius cuts blood flow by more than a third.\n* laminar pipe flow transitions to turbulence above re around 2300 \u2014 and once turbulent, the simple poiseuille solution is gone and the problem becomes far harder.\n\n## what comes next\n\nwe've seen flows driven by pressure and gravity in confined geometries. the next section takes us to the surface: gravity waves on water, shallow-water equations, and the physics of ocean waves.\n\n## check your understanding\n\n1. a pipe of radius $a$ carries fluid at flow rate $q$. you replace it with a pipe of radius $2a$ at the same pressure gradient. by what factor does the flow rate increase? by what factor does the mean velocity change?\n2. in gravity-driven planar flow on an inclined surface, the maximum velocity occurs at the free surface, not at the center. why? where would the maximum occur in pressure-driven channel flow between two no-slip walls?\n3. glaciers follow glen's flow law with $n \\approx 3$. how does the velocity profile across a glacier valley differ qualitatively from the newtonian ($n = 1$) poiseuille profile? sketch both profiles and explain the physical reason for the difference.\n\n## challenge\n\nblood ($\\eta \\approx 3 \\times 10^{-3}$ pa\u00b7s, $\\rho \\approx 1060$ kg/m\u00b3) flows through a capillary of radius 4 $\\mu$m and length 1 mm, driven by a pressure difference of 25 pa. compute the volume flow rate using the hagen-poiseuille law. then check the reynolds number \u2014 is laminar flow a reasonable assumption? now suppose the capillary narrows to radius 3.5 $\\mu$m due to plaque buildup. by what percentage does the flow rate "
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "contapprox",
      "lessonTitle": "Continuum Approximation",
      "x": 0.10420393198728561,
      "y": 0.7814384698867798,
      "searchText": "continuum approximation\n# continuum approximation\n\n## the big idea\n\nhere's the deal: everything is made of atoms. water, steel, glaciers, cheese \u2014 it's all discrete little particles bouncing around. but if you try to track every atom in a glass of water (about $10^{25}$ of them), you'll be dead before your computer finishes the first timestep.\n\nso we do something audacious. we *ignore the atoms*. we pretend the material is perfectly smooth and continuous, like a mathematical function that you can differentiate everywhere. this is the **continuum approximation**, and it's the foundation of everything in this course.\n\nit's the same trick you use every day without thinking about it. when you say \"it's raining,\" you're ignoring individual raindrops. when you look at a photograph, you're ignoring individual pixels. when you check the temperature outside, you're ignoring the fact that some air molecules are screaming along at 600 m/s while others are barely moving. you're *averaging*, and that averaging is the continuum approximation.\n\nbut when does this trick actually work? and when does it break down? that's what this section is about.\n\nas benny lautrup puts it: \"science originates from curiosity and bad eyesight.\" the continuum approximation is the formal version of *not looking too closely*.\n\nthe rule of thumb: it works whenever your measuring stick is vastly larger than the distance molecules travel between collisions. we'll make this precise below.\n\n## density fluctuations \u2014 when is smooth smooth enough?\n\nlet's start with something concrete. you have a box of gas. the density is:\n$$\n\\rho = \\frac{n m}{v}\n$$\nwhere $n$ is the number of molecules, $v$ is the volume, and $m$ is the mass of each molecule.\n\nnow here's the question you should be asking: *if i measure the density in a tiny box versus a slightly different tiny box right next to it, will i get the same answer?*\n\nnot exactly, no. there will be fluctuations \u2014 some boxes have a few more molecules, some have a few less. from basic statistics, the relative fluctuation goes like:\n$$\n\\frac{\\delta \\rho}{\\rho} = \\frac{\\delta n}{n} = \\frac{1}{\\sqrt{n}}\n$$\n\nso if you want a relative precision of $\\epsilon = 10^{-3}$ (one part in a thousand), you need at least $n > \\epsilon^{-2} = 10^6$ molecules in your box. those molecules occupy a certain volume, and the side length of the smallest box that gives you that precision is:\n$$\nl_{\\text{micro}} = \\epsilon^{-2/3} l_{\\text{mol}}\n$$\nwhere $l_{\\text{mol}}$ is the typical spacing between molecules:\n$$\nl_{\\text{mol}} = \\left( \\frac{v}{n} \\right)^{1/3} = \\left( \\frac{m_{\\text{mol}}}{\\rho n_a} \\right)^{1/3}\n$$\n\nfor air at sea level, $l_{\\text{mol}} \\approx 3 \\times 10^{-9}$ m. so $l_{\\text{micro}} \\approx 3 \\times 10^{-7}$ m \u2014 about 300 nanometers. anything bigger than that, and the continuum approximation gives you density to better than 0.1%. that's *tiny*. the continuum approximation works spectacularly well for everyday situations.\n\n[[simulation averaging-volume]]\n\n## macroscopic smoothness \u2014 the slow-change rule\n\nhaving enough molecules in each box is necessary but not sufficient. you also need the density to change *gradually* from one box to the next. if the density jumps wildly between neighboring cells, \"smooth\" is a lie and your derivatives are meaningless.\n\nwe require that the relative change in density between adjacent cells is also less than $\\epsilon$:\n$$\n\\left| \\frac{\\partial \\rho}{\\partial x} \\right| < \\frac{\\epsilon \\, \\rho}{l_{\\text{micro}}}\n$$\n\nthis defines a macroscopic length scale:\n$$\nl_{\\text{macro}} = \\epsilon^{-1} l_{\\text{micro}}\n$$\n\nif your physical situation varies on length scales larger than $l_{\\text{macro}}$, the continuum approximation is valid. for air with $\\epsilon = 10^{-3}$, that's about $l_{\\text{macro}} \\approx 0.3$ mm. still tiny.\n\nbut here's the catch: at *interfaces* between different materials (the surface of a water droplet, the edge of a steel beam), properties change over distances comparable to $l_{\\text{mol}}$ \u2014 far too sharp for the continuum approximation. so we represent these as **surface discontinuities** and handle them separately with boundary conditions.\n\n## velocity fluctuations \u2014 the thermal jitter problem\n\nfor fluids, there's another wrinkle. molecules don't just sit still \u2014 they're buzzing around with thermal energy. the root-mean-square molecular speed is:\n$$\nv_{\\text{mol}} = \\sqrt{\\frac{3 r t}{m_{\\text{mol}}}}\n$$\n\nfor air at room temperature, that's about 500 m/s. if you're measuring the *bulk velocity* of a gentle breeze (say 5 m/s), the thermal jitter is 100 times larger than the signal you're trying to measure.\n\nto keep the velocity fluctuations below $\\epsilon$, you need a bigger box:\n$$\nl_{\\text{micro}}^* = \\left( \\frac{v_{\\text{mol}}}{v} \\right)^{2/3} l_{\\text{micro}}\n$$\n\nfor a 5 m/s breeze in air, $l_{\\text{micro}}^* \\approx 100 \\, l_{\\text{micro}}$. still small enough for the continuum approximation to hold in most practical situations \u2014 but now you see that slow-moving flows need *bigger* averaging volumes than fast ones.\n\n## mean free path \u2014 the drunk crowd at closing time\n\nthere's one more way to think about when the continuum approximation works, and it's perhaps the most vivid.\n\npicture ten thousand people leaving a bar at closing time. they're stumbling around, bouncing off each other, changing direction with every collision. the average distance someone travels before bumping into the next person is the **mean free path** $\\ell$.\n\nfor air at sea level, the mean free path is about $\\ell \\approx 68$ nm \u2014 molecules travel less than a ten-thousandth of a millimeter before colliding. if your measuring stick (the length scale of your problem) is much longer than the mean free path, you can safely average over many collisions and treat the material as continuous.\n\nthe rule of thumb: the continuum approximation holds when\n$$\n\\text{kn} = \\frac{\\ell}{l} \\ll 1\n$$\n\nwhere $l$ is the characteristic length of your problem and kn is the **knud"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "creepingflow",
      "lessonTitle": "Creeping Flow",
      "x": 0.19111928343772888,
      "y": 0.8352689146995544,
      "searchText": "creeping flow\n# creeping flow\n\n## when the fluid forgets its own history\n\nimagine swimming through honey. you push forward, but the moment you stop pushing, you stop moving. there's no gliding, no coasting, no inertia to speak of. the honey is so viscous that it instantly dampens any momentum. the fluid *forgets its own history* \u2014 it has no memory of what it was doing a moment ago.\n\nthis is **creeping flow** (also called **stokes flow**): flow at very low reynolds numbers, where viscosity completely dominates over inertia. it describes heavy oils, biological flows at the cellular scale, magma oozing through rock, and \u2014 importantly for us \u2014 glaciers.\n\n## the stokes equations \u2014 navier-stokes without the hard part\n\nwhen re $\\ll 1$, the advective term $(\\mathbf{v} \\cdot \\nabla)\\mathbf{v}$ in the navier-stokes equation becomes negligible. for steady flow ($\\partial \\mathbf{v}/\\partial t = 0$), the equations simplify dramatically:\n$$\n\\nabla p = \\eta \\nabla^2 \\mathbf{v}, \\qquad \\nabla \\cdot \\mathbf{v} = 0\n$$\n\nthis is the **stokes equation**: pressure gradient balances viscous forces. it's *linear* \u2014 which means superposition works, solutions are unique, and the mathematical machinery is much more tractable than for the full navier-stokes equation.\n\nthe linearity has a strange consequence: creeping flow is **time-reversible**. if you reverse all the forces, the flow runs backwards through exactly the same states. this is why microorganisms can't swim by reciprocal motions (the \"scallop theorem\") \u2014 a scallop opening and closing its shell would go nowhere at re $\\ll 1$.\n\n## drag and lift \u2014 how the fluid pushes back\n\nplace a body in a creeping flow. the fluid exerts a force on the body through the no-slip boundary condition at its surface:\n$$\n\\mathbf{r} = \\oint_s \\sigma \\cdot d\\mathbf{s} = \\mathbf{d} + \\mathbf{l}\n$$\n\nthis reaction force splits into:\n* **drag** $\\mathbf{d}$: the component in the direction of the flow \u2014 it resists the body's motion through the fluid.\n* **lift** $\\mathbf{l}$: the component perpendicular to the flow.\n* there can also be a **torque** that makes the body spin.\n\nthe drag itself has two contributions: **viscous drag** (shear stresses on the surface \u2014 the fluid \"rubbing\" against the body) and **pressure drag** (the pressure difference between the front and back of the body \u2014 the \"suction\" effect).\n\n## the sphere in stokes flow \u2014 a classic result\n\nfor a sphere of radius $a$ moving at speed $u$ through a creeping flow, the drag is:\n$$\nd = 6\\pi \\eta a u\n$$\n\nthis is **stokes' drag law**, and it's one of the most useful results in fluid mechanics. it tells you:\n* drag is proportional to velocity (not velocity squared, as in high-re turbulent flow).\n* drag is proportional to radius (not radius squared).\n* drag is proportional to viscosity.\n\nstokes' law is how we measure the viscosity of fluids (drop a known sphere and time its fall), estimate the settling rate of sediment in water, and understand why fog droplets hang in the air so much longer than raindrops.\n\nin creeping flow, the isobars (lines of constant pressure) stretch far out into the flow \u2014 much further than in nearly ideal (high-re) flow. this is because viscous effects are felt over long distances when inertia is absent.\n\n## non-newtonian creeping flow\n\nfor non-newtonian fluids in creeping flow, we return to cauchy's equation and use a power-law constitutive relation instead of the linear newtonian one. the effective viscosity depends on the strain rate:\n$$\n\\eta_{\\text{eff}} = k \\dot{\\gamma}^{n-1}\n$$\n\nfor glaciers, $n \\approx 3$ (glen's flow law), making ice a strongly shear-thinning material at glacier scales \u2014 it flows more easily where the shear rates are high, which is near the base and the valley walls.\n\n[[simulation stokes-flow-demo]]\n\n## big ideas\n\n* creeping flow (re $\\ll 1$) is the world of viscosity as the absolute ruler: the moment you stop pushing, the fluid stops moving. there is no coasting.\n* the stokes equation is linear \u2014 which is remarkable. superposition works, solutions are unique, and the math becomes tractable. linearity is the direct consequence of dropping the nonlinear advection term.\n* time-reversibility is the strangest consequence of linearity: run the forces in reverse, and the flow retraces its path exactly. this is why the \"scallop theorem\" forbids reciprocal swimming strokes at low re.\n* stokes' drag law $d = 6\\pi\\eta a u$ says drag is proportional to velocity (not $v^2$), to size (not size$^2$), and to viscosity. this is the formula behind sedimentation, aerosol dynamics, and viscometry.\n\n## what comes next\n\nwe've seen what happens when viscosity dominates. now let's go back and meet the other limit: fluids where motion is absent altogether. pressure, buoyancy, floating icebergs \u2014 fluids at rest.\n\n## check your understanding\n\n1. a raindrop of radius 1 mm falls at terminal velocity in air ($\\eta \\approx 1.8 \\times 10^{-5}$ pa\u00b7s, $\\rho_{\\text{air}} = 1.2$ kg/m\u00b3). at terminal velocity, stokes drag equals the net gravitational force. estimate the terminal velocity and check whether re is small enough for stokes' law to apply.\n2. you reverse the direction of the pressure gradient driving a stokes flow around a bump. what happens to the flow field? contrast this with what would happen in a high-reynolds-number flow.\n3. microorganisms can't swim using a single hinged \"flap\" that opens and closes \u2014 the scallop theorem forbids it. what makes flagella and cilia effective at low re, and why does a helix work when a flap doesn't?\n\n## challenge\n\na sphere of radius $a$ and density $\\rho_s$ settles at terminal velocity through a fluid of density $\\rho_f$ and viscosity $\\eta$. derive an expression for the terminal velocity using stokes' drag law. now consider a suspension of many identical spheres at volume fraction $\\phi$ (fraction of space occupied by spheres). the effective viscosity increases as $\\eta_{\\text{eff}} = \\eta(1 + 5\\phi/2)$ for dilute suspensions (einstein's formula). how does the termin"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "dynamics",
      "lessonTitle": "The Heartbeat Equation \u2014 Dynamics of Continua",
      "x": 0.11185731738805771,
      "y": 0.8280162811279297,
      "searchText": "the heartbeat equation \u2014 dynamics of continua\n# the heartbeat equation \u2014 dynamics of continua\n\n## one equation to rule them all\n\nhere's something beautiful: whether you're modeling a glacier grinding through a valley, honey dripping off a spoon, or a steel beam vibrating after being struck \u2014 the fundamental equation is *the same*. it's the cauchy momentum equation, and it's the **heartbeat** of continuum mechanics.\n\neverything we've done so far \u2014 stress tensors, strain tensors, hooke's law \u2014 was building toward this. now we put it all together.\n\n## the ingredients \u2014 mass, momentum, and forces\n\nbefore we get to the big equation, let's make sure we know what we're tracking. for a chunk of material occupying volume $v$:\n\n**mass** \u2014 how much stuff is there:\n$$\nm = \\int_v \\rho \\, dv\n$$\n\n**momentum** \u2014 how much \"oomph\" does it carry:\n$$\n\\mathbf{p} = \\int_v \\rho \\, \\mathbf{v} \\, dv\n$$\n\n**angular momentum** \u2014 how much is it spinning:\n$$\n\\mathbf{l} = \\int_v \\mathbf{x} \\times \\rho \\, \\mathbf{v} \\, dv\n$$\n\n**kinetic energy** \u2014 how much energy is in the motion:\n$$\nk = \\int_v \\frac{1}{2} \\rho \\, v^2 \\, dv\n$$\n\n## conservation of mass \u2014 stuff doesn't disappear\n\nthe simplest conservation law: the mass in a volume $v$ can only change if stuff flows in or out through the surface $s$:\n$$\n\\frac{d}{dt} \\int_v \\rho \\, dv = -\\oint_s \\rho \\, \\mathbf{v} \\cdot \\hat{n} \\, da\n$$\n\nusing the divergence theorem to convert the surface integral into a volume integral:\n$$\n\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho \\, \\mathbf{v}) = 0\n$$\n\nthis is the **continuity equation**. it says: the rate at which density changes at a point equals the rate at which mass flows away from that point. nothing more, nothing less.\n\n## the material derivative \u2014 riding the flow\n\nhere's a subtlety that trips up everyone the first time. there are two ways to watch a flowing river:\n\n* **stand on the bank** (eulerian view): you watch the water flow past you. at your fixed location, the velocity changes over time as different parcels of water arrive.\n* **jump in a boat** (lagrangian view): you ride along with the water. the velocity you experience changes because you're moving to new locations.\n\nthe **material derivative** $d/dt$ captures the boat-rider's perspective:\n$$\n\\frac{dq}{dt} = \\frac{\\partial q}{\\partial t} + (\\mathbf{v} \\cdot \\nabla) q\n$$\n\nthe first term is the *local* change (what happens at your fixed point). the second term is the *advective* change (what changes because you moved to a new location). together, they give the total rate of change *experienced by a moving parcel of material*.\n\nthink of it this way: imagine you're in a hot air balloon drifting east. the temperature at your location changes for two reasons: (1) the air around you might be heating up (local change), and (2) you're drifting into a region that was already warmer (advective change).\n\nin the lagrangian picture, conservation of mass becomes elegantly simple:\n$$\n\\frac{dm}{dt} = 0\n$$\n\nand the material derivative of density:\n$$\n\\frac{d\\rho}{dt} = -\\rho (\\nabla \\cdot \\mathbf{v})\n$$\n\nfor an incompressible material ($\\nabla \\cdot \\mathbf{v} = 0$), the density of each parcel doesn't change as it moves \u2014 which makes sense, since incompressibility means volumes don't change.\n\n## transport of any quantity \u2014 the general recipe\n\nthe material derivative works for *any* quantity, not just density. if $q$ is some specific (per-unit-mass) quantity like temperature or chemical concentration, then the total amount in a volume is $q = \\int_v \\rho \\, q \\, dv$, and:\n$$\n\\frac{dq}{dt} = \\int_v \\rho \\frac{dq}{dt} \\, dv\n$$\n\nthis is the general transport equation. imagine a box full of some quantity $q$, drifting with the flow. what enters and leaves through the boundaries changes $q$:\n$$\n\\frac{\\partial (\\rho q)}{\\partial t} + \\nabla \\cdot (\\rho q \\, \\mathbf{v}) = \\rho \\frac{dq}{dt}\n$$\n\nthis works because the mass conservation terms cancel, leaving only the \"ride along\" derivative.\n\n## cauchy's equation \u2014 the heartbeat\n\nnow we're ready. newton's second law says momentum changes equal forces. for a continuum:\n$$\n\\rho \\frac{d\\mathbf{v}}{dt} = \\mathbf{f} + \\nabla \\cdot \\sigma\n$$\n\nthat's it. that's the heartbeat equation. let's unpack it:\n\n* **left side**: mass per unit volume times acceleration (in the material derivative sense \u2014 following the flow).\n* **$\\mathbf{f}$**: body forces, like gravity ($\\rho \\, \\mathbf{g}$).\n* **$\\nabla \\cdot \\sigma$**: the divergence of the stress tensor \u2014 the net force per unit volume from all the internal stresses acting on the material.\n\nthis is **cauchy's equation**, and it's universal. it doesn't care whether you're dealing with a solid, a liquid, or anything in between. the difference between solids and fluids comes in *later*, when you specify what $\\sigma$ looks like:\n\n* **for an elastic solid**: $\\sigma = \\lambda \\, \\text{tr}(\\varepsilon) \\, \\mathbf{i} + 2\\mu \\, \\varepsilon$ \u2192 you get the **navier-cauchy equation**.\n* **for a viscous fluid**: $\\sigma = -p\\,\\mathbf{i} + 2\\eta \\, \\dot{\\varepsilon}$ \u2192 you get the **navier-stokes equation**.\n\nsame heartbeat. different constitutive law. that's the deep unity of continuum mechanics.\n\n> **reading cauchy's equation physically:** the left side, $\\rho\\,d\\mathbf{v}/dt$, is *how much the velocity of this blob is changing as it rides the flow*. the right side, $\\mathbf{f} + \\nabla \\cdot \\sigma$, is *every force that could act on it, per unit volume* \u2014 gravity pulling it down, pressure pushing from all sides, viscous stresses dragging on its surfaces. newton's second law, applied to a smear of matter.\n\n[[simulation deformation-grid]]\n\n[[simulation ps-wave-animation]]\n\n## big ideas\n\n* the continuity equation is just \"stuff doesn't disappear,\" written as a pde. every conservation law in continuum mechanics has this same structure: a time derivative plus a divergence of flux equals zero.\n* the material derivative $d/dt = \\partial/\\partial t + (\\mathbf{v} \\cdot \\nabla)$ is the derivative that rides along with th"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "elasticity",
      "lessonTitle": "Elasticity",
      "x": 0.10549811273813248,
      "y": 0.9217578172683716,
      "searchText": "elasticity\n# elasticity\n\n## the cheese test\n\nfrom basic mechanics, you know hooke's law: $f = -kx$. a spring, one dimension, done. but the world isn't one-dimensional. when you squeeze a block of gouda cheese, it doesn't just compress downward \u2014 it *bulges outward*. when you stretch a rubber band lengthwise, it gets thinner in the middle. forces and deformations couple across dimensions, and the simple spring constant $k$ isn't enough anymore.\n\nwhy cheese? because it's *perfect* for building intuition. it's solid enough to hold its shape (unlike honey), but soft enough that you can actually *see* the deformation with your eyes (unlike steel). a one cubic meter block of gouda is our mental laboratory for this section. put a weight on top and watch what happens.\n\n## young's modulus \u2014 how stiff is your cheese?\n\nyoung's modulus $e$ answers a simple question: *how hard do i have to pull (per unit area) to stretch the material by a certain fraction of its length?*\n\n$$\ne = \\frac{\\sigma_{xx}}{\\varepsilon_{xx}} = \\frac{f/a}{\\delta l / l} = k \\frac{l}{a}\n$$\n\nit's the spring constant, but normalized by geometry \u2014 force per unit area divided by relative deformation. this lets you compare materials regardless of their shape or size:\n\n| material | young's modulus |\n|----------|----------------|\n| diamond  | $\\sim 1000$ gpa |\n| steel    | $\\sim 200$ gpa  |\n| bone     | $\\sim 15$ gpa   |\n| wood     | $\\sim 10$ gpa   |\n| gouda    | $\\sim 0.3$ gpa  |\n| rubber   | $\\sim 0.01$ gpa |\n\n## poisson's ratio \u2014 the sideways squeeze\n\nnow put that weight on the cheese. it compresses vertically, but it also *bulges outward* horizontally. poisson's ratio $\\nu$ measures this coupling:\n\n$$\n\\nu = -\\frac{\\varepsilon_{\\text{transverse}}}{\\varepsilon_{\\text{longitudinal}}}\n$$\n\nfor most solid materials, $\\nu$ is between 0.2 and 0.5. a poisson's ratio of 0.5 means the material is **incompressible** \u2014 it changes shape but not volume (rubber is close to this). a poisson's ratio of 0 means the dimensions are decoupled \u2014 squeezing vertically has no effect on horizontal dimensions (cork is close to this, which is why it works so well as a bottle stopper).\n\n## generalized hooke's law \u2014 the full 3d picture\n\nin the general case, hooke's law becomes a tensor equation:\n$$\n\\sigma_{ij} = c_{ijkl} \\, \\varepsilon_{kl}\n$$\n\nwhere $c_{ijkl}$ is a rank-4 **stiffness tensor** that relates all components of stress to all components of strain. for a general anisotropic material, this has 21 independent components. for an **isotropic** material (same properties in all directions), it collapses to just two parameters \u2014 the lame coefficients $\\lambda$ and $\\mu$:\n$$\n\\sigma_{ij} = \\lambda \\, \\varepsilon_{kk} \\, \\delta_{ij} + 2\\mu \\, \\varepsilon_{ij}\n$$\n\nthe lame coefficients relate to young's modulus and poisson's ratio by:\n$$\n\\mu = \\frac{e}{2(1+\\nu)}, \\qquad \\lambda = \\frac{e\\nu}{(1+\\nu)(1-2\\nu)}\n$$\n\n## what happens when you stretch too far?\n\nhooke's law is a *linear* relationship \u2014 double the stress, double the strain. but you know from experience that this can't be the whole story. stretch the rubber band far enough and it snaps. squeeze the cheese hard enough and it crumbles.\n\nreal materials have a **yield point** where the linear relationship breaks down. beyond it, the material deforms *permanently* \u2014 it doesn't spring back. this is **plastic deformation**, and it's where materials science gets interesting. we won't dive deep into plasticity in this course, but it's important to know that linear elasticity has limits. the von mises criterion from the previous section tells you *when* those limits are reached.\n\n## work and energy in a deformed material\n\nwhen you deform a material, you do work on it. that work gets stored as elastic potential energy (like compressing a spring). the work per unit volume is:\n$$\nw = \\sigma : \\varepsilon = \\sum_{ij} \\sigma_{ij} \\, \\varepsilon_{ij}\n$$\n\nthe \"$:$\" operator is the **double contraction** \u2014 you multiply corresponding elements and sum them all up. the units work out to $\\text{pa} = \\text{j/m}^3$: energy per unit volume, exactly what you'd expect for stored elastic energy.\n\n## linear elastostatics \u2014 when nothing moves\n\nif a material is in static equilibrium (no acceleration), the forces must balance everywhere:\n$$\n-\\nabla \\cdot \\sigma = \\mathbf{f}\n$$\n\nwhere $\\mathbf{f}$ is the body force density (like gravity). combined with hooke's law ($\\sigma = \\lambda \\, \\text{tr}(\\varepsilon) \\, \\mathbf{i} + 2\\mu \\, \\varepsilon$) and the strain-displacement relation, this gives the **navier-cauchy equation** for elastostatics. it's analytically solvable for simple geometries, and it's what we'll solve numerically with fem for everything else.\n\n## beam profiles and slender rods\n\nmany engineering structures \u2014 bridges, buildings, bones \u2014 can be modeled as slender beams. when a beam bends under load, the top is compressed and the bottom is stretched (or vice versa). there's a **neutral axis** in the middle where the strain is zero.\n\nthe classic euler-bernoulli beam theory gives the deflection $w(x)$ of a beam under load:\n$$\nei \\frac{d^4 w}{dx^4} = q(x)\n$$\n\nwhere $i$ is the second moment of area (a geometric property of the cross-section) and $q(x)$ is the distributed load. this single equation governs how bridges sag, diving boards flex, and tree branches bend in the wind.\n\n## vibrations and sound \u2014 when elasticity meets dynamics\n\npush a material and let go. if it's elastic, it springs back \u2014 and *overshoots*. then it springs back again. this oscillation propagates through the material as a **wave**.\n\nthere are two kinds of elastic waves:\n\n* **p-waves** (pressure/longitudinal): the material compresses and expands along the direction of propagation, like a slinky being pushed and pulled. the displacement is parallel to the wave vector $\\vec{k}$.\n* **s-waves** (shear/transverse): the material shears perpendicular to the direction of propagation, like a rope being wiggled. the displacement is perpendicular to $\\vec{k}$, with tw"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "examdisp",
      "lessonTitle": "The Grand Finale \u2014 A Detective Story in Seven Acts",
      "x": 0.23125572502613068,
      "y": 0.8802330493927002,
      "searchText": "the grand finale \u2014 a detective story in seven acts\n# the grand finale \u2014 a detective story in seven acts\n\n## seven detective stories \u2014 a recap\n\neach act below is a mystery we solved during this course. if any feels unfamiliar, revisit the corresponding chapter with fresh eyes.\n\n**act 1 \u2014 why the iceberg floats.** we opened with an audacious bet: pretend matter is smooth. the continuum approximation let us write down the hydrostatic equation and archimedes' principle, explaining why exactly 90% of an iceberg hides below the waterline \u2014 and why it stays upright once it gets there.\n\n**act 2 \u2014 the glacier's crevasses.** the cauchy stress tensor and its deviatoric part gave us the language to ask *where* inside a material things break. crevasses open at the surface where tensile stress dominates and close at depth where compression wins.\n\n**act 3 \u2014 the earthquake's double punch.** hooke's law in three dimensions spawned two wave speeds: fast compressive p-waves and slower transverse s-waves. the time gap between the two jolts you feel in an earthquake is proportional to your distance from the epicenter.\n\n**act 4 \u2014 one heartbeat, many rhythms.** cauchy's equation turned out to be the single heartbeat behind every continuous material. swap in one constitutive law and you get elastic solids; swap in another and you get viscous fluids, power-law glaciers, or anything in between.\n\n**act 5 \u2014 bernoulli's garden hose.** dropping viscosity gave us euler's equations, bernoulli's theorem, and the ideas of vorticity and circulation \u2014 enough to explain why pinching a hose speeds up the water and why airplanes stay aloft.\n\n**act 6 \u2014 honey, ketchup, and turbulence.** restoring viscosity brought the navier-stokes equations, the reynolds number, and a zoo of exact solutions for pipes and channels. non-newtonian fluids like custard and ketchup showed that real materials can bend the rules in entertaining ways.\n\n**act 7 \u2014 simulating gladys the glacier.** when geometry gets complicated, we turn differential equations into weak forms, project onto finite-element spaces, and let the computer solve the resulting sparse system. fenics turned the stokes equations into a working glacier simulation.\n\n---\n\n[[simulation unified-map]]\n\n[[simulation glacier-cross-section]]\n\n## the whole story in one breath\n\nwe started by pretending matter is smooth. we learned the language of tensors. we pushed on things and watched them push back (stress and strain). we discovered that the same cauchy equation governs everything from steel beams to ocean currents. we solved beautiful exact problems for pipes and channels, then learned fem for everything else. and along the way, rosie stretched, harry dripped, and gladys kept grinding slowly toward the sea.\n\nthat's continuum mechanics. now go pass the exam \u2014 and never look at a river, a rubber tire, or a cube of cheese the same way again.\n\n## big ideas\n\n* one equation \u2014 $\\rho\\,d\\mathbf{v}/dt = \\mathbf{f} + \\nabla\\cdot\\sigma$ \u2014 governs every continuous material. glaciers, arteries, earthquake waves, and ocean tides all dance to the same heartbeat. what distinguishes them is the constitutive law: the material's personal recipe for turning deformation into stress.\n* the route from a differential equation to a computer solution runs through weak form \u2192 galerkin projection \u2192 sparse linear system \u2192 fem solve. every step has a physical justification, not just a mathematical one.\n\n## what comes next\n\nthis is the end of the continuum mechanics arc \u2014 from the audacious claim that matter is smooth, through the language of tensors, through the heartbeat of cauchy's equation, all the way to a working fem simulation of a glacier. you've traveled from abstract principles to executable code.\n\nwhere do you go from here? the tools you now hold \u2014 weak formulations, fem, dimensional analysis, the reynolds number \u2014 transfer directly to heat transfer, electromagnetic field equations, geophysical modeling, and biomechanics. the constitutive law is what changes; the scaffolding stays the same. pick a physical system you care about, write down what $\\sigma$ looks like for it, and you already know how to solve the problem.\n\n## check your understanding\n\n1. cauchy's equation is the same for elastic solids and viscous fluids. write down the two constitutive relations that turn it into the navier-cauchy equation (for solids) and the navier-stokes equation (for fluids). what is the fundamental physical difference between the two?\n2. a tsunami travels across the open pacific at $\\approx 200$ m/s and slows to $\\approx 10$ m/s as it approaches a shoreline where the depth has decreased to 10 m. by what factor does its amplitude increase, assuming energy flux is conserved?\n3. the weak form of a pde requires less smoothness from the solution than the strong form. give a concrete example of a physical situation where the true solution has a kink or discontinuity \u2014 and explain why weak form is the right framework for handling it numerically.\n\n## challenge\n\ndesign a complete computational study of gladys the glacier. her cross-section is a parabolic valley: $y = x^2/w$ for $-w \\leq x \\leq w$, with $w = 500$ m and maximum depth 60 m. ice obeys glen's flow law (power-law rheology with $n = 3$, $k \\approx 2 \\times 10^{-24}$ pa$^{-3}$ s$^{-1}$) under the driving stress of a surface slope of $\\theta = 3\u00b0$. formulate the 2d stokes problem in weak form, identify the appropriate boundary conditions (no-slip at the bedrock, stress-free at the surface), and outline the fenics implementation. without computing, predict qualitatively how the velocity profile across the valley will differ from a newtonian poiseuille flow \u2014 then explain why that difference matters for predicting how fast icebergs calve from the glacier's terminus.\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "fem",
      "lessonTitle": "Finite Element Method",
      "x": 0.3318621516227722,
      "y": 0.9103028178215027,
      "searchText": "finite element method\n# finite element method\n\n## from equations to elements\n\nsome differential equations don't have closed-form solutions \u2014 the geometry is irregular, the material varies, the boundary conditions resist analytical tricks. the **finite element method** (fem) handles this by dividing the domain into small elements, assuming a simple solution on each, and stitching them together.\n\nthe key distinction from finite differences: fd approximates the *derivatives* in the equation. fem approximates the *solution itself*.\n\n## the approximation\n\nwrite the unknown as a weighted sum of known basis functions:\n$$\nu^n(x) = \\sum_{i=1}^n a_i \\, \\phi_i(x)\n$$\n\neach $\\phi_i$ is typically a polynomial that is nonzero only near node $i$. the coefficients $a_i$ are what we solve for. substituting into the governing equation $\\mathcal{l}(u) = f$ produces a **residual**:\n$$\nr^n(x) = \\mathcal{l}(u^n) - f\n$$\n\nfor the exact solution, $r = 0$ everywhere. for our approximation, we need a principled way to make it small.\n\n## galerkin's method\n\nthe standard approach: require the residual to be **orthogonal** to every basis function:\n$$\n\\int_\\omega r^n(x) \\, \\phi_i(x) \\, dx = 0, \\qquad i = 1, \\ldots, n\n$$\n\nthe intuition: the residual and the approximation error are linked. making $r$ orthogonal to the approximation space produces the best approximation within that space. this yields $n$ equations for $n$ unknowns.\n\nthere are other approaches. *least squares* minimizes $\\int (r^n)^2 \\, dx$. *collocation* forces $r^n = 0$ at selected points. galerkin dominates because it preserves symmetry of the underlying operator and connects directly to energy minimization.\n\n## what makes it \"finite element\"\n\ngalerkin works with any basis. fem makes three specific choices:\n\n1. **mesh the domain** into elements \u2014 intervals in 1d, triangles in 2d, tetrahedra in 3d.\n2. **use local basis functions** \u2014 each $\\phi_i$ is a polynomial nonzero only on elements touching node $i$. this is the \"finite\" in finite element: the support is finite.\n3. **assemble** \u2014 global integrals become sums over elements, producing a **sparse** system $\\mathbf{k}\\mathbf{a} = \\mathbf{f}$.\n\nsparsity is what makes fem scale. each node couples only to its neighbors, so systems with millions of unknowns remain tractable.\n\n## the variational perspective\n\nfor elastic problems, the true displacement minimizes the **potential energy**:\n$$\nw(u) = \\frac{1}{2}\\int_\\omega \\varepsilon : \\sigma \\, dv - \\int_\\omega \\mathbf{f} \\cdot u \\, dv\n$$\n\nthe galerkin equations are exactly the optimality conditions for this functional. nature chooses the configuration that minimizes energy; fem finds the best approximation within the element space.\n\n## see it work\n\nthe simulation below solves a 1d bar fixed at one end under a uniform distributed load. the exact displacement is a parabola \u2014 watch how piecewise-linear elements converge to it as you add more elements. toggle the basis functions to see how local hat functions, each scaled by its nodal value, combine into the global solution.\n\n[[simulation fem-1d-bar-sim]]\n\n## big ideas\n\n* fem approximates the solution itself (as a sum of local basis functions), not the derivatives \u2014 this is what separates it from finite differences and gives it flexibility with irregular geometries.\n* galerkin's method turns the residual-minimization problem into a linear system: require the residual to be orthogonal to every basis function, and you get $n$ equations for $n$ unknowns.\n* sparsity is the killer feature: each basis function is nonzero only near its home node, so the global stiffness matrix $\\mathbf{k}$ has almost all zero entries and systems with millions of unknowns stay tractable.\n* the variational perspective reveals why fem converges: nature minimizes potential energy, and fem finds the best approximation within the element space \u2014 a projection onto a finite-dimensional subspace.\n\n## what comes next\n\nwe have the theory. next we get hands dirty with actual computation \u2014 setting up and solving fem problems in python with fenics.\n\n## check your understanding\n\n1. in the galerkin method, we require $\\int r^n \\phi_i \\, dx = 0$ for every basis function $\\phi_i$. why is orthogonality the right condition? what would it mean if the residual were not orthogonal to the approximation space?\n2. fem produces a sparse stiffness matrix. why? which entries of $\\mathbf{k}_{ij}$ are nonzero, and which are zero?\n3. adding more elements (refining the mesh) increases $n$. how does the error in the fem solution typically scale with element size $h$? what does this tell you about the trade-off between accuracy and computational cost?\n\n## challenge\n\nconsider the 1d poisson equation $-d^2u/dx^2 = 1$ on $[0,1]$ with $u(0) = u(1) = 0$, whose exact solution is $u = x(1-x)/2$. implement a 1d fem solver by hand (no libraries): set up the weak form, build the stiffness matrix and load vector for $n = 2, 4, 8, 16$ linear elements, solve each system, and plot the error $\\|u - u^n\\|$ as a function of $n$. verify that the error decreases as $h^2$ (where $h = 1/n$). now repeat with quadratic elements ($n = 2, 4, 8$) and confirm the error scales as $h^3$. what is the computational cost (in terms of matrix size and solve time) of achieving the same error level with linear versus quadratic elements?\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "fluids",
      "lessonTitle": "Fluids at Rest",
      "x": 0.1538933366537094,
      "y": 0.8696458339691162,
      "searchText": "fluids at rest\n# fluids at rest\n\n## what makes a fluid a fluid?\n\npick up a block of cheese and set it on the table. it holds its shape. now pour a glass of water on the table. it spreads everywhere.\n\nthat's the fundamental difference between solids and fluids. a solid resists being deformed \u2014 apply a shear stress and it pushes back with a restoring force, like a spring. a fluid *can't* resist shear. apply even the tiniest shear stress and it flows. it might flow slowly (honey) or quickly (water), but it flows.\n\nmore precisely: a **fluid** is a material with zero shear modulus. it deforms continuously under any applied shear stress. fluids resist compression (you can't easily squeeze water into a smaller volume), but they don't resist shearing.\n\nthis means fluids respond to stress in a very particular way:\n* **normal stress** (compression/tension) \u2192 yes, fluids push back through **pressure**.\n* **shear stress** \u2192 no resistance in equilibrium. any shear causes flow.\n\nat rest, the only stress a fluid can sustain is pressure \u2014 equal in all directions, like being squeezed uniformly from all sides. the stress tensor for a fluid at rest is:\n$$\n\\sigma_{ij} = -p \\, \\delta_{ij}\n$$\n\nthat's it: negative pressure (compression) equally on every face of every tiny cube.\n\n## pressure \u2014 the weight of everything above\n\nwhat *is* pressure? at a molecular level, it's the result of billions of molecules bombarding a surface. but in the continuum picture, we don't need molecules \u2014 we just need the fact that the fluid pushes back against compression.\n\nthe force on a small surface element $d\\mathbf{s}$ due to pressure is:\n$$\nd\\mathbf{f} = -p \\, d\\mathbf{s}\n$$\n\nthe minus sign means the force points *inward* \u2014 the fluid pushes against the surface, trying to expand. the total pressure force on a body submerged in fluid is:\n$$\n\\mathbf{f}_b = -\\oint_s p \\, d\\mathbf{s}\n$$\n\nin a fluid at rest in a gravitational field, the pressure increases with depth. you've felt this in a swimming pool \u2014 your ears pop as you dive deeper. the condition for static equilibrium is:\n$$\n\\nabla p = \\rho \\, \\mathbf{g}\n$$\n\nthis is the **hydrostatic equation**: pressure gradient equals body force density. for a constant-density fluid with gravity pointing down:\n$$\np(z) = p_0 + \\rho g (z_0 - z)\n$$\n\npressure increases linearly with depth. every 10 meters of water adds about 1 atmosphere of pressure.\n\n## equation of state \u2014 how pressure relates to density\n\nfor many applications, we need to know how pressure relates to other properties of the fluid, like density and temperature. this is the **equation of state**.\n\nfor an ideal gas: $p = \\rho r t / m_{\\text{mol}}$. for liquids, the relationship is more complex \u2014 the **bulk modulus** $k = -v \\, dp/dv$ tells you how much the pressure changes when you compress the fluid. water has a bulk modulus of about 2.2 gpa, which is why it's nearly incompressible under everyday conditions.\n\nfor advanced applications (like modeling phase transitions), the **van der waals equation** accounts for molecular interactions and finite molecular size \u2014 but for most of this course, we'll treat fluids as either ideal gases or incompressible liquids.\n\n## buoyancy \u2014 why icebergs float\n\ndrop an object into a fluid. gravity pulls it down. but the pressure of the fluid pushes up on its bottom surface more than it pushes down on its top surface (because pressure increases with depth). the result is an upward **buoyancy force**.\n\nthe gravitational force on the body:\n$$\n\\mathbf{f}_g = \\int_v \\rho_{\\text{body}} \\, \\mathbf{g} \\, dv\n$$\n\nthe buoyancy force from the surrounding fluid pressure:\n$$\n\\mathbf{f}_b = -\\oint_s p \\, d\\mathbf{s}\n$$\n\ncombining them gives **archimedes' principle**:\n$$\n\\mathbf{f} = \\mathbf{f}_g + \\mathbf{f}_b = \\int_v (\\rho_{\\text{body}} - \\rho_{\\text{fluid}}) \\, \\mathbf{g} \\, dv\n$$\n\n*the buoyant force equals the weight of the displaced fluid.* if the object is less dense than the fluid, it floats. if it's denser, it sinks.\n\nfor a uniform gravitational field:\n$$\n\\mathbf{f} = (m_{\\text{body}} - m_{\\text{displaced fluid}}) \\, \\mathbf{g}_0\n$$\n\nhere's a beautiful way to think about it: imagine replacing the object with an equal volume of fluid. that \"fluid object\" would be in perfect equilibrium \u2014 the buoyancy exactly balances its weight. now swap in the real object: if it's lighter, there's a net upward force; if heavier, a net downward force.\n\nas the course quote goes: *\"if the berg is full of water or if it's full of iceberg, it doesn't matter! just imagine an iceberg made of water, a so-called waterberg.\"*\n\n## stability \u2014 will it tip over?\n\nfloating isn't enough. a floating body must also be *stable* \u2014 if you nudge it, it should rock back to its original position, not capsize.\n\nbeyond the buoyant force balance, there must also be a balance of **moments**. the gravitational moment acts through the **center of gravity** $\\mathbf{x}_g$, and the buoyancy moment acts through the **center of buoyancy** $\\mathbf{x}_b$:\n\n$$\n\\mathbf{x}_g = \\frac{1}{m}\\int_v \\mathbf{x} \\, \\rho_{\\text{body}} \\, dv, \\qquad \\mathbf{x}_b = \\frac{1}{m_{\\text{displaced}}}\\int_v \\mathbf{x} \\, \\rho_{\\text{fluid}} \\, dv\n$$\n\nthe total moment is:\n$$\n\\mathbf{m}_{\\text{total}} = (\\mathbf{x}_g - \\mathbf{x}_b) \\times m \\, \\mathbf{g}_0\n$$\n\nfor stability, this moment must be *restoring*: tilting the body should create a moment that pushes it back upright. this is why ships have heavy keels (to lower $\\mathbf{x}_g$) and wide hulls (to raise $\\mathbf{x}_b$ when tilted).\n\n## big ideas\n\n* a fluid is defined by what it cannot do: resist shear. any shear stress, no matter how small, causes continuous flow. at rest, the only allowed stress is isotropic pressure.\n* the hydrostatic equation $\\nabla p = \\rho\\mathbf{g}$ is just cauchy's equation with all velocities set to zero. every 10 meters of water adds roughly one atmosphere of pressure.\n* archimedes' principle is surprisingly deep: a floating body displaces its own weight of fluid \u2014 full stop. you don't need to"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "fluidsinmotion",
      "lessonTitle": "Fluids in Motion",
      "x": 0.1530931442975998,
      "y": 0.8320428729057312,
      "searchText": "fluids in motion\n# fluids in motion\n\n## the garden hose experiment\n\nbefore a single equation, do this experiment in your head. you're holding a garden hose with water flowing steadily out the end. now pinch the hose partway shut with your thumb. what happens?\n\nthe water speeds up. a narrow, fast jet shoots out.\n\n*why?* because the same amount of water has to get through a smaller opening in the same amount of time. water isn't appearing or disappearing \u2014 it's being conserved. that simple observation \u2014 **what goes in must come out** \u2014 is all the continuity equation is saying. the rest is just making that idea precise.\n\n## ideal flows \u2014 the simplest starting point\n\nlet's start with the most idealized fluid imaginable: **incompressible** (you can't squeeze it), **inviscid** (no internal friction), and **irrotational** (no local spinning). this is an \"ideal flow.\" real fluids are never quite this perfect, but ideal flow gives us beautiful results that are surprisingly useful.\n\nfor an ideal fluid at rest, we had $\\sigma = -p\\,\\mathbf{i}$. this remains true in motion \u2014 the only stress is pressure:\n$$\n\\sigma = -p\\,\\mathbf{i} \\qquad \\rightarrow \\qquad \\nabla \\cdot \\sigma = -\\nabla p\n$$\n\nplugging into cauchy's equation, and using incompressibility ($\\nabla \\cdot \\mathbf{v} = 0$), we get the **euler equations**:\n$$\n\\nabla \\cdot \\mathbf{v} = 0, \\qquad \\frac{d\\mathbf{v}}{dt} = \\mathbf{g} - \\frac{\\nabla p}{\\rho}\n$$\n\nfour equations (one scalar continuity equation, three components of the momentum equation) for four unknowns ($v_x$, $v_y$, $v_z$, $p$). the system is closed.\n\n[[simulation elastic-wave]]\n\none striking consequence of incompressibility: taking the divergence of the momentum equation gives a **poisson equation** for pressure. this means pressure is *non-local* \u2014 change the pressure somewhere, and it instantly adjusts everywhere. that's because we've assumed the fluid is perfectly incompressible: pressure waves travel at infinite speed (in reality they travel at the speed of sound, which is very fast but not infinite).\n\n## bernoulli's theorem \u2014 conservation of energy along a streamline\n\nback to the garden hose. the water speeds up when you pinch the opening. but where does the extra kinetic energy come from? it comes from the pressure: the pressure drops where the velocity increases. this trade-off between pressure and velocity is **bernoulli's theorem**.\n\ndefine the **bernoulli function**:\n$$\nh = \\frac{1}{2}v^2 + \\phi + \\frac{p}{\\rho}\n$$\n\nwhere $\\phi$ is the gravitational potential (e.g., $gz$). along a streamline in steady flow:\n$$\n\\frac{dh}{dt} = 0\n$$\n\n$h$ is constant along a flowline. it's an energy conservation statement: kinetic energy + potential energy + pressure energy = constant. divide everything by $g$ and you get what engineers call the **total head** \u2014 a quantity measured in meters that you can literally see as a height.\n\nthe gradient of $h$ connects to vorticity:\n$$\n\\nabla h = \\mathbf{v} \\times (\\nabla \\times \\mathbf{v}) - \\frac{\\partial \\mathbf{v}}{\\partial t}\n$$\n\nin steady, irrotational flow, $\\nabla h = 0$ everywhere \u2014 meaning $h$ is constant not just along streamlines but throughout the entire flow. this is the strongest form of bernoulli's theorem.\n\n## vorticity \u2014 the local spin\n\nimagine dropping a tiny ice skater into a flowing river. at some points, the current is uniform and she glides straight. at other points, the flow is faster on one side than the other, and she starts to **spin**. the rate at which she spins is the **vorticity**:\n$$\n\\boldsymbol{\\omega} = \\nabla \\times \\mathbf{v}\n$$\n\nvorticity is the curl of the velocity field \u2014 it measures the local rotation rate. if the vorticity is zero everywhere, the flow is **irrotational** and things simplify enormously (we can use a velocity potential). if vorticity is nonzero, the flow has internal structure \u2014 think of the swirling eddies behind a rock in a stream.\n\ntaking the curl of euler's equation gives the **vorticity equation**:\n$$\n\\frac{\\partial \\boldsymbol{\\omega}}{\\partial t} = \\nabla \\times (\\mathbf{v} \\times \\boldsymbol{\\omega})\n$$\n\na key consequence: **if a flow starts irrotational, it stays irrotational** (for an ideal fluid). vorticity can't spontaneously appear in an inviscid flow \u2014 you need viscosity (friction) or boundaries to create it. this is **kelvin's circulation theorem**, and it explains why potential flow is such a useful approximation far from surfaces.\n\n## circulation and stokes' theorem\n\nthe **circulation** $\\gamma$ around a closed curve is the line integral of velocity:\n$$\n\\gamma = \\oint_c \\mathbf{v} \\cdot d\\mathbf{l}\n$$\n\nby stokes' theorem, this equals the flux of vorticity through any surface bounded by the curve:\n$$\n\\gamma = \\int_s \\boldsymbol{\\omega} \\cdot d\\mathbf{s}\n$$\n\ncirculation is the \"total spin\" enclosed by a loop. kelvin's theorem says that for an ideal fluid, the circulation around a material loop (one that moves with the fluid) is conserved. vorticity isn't created or destroyed \u2014 it's just transported and stretched by the flow.\n\n## big ideas\n\n* an ideal (inviscid, incompressible, irrotational) flow is governed by the euler equations \u2014 four equations for four unknowns. pressure becomes non-local: it instantly adjusts everywhere because incompressibility makes pressure waves travel infinitely fast.\n* bernoulli's theorem is energy conservation along a streamline: fast flow means low pressure, slow flow means high pressure. it explains garden hoses, airplane wings, and venturi meters.\n* vorticity $\\omega = \\nabla \\times \\mathbf{v}$ measures local spin; kelvin's theorem says vorticity can't spontaneously appear in an ideal fluid \u2014 you need viscosity or boundaries to create it.\n* circulation $\\gamma = \\oint \\mathbf{v} \\cdot d\\mathbf{l}$ is the global version of vorticity: the total \"spin\" enclosed by a loop, conserved in ideal flow.\n\n## what comes next\n\nideal flows are elegant but incomplete \u2014 they can't explain why the water in your cup eventually stops spinning after you stir it. tha"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "gravitywaves",
      "lessonTitle": "Gravity Waves",
      "x": 0.20091521739959717,
      "y": 0.908389151096344,
      "searchText": "gravity waves\n# gravity waves\n\n## ripples, swells, and tsunamis\n\ndrop a stone in a pond. ripples spread outward in concentric circles. stand on a beach and watch the ocean: long, rolling swells march toward shore, steepen, and break. somewhere on the other side of the pacific, a submarine earthquake generates a wave that crosses the entire ocean in hours.\n\nthese are all **gravity waves** \u2014 waves where gravity provides the restoring force. when water is pushed up above its equilibrium level, gravity pulls it back down, and the resulting oscillation propagates outward.\n\nthis section develops the mathematical description of these waves, culminating in the shallow-water equations that govern everything from tidal bores to tsunamis.\n\n## the shallow-water equations\n\nwhen the wavelength is much larger than the water depth (think tsunamis in the open ocean, tidal flows in harbors, or flood waves in rivers), we can average the flow over the depth and arrive at the **shallow-water equations**:\n$$\n\\frac{\\partial v_x}{\\partial t} + (v_x \\nabla_x + v_y \\nabla_y)v_x = -g_0 \\nabla_x \\eta + f\\,v_y\n$$\n$$\n\\frac{\\partial v_y}{\\partial t} + (v_x \\nabla_x + v_y \\nabla_y)v_y = -g_0 \\nabla_y \\eta - f\\,v_x\n$$\n$$\n\\frac{\\partial \\eta}{\\partial t} + \\nabla_x(h\\,v_x) + \\nabla_y(h\\,v_y) = 0\n$$\n\nhere $\\eta$ is the surface elevation, $h$ is the water depth, $f$ is the coriolis parameter (earth's rotation matters for large-scale waves), and $v_x$, $v_y$ are the depth-averaged velocities.\n\nthe first two equations are momentum conservation (newton's second law applied to each column of water). the third is mass conservation (the water surface rises where more water flows in than out).\n\n## the wave equation and dispersion\n\nfor small-amplitude waves over constant depth $d$, the shallow-water equations linearize to give a **2d wave equation** for the surface elevation:\n$$\n\\left(\\frac{\\partial^2}{\\partial t^2} + f^2\\right)\\eta - g_0 d \\, \\nabla_h^2 \\eta = 0\n$$\n\nwithout rotation ($f = 0$), this is a standard wave equation. waves propagate at speed:\n$$\nc = \\sqrt{g_0 d}\n$$\n\nthis is the shallow-water wave speed: it depends on depth, not on wavelength. in the deep ocean ($d \\approx 4000$ m), this gives $c \\approx 200$ m/s $\\approx 700$ km/h \u2014 which is why tsunamis cross oceans in hours.\n\nthe wave period for a wave of wavelength $\\lambda$ is:\n$$\n\\tau \\approx \\frac{\\lambda}{\\sqrt{g_0 d}}\n$$\n\nwith rotation ($f \\neq 0$), the minimum wave frequency is $f$ \u2014 waves slower than one cycle per half-day (at mid-latitudes) are deflected by the coriolis force into rotating patterns rather than propagating freely.\n\n[[simulation dispersion-relation]]\n\n## big ideas\n\n* gravity waves exist because two things compete: gravity pulls displaced water back to its resting level, and inertia carries it past. the oscillation between these two tendencies is the wave.\n* in shallow water ($\\text{wavelength} \\gg \\text{depth}$), the wave speed $c = \\sqrt{g_0 d}$ depends only on depth \u2014 not on wavelength. all frequencies travel at the same speed, so the wave shape is preserved as it propagates.\n* this dispersionless property explains why tsunamis are so dangerous: a pulse of energy crosses entire ocean basins without spreading out, arriving with nearly its original amplitude.\n* earth's rotation (the coriolis force, parameterized by $f$) deflects large-scale gravity waves into rotating patterns and sets a minimum frequency below which free propagation is impossible.\n\n## what comes next\n\nwe've now covered the main exact solutions and wave phenomena in fluid mechanics. to solve the stokes equation for realistic geometries (like gladys the glacier flowing through an irregular valley), we need to reformulate it in **weak form** \u2014 a step that prepares the equation for numerical solution by the finite element method.\n\n## check your understanding\n\n1. the deep pacific ocean has an average depth of about 4000 m. estimate the speed of a tsunami in the open ocean in km/h. how long does it take to cross 8000 km of open water?\n2. as a tsunami approaches shore and the water depth decreases from 4000 m to 10 m, by what factor does the wave speed decrease? what must happen to the wave's amplitude to conserve energy flux?\n3. what is the coriolis parameter $f = 2\\omega\\sin\\phi$ at your latitude $\\phi$? ($\\omega = 7.27 \\times 10^{-5}$ rad/s.) what is the minimum wave period that can propagate freely without being deflected into a rotating pattern?\n\n## challenge\n\na rectangular harbor of length $l = 500$ m and uniform depth $d = 5$ m is closed at one end and open to the ocean at the other. model the harbor as a 1d resonator and find the natural resonant frequencies using the shallow-water wave equation with appropriate boundary conditions (zero velocity at the closed end, zero surface elevation perturbation at the open end). at which frequencies will the harbor experience resonance (a \"harbor seiche\")? if an incoming ocean swell has a period of 100 seconds, is the harbor at risk? what harbor geometry would bring a resonant mode to exactly that period?\n"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "pythonpackages",
      "lessonTitle": "Python Packages \u2014 Let's Play With the Universe",
      "x": 0.3028824031352997,
      "y": 0.9372503161430359,
      "searchText": "python packages \u2014 let's play with the universe\n# python packages \u2014 let's play with the universe\n\n## your computational toolkit\n\nyou've spent the last several sections building up the theory: tensors, stress, strain, elasticity, fluid dynamics, the finite element method. now it's time to *compute*. the good news: with modern python packages, you can go from \"here's my differential equation\" to \"here's a beautiful simulation of a bending beam\" in about 20 lines of code.\n\nthis section introduces the packages you'll use throughout the rest of the course. keep it open as a reference.\n\n## fenics \u2014 the heavy lifter\n\nfenics is a powerful finite element library that lets you solve pdes by writing them in a form that looks almost like the math. here's a taste \u2014 a complete script that computes the deformation of a 1d elastic bar under gravity:\n\n```python\nfrom fenics import *\n\n# create a mesh: 100 elements along a unit interval\nmesh = unitintervalmesh(100)\n\n# define the function space: linear lagrange elements\nv = functionspace(mesh, 'lagrange', 1)\n\n# boundary condition: fixed at x=0\nbc = dirichletbc(v, constant(0), 'near(x[0], 0)')\n\n# define the problem: find u such that\n# integral of (du/dx * dw/dx) dx = integral of f * w dx\nu = trialfunction(v)      # the unknown displacement\nw = testfunction(v)        # the weight/test function\nf = constant(-1.0)        # body force (gravity, pointing down)\n\n# this is the weak form \u2014 compare to your notes!\na = inner(grad(u), grad(w)) * dx   # stiffness term\nl = f * w * dx                      # load term\n\n# solve it\nu_sol = function(v)\nsolve(a == l, u_sol, bc)\n\n# that's it. u_sol now contains the displacement field.\nplot(u_sol)\n```\n\nthat's 15 lines of real code, and you've just solved an elasticity problem. the key fenics functions:\n\n| function | what it does |\n|----------|-------------|\n| `unitintervalmesh(n)` | creates a 1d mesh with $n$ elements |\n| `rectanglemesh(...)` | creates a 2d rectangular mesh |\n| `functionspace(mesh, type, degree)` | defines the approximation space. `'lagrange', 1` = linear polynomials |\n| `vectorfunctionspace(...)` | for vector-valued problems (2d/3d displacements) |\n| `trialfunction(v)` | the unknown function you're solving for |\n| `testfunction(v)` | the weight function in the weak form |\n| `dirichletbc(v, value, boundary)` | boundary conditions: \"fix this value on this boundary\" |\n| `inner(a, b)` | the double-dot product ($a : b$) |\n| `solve(a == l, u, bcs)` | find the coefficients that satisfy the weak form |\n\nfor 2d problems, the syntax extends naturally:\n\n```python\nmesh = rectanglemesh(point(0, 0), point(width, height), nx, ny)\nv = vectorfunctionspace(mesh, 'lagrange', 1)\n\n# boundary condition: fixed bottom edge\ndef bottom(x, on_boundary):\n    return on_boundary and near(x[1], 0)\n\nbc = dirichletbc(v, constant((0, 0)), bottom)\n```\n\nfenics handles the meshing, assembly, and linear algebra behind the scenes. your job is to express the physics in weak form \u2014 and you already know how to do that from the previous sections.\n\n## sympy \u2014 algebra without tears\n\nsympy does symbolic computation inside python. it's invaluable for tensor calculations where you need exact expressions rather than numbers:\n\n```python\nfrom sympy import symbols, diff, simplify, matrix\n\nx, y, z = symbols('x y z')\n\n# define a velocity field\nvx = x**2 * y\nvy = -x * y**2\n\n# compute the strain rate tensor\neps_xx = diff(vx, x)\neps_yy = diff(vy, y)\neps_xy = (diff(vx, y) + diff(vy, x)) / 2\n\nprint(f\"strain rate tensor:\")\nprint(f\"  eps_xx = {eps_xx}\")\nprint(f\"  eps_yy = {eps_yy}\")\nprint(f\"  eps_xy = {eps_xy}\")\n```\n\nuse `lambdify` to convert symbolic expressions into fast numerical functions when you need to evaluate them on arrays.\n\n## plotly \u2014 interactive 3d visualization\n\nmatplotlib is great for 2d plots, but for 3d stress fields and deformation surfaces, plotly gives you interactive visualizations you can rotate, zoom, and explore:\n\n```python\nimport plotly.graph_objects as go\n\n# plot a 3d surface of a stress field\nfig = go.figure(data=[go.surface(z=stress_field, x=x, y=y)])\nfig.update_layout(title='von mises stress')\nfig.show()\n```\n\n## shapely \u2014 geometric operations\n\nfor setting up domain geometries, computing cross-sections, and geometric preprocessing:\n\n```python\nfrom shapely.geometry import polygon\n\n# define a glacier cross-section\nglacier = polygon([(0, 0), (10, 0), (8, 5), (2, 5)])\nprint(f\"area: {glacier.area}\")\nprint(f\"centroid: {glacier.centroid}\")\n```\n\n## rasterio \u2014 working with real terrain data\n\nfor real-world applications (modeling glacier flow over actual terrain), rasterio reads geotiff and other raster formats used in geographic information systems:\n\n```python\nimport rasterio\nwith rasterio.open('terrain.tif') as src:\n    elevation = src.read(1)  # numpy array of elevation data\n```\n\n## gmsh \u2014 custom meshes\n\nfor domains more complex than rectangles, gmsh generates high-quality finite element meshes. you can define geometries programmatically or through its gui, export the mesh, and import it into fenics. see [gmsh.info](https://gmsh.info/).\n\n## gridap (julia)\n\nif you're curious about fem in julia, gridap offers a similar high-level interface. the syntax and workflow are analogous to fenics, which is reassuring: the *concepts* transfer across languages. see [gridap.github.io](https://gridap.github.io).\n\n## big ideas\n\n* fenics lets you write a pde in weak form almost exactly as you'd write it on paper \u2014 the code mirrors the math, so bugs are easy to spot and physics is easy to change.\n* sympy closes the gap between pencil-and-paper tensor algebra and numerical computation: derive the expression symbolically, then `lambdify` it for fast array evaluation.\n* the workflow from physical problem to simulation is now a closed loop: write the pde, express it in weak form (by hand), encode the weak form in fenics, mesh the domain (gmsh), and visualize the result (plotly).\n* the same conceptual structure \u2014 domain, function space, trial/test functions, bilinear form, li"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "stressandstrain",
      "lessonTitle": "Stress and Strain",
      "x": 0.05792418494820595,
      "y": 0.9322887063026428,
      "searchText": "stress and strain\n# stress and strain\n\n## getting physical \u2014 the rubber band experiment\n\nbefore we write a single equation, try this. take a rubber band and hold it between your fingers. now pull it straight \u2014 you feel it resist, and your fingers are being pulled toward each other. that's **normal stress**: force perpendicular to the surface, pulling the material apart.\n\nnow try twisting the rubber band while holding it taut. feel that sideways tug, the way the material wants to slide past itself? that's **shear stress**: force parallel to the surface.\n\nevery point inside a stressed material experiences some combination of both. the stress tensor is just the bookkeeping system that tracks all of it.\n\n## the stress tensor \u2014 forces on imaginary surfaces\n\nimagine slicing through a stressed material with an imaginary plane. the orientation of that plane is described by its outward normal vector $\\hat{n}$. the **traction vector** \u2014 the force per unit area that the material on one side of the cut exerts on the other \u2014 is:\n$$\nt_i = \\sigma_{ij} \\, n_j\n$$\n\nthis is the stress tensor's job: you give it a direction (the normal to your imaginary cut), and it gives you back the force per unit area on that surface. it's a machine that converts orientations into forces.\n\nthe stress tensor is symmetric ($\\sigma_{ij} = \\sigma_{ji}$) because angular momentum must be conserved \u2014 otherwise every little cube of material would start spontaneously spinning. this symmetry means that in 3d, we have six independent stress components, not nine.\n\n[[simulation stress-strain-sim]]\n\n## principal stresses \u2014 finding the sweet spot\n\nhere's something remarkable: no matter how complicated the stress state, there's always a special orientation where all the shear stresses vanish. in this **principal coordinate system**, the stress tensor is diagonal:\n$$\n\\sigma = \\begin{pmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 \\\\ 0 & 0 & \\sigma_3 \\end{pmatrix}\n$$\n\nthe values $\\sigma_1$, $\\sigma_2$, $\\sigma_3$ are the **principal stresses** \u2014 the eigenvalues of $\\sigma_{ij}$. they tell you the maximum and minimum normal stresses at that point, and the eigenvectors tell you which directions they act in.\n\nfinding principal stresses is just an eigenvalue problem. if you know the stress tensor in any coordinate system, you can always rotate to the principal system.\n\n## the strain tensor \u2014 measuring deformation\n\nwhen you push on a material, it deforms. the **strain tensor** quantifies that deformation. for small displacements $u_i$ from the original position:\n$$\n\\varepsilon_{ij} = \\frac{1}{2}\\left(\\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i}\\right)\n$$\n\nthe entries have clear physical meanings:\n\n* **normal strain** ($\\varepsilon_{ii}$): fractional change in length along axis $i$. positive means stretching, negative means compressing.\n* **shear strain** ($\\varepsilon_{ij}$, $i \\neq j$): change in angle between two originally perpendicular lines. it measures how much the material *skews*.\n\n## hooke's law \u2014 the simplest possible relationship\n\nso stress causes strain, and strain implies stress. what's the relationship between them?\n\nfor a **linear elastic, isotropic** material (meaning it responds the same way in all directions, and the response is proportional to the load), the relationship is beautifully simple:\n$$\n\\sigma_{ij} = \\lambda \\, \\varepsilon_{kk} \\, \\delta_{ij} + 2\\mu \\, \\varepsilon_{ij}\n$$\n\nhere $\\lambda$ and $\\mu$ are the **lame parameters**, material constants that you can look up for steel, rubber, ice, or cheese. equivalently, using young's modulus $e$ and poisson's ratio $\\nu$:\n$$\n\\varepsilon_{ij} = \\frac{1+\\nu}{e}\\,\\sigma_{ij} - \\frac{\\nu}{e}\\,\\sigma_{kk}\\,\\delta_{ij}\n$$\n\nwhat do these constants mean physically?\n\n* **young's modulus** $e$: how stiff is the material? pull on it \u2014 $e$ tells you how much force per unit area you need to stretch it by a given fraction. steel: $e \\approx 200$ gpa. rubber: $e \\approx 0.01$ gpa.\n* **poisson's ratio** $\\nu$: when you stretch something in one direction, how much does it thin in the other directions? for most materials, $\\nu \\approx 0.3$. for incompressible materials (like rubber), $\\nu \\approx 0.5$.\n\n[[simulation stress-strain-curve]]\n\n## mohr's circle \u2014 the world's most useful clock face for stress\n\nyou have a stress state. you want to know: if i cut through the material at some angle, what normal stress and shear stress will i see on that surface? you *could* do the matrix rotation by hand every time. or you could use **mohr's circle**.\n\nhere's how it works. given principal stresses $\\sigma_1 > \\sigma_2 > \\sigma_3$:\n\n1. draw a horizontal axis for normal stress $\\sigma_n$ and a vertical axis for shear stress $\\tau$.\n2. plot the points $(\\sigma_1, 0)$, $(\\sigma_2, 0)$, $(\\sigma_3, 0)$ on the horizontal axis.\n3. draw circles: one connecting $\\sigma_1$ and $\\sigma_3$ (the big outer circle), one connecting $\\sigma_1$ and $\\sigma_2$, and one connecting $\\sigma_2$ and $\\sigma_3$.\n4. every possible stress state on any plane through that point lies *on or between* these three circles.\n\nthink of it as a clock face for stress. the position on the circle tells you the normal and shear stress on a plane at a given angle. as you \"rotate the clock hand\" (change the orientation of your imaginary cut), you sweep around the circle.\n\nthe maximum shear stress jumps out immediately: it's the radius of the biggest circle:\n$$\n\\tau_{\\max} = \\frac{\\sigma_1 - \\sigma_3}{2}\n$$\n\nno eigenvalue calculation needed. just draw the circle, read off the answer. engineers have been doing this on napkins for over a century.\n\n[[simulation mohr-circle]]\n\n## big ideas\n\n* the stress tensor is a machine: feed it a surface orientation, get back the force per unit area on that surface. its symmetry is not a coincidence \u2014 it's angular momentum conservation.\n* principal stresses are the eigenvalues of the stress tensor; they represent the stress state in the \"sweet spot\" basis where all shear vanishes.\n* hook"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "tensorfundamentals",
      "lessonTitle": "Tensor Fundamentals",
      "x": 0.061867304146289825,
      "y": 0.9165616035461426,
      "searchText": "tensor fundamentals\n# tensor fundamentals\n\n## the ant on the rubber sheet\n\nimagine you're an ant walking on a rubber sheet. someone is stretching the sheet \u2014 pulling it to the right, squeezing it from above. as you walk, the ground under your feet stretches in one direction and compresses in another. the way the sheet deforms around you depends on *which direction you're facing*. that direction-dependent description of stretching and squeezing \u2014 that's a tensor.\n\na scalar (like temperature) tells you one number at each point. a vector (like velocity) tells you a magnitude and a direction. a tensor tells you something richer: it tells you how a quantity *changes depending on which direction you look*. in continuum mechanics, we need tensors because the forces inside a material aren't just big or small \u2014 they act differently in different directions. a beam might be compressed vertically but stretched horizontally, all at the same point.\n\nnow let's give these ideas precise names so we don't have to wave our hands anymore.\n\n## the cauchy stress tensor \u2014 six little hands\n\nimagine cutting a tiny cube out of the interior of a stressed material. on each face of that cube, the surrounding material is pushing and pulling. it's like having **six little hands** pressing and twisting on every tiny cube inside the material.\n\nthe cauchy stress tensor $\\sigma$ captures all of this. in 3d, it's a $3 \\times 3$ matrix:\n$$\n\\sigma = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\sigma_{13}\\\\\n\\sigma_{21} & \\sigma_{22} & \\sigma_{23}\\\\\n\\sigma_{31} & \\sigma_{32} & \\sigma_{33}\\\\\n\\end{pmatrix}\n$$\n\nwhat do the entries mean?\n\n* **diagonal elements** ($\\sigma_{11}$, $\\sigma_{22}$, $\\sigma_{33}$) are the **normal stresses** \u2014 they push or pull straight into each face of the cube. positive means tension (pulling apart), negative means compression (squeezing together).\n* **off-diagonal elements** ($\\sigma_{12}$, $\\sigma_{13}$, etc.) are the **shear stresses** \u2014 they slide the faces sideways, like rubbing your hands together.\n\nthe columns of $\\sigma$ are the **traction vectors**: the force per unit area on each face. every element has units of pascal ($\\text{n/m}^2$).\n\nfor the tensor to be physically consistent (no spontaneously spinning cubes!), angular momentum conservation requires it to be symmetric: $\\sigma = \\sigma^t$. that means $\\sigma_{12} = \\sigma_{21}$, and so on \u2014 only 6 independent components in 3d, not 9.\n\nin 2d and 1d, the tensor shrinks accordingly:\n$$\n\\sigma_{2d} = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12}\\\\\n\\sigma_{21} & \\sigma_{22}\\\\\n\\end{pmatrix}, \\qquad\n\\sigma_{1d} = \\sigma_{11}\n$$\n\nhere's a key insight: **normal and shear stresses are a matter of perspective**. they depend on which coordinate system you choose, which is arbitrary. there always exists a special basis \u2014 the eigenbasis \u2014 where the stress tensor is purely diagonal and all the shear stresses vanish. the stresses in that basis are called the **principal stresses**, and they represent the purest description of the stress state at that point.\n\n## the stress deviator \u2014 relative to what?\n\nsometimes you don't care about the total stress \u2014 you care about how the stress *deviates* from uniform pressure. think about building a house. the materials were tested at atmospheric pressure. the house will stand at atmospheric pressure. so you want to know: how much *extra* stress does the structure experience beyond the background pressure?\n\nthe **stress deviator** strips away the uniform pressure part:\n$$\ns = \\sigma - p\\,\\mathbf{i} \\qquad \\text{where} \\qquad p = \\frac{1}{3}\\text{tr}(\\sigma)\n$$\n\nwritten out:\n$$\n\\begin{pmatrix}\ns_{11} & s_{12} & s_{13}\\\\\ns_{21} & s_{22} & s_{23}\\\\\ns_{31} & s_{32} & s_{33}\\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sigma_{11} - p & \\sigma_{12} & \\sigma_{13}\\\\\n\\sigma_{21} & \\sigma_{22} - p & \\sigma_{23}\\\\\n\\sigma_{31} & \\sigma_{32} & \\sigma_{33} - p\\\\\n\\end{pmatrix}\n$$\n\nthe deviator is what matters for predicting *shape change* and *failure*. uniform pressure changes volume but doesn't change shape \u2014 it's the deviator that warps, bends, and eventually breaks things.\n\n## invariants \u2014 what doesn't change when you rotate\n\nthe stress tensor looks different in different coordinate systems, but certain quantities remain the same no matter how you rotate your axes. these **invariants** are the truly physical quantities.\n\nfor the cauchy stress tensor, the three invariants are:\n\n$i_1 = \\sigma_1 + \\sigma_2 + \\sigma_3$\n\n$i_2 = \\sigma_1\\sigma_2 + \\sigma_2\\sigma_3 + \\sigma_3\\sigma_1$\n\n$i_3 = \\sigma_1\\sigma_2\\sigma_3$\n\nfor the stress deviator:\n\n$j_1 = s_{kk} = 0$ (by construction \u2014 we removed the pressure)\n\n$j_2 = \\frac{1}{2}\\text{tr}(s^2) = \\frac{1}{2}\\left(\\text{tr}(\\sigma^2) - \\frac{1}{3}\\text{tr}(\\sigma)^2\\right)$\n\n$j_3 = \\det(s) = \\frac{1}{3}\\left(\\text{tr}(\\sigma^3) - \\text{tr}(\\sigma^2)\\text{tr}(\\sigma) + \\frac{2}{9}\\text{tr}(\\sigma)^3\\right)$\n\nthe $j_2$ invariant is especially important because it connects to the **von mises yield criterion**: a practical rule for predicting when a material will permanently deform. for many metals, the ratio of shear yield stress to tensile yield stress is:\n$$\n\\frac{\\sigma_{\\text{shear,yield}}}{\\sigma_{\\text{tensile,yield}}} = \\frac{1}{\\sqrt{3}} \\approx 0.577\n$$\n\nwhen this holds, the material will start to permanently deform when:\n$$\n\\sigma_{\\text{von mises}} = \\sqrt{3 j_2}\n$$\nexceeds the yield strength. this works regardless of your choice of coordinates \u2014 that's the power of invariants.\n\n## the cauchy strain tensor \u2014 measuring deformation\n\nso far we've described the *forces* inside a material. but what about the *deformation* itself? if you push on rosie the rubber band, how much does she actually stretch?\n\nconsider a velocity field describing motion in a continuum:\n$$\n\\mathbf{v}(x,y,z) = \\begin{pmatrix} v_x(x,y,z)\\\\ v_y(x,y,z)\\\\ v_z(x,y,z) \\end{pmatrix}\n$$\n\nthe **cauchy strain tensor** (or strain rate tensor) captures how the material deforms:\n$$\n\\epsilon = \\begin{pmatrix}\n\\frac{\\partial v_x}{\\partial x}"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "viscousflow",
      "lessonTitle": "Viscous Flow",
      "x": 0.17456774413585663,
      "y": 0.7956547737121582,
      "searchText": "viscous flow\n# viscous flow\n\n## the battle in a high-school cafeteria\n\nimagine a high-school cafeteria at lunchtime. a crowd of students is streaming toward the door. their inertia wants to carry them straight ahead \u2014 they've got momentum and they want to keep going. but there's also social friction: people bumping shoulders, friends grabbing friends, the crowd *smearing out* the individual trajectories.\n\nthat's the battle at the heart of viscous flow: **inertia** (the tendency to keep going straight) versus **viscosity** (the tendency to smear everything out). the reynolds number tells you who's winning.\n\nbut we're getting ahead of ourselves. let's start with what viscosity actually is.\n\n## viscosity \u2014 how sticky is your fluid?\n\nviscosity is a fluid's resistance to being sheared. honey has high viscosity (hard to stir). water has low viscosity (easy to stir). air has very low viscosity (you barely notice it).\n\nimagine two parallel plates with fluid between them. you slide the top plate to the right while holding the bottom plate still. the fluid in between develops a velocity gradient \u2014 fast near the top plate, stationary near the bottom. **newton's law of viscosity** says:\n$$\n\\sigma_{xy} = \\eta \\frac{dv_x}{dy}\n$$\n\nthe shear stress is proportional to the velocity gradient. the proportionality constant $\\eta$ is the **dynamic viscosity** (units: pa$\\cdot$s). water: $\\eta \\approx 10^{-3}$ pa$\\cdot$s. honey: $\\eta \\approx 2$\u2013$10$ pa$\\cdot$s. the fluid between the plates is being *sheared*, and viscosity determines how hard it pushes back.\n\na useful variant is the **kinematic viscosity**:\n$$\n\\nu = \\frac{\\eta}{\\rho}\n$$\n\nthis is the viscosity \"per unit density\" \u2014 it's what shows up most often in the equations. for water: $\\nu \\approx 10^{-6}$ m$^2$/s. for air: $\\nu \\approx 1.5 \\times 10^{-5}$ m$^2$/s.\n\n## velocity-driven planar flow \u2014 viscosity as diffusion\n\nthe simplest viscous flow problem: one plate at rest, one plate suddenly set in motion. the velocity profile evolves as:\n$$\n\\frac{\\partial v_x}{\\partial t} = \\nu \\frac{\\partial^2 v_x}{\\partial y^2}\n$$\n\nthis is a **diffusion equation** \u2014 exactly the same form as heat diffusion! viscosity *diffuses* momentum the same way thermal conductivity diffuses heat. the viscous layer spreads outward from the moving plate at a rate $\\sim \\sqrt{\\nu t}$.\n\n## the viscous stress tensor\n\nfor a general incompressible newtonian fluid, the stress tensor has two parts \u2014 pressure and viscous shear:\n$$\n\\sigma_{ij} = -p\\,\\delta_{ij} + \\eta\\left(\\frac{\\partial v_i}{\\partial x_j} + \\frac{\\partial v_j}{\\partial x_i}\\right)\n$$\n\nor in compact notation:\n$$\n\\sigma = -p\\,\\mathbf{i} + 2\\eta\\,\\dot{\\varepsilon}\n$$\n\nwhere $\\dot{\\varepsilon}$ is the strain rate tensor. compare this to the elastic solid: $\\sigma = -p\\,\\mathbf{i} + 2\\mu\\,\\varepsilon$. the *structure* is identical \u2014 the difference is that fluids resist the *rate* of deformation while solids resist the deformation itself.\n\n## the navier-stokes equation \u2014 nature's accounting book for fluid motion\n\nso you want to know how the velocity of a viscous fluid changes over time? here's the accounting book nature uses. plug the viscous stress into cauchy's equation, assume incompressible, isotropic, homogeneous newtonian fluid:\n$$\n\\frac{\\partial \\mathbf{v}}{\\partial t} + (\\mathbf{v} \\cdot \\nabla)\\mathbf{v} = \\mathbf{g} - \\frac{1}{\\rho_0}\\nabla p + \\nu \\nabla^2 \\mathbf{v}, \\qquad \\nabla \\cdot \\mathbf{v} = 0\n$$\n\nevery term has a physical meaning:\n\n| term | what it means |\n|------|--------------|\n| $\\partial \\mathbf{v}/\\partial t$ | local acceleration \u2014 how velocity changes at a fixed point |\n| $(\\mathbf{v} \\cdot \\nabla)\\mathbf{v}$ | advection \u2014 momentum carried by the flow itself |\n| $\\mathbf{g}$ | gravity (or other body forces) |\n| $-\\nabla p / \\rho_0$ | pressure gradient \u2014 fluid flows from high to low pressure |\n| $\\nu \\nabla^2 \\mathbf{v}$ | viscous diffusion \u2014 friction smears out velocity gradients |\n\nthe assumptions behind this equation: the fluid is **incompressible** ($\\nabla \\cdot \\mathbf{v} = 0$), **newtonian** ($\\nu$ is constant), and **isotropic** (same material properties in all directions).\n\nthese equations are among the most studied in all of physics. they can produce laminar flow, turbulence, boundary layers, vortex streets, and chaos \u2014 all from four lines of math.\n\n## the reynolds number \u2014 who wins the cafeteria battle?\n\nback to our cafeteria. the reynolds number quantifies the competition between inertia and viscosity:\n$$\n\\text{re} = \\frac{|\\text{advective term}|}{|\\text{viscous term}|} \\approx \\frac{u^2/l}{\\nu u / l^2} = \\frac{ul}{\\nu}\n$$\n\nwhere $u$ is a characteristic velocity and $l$ is a characteristic length.\n\n* **re $\\ll$ 1**: viscosity dominates. the flow is smooth, predictable, and *creeping*. think of bacteria swimming through mucus, or honey pouring off a spoon. harry the honey drop lives here.\n* **re $\\sim$ 1**: a fair fight. inertia and viscosity are comparable.\n* **re $\\gg$ 1**: inertia dominates. the flow becomes chaotic, turbulent, unpredictable. think of smoke rising from a campfire, or the wake behind a fast boat.\n\nsome examples:\n\n| system | re |\n|--------|----|\n| bacterium swimming | $\\sim 10^{-4}$ |\n| blood flow in capillaries | $\\sim 10^{-3}$ |\n| honey pouring | $\\sim 10^{-1}$ |\n| water in a pipe | $\\sim 10^3$ |\n| airplane wing | $\\sim 10^7$ |\n| ocean currents | $\\sim 10^9$ |\n\n## newtonian vs. non-newtonian \u2014 when the rules change\n\neverything above assumes the fluid is **newtonian**: the stress is linearly proportional to the strain rate. but many fluids don't play by these rules.\n\n* **shear-thinning** (pseudoplastic, $n < 1$): viscosity *decreases* under shear. examples: ketchup, blood, paint. this is why you shake the ketchup bottle \u2014 the shearing makes it flow.\n* **shear-thickening** (dilatant, $n > 1$): viscosity *increases* under shear. examples: cornstarch in water (oobleck), wet sand at the beach.\n\nthe power-law model captures this: $\\sigma = k \\dot{\\gamma}^n$, where $n$ is the flow"
    },
    {
      "topicId": "continuum-mechanics",
      "topicTitle": "Continuum Mechanics",
      "routeSlug": "continuum-mechanics",
      "lessonSlug": "weakstokes",
      "lessonTitle": "Weak Stokes Formulation",
      "x": 0.2813628017902374,
      "y": 0.8852092027664185,
      "searchText": "weak stokes formulation\n# weak stokes formulation\n\n## why \"weak\"? \u2014 from perfect to practical\n\nthe stokes equation $\\nabla p = \\eta \\nabla^2 \\mathbf{v}$ with $\\nabla \\cdot \\mathbf{v} = 0$ is the **strong form**: it demands that the equation is satisfied at every single point in the domain. that's a tall order when your domain is a glacier valley with jagged rock walls and complex boundary conditions.\n\nthe **weak form** relaxes this demand. instead of requiring the equation to hold pointwise, we require it to hold *on average* \u2014 specifically, when integrated against test functions. this might sound like we're giving something up, but we're actually gaining something enormous: the weak form can be solved numerically using the finite element method.\n\nthink of it this way: the strong form is like requiring a student to know the answer at every point on the exam. the weak form is like requiring the student to get the right average score. the average-score requirement is easier to enforce, and for well-posed problems, it's equivalent to the pointwise requirement.\n\n## the cauchy momentum balance in weak form\n\nstart with the cauchy momentum balance for steady stokes flow:\n$$\n-\\nabla \\cdot \\sigma + \\nabla p = \\mathbf{f}, \\qquad \\nabla \\cdot \\mathbf{v} = 0\n$$\n\nwhere $\\sigma = 2\\eta\\,\\dot{\\varepsilon}$ for a newtonian fluid and $\\mathbf{f}$ is the body force.\n\nto get the weak form, we multiply the momentum equation by a **test function** $\\mathbf{w}$ (a vector function that vanishes on the boundary where we've prescribed velocities), and integrate over the domain $\\omega$:\n$$\n\\int_\\omega 2\\eta\\,\\dot{\\varepsilon}(\\mathbf{v}) : \\dot{\\varepsilon}(\\mathbf{w}) \\, dv - \\int_\\omega p \\, (\\nabla \\cdot \\mathbf{w}) \\, dv = \\int_\\omega \\mathbf{f} \\cdot \\mathbf{w} \\, dv + \\int_{\\gamma_n} \\mathbf{t} \\cdot \\mathbf{w} \\, ds\n$$\n\nsimilarly, the incompressibility constraint is tested against a scalar test function $q$:\n$$\n\\int_\\omega q \\, (\\nabla \\cdot \\mathbf{v}) \\, dv = 0\n$$\n\nnotice what happened: the second derivatives that were in the strong form got distributed (via integration by parts) between $\\mathbf{v}$ and $\\mathbf{w}$. we no longer need $\\mathbf{v}$ to be twice differentiable \u2014 once differentiable is enough. this is the technical payoff of the weak form: weaker smoothness requirements on the solution.\n\nwhy does this matter in practice? building $c^1$ finite elements \u2014 basis functions that are continuously differentiable across element boundaries \u2014 is painful: the elements need many degrees of freedom and complicated shape functions. building $c^0$ elements \u2014 merely continuous, with possible kinks at element edges \u2014 is easy: simple triangles with linear or quadratic polynomials will do. integration by parts trades one order of smoothness from the solution for one on the test function, letting us use those simple $c^0$ elements instead.\n\n## from weak form to linear system\n\nin the finite element method (next section), we'll approximate $\\mathbf{v}$ and $p$ as combinations of basis functions. the weak form then becomes a system of linear equations:\n$$\n\\begin{pmatrix} \\mathbf{k} & \\mathbf{g}^t \\\\ \\mathbf{g} & \\mathbf{0} \\end{pmatrix} \\begin{pmatrix} \\mathbf{u} \\\\ \\mathbf{p} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{f} \\\\ \\mathbf{0} \\end{pmatrix}\n$$\n\nwhere $\\mathbf{k}$ is the viscosity matrix, $\\mathbf{g}$ enforces incompressibility, $\\mathbf{u}$ contains the unknown velocities at mesh nodes, and $\\mathbf{p}$ contains the unknown pressures. this is a **saddle-point system** \u2014 it requires careful choice of finite element spaces to ensure stability (the inf-sup condition).\n\n[[simulation fem-convergence]]\n\n## big ideas\n\n* the strong form demands the equation holds at every point. the weak form only demands it hold on average \u2014 when integrated against any test function. for well-posed problems, these two demands are equivalent, but the weak form is far easier to enforce numerically.\n* integration by parts is the magic trick: it redistributes one derivative from the solution onto the test function, halving the smoothness required of each. this is not a compromise \u2014 it's what allows fem to use piecewise-polynomial approximations.\n* the incompressibility constraint $\\nabla \\cdot \\mathbf{v} = 0$ enters the weak form as a separate equation tested against pressure test functions, producing the saddle-point structure that couples velocity and pressure.\n* the inf-sup condition (or lbb condition) is not bureaucratic pedantry: violate it by choosing the wrong finite element spaces, and your pressure solution will be polluted by spurious oscillations.\n\n## what comes next\n\nwe have the equations in the right form. now we need to learn how to actually *solve* them on a computer. that's the finite element method \u2014 where we cut the domain into tiny pieces and solve for each one.\n\n## check your understanding\n\n1. what does it mean for a test function $\\mathbf{w}$ to \"vanish on the boundary where velocities are prescribed\"? what would go wrong physically if test functions were allowed to be nonzero on dirichlet boundaries?\n2. the weak form has first derivatives of both $\\mathbf{v}$ and $\\mathbf{w}$, while the strong form has second derivatives of $\\mathbf{v}$ alone. integration by parts made this trade. why is first-differentiability easier to achieve in a finite element approximation than second-differentiability?\n3. the saddle-point matrix $\\begin{pmatrix} \\mathbf{k} & \\mathbf{g}^t \\\\ \\mathbf{g} & \\mathbf{0} \\end{pmatrix}$ has a zero block in the lower right. why is there no pressure-pressure coupling, and what does this say about how pressure is determined in incompressible flow?\n\n## challenge\n\n**challenge.** apply what you have learned here to build a finite-element solver \u2014 you will do exactly this in the next lesson on the finite element method, where you derive the weak form of a 1d poisson problem, assemble the stiffness matrix with hat functions, and compare numerical and exact solutions.\n"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "bacterial-growth",
      "lessonTitle": "Bacterial Growth Physiology",
      "x": 0.08604995161294937,
      "y": 0.5436693429946899,
      "searchText": "bacterial growth physiology\n# bacterial growth physiology\n\n## where we are headed\n\nthis is the final lesson, and it is where everything comes together. we have modeled individual genes, noise, regulation, feedback, signaling, and networks. now we zoom all the way out and ask the biggest question of all: how does a cell *grow*? how does a bacterium take in nutrients and convert them into more of itself, doubling in size and dividing every twenty minutes? the answer involves a stunning act of resource allocation \u2014 the cell must decide, moment by moment, how to divide its limited protein budget between making the machinery for growth (ribosomes) and making everything else. and it turns out that a single small molecule, ppgpp, acts as the cell's internal text message saying \"i'm starving \u2014 stop making ribosomes!\"\n\n## the growth curve\n\njacques monod, working in the 1940s, carefully measured how bacterial populations grow over time and identified several distinct phases. we focus on the **exponential growth phase**, where bacteria have settled into a steady rhythm: every cell component doubles once per generation, the nutrient supply is constant, and the population increases exponentially.\n\nas frederick c. neidhardt later wrote about encountering this simple exponential curve for the first time: it was a transforming experience. there is something profound about the fact that this enormously complex biochemical machine \u2014 thousands of genes, thousands of reactions \u2014 achieves such simple, predictable behavior.\n\n## defining steady-state growth\n\nduring balanced exponential growth, three conditions hold:\n\n1. **intrinsic parameters** of the cell (composition, ratios of components) remain constant.\n2. **extensive parameters** (total protein, total rna, cell mass) all increase exponentially with precisely the same doubling time.\n3. **growth conditions** (temperature, nutrient concentrations) remain constant.\n\n> *this is a remarkable state: the cell is a machine that is building a copy of itself, and during balanced growth it does so with perfect proportionality \u2014 everything doubles together.*\n\n## monod's growth law\n\nmonod discovered that the growth rate $\\lambda$ depends on the concentration of the limiting nutrient $s$ through a saturating function:\n\n> **key equation \u2014 growth rate is proportional to ribosome fraction**\n> $$\n> \\lambda = k \\cdot \\phi_\\mathrm{r}\n> $$\n> a bacterium's growth rate equals the translation speed per ribosome times the fraction of its protein that is ribosomes \u2014 one of the most robust quantitative laws in microbiology.\n\n$$\n\\lambda = \\lambda_\\mathrm{max} \\frac{s}{k_s + s}.\n$$\n\n> *at low nutrient concentration, growth rate increases linearly with $s$. at high concentration, the cell is growing as fast as it can and adding more nutrient makes no difference. the half-saturation constant $k_s$ is the concentration at which the cell grows at half its maximum rate.*\n\nthis is a **michaelis-menten** function \u2014 the same mathematical form we saw in the hill function with $n = 1$. it appears here because nutrient uptake enzymes, like all enzymes, saturate when their substrate is abundant.\n\n[[simulation michaelis-menten]]\n\nvary the nutrient concentration $s$ and watch the growth rate $\\lambda$ respond. at very low $s$, growth is nearly proportional to nutrients. at very high $s$, adding more makes no difference \u2014 the cell is growing as fast as its ribosomes allow. find $k_s$: the concentration where growth is exactly half-maximal.\n\n## growth as protein production\n\nhere is a key simplification: 55% of the total dry weight of a bacterium is protein. so to a first approximation, **bacterial growth is protein synthesis**. the equation for total protein mass $m$ is:\n\n$$\n\\frac{\\mathrm{d} m}{\\mathrm{d} t} = \\lambda \\cdot m.\n$$\n\n> *this is just exponential growth: the more protein the cell has, the faster it makes more. this is possible because some of that protein is the very machinery (ribosomes) that makes protein.*\n\nif protein synthesis is the rate-limiting step, we can be more specific:\n\n$$\n\\frac{\\mathrm{d} m}{\\mathrm{d} t} = k \\cdot n_r^a,\n$$\n\nwhere $k$ is the translation rate per ribosome and $n_r^a$ is the total number of actively translating ribosomes.\n\n> *all the cell's growth ultimately depends on how many ribosomes it has and how fast they work. this is the central equation of bacterial growth physiology.*\n\n## the proteome pie: three slices\n\nwe can divide the cell's protein into three functional categories:\n\n* **metabolic and transport proteins**: fraction $\\phi_\\mathrm{p} = m_\\mathrm{p}/m$. these bring nutrients in and process them.\n* **ribosomes** (and associated factors): fraction $\\phi_\\mathrm{r} = m_\\mathrm{r}/m$. these make protein.\n* **everything else** (housekeeping, dna replication, etc.): fraction $\\phi_\\mathrm{q} = m_\\mathrm{q}/m$.\n\nsince these are the only categories:\n\n$$\n\\phi_\\mathrm{p} + \\phi_\\mathrm{r} + \\phi_\\mathrm{q} = 1.\n$$\n\n> *the cell has a fixed budget. every ribosome it makes is a metabolic enzyme it did not make, and vice versa. growth physiology is fundamentally a problem of **resource allocation**.*\n\n[[figure proteome-pie-charts]]\n\n[[simulation proteome-allocation]]\n\nadjust the nutrient quality slider to see how the cell redistributes its protein budget. watch how the growth rate indicator responds as you shift investment between ribosomes ($\\phi_r$) and metabolic enzymes ($\\phi_p$). the constraint $\\phi_r + \\phi_p + \\phi_q = 1$ means every extra ribosome comes at the cost of a metabolic enzyme. find the allocation that maximizes growth rate \u2014 and notice that it depends on the nutrient environment.\n\n## the allocation dilemma\n\nhere is the dilemma the cell faces. metabolic enzymes ($\\phi_\\mathrm{p}$) bring nutrients into the cell. ribosomes ($\\phi_\\mathrm{r}$) convert those nutrients into new protein. for the cell to grow efficiently, **nutrient influx must match nutrient usage**. too many metabolic enzymes and not enough ribosomes? nutrients pile up but cannot "
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "differential-equations",
      "lessonTitle": "Differential Equations in a Nutshell",
      "x": 0.07599782943725586,
      "y": 0.5955976247787476,
      "searchText": "differential equations in a nutshell\n# differential equations in a nutshell\n\n## where we are headed\n\nthis is our first lesson, and we need a language for talking about how things change over time. that language is differential equations. by the end of this lesson you will see that one simple idea \u2014 *the rate of change equals what flows in minus what flows out* \u2014 is enough to explain how molecules accumulate, decay, and reach a steady state inside a living cell. everything else in this course builds on top of this.\n\n## the bathtub picture\n\nimagine you are filling a bathtub. water flows in at constant rate $k$ (molecules per minute) and drains out at rate proportional to how full it is: $\\gamma n$. the net rate of change is production minus loss:\n\n> **key equation \u2014 the bathtub equation**\n> $$\n> \\dot{n} = k - \\gamma n\n> $$\n> the rate of change of molecule number equals the production rate minus the degradation rate: what flows in minus what flows out.\n\nat steady state the tub is neither rising nor falling, so $\\dot{n} = 0$. solve: $n_\\mathrm{ss} = k / \\gamma$. every steady state in this entire course is of this form.\n\nthis is exactly how molecules work inside a cell. mrna is produced at some rate by the transcription machinery, and it is degraded at a rate proportional to how much is present. the steady-state concentration is simply the ratio of production to degradation. nature finds this balance automatically. let us now derive this step by step.\n\n[[figure bathtub-cartoon]]\n\n[[figure approach-to-steady-state]]\n\n## creation: molecules appearing at a constant rate\n\nsuppose molecules (say, mrna) are created at a constant rate $k$ \u2014 the \"birth rate\" \u2014 (molecules per unit time). after a short time $\\delta t$, the number of molecules goes from $n(t)$ to\n\n$$\nn(t+\\delta t) = n(t) + k \\, \\delta t.\n$$\n\n> *in words: the new count equals the old count plus however many were made during that interval.*\n\nrearranging and taking the limit $\\delta t \\rightarrow 0$ gives us our first differential equation:\n\n$$\n\\frac{\\mathrm{d} n(t)}{\\mathrm{d} t} = k.\n$$\n\n> *this says: the number of molecules increases at a constant rate, no matter how many are already there.*\n\nthe solution is simply $n(t) = n(0) + kt$ \u2014 a straight line. molecules pile up forever. clearly something is missing.\n\n## degradation: molecules falling apart\n\nnow suppose molecules are degraded at a rate $\\gamma$ \u2014 the \"death rate constable\" \u2014 (per unit time). the number destroyed in a short interval $\\delta t$ is $\\gamma \\, n(t) \\, \\delta t$ \u2014 it depends on how many molecules are present. following the same logic:\n\n$$\n\\frac{\\mathrm{d} n(t)}{\\mathrm{d} t} = -\\gamma \\, n(t).\n$$\n\n> *this says: the more molecules you have, the faster they disappear.*\n\nthe solution is $n(t) = n(0) \\, e^{-\\gamma t}$ \u2014 exponential decay. you can also think of this in terms of the **half-life** $t_{1/2} = \\ln 2 / \\gamma$, the time it takes for half the molecules to be gone. or equivalently, the **time constant** $\\tau = 1/\\gamma$, which is the time for the number to drop to about 37% of its starting value.\n\n## the full picture: creation and degradation together\n\nnow combine both processes. this is the bathtub equation:\n\n$$\n\\frac{\\mathrm{d} n(t)}{\\mathrm{d} t} = k - \\gamma \\, n(t).\n$$\n\n> *in words: the rate of change equals what's being made minus what's falling apart.*\n\nat **steady state** nothing is changing, so we set the left side to zero:\n\n$$\n0 = k - \\gamma \\, n_\\mathrm{ss} \\quad \\longrightarrow \\quad n_\\mathrm{ss} = \\frac{k}{\\gamma}.\n$$\n\n> *the steady-state number of molecules is just the production rate divided by the degradation rate. that's it. this beautifully simple result shows up everywhere in biology.*\n\nif you start with zero molecules, the system climbs toward $n_\\mathrm{ss}$ on a timescale set by $1/\\gamma$ \u2014 fast degradation means the system reaches steady state quickly. if you start above $n_\\mathrm{ss}$, the excess decays away on the same timescale.\n\n[[simulation bathtub-dynamics]]\n\ntry varying the production rate $k$ and the degradation rate $\\gamma$. watch three scenarios: the default parameters, fast degradation, and slow degradation. notice how the steady-state level $n_{ss} = k/\\gamma$ changes, and how the time to reach steady state depends only on $\\gamma$. toggle the analytical solution overlay to check the exponential approach.\n\n## why does nature do it this way?\n\nyou might ask: why does a cell bother degrading its own mrna? it costs energy to make it, so why destroy it? the answer is speed. a cell that only produces molecules (no degradation) can never reduce their levels \u2014 it is stuck. but a cell that both produces and degrades can change its steady state simply by adjusting the production rate. the faster the degradation, the quicker the response. this is why many mrnas in bacteria have half-lives of just a few minutes.\n\n## check your understanding\n\n* if you double the production rate $k$ while keeping $\\gamma$ fixed, what happens to the steady-state level?\n* a certain mrna has a half-life of 3 minutes. what is its degradation rate $\\gamma$?\n* why does faster degradation lead to faster response, even though it seems wasteful?\n\n## challenge\n\nsuppose a gene starts producing mrna at rate $k = 10$ molecules per minute, starting from zero, with degradation rate $\\gamma = 0.2$ per minute. without solving the differential equation, try to guess the steady-state level and the approximate time to get there. then solve $\\dot{n} = k - \\gamma n$ and check your guess. how close were you?\n\n## big ideas\n\n* **the bathtub equation** $\\dot{n} = k - \\gamma n$ is the foundation of almost every model in this course.\n* **steady state** is where production balances degradation: $n_\\mathrm{ss} = k / \\gamma$.\n* **the degradation rate $\\gamma$ sets the response time** \u2014 fast turnover means the cell can change its mind quickly.\n\nthe bathtub equation gives us the simplest model of a single molecular species. but real cells make proteins from mrna, introducing a second v"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "feedback-loops",
      "lessonTitle": "Feedback Loops in Biological Systems",
      "x": 0.00779470382258296,
      "y": 0.5195009112358093,
      "searchText": "feedback loops in biological systems\n# feedback loops in biological systems\n\n## where we are headed\n\nin [transcriptional regulation](./transcriptional-regulation) you saw how the hill function lets a cell turn gene expression up or down in response to a signal. that is powerful, but it is still a one-way street: a signal comes in, the gene responds. now we close the loop. what happens when a gene product influences its *own* production? this simple act \u2014 feedback \u2014 is what gives cells memory, decision-making ability, and the capacity to oscillate. switches, clocks, and irreversible commitments all emerge from feedback, and you will see exactly why.\n\n## why feedback matters\n\na gene product that influences its own production creates a **feedback loop**, and the sign of that influence determines everything:\n\n* **negative feedback** (the gene product represses itself): promotes homeostasis, speeds up the response, and reduces noise. the system finds a stable set point and sticks to it.\n* **positive feedback** (the gene product activates itself): generates bistability, memory, and irreversible switching. the system can commit to one of two states and stay there.\n\n## negative autoregulation\n\nsuppose a transcription factor $x$ represses its own promoter. using the hill function from [transcriptional regulation](./transcriptional-regulation):\n\n> **key equation \u2014 negative autoregulation**\n> $$\n> \\frac{dx}{dt} = \\frac{\\beta}{1 + (x/k)^n} - \\gamma x\n> $$\n> the gene product represses its own production: when $x$ is high, the hill term shrinks, slowing production and self-correcting toward steady state.\n\nhere $\\beta$ is the maximal production rate (the \"factory at full power\"), $k$ is the repression threshold (the \"sensitivity dial\"), $n$ is the hill coefficient (the \"sharpness knob\"), and $\\gamma$ is the degradation rate (the \"death rate constable\").\n\n> *in words: when $x$ is low, the repression term is small and production runs at nearly full speed. as $x$ builds up, it starts sitting on its own promoter and slowing down its own production. the system self-corrects.*\n\nthis self-correction gives negative autoregulation three beautiful properties:\n\n* **faster response time**: the system overshoots at first (production starts at full blast when $x$ is low), then self-limits. this gets to steady state faster than a gene without feedback.\n* **reduced noise**: if $x$ fluctuates above the mean, the increased repression pushes it back down. if it drops below, repression eases and production speeds up. the feedback acts like a thermostat.\n* **robustness**: the steady-state level is less sensitive to parameter variations. if the degradation rate changes slightly, the feedback compensates.\n\n## finding the steady state graphically\n\nhere is a powerful trick for analyzing any one-dimensional feedback circuit. think of it as drawing two curves on the same piece of paper:\n\n1. draw the **production rate** $f(x) = \\beta/(1 + (x/k)^n)$ as a function of $x$. for negative feedback, this curve starts high and decreases.\n2. draw the **degradation rate** $\\gamma x$ \u2014 a straight line through the origin.\n3. **where they cross is where the system is happy and stays put.** that intersection is the steady state.\n\n> *at the crossing point, production exactly balances degradation \u2014 nothing changes. if $x$ is below the crossing point, production exceeds degradation, so $x$ increases. if $x$ is above, degradation wins, so $x$ decreases. the system is pulled toward the crossing like a ball rolling to the bottom of a valley.*\n\n[[figure graphical-steady-state-construction]]\n\nto check stability: if the production curve is *below* the degradation line to the right of the crossing (and above to the left), the fixed point is stable. if the slopes tell the opposite story, it is unstable.\n\n[[simulation steady-state-regulation]]\n\nstart with the degradation rate $\\gamma$ low and watch the production and degradation curves cross at one point (stable steady state). now slowly increase $\\gamma$ \u2014 the crossing point moves. for negative feedback, you always get exactly one crossing. switch to positive feedback mode and try the same thing \u2014 you should be able to find a parameter range where three crossings appear: two stable and one unstable. that is bistability.\n\n[[simulation production-degradation-crossings]]\n\nthis simulation makes the graphical construction explicit: the blue s-shaped curve is the production rate $f(x)$ for positive feedback, and the red line is the degradation rate $\\gamma x$. adjust the hill coefficient and degradation rate to watch the number of crossings change from one (monostable) to three (bistable). the green dots are stable steady states; the grey dot is unstable.\n\n## positive feedback and bistability\n\nnow suppose the transcription factor *activates* its own production:\n\n$$\n\\frac{dx}{dt} = \\frac{\\beta (x/k)^n}{1 + (x/k)^n} + \\beta_0 - \\gamma x,\n$$\n\nwhere $\\beta_0$ is a small basal (leak) production rate \u2014 even without feedback, a trickle of $x$ is made.\n\nfor sufficiently cooperative activation ($n \\geq 2$), the production curve can be s-shaped, and it may cross the degradation line at **three points**: two stable (low and high expression) and one unstable in between. this is **bistability**.\n\n> *look at the picture: a straight degradation line can cross an s-curve once (monostable) or three times (two stable states + one unstable). the cell now has **memory** \u2014 it can sit in the low or high state forever, even after the signal is gone. that is how a stem cell stays a stem cell and how a phage decides to lysogenise.*\n\na push large enough to cross the unstable middle point flips the switch. the positive feedback maintains itself \u2014 once a cell commits to the high state, it remains there even after the inducing signal is removed.\n\n[[figure bifurcation-diagram-positive-feedback]]\n\n[[simulation bifurcation-diagram]]\n\ndrag the degradation rate slider slowly from low to high and watch the stable states appear and disappear. at"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "gene-expression-noise",
      "lessonTitle": "Quantifying Noise in Gene Expression",
      "x": 0.15841762721538544,
      "y": 0.5546092391014099,
      "searchText": "quantifying noise in gene expression\n# quantifying noise in gene expression\n\n## where we are headed\n\nin [mutational analysis](./mutational-analysis) you saw how the poisson distribution describes rare mutational events \u2014 and you met the fano factor as a statistical fingerprint. now we turn those same tools on **gene expression itself**. remember the bathtub equation from [differential equations](./differential-equations), with its clean, smooth steady state? that steady state is a *lie*. or rather, it is the average of something much messier. inside a real cell, molecules are made and destroyed one at a time by random collisions, and when the numbers are small \u2014 tens or hundreds of molecules, not trillions \u2014 the randomness matters enormously. today we find out just how noisy gene expression really is, and why that noise is not just a nuisance but a tool that cells use to make life-or-death decisions.\n\n## the central dogma, revisited\n\nyou already know the central dogma:\n\n$$\n\\text{dna} \\xrightarrow{\\text{transcription}} \\text{mrna} \\xrightarrow{\\text{translation}} \\text{protein}\n$$\n\ntwo molecular machines drive this process: **rna polymerase** transcribes dna into mrna, and the **ribosome** translates mrna into protein. in our deterministic model, these machines hum along at constant rates. but in reality, every molecular encounter \u2014 polymerase finding the promoter, ribosome latching onto the mrna \u2014 is a random event driven by diffusion. the molecules are doing a drunken walk through the cytoplasm, and sometimes they find their target quickly, sometimes slowly.\n\n## measuring the noise\n\nto see noise in action, biologists fuse a **reporter protein** like gfp (green fluorescent protein) to a gene of interest. the brighter the cell glows, the more protein it has.\n\n* **bulk measurement** (a tube full of billions of cells): you see the population average. smooth. boring.\n* **single-cell measurement** (microscopy or flow cytometry): you see the full distribution. and it is *wide*. genetically identical cells, grown in the same conditions, can have two-fold or even ten-fold differences in protein level.\n\nthis is not measurement error. it is real, biological noise.\n\n## why is gene expression noisy?\n\nthe fundamental reason is **small numbers**. a typical *e. coli* cell has one or two copies of each gene, perhaps a handful of mrna molecules, and maybe a few hundred to a few thousand proteins from each gene. when you are working with numbers this small, random fluctuations are unavoidable \u2014 it is the same reason that flipping a coin 10 times gives a much noisier fraction of heads than flipping it 10,000 times.\n\nthis noise has real biological consequences:\n\n* **competence in *b. subtilis***: noisy expression of the master regulator *comk* causes a small fraction of cells to stochastically flip into a state where they can take up dna from the environment \u2014 a survival strategy under stress.\n* **persister cells**: in a genetically uniform bacterial population, noise creates rare cells with low metabolic activity that survive antibiotic treatment, even without resistance mutations.\n\n## quantifying noise: the coefficient of variation\n\nthe **coefficient of variation** (cv) measures noise as the ratio of the standard deviation to the mean:\n\n> **key equation \u2014 noise decomposition**\n> $$\n> \\eta_{\\mathrm{total}}^2 = \\eta_{\\mathrm{int}}^2 + \\eta_{\\mathrm{ext}}^2\n> $$\n> total noise decomposes exactly into intrinsic noise (independent random firing of each gene copy) and extrinsic noise (shared fluctuations across the cell).\n\n$$\n\\eta = \\frac{\\sigma}{\\langle n \\rangle} = \\frac{\\sqrt{\\langle (n - \\langle n \\rangle)^2 \\rangle}}{\\langle n \\rangle},\n$$\n\nwhere $n$ is the protein copy number in a single cell, and the angle brackets denote the average over the population.\n\n> *in words: the cv tells you how wide the distribution is relative to its center. a cv of 0.5 means the typical fluctuation is about half the mean \u2014 that is very noisy.*\n\n## decomposing noise: the elowitz experiment\n\nhere is one of the most clever experiments in modern biology. in 2002, michael elowitz and colleagues asked: where does the noise come from? is it because individual gene copies fire randomly (**intrinsic noise**), or because the whole cell's environment fluctuates \u2014 varying numbers of ribosomes, polymerases, and metabolites from cell to cell (**extrinsic noise**)?\n\nthe trick: put two *different-colored* fluorescent reporters (cfp and yfp) under the control of *identical* promoters in the same cell. if the noise is all extrinsic (upstream fluctuations shared by both genes), the two colors go up and down together \u2014 every cell lands on the diagonal of a cfp-vs-yfp plot. if the noise is intrinsic (independent random firing), the colors fluctuate independently \u2014 cells scatter off the diagonal.\n\nthe math makes this precise. imagine two glasses of water, one blue and one red, and you are trying to figure out why the water levels fluctuate. one glass might be noisy because the faucet drips unpredictably (intrinsic \u2014 each glass has its own random drip). the other source of variation is that some days you leave the tap on longer (extrinsic \u2014 both glasses get more or less water together).\n\n**extrinsic noise** (correlated fluctuations):\n\n$$\n\\eta_{\\mathrm{ext}}^2 = \\frac{\\langle n^{(1)} n^{(2)} \\rangle - \\langle n^{(1)} \\rangle \\langle n^{(2)} \\rangle}{\\langle n^{(1)} \\rangle \\langle n^{(2)} \\rangle}.\n$$\n\n**intrinsic noise** (uncorrelated fluctuations):\n\n$$\n\\eta_{\\mathrm{int}}^2 = \\frac{\\langle (n^{(1)} - n^{(2)})^2 \\rangle}{2\\,\\langle n^{(1)} \\rangle \\langle n^{(2)} \\rangle}.\n$$\n\nand the total noise decomposes exactly:\n\n$$\n\\eta_{\\mathrm{total}}^2 = \\eta_{\\mathrm{int}}^2 + \\eta_{\\mathrm{ext}}^2.\n$$\n\n> *the beauty of this decomposition is that you can measure it directly from the two-color data, without knowing anything about the underlying mechanism.*\n\n[[simulation gene-expression-noise]]\n\nstart with a high production rate and watch the protein trace \u2014"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "gene-regulatory-networks",
      "lessonTitle": "Gene Regulatory Networks",
      "x": 0.00798653345555067,
      "y": 0.5009034276008606,
      "searchText": "gene regulatory networks\n# gene regulatory networks\n\n## where we are headed\n\nover the past several lessons you have built up a toolkit: differential equations, the hill function, feedback loops, and signaling. now we step back and look at the big picture. inside every cell, hundreds of genes regulate each other in a vast interconnected **network**. how do we make sense of something so complex? the answer, it turns out, is that these networks are not random tangles \u2014 they are built from a small set of recurring patterns called **motifs**. these are the lego bricks that nature uses over and over, and once you learn to recognize them, you can read a regulatory network the way an engineer reads a circuit diagram.\n\n## types of regulation\n\nevery regulatory interaction falls into one of two categories:\n\n* **positive regulation**: gene a activates gene b, symbolized as $a \\rightarrow b$. more of a means more of b.\n* **negative regulation**: gene a represses gene b, symbolized as $a \\dashv b$. more of a means less of b.\n\nthat is the entire alphabet. every regulatory network, no matter how complex, is spelled with just these two letters. the magic is in the combinations.\n\n## from small genomes to complex regulation\n\nas genome size increases, the fraction of genes devoted to regulation increases *faster* than the total number of genes. a simple bacterium might dedicate 5% of its genes to regulation; a complex organism dedicates far more. regulation is expensive, but it pays for itself: the more complex the environment, the more a cell needs to coordinate its response.\n\n## the motif zoo: circuit patterns with personalities\n\neven the fruit fly *drosophila* has a regulatory network with thousands of genes and tens of thousands of interactions. staring at the whole thing is hopeless. but here is the key insight from uri alon and others: certain small subpatterns \u2014 **network motifs** \u2014 appear far more often than you would expect by chance. evolution has discovered that these motifs are *useful*, and keeps reusing them. let us meet the family, each with its own personality.\n\n### negative autoregulation \u2014 \"the thermostat\"\n\nwe already know this one from the feedback lesson: a gene represses itself. it is the single most common motif in *e. coli* \u2014 about half of all transcription factors repress their own promoter.\n\n**personality: stability and speed.** it locks the protein level to a set point and gets there fast.\n\n### positive autoregulation \u2014 \"the commitment device\"\n\na gene activates itself. with strong cooperativity, this creates bistability \u2014 two stable states with a barrier between them.\n\n**personality: memory and irreversibility.** once the cell flips to the high state, it stays there. the *comk* system in *b. subtilis* uses this to commit a rare fraction of cells to competence.\n\n### the feed-forward loop \u2014 \"the delay timer\"\n\nthis is the most interesting motif, and it comes in eight flavors (depending on whether each arrow is activation or repression). the most common is the **coherent type-1 feed-forward loop**:\n\n> **key equation \u2014 feed-forward loop logic**\n> $$\n> x \\rightarrow y \\rightarrow z, \\qquad x \\rightarrow z\n> $$\n> gene x activates z through a fast direct path and a slow indirect path via y; with and-gate logic, only sustained signals pass through \u2014 brief noise is filtered out.\n\ngene $x$ activates gene $z$ through two paths: a fast direct path, and a slow indirect path through $y$. if $z$ requires *both* $x$ and $y$ to be present (and logic), then $z$ only turns on after $x$ has been on long enough for $y$ to accumulate. a brief pulse of $x$ does nothing \u2014 only a sustained signal gets through.\n\n> *think of it as a spam filter for your genes. a brief, noisy fluctuation in $x$ will not accidentally turn on $z$. the signal has to be real and persistent. this is how the arabinose utilization genes ($ara$) work in *e. coli*: crp activates arac, and both crp and arac must be present to turn on the $ara$ genes.*\n\nthe **incoherent type-1 feed-forward loop** does the opposite: $x$ activates $z$ directly but also activates $y$, which *represses* $z$. the result is a pulse \u2014 $z$ goes up fast (through the direct path) and then comes back down (when the repressor $y$ catches up).\n\n**personality: pulse generation and response acceleration.**\n\n### the toggle switch \u2014 \"the memory bank\"\n\ntwo genes mutually repress each other:\n\n$$\nu \\dashv v, \\qquad v \\dashv u.\n$$\n\nwe met this in the feedback lesson as the genetic toggle switch. in the network context, it is the fundamental unit of **binary memory** \u2014 the cell can be in state \"high-$u$\" or state \"high-$v$,\" and it remembers which one. the lambda phage lysis-lysogeny decision (ci vs. cro) is the textbook example.\n\n### the single input module \u2014 \"the temporal program\"\n\none transcription factor $x$ controls a set of target genes $z_1, z_2, \\ldots, z_n$, each with a different activation threshold:\n\n$$\nx \\rightarrow z_1, \\quad x \\rightarrow z_2, \\quad x \\rightarrow z_3.\n$$\n\nas $x$ gradually increases, the targets turn on one by one in order of their thresholds.\n\n**personality: temporal ordering.** this is how *e. coli* builds its flagellum \u2014 a single master regulator activates the structural genes in the correct assembly order.\n\n[[simulation motif-gallery]]\n\nclick through the four motifs to see their step responses. negative autoregulation reaches steady state faster than a gene without feedback. positive feedback shows two trajectories converging to different steady states \u2014 bistability. the feed-forward loop delays its output until the intermediate accumulates, filtering brief noise. the toggle switch flips from one state to the other when hit with a pulse and stays there \u2014 biological memory.\n\n## identifying feedback sign: the multiplication rule\n\ngiven a closed loop, you can determine whether it is positive or negative feedback by a simple trick. assign $+1$ to each activation arrow ($\\rightarrow$) and $-1$ to each repression arrow ($\\dashv$), then multiply aro"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "mutational-analysis",
      "lessonTitle": "Probability for Mutational Analysis",
      "x": 0.1610666960477829,
      "y": 0.5715514421463013,
      "searchText": "probability for mutational analysis\n# probability for mutational analysis\n\n## where we are headed\n\nin the last two lessons you learned to write differential equations for production and degradation, and you saw how mrna and protein form a coupled two-timescale system. those equations describe the average beautifully \u2014 but life is not average. every molecular event is random, and randomness shows up first and most dramatically in dna replication. today we zoom in on **mutations**: dna polymerase is one of the most astonishing proofreaders in nature, making fewer than one error per billion bases copied. how does it achieve that? and when errors *do* slip through, what does the statistics of their appearance look like? the probability distributions we develop here \u2014 **binomial** and **poisson** \u2014 will become our toolkit for understanding randomness throughout this course, from rare mutational events to the noise in gene expression that we tackle in the very next lesson.\n\n## what causes mutations?\n\nthere are two broad categories:\n\n**spontaneous mutations** arise from chemical mistakes during dna replication. even under perfect conditions, about 5,000 potentially mutagenic events happen per day in a human cell:\n* **depurination**: an adenine or guanine base simply falls off the sugar backbone.\n* **deamination**: cytosine spontaneously converts to uracil.\n\n**induced mutations** are caused by external agents \u2014 uv light, ionizing radiation, or chemical mutagens. these account for roughly 100 additional dna lesions per day in a human cell.\n\n## the fidelity of the central dogma\n\nnot every step in gene expression is equally accurate. the error rates tell a striking story:\n\n* **dna replication**: $\\sim 1$ error per $10^9$ bases (a commonly cited round number; modern estimates after all correction layers are closer to $10^{-10}$) \u2014 almost unbelievably precise.\n* **transcription**: $\\sim 1$ error per $10^6$ bases \u2014 good, but a thousand times worse.\n* **translation**: $\\sim 1$ error per $10^4$ bases \u2014 the least accurate step.\n\n> *think about what this means: evolution has invested enormous machinery into keeping dna replication faithful, because errors there are permanent and heritable. errors in transcription and translation are temporary \u2014 the bad mrna or protein will be degraded soon enough.*\n\n## how does dna polymerase proofread so well?\n\nimagine a typist who makes a mistake every few hundred keystrokes. that is roughly the error rate of base-pairing alone \u2014 the free energy difference between a correct and incorrect base pair (4--13 kj/mol) is only enough to give an error rate of about $10^{-2}$. so how does the cell get from $10^{-2}$ down to $10^{-9}$?\n\nthe answer is **two extra layers of proofreading**, each multiplying the fidelity:\n\n**editing by dna polymerase** \u2014 dna polymerase does not just add bases; it also checks the last base it added. if the fit is wrong, the polymerase backs up and removes it. this is like a typist who pauses after every keystroke to check and erase mistakes before moving on.\n\n**strand-directed mismatch repair** \u2014 after replication, dedicated repair enzymes scan the new dna strand for mismatches. they can tell which strand is new (in bacteria, by its methylation pattern) and recruit dna polymerase to fix the error. this is like a second proofreader who reads the whole manuscript after the typist is done.\n\neach layer improves fidelity by roughly a factor of 100--1000, and together they achieve the extraordinary $10^{-9}$ error rate.\n\n## recombination\n\nduring meiosis (in sexually reproducing organisms), chromosomes exchange segments through **crossing-over**. this is not an error \u2014 it is a deliberate reshuffling that generates genetic diversity, the raw material for evolution.\n\n## working with mutants\n\nbacteria are a geneticist's dream: you can grow a billion of them overnight in a single milliliter. to find mutants in this ocean of cells, we use:\n\n* **genetic selection**: design conditions where only mutants survive. for example, grow bacteria on a plate containing an antibiotic \u2014 only resistant mutants form colonies.\n* **genetic screens**: check every colony individually for the phenotype of interest. more labor-intensive, but necessary when there is no way to select directly.\n\n## the statistics of rare events\n\nnow suppose you watch cells divide and count how many carry a new mutation. each cell division is an independent \"trial\" with a tiny probability $p$ of producing a mutant. after $n$ divisions, how many mutants do you expect? this is exactly the setting for the **binomial distribution**.\n\n## the binomial distribution\n\nif each of $n$ independent trials has probability $p$ of success, the probability of getting exactly $k$ successes is:\n\n> **key equation \u2014 the poisson distribution**\n> $$\n> p_m(k) = \\frac{m^k}{k!} e^{-m}\n> $$\n> when $n$ is large and $p$ is small (with $m = np$ moderate), the binomial simplifies to the poisson: the universal distribution for counting rare, independent events.\n\n$$\np_n(k) = \\binom{n}{k} p^k (1-p)^{n-k}.\n$$\n\n> *in words: choose which $k$ trials are the \"successes\" (the binomial coefficient), multiply by the probability of $k$ successes and $n-k$ failures.*\n\nthe mean is $\\mu = np$ and the variance is $\\sigma^2 = np(1-p)$.\n\n## the poisson limit\n\nin biology, $n$ is typically enormous (millions of cell divisions) and $p$ is tiny (one error per billion bases), but the product $m = np$ is a modest number. in this limit, the binomial distribution simplifies beautifully to the **poisson distribution**:\n\n$$\np_m(k) = \\frac{m^k}{k!} e^{-m}.\n$$\n\n> *the poisson distribution has a remarkable property: its mean and variance are both equal to $m$. this is exactly where the fano factor $f = 1$ comes from \u2014 a poisson process has $f = \\text{var}/\\text{mean} = 1$.*\n\nthis connects directly to what we learned about noise: if gene expression events (transcription, translation) are independent poisson events, the fano factor is 1. when we see $f > 1$, somethin"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "signal-transduction",
      "lessonTitle": "Signal Transduction",
      "x": 0.06621628254652023,
      "y": 0.5018814206123352,
      "searchText": "signal transduction\n# signal transduction\n\n## where we are headed\n\nwatch this tiny bug. an *e. coli* bacterium \u2014 no brain, no eyes, no map \u2014 is swimming through a sugar gradient, and it is doing calculus with its body. it detects a concentration change of one extra molecule per cell volume per micron, across five orders of magnitude of background concentration, all while being hammered by brownian motion. it is the most sensitive navigation system in nature, and today you are going to understand exactly how it works.\n\nin [feedback loops](./feedback-loops) we saw how feedback creates switches and oscillators *inside* the cell. now we zoom out and ask: how does the cell sense what is happening *outside*? the answer \u2014 **signal transduction** \u2014 is how cells read their mail, and it involves some of the most elegant molecular engineering in all of biology.\n\n## what is a signal?\n\na signal is anything the cell can detect:\n* **chemical**: food molecules, quorum-sensing signals, hormones, ions, gases.\n* **physical**: pressure, temperature, light.\n\nthe signal binds to a specific **receptor** on the cell surface, triggering a cascade of events inside the cell. this cascade is signal transduction.\n\n## why so many steps?\n\nyou might ask: why not just have the signal directly flip a gene on or off? why the elaborate chain of intermediate molecules? because intermediate steps give the cell superpowers:\n\n* **amplification**: a single receptor event can activate thousands of downstream molecules.\n* **noise filtering**: intermediate steps smooth out random fluctuations.\n* **integration**: the cell can combine information from multiple signals.\n* **memory**: some signaling states persist after the signal is gone.\n* **adaptation**: the cell can reset its sensitivity, responding to *changes* rather than absolute levels.\n\nvery often, these intermediate steps involve a chain of **phosphorylation** events \u2014 one protein adds a phosphate group to the next, passing the signal along like a bucket brigade.\n\n## bacterial chemotaxis: nature's gps\n\nbacterial chemotaxis is one of the best-understood signaling systems in all of biology, and it is breathtaking. *e. coli* can detect the tiniest gradient of food \u2014 a concentration change of just one molecule per cell volume per micron \u2014 and steer toward it. it can do this across five orders of magnitude of background concentration. and it does all of this while being buffeted by brownian motion, with no brain and no map.\n\n### how bacteria swim\n\nsuppose you are a bacterium. you are about 2 micrometers long, and you have several flagella \u2014 helical propellers driven by molecular motors embedded in your membrane.\n\n* when the motors spin **counterclockwise**, the flagella bundle together into a single propeller and you swim in a straight line. this is a **run**.\n* when one or more motors switch to **clockwise**, the bundle flies apart and you tumble randomly, reorienting in a new direction. this is a **tumble**.\n\na run lasts about 1 second, then you tumble and pick a new direction at random. on average, you go nowhere \u2014 it is a random walk. but here is the trick nature discovered: **if things are getting better (more food), you run longer. if things are getting worse, you tumble sooner.** this bias turns a random walk into a directed climb up the food gradient.\n\n### the molecular circuit\n\nthe switch between running and tumbling is controlled by a small signaling network:\n\n* **chea** is a kinase (an enzyme that adds phosphate groups) associated with the receptor. when the receptor detects *less* attractant, chea becomes more active.\n* **chey** receives a phosphate from chea. phosphorylated chey (chey-p) binds to the flagellar motor and makes it switch to clockwise rotation (tumble).\n\n> *so the logic is: less food \u2192 more chea activity \u2192 more chey-p \u2192 more tumbling. more food \u2192 less chea activity \u2192 less chey-p \u2192 longer runs toward the food. simple and beautiful.*\n\n### the problem of saturation\n\nbut wait \u2014 if tumbling frequency depends on the *absolute* concentration of attractant, the system will saturate quickly. at high background concentrations, the receptors are all occupied regardless of the gradient. the bacterium goes blind. if tumbling frequency ignores concentration entirely, the bacterium cannot detect any gradient at all. how does nature solve this?\n\n### adaptation: the receptor's memory\n\nthe answer is **adaptation**, and it is one of the most beautiful ideas in molecular biology. the cell does not respond to the absolute concentration \u2014 it responds to *changes* in concentration. after a step increase in attractant, the tumbling frequency drops immediately (the cell runs), but then gradually returns to its baseline over about a minute. the cell has adapted.\n\nthink of the receptor as a little spring. when attractant binds, the spring relaxes (signal goes down). but then the enzyme **cher** slowly adds methyl groups to the receptor, stiffening the spring back to its original tension. this **methylation** is the receptor's memory \u2014 it records what the \"normal\" level of attractant is and resets the response.\n\nthe enzyme **cheb** (which removes methyl groups) provides the opposing force, and the balance between cher and cheb sets the adaptation level.\n\n> **key equation \u2014 adaptation via methylation**\n> $$\n> \\frac{\\mathrm{d} [e_m]}{\\mathrm{d} t} = k^r [\\mathrm{cher}] - \\frac{k^b b^{*}(l) \\, e_m}{k_m^b + e_m}\n> $$\n> methylation rises at a constant rate (cher) and falls at a rate depending on receptor activity: the balance resets the receptor's baseline, giving the cell perfect adaptation.\n\n> *the first term says cher adds methyl groups at a constant rate. the second term says cheb removes them at a rate that depends on receptor activity $b^*(l)$, which in turn depends on the ligand concentration $l$. the balance of these two processes is what gives the cell its remarkable ability to adapt.*\n\n[[figure chemotaxis-adaptation]]\n\n[[simulation chemotaxis-adaptation]]\n\nstep up the ligand "
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "transcription-translation",
      "lessonTitle": "Differential Equations for Transcription and Translation",
      "x": 0.07114197313785553,
      "y": 0.5800749659538269,
      "searchText": "differential equations for transcription and translation\n# differential equations for transcription and translation\n\n## where we are headed\n\nin the first lesson we saw how a single molecule type reaches a steady state through the balance of production and degradation. now it is time to apply that same idea to the two-step process at the heart of every cell: dna is transcribed into mrna, and mrna is translated into protein. the beautiful thing is that these two steps operate on very different timescales \u2014 and that difference has profound consequences for how cells respond to change.\n\n## two timescales, one system\n\nimagine tuning a radio in a noisy room. the raw signal from the antenna jitters up and down rapidly \u2014 that is like mrna, which is made and destroyed on a timescale of minutes in a bacterium. but what you actually hear through the speaker is smoothed out, because the speaker (like a capacitor in the circuit) averages over those rapid fluctuations. that smooth output is like the protein level: it changes slowly, following the mrna signal with a delay.\n\nthis two-timescale structure is not a bug \u2014 it is a feature. it means protein levels are naturally buffered against rapid noise in transcription. nature builds its own low-pass filter.\n\n[[figure two-timescale-response]]\n\n## the equations\n\nfor **transcription**, mrna is produced at rate $k_\\mathrm{m}$ and degraded at rate $\\gamma_\\mathrm{m}$:\n\n> **key equation \u2014 coupled transcription-translation**\n> $$\n> \\frac{\\mathrm{d} n_\\mathrm{m}}{\\mathrm{d} t} = k_\\mathrm{m} - \\gamma_\\mathrm{m} \\, n_\\mathrm{m}, \\qquad \\frac{\\mathrm{d} n_\\mathrm{p}}{\\mathrm{d} t} = k_\\mathrm{p} \\, n_\\mathrm{m} - \\gamma_\\mathrm{p} \\, n_\\mathrm{p}\n> $$\n> mrna follows a bathtub equation; protein production depends on how much mrna is present, coupling the two timescales.\n\n> *the first equation is exactly the [bathtub equation](./differential-equations), applied to mrna.*\n\nfor **translation**, protein is produced at a rate proportional to the number of mrna molecules (more mrna means more ribosomes translating) and degraded at rate $\\gamma_\\mathrm{p}$.\n\n> *notice the key difference: the production of protein depends on how much mrna is present. these two equations are coupled.*\n\nthe parameters:\n* $n_\\mathrm{m}(t)$: number of mrna molecules at time $t$\n* $n_\\mathrm{p}(t)$: number of protein molecules at time $t$\n* $k_\\mathrm{m}$: transcription rate (mrna molecules per unit time)\n* $k_\\mathrm{p}$: translation rate (proteins per mrna per unit time)\n* $\\gamma_\\mathrm{m}$: mrna degradation rate\n* $\\gamma_\\mathrm{p}$: protein degradation rate (often dominated by dilution through cell growth)\n\n## steady state\n\nat steady state, mrna reaches $n_\\mathrm{m}^\\mathrm{ss} = k_\\mathrm{m} / \\gamma_\\mathrm{m}$, just as before. plugging this into the protein equation gives:\n\n$$\nn_\\mathrm{p}^\\mathrm{ss} = \\frac{k_\\mathrm{p} \\, k_\\mathrm{m}}{\\gamma_\\mathrm{p} \\, \\gamma_\\mathrm{m}}.\n$$\n\n> *the steady-state protein level depends on all four parameters: both production rates and both degradation rates. change any one, and the protein level shifts.*\n\n## number of molecules versus concentration\n\nso far we have been counting molecules. but experimentally (and theoretically) it is often more convenient to work with **concentrations** \u2014 number per volume. if the cell has volume $v$, we define $c_\\mathrm{m}(t) = n_\\mathrm{m}(t)/v$ and $c_\\mathrm{p}(t) = n_\\mathrm{p}(t)/v$. the equations keep the same form:\n\n$$\n\\frac{\\mathrm{d} c_\\mathrm{m}(t)}{\\mathrm{d} t} = k_\\mathrm{m} - \\gamma_\\mathrm{m} \\, c_\\mathrm{m}(t),\n$$\n\n$$\n\\frac{\\mathrm{d} c_\\mathrm{p}(t)}{\\mathrm{d} t} = k_\\mathrm{p} \\, c_\\mathrm{m}(t) - \\gamma_\\mathrm{p} \\, c_\\mathrm{p}(t).\n$$\n\n> *the switch from number to concentration is just a rescaling \u2014 the physics does not change. but be careful: when cells grow and divide, the volume changes, and dilution acts like an extra degradation term.*\n\n## scaling and dimensionless variables\n\nthere is one more rescaling trick that will pay dividends throughout this course. instead of working with raw numbers or concentrations, measure time in units of the half-life ($\\tilde{t} = \\gamma \\, t$) and concentration in units of some characteristic scale \u2014 for example, the steady-state level $n_\\mathrm{ss}$ itself. define $\\tilde{n} = n / n_\\mathrm{ss}$. then the bathtub equation becomes:\n\n$$\n\\frac{\\mathrm{d} \\tilde{n}}{\\mathrm{d} \\tilde{t}} = 1 - \\tilde{n}.\n$$\n\nall the parameters have disappeared. the solution $\\tilde{n}(\\tilde{t}) = 1 - e^{-\\tilde{t}}$ is universal: *every* bathtub equation, regardless of its specific $k$ and $\\gamma$, follows the same dimensionless curve. this is the power of nondimensionalization \u2014 it strips away the details and reveals the universal behavior. when we later encounter the hill function, the natural concentration scale will be the dissociation constant $k$, and measuring in units of $k$ will again collapse many different systems onto a single curve.\n\n## why does nature do it this way?\n\nwe saw in the [previous lesson](./differential-equations) that degradation lets a cell change its mind quickly. the two-step design adds two further advantages: **amplification** (each mrna is translated many times, turning one gene into thousands of protein copies) and **independent control** (the cell can regulate transcription and translation separately \u2014 two knobs instead of one).\n\n## check your understanding\n\n* if mrna degradation becomes much faster ($\\gamma_\\mathrm{m} \\gg \\gamma_\\mathrm{p}$), what happens to the protein response time? why?\n* a gene is transcribed 10 times per minute and each mrna is translated 20 times before it is degraded. roughly how many proteins does the cell make from this gene per minute?\n* why is protein degradation in bacteria often dominated by dilution (cell division) rather than active degradation?\n\n## challenge\n\nin *e. coli*, a typical mrna has a half-life of about 3 minutes, and a typical protein has an intrinsic half-life of about 1 hour, but in fast-gr"
    },
    {
      "topicId": "dynamical-models",
      "topicTitle": "Dynamical Models",
      "routeSlug": "dynamical-models",
      "lessonSlug": "transcriptional-regulation",
      "lessonTitle": "Transcriptional Regulation and the Hill Function",
      "x": 0.0,
      "y": 0.5600228309631348,
      "searchText": "transcriptional regulation and the hill function\n# transcriptional regulation and the hill function\n\n## where we are headed\n\nover the last four lessons you have built the foundations: differential equations for production and degradation, the two-timescale coupling of mrna and protein, probability distributions for rare events, and the noisy reality of gene expression. through all of that, we treated the transcription rate as a constant \u2014 the gene is either on or off, and we did not ask what controls it. but in a real cell, genes are regulated. a repressor protein sits on the promoter and blocks transcription; an activator protein recruits rna polymerase and turns it on. today we derive the mathematical function that describes this regulation \u2014 the **hill function** \u2014 and you will see how nature turns a gentle, graded response into a sharp, switch-like one. this is one of the most important functions in all of quantitative biology.\n\n## the committee picture\n\nbefore any math, think about it this way. suppose a gene's promoter is like a conference room, and the gene only turns on when the room is occupied by a transcription factor. if only *one* molecule needs to sit down, then the gene turns on gradually as the concentration of that molecule increases \u2014 some rooms are occupied, some are not, and the fraction increases smoothly.\n\nbut now suppose the promoter requires a *committee*: four transcription factor molecules must all be present at the same time to turn the gene on. at low concentrations, the chance of all four showing up simultaneously is tiny. at high concentrations, it is almost certain. the transition from \"almost never\" to \"almost always\" happens over a narrow range of concentration. you have turned a gentle slope into a switch.\n\nthat is the hill function. the hill coefficient $n$ is the size of the committee.\n\n## deriving the hill function from binding kinetics\n\n### a single transcription factor\n\nconsider a transcription factor (tf) that binds to and represses a promoter. the promoter can be in two states: **free** (gene on) or **occupied** (gene off). the binding rate depends on tf concentration, but the unbinding rate does not:\n\n$$\n\\frac{\\mathrm{d} p_\\mathrm{free}}{\\mathrm{d} t} = - v_\\mathrm{bind} \\, c_\\mathrm{p} \\, p_\\mathrm{free} + k_\\mathrm{unbind} \\, (1 - p_\\mathrm{free}).\n$$\n\n> *in words: free promoters get occupied at a rate proportional to tf concentration, and occupied promoters become free at a constant unbinding rate.*\n\nwe assume binding and unbinding are much faster than transcription itself, so the promoter reaches a quasi-steady state. setting the derivative to zero:\n\n$$\np_\\mathrm{free} = \\frac{1}{1 + c_\\mathrm{p}/k}, \\qquad \\text{where } k = \\frac{k_\\mathrm{unbind}}{v_\\mathrm{bind}}.\n$$\n\n> *$k$ is the **dissociation constant** \u2014 the concentration at which the promoter is occupied half the time. when $c_\\mathrm{p} = k$, you get $p_\\mathrm{free} = 1/2$. below $k$, the gene is mostly on; above $k$, the gene is mostly off.*\n\n### dimers: the committee of two\n\nnow suppose the transcription factor must form a **dimer** (a pair) before it can bind the promoter. the binding rate becomes proportional to $c_\\mathrm{p}^2$ instead of $c_\\mathrm{p}$:\n\n$$\np_\\mathrm{free} = \\frac{1}{1 + (c_\\mathrm{p}/k_2)^2}.\n$$\n\n> *the exponent of 2 makes the transition from \"on\" to \"off\" steeper. the gene does not care about individual molecules \u2014 it waits until the concentration is high enough that pairs are common.*\n\n### the general hill function\n\nbecause each extra bound repressor multiplies the probability of the next one binding (cooperativity), the fraction bound becomes $[c_\\mathrm{p}/k]^n / (1 + [c_\\mathrm{p}/k]^n)$. the gene is active only when the promoter is *empty*, so transcription rate is maximal rate times $(1 - \\text{fraction bound})$ \u2014 giving us the hill repression function you see everywhere in biology.\n\nfor a complex of $n$ molecules (hill coefficient $n$), the pattern generalizes:\n\n> **key equation \u2014 the hill function**\n> $$\n> p_\\mathrm{free} = \\frac{1}{1 + (c_\\mathrm{p}/k)^n}\n> $$\n> the fraction of time the promoter is free (gene on) decreases as repressor concentration rises; the hill coefficient $n$ controls how switch-like the transition is.\n\n[[figure hill-function-overlay]]\n\nplay with this in your head:\n* **$n = 1$**: a gentle, hyperbolic curve. the gene gradually turns off as repressor concentration increases.\n* **$n = 2$**: steeper. the transition sharpens.\n* **$n = 4$**: very steep. the gene is either fully on or fully off, with a narrow transition zone around $c_\\mathrm{p} = k$.\n\n> *look how nature turns a gentle slope into a switch! the higher the hill coefficient, the more \"all-or-nothing\" the response. this is the mathematical heart of biological decision-making.*\n\n[[simulation hill-function]]\n\nset the hill coefficient to $n = 1$ and slowly increase the repressor concentration. watch how gradually the gene turns off. now crank $n$ up to 4 and repeat. see how the transition sharpens into a switch? find the concentration where the gene is exactly at 50% \u2014 that is $k$, the dissociation constant. the cell has just invented a tunable amplifier.\n\n## repression\n\nfor a self-repressing gene, the transcription rate depends on the repressor (protein) concentration through the hill function:\n\n$$\n\\frac{\\mathrm{d} c_\\mathrm{m}}{\\mathrm{d} t} = \\frac{\\alpha_\\mathrm{m}}{1 + (c_\\mathrm{p}/k)^n} - \\gamma_\\mathrm{m} \\, c_\\mathrm{m},\n$$\n\n$$\n\\frac{\\mathrm{d} c_\\mathrm{p}}{\\mathrm{d} t} = k_\\mathrm{p} \\, c_\\mathrm{m} - \\gamma_\\mathrm{p} \\, c_\\mathrm{p}.\n$$\n\n> *when protein is scarce ($c_\\mathrm{p} \\ll k$), the hill function is close to 1 and mrna is produced at full rate $\\alpha_\\mathrm{m}$. when protein is abundant ($c_\\mathrm{p} \\gg k$), the hill function drops to nearly zero and transcription is shut off.*\n\nmore generally, if the repressor is a different protein (not the gene's own product), we replace $c_\\mathrm{p}$ with the concentration of the regulating transcription facto"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "bayesian-inversion",
      "lessonTitle": "Bayesian Inversion",
      "x": 0.6200622320175171,
      "y": 0.7276161909103394,
      "searchText": "bayesian inversion\n# bayesian inversion\n\nhere's a question that might have been nagging you. in the [regularization lesson](./regularization), we added a penalty term $\\epsilon^2\\|\\mathbf{m}\\|^2$ and said \"this keeps the model from going crazy.\" but *why that particular penalty*? why not something else? and how do we know if $\\epsilon$ is too big or too small?\n\nthe beautiful thing is that there's an answer \u2014 and it comes from probability. every regularization choice has a precise probabilistic meaning. the penalty is a **prior belief** about the model. the data misfit is a **likelihood**. and the regularized solution is the **most probable model** given both your beliefs and your data.\n\nonce you see this, regularization stops feeling like a hack and starts feeling like honest science.\n\n---\n\n## regularization as a gaussian prior\n\nthe tikhonov objective\n\n$$\ne_\\epsilon(\\mathbf{m}) = \\|\\mathbf{d} - \\mathbf{gm}\\|^2 + \\epsilon^2\\|\\mathbf{m}\\|^2\n$$\n\nis equivalent to maximizing the posterior probability\n\n$$\n\\sigma(\\mathbf{m}) = \\rho_m(\\mathbf{m})\\,l(\\mathbf{m}), \\qquad l(\\mathbf{m}) = \\rho_d(g(\\mathbf{m}))\n$$\n\nwhere the prior $\\rho_m$ is a zero-mean gaussian with covariance proportional to $\\epsilon^{-2}\\mathbf{i}$, and the likelihood $l$ assumes gaussian noise on the data. (important caveat: this equivalence holds specifically under gaussian noise and a quadratic prior. non-gaussian noise or non-quadratic penalties lead to different posterior shapes \u2014 but the core insight that regularization encodes prior belief remains universal.)\n\n> minimizing the tikhonov objective is mathematically identical to finding the mode of a gaussian posterior. **regularization is a prior in disguise.**\n\nwhat does that mean in plain language? the prior says: \"i expect the model parameters to be small \u2014 close to zero \u2014 and i'm about $1/\\epsilon$ uncertain about each one.\" large $\\epsilon$ means a tight prior (you're very confident the model is small). small $\\epsilon$ means a loose prior (you're letting the data do the talking).\n\n[[figure gaussian-process]]\n\nlook at this picture carefully. each colored line is a sample from the prior distribution \u2014 a possible model *before* seeing any data. the prior is telling the model: \"i expect you to be smooth, like a gentle hillside, not like the alps on a bad day.\" the spread of the lines shows your uncertainty. when you combine this prior with data, the posterior (shaded region) shrinks \u2014 the data has taught you something.\n\n[[simulation prior-likelihood-posterior]]\n\n[[simulation posterior-walker-arena]]\n\nthings to look for in the simulation:\n\n* set the prior wide and the noise tight \u2014 the posterior snaps to the data (data-dominated regime, small $\\epsilon$)\n* set the prior tight and the noise wide \u2014 the posterior barely moves from the prior (prior-dominated regime, large $\\epsilon$)\n* watch the effective $\\epsilon$ ratio: it is exactly the tikhonov regularization parameter from the [previous lesson](./regularization)\n* the posterior is always narrower than both the prior and the likelihood \u2014 combining information always reduces uncertainty\n\n---\n\n## weighted formulation: data and model covariance\n\nthe basic tikhonov formulation assumes all data points have equal uncertainty and the model is isotropically smooth. in the real world? never.\n\nsome seismometers are more precise than others. some model parameters are better constrained by geology than others. to handle this, introduce **data covariance** $\\mathbf{c}_d$ and **model covariance** $\\mathbf{c}_m$:\n\n$$\n\\mathbf{v}^t\\mathbf{v} = \\mathbf{c}_d^{-1}, \\qquad \\mathbf{w}^t\\mathbf{w} = \\mathbf{c}_m^{-1}.\n$$\n\ntransform to \"whitened\" variables:\n\n$$\n\\bar{\\mathbf{d}} = \\mathbf{vd}, \\qquad \\bar{\\mathbf{m}} = \\mathbf{wm}, \\qquad \\bar{\\mathbf{g}} = \\mathbf{vgw}^{-1}.\n$$\n\nnow solve the standard tikhonov problem in these new variables. the effect: noisy data points get downweighted automatically. model parameters with strong prior constraints are penalized more heavily.\n\nnotice what we just did \u2014 we turned our beliefs about noise and about geology into two simple matrices. that's the whole game of science: turn intuition into numbers, then let the numbers argue with the data.\n\n---\n\n## the linear-gaussian closed form\n\nwhen both the prior and the noise are gaussian and the forward model is linear, the posterior is also gaussian \u2014 and we can write it down exactly. no sampling, no optimization, just algebra.\n\nthe posterior mean (which is also the map estimate) is:\n\n$$\n\\hat{\\mathbf{m}} = \\mathbf{c}_m\\mathbf{g}^t(\\mathbf{g}\\mathbf{c}_m\\mathbf{g}^t + \\mathbf{c}_d)^{-1}(\\mathbf{d} - \\mathbf{g}\\mathbf{m}_{\\text{prior}}) + \\mathbf{m}_{\\text{prior}},\n$$\n\nand the posterior covariance is:\n\n$$\n\\tilde{\\mathbf{c}}_m = \\mathbf{c}_m - \\mathbf{c}_m\\mathbf{g}^t(\\mathbf{g}\\mathbf{c}_m\\mathbf{g}^t + \\mathbf{c}_d)^{-1}\\mathbf{g}\\mathbf{c}_m.\n$$\n\nset $\\mathbf{c}_m = \\epsilon^{-2}\\mathbf{i}$ and $\\mathbf{c}_d = \\mathbf{i}$, and this reduces to the tikhonov formula from the [regularization lesson](./regularization). the \"regularization parameter\" $\\epsilon$ was the ratio of noise standard deviation to prior standard deviation all along.\n\nnotice something remarkable about $\\tilde{\\mathbf{c}}_m$: it depends on $\\mathbf{g}$, $\\mathbf{c}_m$, and $\\mathbf{c}_d$, but **not on the data** $\\mathbf{d}$. the data shifts the posterior mean but doesn't change its shape. this means you can compute how uncertain your answer will be *before you collect a single measurement* \u2014 which is the foundation of experimental design. if two survey configurations give different $\\tilde{\\mathbf{c}}_m$, you can pick the one with smaller uncertainty and know you're making an optimal investment.\n\n---\n\n## from point estimates to posterior exploration\n\nso far, we've been finding the *peak* of the posterior \u2014 the single most probable model. that's useful, but it's not the whole story.\n\nthink about it. the earth doesn't care which of ten different fault geometries you pick \u2014 they all wiggle the surf"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "geophysical-inversion",
      "lessonTitle": "Geophysical Inversion Examples",
      "x": 0.6729611158370972,
      "y": 0.8244470953941345,
      "searchText": "geophysical inversion examples\n# geophysical inversion examples\n\nthis is where everything we've built comes together. we've developed regularization, the bayesian framework, and monte carlo sampling. now let's point these tools at real geophysical problems and see what happens.\n\ntwo case studies: a fault beneath the surface, and a glacier hiding its bed. in both cases, the answer isn't a number \u2014 it's a distribution.\n\n---\n\n## example 1: vertical fault inversion\n\n### the problem\n\nan earthquake ruptures along a fault buried beneath the surface. the rupture displaces the ground \u2014 gps stations and satellite radar measure how much the surface moved. the question: what is the geometry of the fault? how deep does it go? how much did it slip?\n\n[[figure vertical-fault-diagram]]\n\n### the forward model\n\nfor a vertical strike-slip fault in an elastic half-space, the surface displacement at a point $x$ depends on:\n\n* **fault depth** $d$: how deep the top of the fault is\n* **fault slip** $s$: how much the two sides moved past each other\n* **fault dip** $\\delta$: the angle of the fault plane\n\nthe displacement is a nonlinear function of these parameters. given the geometry, we can compute what the surface *should* look like \u2014 that's the forward model. but given the surface measurements, multiple fault configurations can produce nearly identical displacements.\n\n### the data\n\nsuppose we measured surface displacements at 20 stations along a profile perpendicular to the fault:\n\n* station 1 (0.5 km from fault): 3.1 cm displacement\n* station 5 (2.5 km): 2.4 cm\n* station 10 (5.0 km): 1.2 cm\n* station 15 (7.5 km): 0.4 cm\n* station 20 (10.0 km): 0.1 cm\n\nthe displacements decay with distance \u2014 but *how* they decay encodes the fault geometry. a shallow fault produces a sharp near-field signal. a deep fault produces a broader, gentler pattern. a steeply dipping fault looks different from a gently dipping one.\n\n### running the mcmc\n\nwe set up a metropolis sampler (see [monte carlo methods](./monte-carlo-methods) for the algorithm) to explore the posterior over fault parameters \u2014 starting from an initial guess, proposing random perturbations, and accepting or rejecting via the likelihood ratio. after 50,000 iterations, the chain has mapped out the posterior.\n\n[[simulation vertical-fault-mcmc]]\n\nthings to look for in the simulation:\n\n* the chain doesn't settle on one answer \u2014 it wanders through a cloud of plausible fault configurations\n* look for the elongated depth-vs-slip trade-off: shallow+large-slip and deep+small-slip produce similar surface data\n* the total seismic moment (depth x slip) is tightly constrained even when individual parameters aren't\n\n[[simulation tradeoff-cloud]]\n\n### what the posterior tells us\n\nwatch the simulation carefully. the chain doesn't settle on a single answer \u2014 it wanders through a cloud of plausible fault configurations. some things are well-determined: the total seismic moment (roughly, depth \u00d7 slip) is tightly constrained because it controls the total amount of surface displacement. but there's a **trade-off**: a shallow fault with large slip can produce similar surface displacements as a deeper fault with smaller slip.\n\nthis shows up as an elongated, tilted cloud in the depth-vs-slip scatter plot. the data alone cannot break this trade-off. you'd need additional information \u2014 maybe insar data from a different viewing angle, or teleseismic waveforms \u2014 to shrink the posterior further.\n\nthe earth doesn't care which of these fault models you pick \u2014 they all explain the surface measurements equally well. the whole cloud of models is the honest answer. a single \"best\" model would hide this fundamental ambiguity.\n\n---\n\n## example 2: glacier bed topography\n\n### the problem\n\na glacier flows down a valley, and we want to know the shape of the bedrock underneath. why? because bed topography controls ice flow, determines how the glacier will respond to climate warming, and governs whether meltwater can drain or gets trapped.\n\nbut the bed is buried under hundreds of meters of ice. we can't see it directly.\n\n[[figure glacier-valley-diagram]]\n\n### what we can measure\n\nwe have two types of data:\n\n* **surface velocity**: ice flows faster where it's thicker \u2014 roughly proportional to the fourth power of thickness (for $n = 3$ in glen's flow law, the shallow-ice approximation gives velocity $\\sim h^{n+1} = h^4$). measured by tracking features in satellite images.\n* **surface elevation**: precisely measured by gps or lidar. the bed elevation equals surface elevation minus ice thickness.\n\nthe forward model connects bed topography to surface observables through the equations of ice flow. it's nonlinear \u2014 the relationship between bed shape and surface velocity involves a power law and depends on the local slope.\n\n### running the mcmc\n\nwe parameterize the bed as a smooth curve (control points with interpolation) and explore the posterior with the same metropolis algorithm from [monte carlo methods](./monte-carlo-methods): propose perturbations to each control point, run the forward model, accept or reject. after 30,000 iterations, we have a family of plausible bed profiles.\n\n[[simulation glacier-thickness-mcmc]]\n\nthings to look for in the simulation:\n\n* the posterior is narrow where ice is thin (good constraint) and wide where ice is thick (poor constraint)\n* some sampled beds show an overdeepening; others don't \u2014 the data cannot resolve this feature\n* the smoothness prior prevents geologically absurd bed shapes while the data pulls toward the observations\n\n### a family of possible beds\n\nthe result isn't one bed profile \u2014 it's a family of plausible profiles. some key features emerge:\n\n**well-constrained regions:** where the glacier is thin and surface data is dense, the posterior is narrow. we know the bed pretty well there.\n\n**poorly-constrained regions:** near the glacier center, where the ice is thickest, many different bed shapes produce similar surface patterns. the posterior is wide. we're honestly u"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "information-entropy",
      "lessonTitle": "Information, Entropy, and Uncertainty",
      "x": 0.6370368599891663,
      "y": 0.7004162073135376,
      "searchText": "information, entropy, and uncertainty\n# information, entropy, and uncertainty\n\nin 1948, claude shannon asked a deceptively simple question: **how much does one message reduce my uncertainty?** he was thinking about telegraph wires and telephone cables, but his answer turned out to be universal. it applies to everything from compression algorithms to dna to \u2014 you guessed it \u2014 inverse problems.\n\nhere's the connection. you start an inverse problem with uncertainty about the model (the prior). you collect data. after inversion, you have less uncertainty (the posterior). shannon's framework lets you *measure* exactly how much the data taught you \u2014 in bits, in nats, in precise mathematical units.\n\nthat's what this lesson is about: quantifying information. and it turns out to be the deepest reason we regularize.\n\n---\n\n## shannon entropy: measuring uncertainty\n\nif $x$ is a random variable taking values with probabilities $p_i$, the **shannon entropy** is:\n\n$$\nh(x) = -\\sum_i p_i \\log p_i.\n$$\n\nwhat does this mean? think of it as the average surprise. if one outcome is nearly certain ($p_1 \\approx 1$), there's almost no surprise when it happens \u2014 entropy is low. if all outcomes are equally likely, every observation is maximally surprising \u2014 entropy is high.\n\n[[figure claude-shannon]]\n\na fair coin: $h = \\log 2 \\approx 0.693$ nats. a loaded coin (99% heads): $h \\approx 0.056$ nats. the loaded coin is boring \u2014 you almost always know what's coming. the fair coin keeps you guessing.\n\n[[simulation entropy-demo]]\n\nthings to look for in the simulation:\n\n* drag the distribution toward uniform \u2014 entropy climbs to its maximum (maximum ignorance)\n* concentrate probability on a single outcome \u2014 entropy drops toward zero (you know the answer)\n* every inverse problem starts somewhere on this curve, and data moves you toward the minimum\n\ndrag the probability slider and watch entropy change. the maximum is at uniform distribution (maximum ignorance). the minimum is at a spike (you know the answer). every inverse problem starts somewhere on this curve \u2014 and data moves you toward the minimum.\n\n---\n\n## kl divergence: the extra surprise\n\nnow suppose you have two distributions: $p$ (the truth) and $q$ (your model). the **kullback-leibler divergence** measures how much *extra* surprise you experience by using the wrong model:\n\n$$\nd_{\\mathrm{kl}}(p \\| q) = \\sum_i p(i) \\log \\frac{p(i)}{q(i)}.\n$$\n\nthink of $d_{\\mathrm{kl}}(p \\| q)$ as the \"extra surprise tax\" you pay every time you use model $q$ when reality follows $p$. using the wrong model always costs you \u2014 never the other way around.\n\nif your model $q$ matches reality $p$ perfectly, there's no extra surprise: $d_{\\mathrm{kl}} = 0$. if your model assigns low probability to events that actually happen often, you're constantly caught off guard \u2014 $d_{\\mathrm{kl}}$ is large.\n\nthree things to remember:\n\n* $d_{\\mathrm{kl}} = 0$ *only* when the distributions match exactly\n* it's always non-negative (you can't be *less* surprised by using the wrong model)\n* it's **asymmetric**: $d_{\\mathrm{kl}}(p \\| q) \\neq d_{\\mathrm{kl}}(q \\| p)$. the asymmetry has a sharp intuition: if you believe a narrow distribution $q$ but the truth $p$ is broad, you are only mildly surprised \u2014 most of $p$'s mass still overlaps $q$. but if you believe a broad $q$ when the truth is narrow, your model wastes probability on events that never happen, and every observation quietly penalizes you. being confidently wrong about likely events costs far more than being vaguely wrong about rare ones.\n\n[[simulation kl-divergence]]\n\n[[simulation entropy-kl-calculator]]\n\nthings to look for in the simulation:\n\n* make the two distributions identical and verify $d_{\\mathrm{kl}} = 0$\n* shift one distribution's mean and watch the divergence grow \u2014 the \"surprise tax\" increases\n* swap $p$ and $q$ and notice the asymmetry: $d_{\\mathrm{kl}}(p \\| q) \\neq d_{\\mathrm{kl}}(q \\| p)$\n\n---\n\n## why this matters for inverse problems\n\nthese aren't abstract mathematical toys. they connect directly to every concept in this course.\n\n### measuring what the data taught you\n\nremember the [bayesian framework](./bayesian-inversion)? you start with a prior, collect data, and get a posterior. the kl divergence between posterior and prior \u2014\n\n$$\nd_{\\mathrm{kl}}(\\text{posterior} \\| \\text{prior})\n$$\n\n\u2014 measures exactly how much the data changed your beliefs. large value: the data was highly informative, you learned a lot. small value: the data told you nothing you didn't already know. this is the quantitative answer to \"was the experiment worth running?\"\n\n### why we regularize (the deep reason)\n\nhere's something most textbooks don't tell you. regularization is about controlling how much information you extract from the data.\n\n**overfitting** ($\\epsilon \\approx 0$): you're extracting *everything* from the data \u2014 including the noise. the inferred model has low entropy relative to the prior (you've committed to very specific parameter values), but much of that \"information\" is actually noise. you've fooled yourself into thinking you know more than you do.\n\n**underfitting** ($\\epsilon$ huge): you're ignoring the data entirely. the posterior is basically the prior. entropy stays high \u2014 you haven't learned anything.\n\nthe right regularization extracts the signal and leaves the noise behind. information theory gives you a principled way to find that balance: maximize the genuine information gain while penalizing overfitting.\n\n### model comparison and selection\n\nwhen two different models both fit the data, which is better? residuals alone can't answer this \u2014 a more complex model will always fit better. kl divergence provides a principled comparison: which model's predicted distribution is closest to the observed data distribution? this is the information-theoretic foundation of model selection criteria like aic and bic.\n\n### monte carlo diagnostics\n\nwhen running [mcmc](./monte-carlo-methods), the entropy of the sampled posterior tells you whether the chain ha"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "linear-tomography",
      "lessonTitle": "Linear Tomography",
      "x": 0.6207020878791809,
      "y": 0.823287308216095,
      "searchText": "linear tomography\n# linear tomography\n\nhere's a puzzle. you can't see inside the earth. you can't drill deep enough, and even if you could, you'd only see one tiny spot. but earthquakes send seismic waves criss-crossing through the interior, and we have seismometers all over the surface recording when those waves arrive.\n\neach wave takes a path through the earth. if it passes through slow material, it arrives late. if it passes through fast material, it arrives early. so the arrival time carries information about the material it crossed.\n\nnow: given thousands of arrival times, can you reconstruct the velocity structure of the interior?\n\n*yes.* and the method is called **tomography** \u2014 literally, \"drawing by slices.\" it's the same principle behind a ct scan at the hospital, except the patient is the entire planet.\n\n---\n\n## forward model: rays through the earth\n\nfor a ray traveling along path $\\gamma$, the travel-time anomaly (how late it is compared to a reference model) is:\n\n$$\nt_\\gamma = \\int_\\gamma s(u)\\,du,\n$$\n\nwhere $s(u)$ is the slowness anomaly along the path. slow region? the integral is large. fast region? small.\n\nit's like asking a tourist who walked through 50 cities which ones slowed them down. each tourist only knows about the cities they actually visited \u2014 and each answer only lights up a few boxes on the map. but if you have *hundreds* of tourists taking different routes, you can piece together the whole map.\n\n---\n\n## from rays to matrix form\n\ndiscretize the earth into a grid of cells. each cell has an unknown slowness anomaly. each ray passes through some cells and misses others. this turns the continuous integral into a matrix equation:\n\n$$\n\\mathbf{d} = \\mathbf{gm}.\n$$\n\n* **rows** of $\\mathbf{g}$: one per ray (measurement)\n* **columns** of $\\mathbf{g}$: one per grid cell (unknown)\n* **entry** $g_{ij}$: the length of ray $i$ inside cell $j$ (zero if the ray misses that cell)\n* $\\mathbf{m}$: unknown slowness anomalies in each cell\n* $\\mathbf{d}$: measured travel-time anomalies\n\nmost entries of $\\mathbf{g}$ are zero \u2014 each ray only crosses a small fraction of the grid. this sparsity is what makes large-scale tomography computationally feasible.\n\n---\n\n## building the sensitivity matrix\n\nhere's the code that constructs $\\mathbf{g}$ for a simple 2d cross-borehole geometry with diagonal rays:\n\n```python\ndef make_g(n=13):\n    g_right = [np.eye(n, k=1 + i).flatten() for i in range(n - 2)]\n    g_left = [np.flip(np.eye(n, k=-(1 + i)), axis=0).flatten() for i in range(n - 2)]\n    z = np.zeros((1, n**2))\n    g = np.concatenate([z, g_left[::-1], z, z, g_right, z])\n    return g * (2**0.5) * 1000\n```\n\neach row is one ray. the non-zero entries mark which cells that ray passes through. the $\\sqrt{2} \\times 1000$ factor accounts for diagonal path length and unit conversion. stare at this for a moment \u2014 each row is literally the ray's footprint on the grid.\n\n---\n\n## synthetic experiment: test before you trust\n\na reliable inversion workflow always starts with **synthetic data**. you know the answer, so you can check whether your method recovers it.\n\n1. define a \"true\" anomaly model (something with clear features)\n2. compute noiseless data: $\\mathbf{d}_{\\text{true}} = \\mathbf{gm}_{\\text{true}}$\n3. add controlled noise: $\\mathbf{d}_{\\text{obs}} = \\mathbf{d}_{\\text{true}} + \\boldsymbol{\\eta}$\n4. recover $\\hat{\\mathbf{m}}$ and compare with the truth\n\nthis reveals where your setup has resolving power and where it doesn't \u2014 *before* you ever touch real data. skip this step and you're flying blind.\n\n---\n\n## inversion: regularized reconstruction\n\ntomographic systems are typically underdetermined (more unknowns than data) and noisy. so we use regularized inversion:\n\n$$\n\\hat{\\mathbf{m}} = (\\mathbf{g}^t\\mathbf{g} + \\epsilon^2\\mathbf{i})^{-1}\\mathbf{g}^t\\mathbf{d}_{\\text{obs}}.\n$$\n\nthe target is a model that:\n\n* fits the data within its uncertainty\n* stays stable under noise\n* avoids unrealistic spatial oscillations\n\nif you've been following along, this should feel familiar \u2014 it's exactly the tikhonov formula from the [regularization lesson](./regularization), applied to a concrete problem.\n\n[[simulation ray-smearing-explorer]]\n\n[[simulation linear-tomography]]\n\nthings to look for in the simulation:\n\n* compare the true anomaly pattern with the recovered image \u2014 where does the reconstruction succeed? where does it smear?\n* change the regularization parameter and watch the image sharpen (small $\\epsilon$) or blur (large $\\epsilon$)\n* notice which cells are well-resolved (crossed by many rays) and which are ghostly (sparse coverage)\n\nresolution and coverage \u2014 how to diagnose which parts of your image the data can actually resolve \u2014 are critical to interpreting any tomographic result. we return to these tools in the [geophysical examples](./geophysical-inversion), where spike tests and checkerboard tests reveal the resolving power (and blind spots) of real survey geometries.\n\n---\n\n## the bridge: linear to nonlinear\n\neverything we've done so far relies on a crucial assumption: the forward model is **linear**. travel time is a linear function of slowness. the matrix $\\mathbf{g}$ doesn't depend on the model.\n\nin the real earth, this is almost never exactly true. wave propagation depends on the velocity structure (rays bend). material properties interact nonlinearly. the forward map $g(\\mathbf{m})$ is a complicated function, not a simple matrix multiplication.\n\nwhen the forward model is nonlinear, we can no longer find the answer with a single matrix inversion \u2014 even a regularized one. we have to stop hunting for one best model and start exploring a *family* of plausible models. that's where [monte carlo methods](./monte-carlo-methods) come in.\n\n---\n\n## big ideas\n* the sensitivity matrix $\\mathbf{g}$ is not just a computational object \u2014 each row is the literal geometric footprint of one measurement on the model. building it forces you to think carefully about what your data actually sees.\n* always validate on sy"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "monte-carlo-methods",
      "lessonTitle": "Monte Carlo Methods",
      "x": 0.6279522776603699,
      "y": 0.5911082625389099,
      "searchText": "monte carlo methods\n# monte carlo methods\n\nimagine you're a hiker trying to find the deepest point in a dark valley. you can't see the terrain \u2014 you can only feel the ground under your feet. you take a step. if it goes downhill, great, keep going. if it goes uphill, you *usually* step back \u2014 but sometimes, with a small probability, you go uphill anyway. why? because the downhill direction might lead to a shallow puddle, while the uphill step might take you over a ridge to a much deeper valley beyond.\n\nthat's the essential idea behind markov chain monte carlo. but let's build up to it.\n\n---\n\n## why sampling?\n\nin the [bayesian lesson](./bayesian-inversion), we wrote down the posterior distribution $p(\\mathbf{m} \\mid \\mathbf{d})$. that's the full answer to the inverse problem \u2014 it tells you which models are plausible and which aren't.\n\nbut here's the problem: for anything beyond simple linear-gaussian cases, you can't compute the posterior analytically. it lives in a high-dimensional space, it might have multiple peaks, curved ridges, and heavy tails. you can't integrate it, you can't visualize it directly, and you certainly can't summarize it with a single number.\n\nwhat you *can* do is **sample** from it. generate thousands of models, each drawn from the posterior distribution. the collection of samples *is* the answer \u2014 means, variances, correlations, confidence intervals all come from the samples.\n\n---\n\n## monte carlo integration: the basic idea\n\nthe simplest form of monte carlo is remarkably straightforward. want to compute the integral $\\int f(\\mathbf{x})\\,p(\\mathbf{x})\\,d\\mathbf{x}$? draw $n$ samples from $p$, evaluate $f$ at each one, and average:\n\n$$\n\\int f(\\mathbf{x})\\,p(\\mathbf{x})\\,d\\mathbf{x} \\approx \\frac{1}{n}\\sum_{i=1}^n f(\\mathbf{x}_i).\n$$\n\nthe error shrinks as $o(n^{-1/2})$ \u2014 regardless of dimension. that's the magic. in one dimension, quadrature rules are faster. in ten dimensions, they choke. in a hundred dimensions, monte carlo is the only game in town.\n\n---\n\n## geometric intuition: throwing darts\n\nbefore full inversion, let's build intuition. suppose you want to estimate the volume of a sphere inscribed in a cube. throw random darts at the cube. count how many land inside the sphere. the ratio estimates the volume fraction.\n\n[[simulation sphere-in-cube-mc]]\n\nthings to look for in the simulation:\n\n* watch the estimate converge \u2014 noisy at first, then settling down\n* increase the dimension and see how the sphere's volume fraction collapses (the curse of dimensionality in action)\n* notice the $1/\\sqrt{n}$ convergence: 100x more samples for 10x more accuracy\n\n[[simulation monte-carlo-integration]]\n\n---\n\n## the problem with rejection sampling\n\nfor simple distributions, you can use **rejection sampling**: propose a candidate from some easy distribution, accept it with probability proportional to the target density:\n\n$$\np_{\\text{accept}} = \\frac{p(\\mathbf{x}_{\\text{cand}})}{m}, \\qquad m \\ge \\max_{\\mathbf{x}} p(\\mathbf{x}).\n$$\n\nwith a proposal density $q$:\n\n$$\np_{\\text{accept}} = \\frac{p(\\mathbf{x}_{\\text{cand}})}{m\\,q(\\mathbf{x}_{\\text{cand}})}.\n$$\n\nthis works fine in low dimensions. but in high dimensions, the acceptance rate plummets. think about it: in 100 dimensions, almost all the volume of the cube is in the corners, far from the target distribution. you'd throw billions of darts and accept almost none.\n\nwe need a smarter strategy.\n\n---\n\n## markov chain monte carlo (mcmc)\n\nhere's the key insight: instead of generating independent samples from a hard distribution, build a **chain** of samples where each step depends on the previous one. design the chain so that it spends more time in high-probability regions \u2014 exactly where the posterior puts its weight.\n\nthe simplest version is the **metropolis algorithm**:\n\n1. start at some model $\\mathbf{m}_{\\text{current}}$\n2. propose a new model: $\\mathbf{m}_{\\text{new}} = \\mathbf{m}_{\\text{current}} + \\text{random perturbation}$\n3. accept the new model with probability:\n\n$$\np_{\\text{accept}} = \\min\\left(1, \\frac{p(\\mathbf{m}_{\\text{new}})}{p(\\mathbf{m}_{\\text{current}})}\\right)\n$$\n\n4. if accepted, move to $\\mathbf{m}_{\\text{new}}$. if rejected, stay put. either way, record the current position.\n5. repeat. many, many times.\n\nimagine the posterior probability landscape as a hilly terrain. the metropolis walker always prefers to walk uphill (better fit). but occasionally it takes a small downhill step \u2014 with probability exactly equal to the ratio of probabilities. that tiny chance of going downhill is what lets the walker escape local valleys and explore the entire mountain range.\n\nthe beautiful thing: if the new model is *more* probable, you always accept. if it's *less* probable, you sometimes accept anyway \u2014 and the probability of accepting is exactly the ratio of the densities. this is what keeps you from getting trapped in local optima.\n\nafter enough steps, the chain \"forgets\" where it started and its samples are drawn from the true posterior. the histogram of visited models *is* the posterior distribution.\n\n---\n\n## practical diagnostics: is the chain working?\n\nrunning mcmc is easy. knowing whether it *worked* is the hard part. here's what to watch:\n\n**burn-in.** the chain starts wherever you initialized it, which might be far from the posterior. the initial samples are garbage \u2014 discard them. how many? plot the chain's trajectory and look for where it \"settles down.\"\n\n**acceptance rate.** if you accept everything (rate ~100%), your steps are too tiny and you're barely moving. if you accept almost nothing (rate ~1%), your steps are too big and you keep proposing implausible models. for gaussian targets, optimal acceptance is around 23% in high dimensions, 44% in one dimension.\n\n**mixing.** does the chain explore the full posterior, or does it get stuck in one region? multiple chains started from different points should converge to the same distribution.\n\n**autocorrelation.** consecutive samples are correlated (each step is a small "
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "regularization",
      "lessonTitle": "Regularization \u2014 The First Rescue",
      "x": 0.6373345255851746,
      "y": 0.7480583190917969,
      "searchText": "regularization \u2014 the first rescue\n# regularization \u2014 the first rescue\n\nimagine you're trying to hang a clothesline between two poles, and all you have are noisy measurements of where the line sags. if you force the clothesline to pass through every single measurement \u2014 including the ones corrupted by wind, by your shaky hands, by a bird that landed on the line mid-measurement \u2014 you get a wild, jagged monstrosity that zigzags between the poles. nobody would hang laundry on that.\n\nso instead, you add a little stiffness. you tell the clothesline: \"sure, get close to the measurements, but also, *don't be crazy*. be smooth.\" that compromise between fitting the data and staying sane? that's regularization. and it's the single most important idea in this course.\n\n---\n\n## the setup\n\nwe start with a linear forward problem plus noise:\n\n$$\n\\mathbf{d} = \\mathbf{gm} + \\boldsymbol{\\eta},\n$$\n\nwhere $\\boldsymbol{\\eta}$ is measurement noise. we want to recover $\\mathbf{m}$ from $\\mathbf{d}$.\n\nwhen $\\mathbf{g}$ is well-conditioned, you just solve a least-squares problem and go home. but when $\\mathbf{g}$ is ill-conditioned or rank-deficient \u2014 and in inverse problems, it almost always is \u2014 direct inversion amplifies noise into garbage. you saw this in the [hadamard example](./home). the mathematics is trying to fit noise as if it were signal, and the result is catastrophic.\n\n---\n\n## the l-curve: two forces fighting\n\nbefore we write down any formula, let's understand what we're doing geometrically.\n\nsuppose we try many different amounts of regularization \u2014 call it $\\epsilon$ \u2014 ranging from \"barely any\" to \"crushing.\" for each $\\epsilon$, we solve the problem and record two numbers:\n\n1. **data misfit**: how well does the model explain the data? (residual norm $\\|\\mathbf{d} - \\mathbf{gm}\\|$)\n2. **model norm**: how wild is the model? ($\\|\\mathbf{m}\\|$)\n\nplot these against each other in log-log space. you get an l-shaped curve. on one arm, the misfit is small but the model is insane (fitting noise). on the other arm, the model is simple but the misfit is huge (ignoring data). the corner of the l \u2014 where the two forces reach a compromise \u2014 that's your sweet spot.\n\nthis is the **l-curve method**, and it's pure gold. the data is screaming \"fit me!\" and the model norm is whispering \"don't get crazy.\" the corner is where they shake hands.\n\n---\n\n## the tikhonov objective\n\nnow the formula. we minimize:\n\n$$\nj(\\mathbf{m}) = \\|\\mathbf{d} - \\mathbf{gm}\\|^2 + \\epsilon^2\\|\\mathbf{m}\\|^2.\n$$\n\ntwo terms, fighting:\n\n* **first term**: make the model explain the data\n* **second term**: keep the model from going wild\n\nthe parameter $\\epsilon$ is the referee. small $\\epsilon$: the data dominates, noise gets amplified. large $\\epsilon$: the penalty dominates, you get a boring model that ignores the data. just right: you extract the signal and leave the noise behind.\n\nthe closed-form solution is:\n\n$$\n\\hat{\\mathbf{m}} = (\\mathbf{g}^t\\mathbf{g} + \\epsilon^2\\mathbf{i})^{-1}\\mathbf{g}^t\\mathbf{d}.\n$$\n\nthat $\\epsilon^2\\mathbf{i}$ term is doing all the heavy lifting. without it, $\\mathbf{g}^t\\mathbf{g}$ might be nearly singular and the solution explodes. with it, every eigenvalue gets pushed away from zero. the matrix becomes invertible, the solution becomes stable, and you can breathe again.\n\n---\n\n## choosing $\\epsilon$ in practice\n\nhere's the practical recipe:\n\n1. **sweep** $\\epsilon$ over a log-scale range (say, $10^{-4}$ to $10^{2}$)\n2. for each value, solve the regularized problem\n3. **plot** the l-curve (residual norm vs. model norm)\n4. pick the corner \u2014 the simplest model that still explains the data within its uncertainty\n\nother approaches exist: the discrepancy principle (choose $\\epsilon$ so the residual matches the expected noise level), cross-validation, and bayesian model selection. but the l-curve is intuitive, visual, and often your best first move.\n\n**rule of thumb you can remember forever:** sweep $\\epsilon$ on a log scale. plot the l-curve. pick the corner \u2014 the simplest model that still fits the data within its noise level. everything else is either drunk (overfitting noise) or dead (ignoring data).\n\n[[simulation l-curve-construction]]\n\n[[simulation tikhonov-regularization]]\n\nthings to look for in the simulation:\n\n* drag $\\epsilon$ toward zero and watch the solution go wild \u2014 that's noise amplification in action\n* find the \"sober\" region where the model captures real structure without oscillating\n* compare the residual norm at different $\\epsilon$ values \u2014 the corner of the l-curve is where the trade-off bends\n\n---\n\n## the drunk, the sober, and the dead\n\nhere's a picture that will stay with you. three regularization regimes, three personalities:\n\n**the drunk** ($\\epsilon \\approx 0$). no regularization. the solution lurches through every data point, noise included \u2014 wild, oscillatory, physically absurd. it has memorized the measurement errors and mistaken them for geology. this is **overfitting**.\n\n**the sober** ($\\epsilon$ just right). the solution captures every real feature while ignoring the noise. smooth where the earth is smooth, sharp where the earth is sharp. clear-eyed. this is the answer you want.\n\n**the dead** ($\\epsilon$ huge). too much regularization. the solution is flat, featureless, comatose \u2014 so terrified of complexity that it sees nothing at all. this is **underfitting**.\n\nyour entire job in regularization is finding the sober regime \u2014 where the party ends and science begins.\n\n---\n\n## big ideas\n* noise amplification is not bad luck \u2014 it is the mathematical signature of an ill-conditioned problem. regularization is how you fight back.\n* the l-curve is a geometric picture of a tug-of-war: data fit on one side, model sanity on the other. the corner is where the two forces reach a truce.\n* small $\\epsilon$ is drunk (fits noise), large $\\epsilon$ is dead (ignores data). your job is to find the sober one in between.\n* regularization is not a numerical patch applied after the physics is done \u2014 it is where your"
    },
    {
      "topicId": "inverse-problems",
      "topicTitle": "Inverse Problems",
      "routeSlug": "inverse-problems",
      "lessonSlug": "tikhonov",
      "lessonTitle": "Iterative Methods and Large-Scale Tricks",
      "x": 0.607130765914917,
      "y": 0.7983827590942383,
      "searchText": "iterative methods and large-scale tricks\n# iterative methods and large-scale tricks\n\nwe now know what [regularization](./regularization) does and *why* it works \u2014 it's a gaussian prior ([bayesian inversion](./bayesian-inversion)). the tikhonov formula gives a closed-form map estimate. one matrix inversion, done.\n\nbut here's the catch. that formula requires building $\\mathbf{g}^t\\mathbf{g} + \\epsilon^2\\mathbf{i}$ and inverting it. if your model has 100 parameters, no problem. if it has a million parameters \u2014 a 3d seismic tomography model, a full-waveform inversion grid, a climate reanalysis \u2014 that matrix has $10^{12}$ entries. you can't even store it, let alone invert it.\n\nso we need a different strategy: instead of solving the problem in one shot, we *walk* toward the answer, one step at a time. that's iterative optimization.\n\n---\n\n## steepest descent\n\nthe simplest idea: at each step, move in the direction that decreases the objective fastest. for the tikhonov objective\n\n$$\nj(\\mathbf{m}) = \\|\\mathbf{d} - \\mathbf{gm}\\|^2 + \\epsilon^2\\|\\mathbf{m}\\|^2,\n$$\n\nthe gradient is\n\n$$\n\\nabla j = -2\\mathbf{g}^t(\\mathbf{d} - \\mathbf{gm}) + 2\\epsilon^2\\mathbf{m}.\n$$\n\nthe update rule:\n\n$$\n\\mathbf{m}_{k+1} = \\mathbf{m}_k - \\alpha_k \\nabla j(\\mathbf{m}_k),\n$$\n\nwhere $\\alpha_k$ is the step size (learning rate).\n\nsounds simple. and it is \u2014 but the devil is in the details.\n\n[[simulation steepest-descent]]\n\nthings to look for in the simulation:\n\n* try extreme learning rates \u2014 watch the iterates overshoot and oscillate (too large) or crawl (too small)\n* find the goldilocks step size where the path curves smoothly toward the minimum\n* notice how the path zigzags in narrow valleys \u2014 that's why conjugate gradients and l-bfgs exist\n\nwatch what happens in the simulation above:\n\n* **step size too large:** the iterates overshoot, oscillate wildly, or diverge entirely. the algorithm is trying to sprint down a narrow valley and keeps bouncing off the walls.\n* **step size too small:** convergence is glacially slow. you're tiptoeing toward the answer and might not get there in your lifetime.\n* **just right:** smooth convergence to the regularized solution \u2014 the same answer the closed-form gives, but obtained without ever building the full matrix.\n\nthe beautiful thing is that each iteration only requires *matrix-vector products* $\\mathbf{g}\\mathbf{v}$ and $\\mathbf{g}^t\\mathbf{v}$ \u2014 not the full matrix $\\mathbf{g}^t\\mathbf{g}$. for large sparse systems, this is the difference between feasible and impossible.\n\n---\n\n## beyond steepest descent\n\nsteepest descent works but it can be slow, especially when the problem is ill-conditioned (eigenvalues spanning many orders of magnitude \u2014 exactly the situation in inverse problems). better options:\n\n* **conjugate gradients:** uses information from previous steps to avoid redundant search directions. converges much faster for quadratic objectives.\n* **l-bfgs:** approximates the curvature of the objective using a limited memory of past gradients. the workhorse of large-scale optimization.\n* **truncated newton methods:** solve the newton system approximately using a few cg iterations. excellent for nonlinear problems.\n\nthe common thread: none of these need the full hessian matrix. they all work with matrix-vector products, which means they scale to the problems that matter.\n\n[[simulation l-curve-construction]]\n\n[[simulation conjugate-gradient-race]]\n\nthings to look for in the simulation:\n\n* cg converges in exactly 2 steps for any 2d quadratic \u2014 regardless of condition number\n* crank the condition number up and watch sd zigzag wildly while cg cuts straight through\n* check the cost-vs-iteration chart: sd shows linear convergence (straight line on log scale), cg shows superlinear (drops to machine zero in 2 steps)\n* try different start positions \u2014 cg's 2-step convergence is universal for 2d quadratics\n\n---\n\n## why the penalty term is physics, not math\n\nit's tempting to think of the regularization term $\\epsilon^2\\|\\mathbf{m}\\|^2$ as a mathematical trick \u2014 something we added to make the numerics work. but that misses the point entirely.\n\nthe penalty term encodes **real physical beliefs** about the world:\n\n* **$\\|\\mathbf{m}\\|^2$ (tikhonov):** the model should have bounded energy. don't put structure where you don't need it.\n* **$\\|\\nabla \\mathbf{m}\\|^2$ (smoothness):** neighboring parameters should be similar. the earth doesn't change density by a factor of ten from one meter to the next.\n* **$\\|\\mathbf{m}\\|_1$ (sparsity):** most parameters should be zero. the model is simple, with a few localized features.\n\neach choice tells the inversion something different about what \"reasonable\" looks like. this mirrors how coarse-grid parameterizations work in climate and earth-system models \u2014 the grid resolution itself imposes a smoothness assumption.\n\n[[figure climate-grid]]\n\nnotice what we're really doing: encoding our physical intuition as mathematics. the penalty term is where domain knowledge enters the computation. get it right, and you extract signal. get it wrong, and you hallucinate structure or miss it entirely.\n\n---\n\n## when iterative methods shine\n\nuse the closed-form solution when you can. use iterative methods when you must \u2014 and you must when:\n\n* the model has more than ~$10^4$ parameters\n* $\\mathbf{g}$ is only available as a function (you can compute $\\mathbf{g}\\mathbf{v}$ but never write down $\\mathbf{g}$ explicitly)\n* the forward model is nonlinear (you linearize at each step and solve iteratively)\n* you want to monitor convergence and stop early as an implicit regularization strategy\n\nearly stopping is itself a form of regularization: iterative methods typically fit the large-scale features first and the noise last. stopping before full convergence can give you a better model than running to completion.\n\n---\n\n## big ideas\n* the bottleneck in large-scale inversion is never the physics \u2014 it is the cost of building and inverting a dense matrix. iterative methods dissolve this bottleneck "
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement-learning",
      "lessonSlug": "average-reward-online-rl",
      "lessonTitle": "Online RL in Average-Reward and Discounted Settings",
      "x": 0.7782280445098877,
      "y": 0.5257955193519592,
      "searchText": "online rl in average-reward and discounted settings\n# online rl in average-reward and discounted settings\n\n## what if the game never ends?\n\nevery algorithm we have built so far assumed the game eventually stops. an episode starts, rewards accumulate, the episode ends, and you get a clean total. but what about a server that runs 24/7? a robot that never shuts down? a factory production line that just keeps going? there is no \"end of episode.\" there is no final score to look back on.\n\nwhen the game never ends, the discounted return $\\sum_{t=0}^{\\infty}\\gamma^t r_t$ still makes mathematical sense \u2014 the geometric discount makes far-future rewards negligible. but as we discussed in the [mdp lesson](./mdp-and-dp), the discount factor becomes philosophically awkward for continuing tasks: there is no natural reason why tomorrow's reward should be worth less than today's. there is a more natural way to think about never-ending tasks.\n\n## discounted rl: a quick recap\n\nin the discounted setting, you optimize\n\n$$\n\\mathbb{e}\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_t\\right].\n$$\n\nthe discount factor $\\gamma < 1$ ensures this sum is finite. rewards far in the future are worth exponentially less than immediate rewards. all the value functions, bellman equations, and algorithms we have discussed work in this framework.\n\nwe discussed the philosophical implications of discounting in the [mdp lesson](./mdp-and-dp). the average-reward formulation resolves many of those concerns by removing $\\gamma$ entirely. for many continuing tasks, what you really care about is: over the long run, what is my average reward per step?\n\n## average-reward rl: the steady-state view\n\nin the average-reward formulation, you target the **long-run gain**:\n\n$$\ng^\\pi(s)=\\lim_{t\\to\\infty}\\frac{1}{t}\\mathbb{e}\\left[\\sum_{t=1}^{t}r_t \\mid s_1=s\\right].\n$$\n\nthis is the average reward per time step, in the limit. for ergodic mdps (where every state is reachable from every other state under any reasonable policy), the gain $g^\\pi$ does not depend on the starting state \u2014 it is a single number that characterizes the steady-state performance of policy $\\pi$.\n\nthink of it like the average speed of a car on a long road trip. at the start, you accelerate and brake through city traffic. eventually, you hit the highway and settle into a cruising speed. the average speed over the whole trip converges to something close to your cruising speed. the city traffic part is the transient; the highway cruising is the steady state.\n\n## gain and bias: steady state plus transient\n\nthe **gain-bias decomposition** separates value into two pieces. the **gain** $g^\\pi$ captures the steady-state average reward \u2014 how well you do per step, once the system has settled. the **bias** $h^\\pi(s)$ captures the transient advantage \u2014 how much better or worse it is to start in state $s$ compared to starting in steady state.\n\ntogether, they satisfy the bellman-like equation: $g^\\pi + h^\\pi(s) = r^\\pi(s) + \\sum_{s'} p^\\pi(s'|s) h^\\pi(s')$. the gain is like the horizontal line of steady-state performance, and the bias tells you how far above or below that line you are at the beginning, depending on where you start.\n\npicture a graph of cumulative reward over time. after a while, the curve becomes approximately a straight line with slope $g^\\pi$. the bias measures the vertical offset \u2014 some starting states get a head start (positive bias) and others start behind (negative bias), but eventually they all settle into the same slope.\n\n## algorithms for continuing tasks\n\n**relative value iteration** is the average-reward analog of value iteration. instead of discounting, you subtract the gain at each step to keep the values from growing unboundedly.\n\n**differential td** and **differential q-learning** replace the discount factor with an estimate of the average reward. the td error becomes $r - \\bar{r} + v(s') - v(s)$, where $\\bar{r}$ is a running estimate of the average reward per step.\n\n**r-learning** maintains separate estimates of the average reward and the relative action-values, updating both as experience accumulates.\n\n## regret in online rl\n\nwhen you bring the online learning perspective to rl, regret compares your cumulative reward to the optimal policy in the same environment class. you do not know the transition probabilities; you have to learn them by exploring. the regret measures how much reward you sacrifice during the learning phase.\n\nmodern methods attack this with **optimism** or **posterior sampling**. **ucrl-style** algorithms build confidence sets for the transition probabilities and plan optimistically within those sets \u2014 just like ucb1 was optimistic about arm means, ucrl is optimistic about the mdp dynamics. **psrl-style** (posterior sampling rl) algorithms maintain a bayesian posterior over mdps, sample one, solve it, and follow that policy for a while before re-sampling. both achieve near-optimal regret scaling in tabular mdps. these methods \u2014 particularly ucrl and psrl \u2014 dominate modern regret analysis for tabular online rl and form the theoretical backbone of model-based rl, with their optimism and posterior-sampling principles extending to continuous and function-approximation settings. this closes the loop between the online learning theory from the beginning of the course and the full rl setting.\n\n[[simulation average-reward-vs-discounted]]\n\n## big ideas\n* the discount factor smuggles in a philosophical assumption (see the [mdp lesson](./mdp-and-dp) for the full discussion). average reward refuses that assumption and asks instead what happens in steady state.\n* gain and bias are the mdp analog of mean and variance: the gain captures steady-state performance, the bias captures the transient advantage of good starting positions.\n* the exploration-exploitation trade-off (see the [bandits lesson](./bandits-ucb-exp3)) remains central here \u2014 ucrl extends the same optimism principle to mdp dynamics, closing the loop between bandit theory and full rl.\n* the regret fram"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement-learning",
      "lessonSlug": "bandits-ucb-exp3",
      "lessonTitle": "Stochastic and Adversarial Bandits: UCB1 and EXP3",
      "x": 0.7779847979545593,
      "y": 0.42795655131340027,
      "searchText": "stochastic and adversarial bandits: ucb1 and exp3\n# stochastic and adversarial bandits: ucb1 and exp3\n\nyou are standing in front of $k$ slot machines. each one has a lever. you get to pull one lever per round, for $t$ rounds total. the machine you pull spits out a reward (or a loss). the machines you did *not* pull stay silent. your job: collect as much reward as possible.\n\nthis is the multi-armed bandit problem, and it is the place where exploration meets exploitation head-on.\n\n## ucb1: optimism in the face of uncertainty\n\nimagine you have just moved to a new city, and you are trying to find the best restaurant. you try a few places. the thai place was great \u2014 4.5 stars in your personal rating. the italian place was decent \u2014 3.8 stars. but there is a sushi restaurant you have only been to once, and it was okay \u2014 3.5 stars on that one visit.\n\nshould you go back to the thai place? maybe. but here is the thing: you have been to the thai place ten times, so you have a pretty good estimate of how good it is. the sushi place, you visited only once. maybe that one visit was a bad night. maybe the sushi place is actually a 4.8 if you went back. you do not know because you have not given it enough chances.\n\n*try it yourself: you have pulled arm a ten times (average reward 0.5) and arm b twice (average reward 0.6). which arm should you try next, and why? think about it before reading the formula.*\n\nucb1 handles this dilemma with a beautiful principle: **be optimistic about what you do not know**. for each arm $a$, you compute an upper confidence bound \u2014 your best estimate of that arm's average reward, plus a bonus for how uncertain you are:\n\n$$\na_t \\in \\arg\\max_a \\left[\\widehat{\\mu}_a(t) + \\sqrt{\\frac{2\\log t}{n_a(t)}}\\right].\n$$\n\nhere $\\widehat{\\mu}_a(t)$ is the empirical mean reward of arm $a$ so far, and $n_a(t)$ is how many times you have pulled it. the square root term is the \"optimism bonus\" \u2014 it is big when $n_a(t)$ is small (you have not tried this arm much) and shrinks as you pull the arm more and get a better estimate.\n\nso ucb1 is like the restaurant reviewer who goes back to the good places but *also* keeps trying the underexplored ones, just in case there is a hidden gem. over time, the bonus terms shrink, and you naturally settle on the best arm.\n\nthe regret of ucb1 depends on the gaps between the arms. if arm $a$ has a suboptimality gap $\\delta_a$ (how much worse it is than the best arm), then the gap-dependent regret is $o\\!\\left(\\sum_{a:\\delta_a>0}\\frac{\\log t}{\\delta_a}\\right)$. arms that are almost as good as the best get pulled more before you can distinguish them. arms that are clearly worse get eliminated quickly. the gap-free version, which does not assume you know the gaps, gives $o(\\sqrt{kt\\log t})$.\n\n## exp3: when the world is adversarial\n\nucb1 relies on a critical assumption: the rewards come from fixed distributions. but what if the adversary can change the payoffs each round? what if the slot machines are rigged, and someone is rewriting the payout tables every night?\n\nthat is the adversarial bandit setting, and ucb1 falls apart. you need exp3 \u2014 the exponential-weight algorithm for exploration and exploitation.\n\nexp3 is hedge's cousin, adapted for the bandit world. remember hedge? it worked in the full-information setting because you could see all the losses. in the bandit setting, you only see the loss of the arm you pulled. so you have to *estimate* the losses of the arms you did not pull. here is how:\n\nyou maintain weights $w_t(i)$ on each arm, just like hedge. you build a sampling distribution that mixes exploration (trying everything) with exploitation (favoring high-weight arms). then you pull an arm, observe the loss, and here comes the clever part \u2014 you construct an importance-weighted loss estimate.\n\nyou only see the loss of the arm you pulled, but you pretend you can estimate the loss of every arm. for the arm you did pull, you take the observed loss and divide it by how likely you were to pull that arm. it is like correcting for the fact that you only measured one kid's height but want to know the class average. if you measured a kid you were very likely to measure, the correction is small. if you measured a kid you almost never pick, the correction is large \u2014 that one measurement carries a lot of weight.\n\nthis importance-weighted estimate is unbiased \u2014 on average it gets the right answer \u2014 but it has high variance. the exploration mixing helps control that variance by ensuring you pull every arm with at least some minimum probability.\n\nthe regret of exp3 scales as\n\n$$\nr_t = o(\\sqrt{kt\\log k}).\n$$\n\nthe lower bound for adversarial bandits is $\\omega(\\sqrt{kt})$, so exp3 is near-optimal, off by just a $\\sqrt{\\log k}$ factor. that is a beautifully tight result.\n\n[[simulation multi-armed-bandit]]\n[[simulation bandit-regret-comparison]]\n[[simulation bernoulli-trials]]\n\n## big ideas\n* ucb1's \"optimism in the face of uncertainty\" is not just a heuristic \u2014 it is the right bayesian instinct. treat unknown arms as potentially great, try them, and let data correct you.\n* the gap-dependent regret bound reveals a truth: pulling an arm that is only slightly suboptimal is the expensive part, because you need many samples to distinguish it from the best.\n* exp3's importance-weighted loss estimate is an act of statistical imagination: you never saw the loss of the arm you did not pull, so you construct an unbiased proxy. the noise is the cost of that imagination.\n* stochastic and adversarial bandits require completely different algorithms. importing ucb1 into an adversarial environment, or using exp3 in a stochastic one, leaves significant regret on the table.\n\n## what comes next\n\nucb1 and exp3 treat every round as identical \u2014 no side information distinguishes one round from the next. but in practice, you usually have a clue before you act: a user profile, a patient chart, the current state of a system. the next lesson introduces exp4, which combines exp3's logic with a pool"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement-learning",
      "lessonSlug": "contextual-bandits-exp4",
      "lessonTitle": "Contextual Bandits and EXP4",
      "x": 0.7947332262992859,
      "y": 0.4495924711227417,
      "searchText": "contextual bandits and exp4\n# contextual bandits and exp4\n\n## the doctor's dilemma\n\na doctor sees a patient walk through the door. the patient has a specific medical chart \u2014 age, blood pressure, lab results, symptoms. the doctor has three possible treatments. she picks one. the patient either gets better or does not. the doctor never finds out what would have happened under the other two treatments for *this specific patient*.\n\nthat is a contextual bandit problem. the medical chart is the **context**. the treatment choice is the **action**. the outcome is the **bandit feedback** \u2014 you only see the result of the action you took. but unlike plain bandits, the context gives you leverage. patients who look similar to this one might tell you something about which treatment works here.\n\nthis is arguably the most practically important setting in the entire course. personalized advertising works this way \u2014 the user's profile is the context, the ad shown is the action, and whether they click is the feedback. adaptive interfaces, recommendation engines, medical decision support, dynamic pricing \u2014 all contextual bandits.\n\n## how exp4 works\n\nexp4 stands for exponential-weight algorithm for exploration and exploitation using expert advice. it is exp3's big sibling, designed for the contextual setting.\n\nyou have a pool of experts (or policies), and each expert has an opinion about what to do in any given context. when context $x_t$ arrives, each expert proposes a distribution over actions \u2014 \"for this patient, i would prescribe treatment a with 70% probability and treatment b with 30%.\" the learner then mixes these expert opinions, weighted by how well each expert has done so far.\n\nhere is the step-by-step: the context $x_t$ arrives. each expert $e$ proposes an action distribution for this context. you form a mixture distribution over experts, weighted by their accumulated performance. you sample an action from the induced distribution over actions. you observe only the loss of the action you chose. you build an importance-weighted loss estimate (just like exp3), and you update the expert weights exponentially.\n\nthe importance weighting is the same trick as before \u2014 you correct for the fact that you only observed one action's loss by dividing by the probability of having chosen that action. this lets you update *all* expert weights, even though you only saw feedback for one action.\n\n## why the regret bound is magical\n\nexp4 achieves regret that scales roughly with $\\sqrt{t \\log |\\mathcal{e}|}$, where $\\mathcal{e}$ is the expert class (up to constants and the bandit feedback penalty).\n\nhere is why that is remarkable. if you have a million experts \u2014 a million possible policies, each one mapping contexts to actions differently \u2014 the $\\log$ term is only about 14. so you are basically paying the bandit price times $\\sqrt{14}$. whether you compete against ten policies or a million, the penalty is gentle. that is the power of exponential weights: the logarithmic dependence on the number of experts means you can afford to have a huge, rich policy class without blowing up the regret.\n\ncompare this to what would happen if you tried each expert one at a time. with a million experts, you would need a million rounds just to try each one once. exp4 finds the best expert in far fewer rounds because it updates all of them in parallel using the importance-weighted feedback trick.\n\n[[simulation contextual-bandit-exp4]]\n\n## big ideas\n* context is the difference between one-size-fits-all and actually useful. the same action can be optimal for one patient and harmful for another; the context is what tells the algorithm which is which.\n* logarithmic dependence on the policy class size is the exponential weights miracle again. it means you can define an astronomically large space of possible policies and still compete with the best one at almost no extra cost.\n* importance weighting is the universal bridge between what you observed and what you needed to know. it appears in monte carlo estimation, causal inference, and off-policy evaluation \u2014 all for the same reason.\n* exp4 reveals that \"learning with experts\" and \"learning with contexts\" are the same problem viewed from different angles: experts are just policies, and policies are just functions from contexts to actions.\n\n## from stateless to stateful decisions\n\nso far, every decision we have made is stateless \u2014 the context arrives, we act, we see a reward, and the slate is wiped clean. but many real problems have *state*: the action you take now changes the world you will face next. a robot stepping left is now in a different place; a doctor prescribing drug a has changed the patient's chemistry for tomorrow's decision. when the context depends on your own previous actions, you have a markov decision process, and the tools we have built \u2014 regret, optimism, importance sampling \u2014 will need to be extended to handle this richer structure.\n\n## what comes next\n\nthat leap is the subject of the next lesson. in a markov decision process, actions reshape the state of the world, and the state determines future rewards. a bad move today can trap you in bad states for many rounds; a good move can unlock better states later. the next lesson introduces mdps and dynamic programming: the framework for planning when your decisions have consequences that ripple through time.\n\n## check your understanding\n1. in exp4, each expert proposes a distribution over actions (not just a single action). why is this generality necessary for the contextual setting, and what would go wrong if experts were forced to be deterministic?\n2. why does exp4's regret scale with $\\sqrt{t \\log |\\mathcal{e}|}$ rather than $\\sqrt{t |\\mathcal{e}|}$? trace back to where the logarithm enters the analysis.\n3. compare the contextual bandit problem to supervised classification. what does the contextual bandit setting have that supervised learning does not, and what does it lack?\n\n## challenge\nexp4 requires the ability to compu"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement-learning",
      "lessonSlug": "deep-rl-dqn",
      "lessonTitle": "Function Approximation and Deep Q-Learning",
      "x": 0.8590693473815918,
      "y": 0.5581356287002563,
      "searchText": "function approximation and deep q-learning\n# function approximation and deep q-learning\n\n## when the lookup table does not fit\n\nimagine you are trying to build a q-table for a spaceship with a million rooms. each room has ten possible actions. that is ten million entries in your table. now imagine the spaceship is actually an atari game with pixels on the screen \u2014 the state space is every possible arrangement of pixels, which is astronomically large. your lookup table would need more entries than there are atoms in the universe. it simply does not fit.\n\nso we do something radical: instead of storing one value for every state-action pair, we build a **function approximator** \u2014 a machine that takes in a state (or a state-action pair) and outputs a value estimate. we *guess* the values using a parameterized model. for simple problems, a linear model works. for complex, high-dimensional problems like game screens or robot sensors, we use a deep neural network. that network takes the raw state as input and outputs q-values for every action.\n\n## the neural network as a value guesser\n\nthe idea is elegant: train a neural network $q(s,a;\\theta)$ to approximate $q^*(s,a)$. you collect experience by interacting with the environment, and you minimize the td error \u2014 the gap between the network's prediction and the bootstrap target $r + \\gamma \\max_{a'} q(s',a';\\theta)$.\n\nbut then something goes wrong. the network starts dreaming and hallucinating. values diverge. training becomes unstable. why? three problems conspire against you.\n\nfirst, the training data is **correlated**. consecutive transitions in an episode are nearly identical \u2014 the game barely changes between one frame and the next. imagine the network trying to learn from the last game it played, but it keeps replaying the exact same five seconds over and over. that is like studying only page 47 of the textbook. it overfits to recent experience and forgets everything else.\n\nsecond, the target is alive. in ordinary supervised learning the label \"cat\" never changes. in q-learning the bootstrap target is $r + \\gamma \\max_{a'} q(s',a';\\theta)$. every time you update $\\theta$, the target moves. it is like teaching a student while simultaneously rewriting the answer key \u2014 the network keeps chasing its own tail.\n\nthird, we are combining function approximation, bootstrapping, and off-policy learning \u2014 the **deadly triad** from [td learning](./td-sarsa-qlearning). without careful engineering, the system explodes.\n\n## dqn: engineering away the instability\n\ndqn (deep q-network) was the breakthrough that showed you *can* make deep rl work. it solves the three problems above with two key tricks.\n\n**the replay buffer** is the solution to correlated data. instead of training on transitions in order, you store every transition $(s, a, r, s')$ in a big memory buffer. when it is time to train, you sample a random mini-batch from the buffer. this mixes up the pages \u2014 you might get a transition from episode 1, another from episode 50, another from episode 200, all in the same batch. the samples are decorrelated, and the network sees a diverse diet of experience.\n\nthe replay buffer also solves **catastrophic forgetting** \u2014 the tendency of neural networks to forget old knowledge when trained on new data. without the buffer, the network would learn to play the current level of the game and completely forget how to play the earlier levels. by replaying old transitions, the network retains its accumulated knowledge.\n\n**the target network** is the solution to moving targets. instead of using the current network to compute the bootstrap target, you maintain a *separate* copy of the network \u2014 the target network \u2014 and only update it periodically (say, every 10,000 steps). between updates, the target is fixed, so the optimization is more like supervised learning. you are still bootstrapping, but the target is stable enough for the gradient descent to make progress without chasing itself in circles.\n\nadditional stabilization tricks include **gradient clipping** (preventing the gradients from becoming too large), **robust loss functions** (like huber loss instead of squared error), and **double-q learning** (using two networks to reduce overestimation of q-values, where one network selects the best action and the other evaluates it).\n\n## beyond dqn\n\ndqn opened the floodgates, but it is just the beginning. **policy gradient methods** directly optimize the policy without learning q-values at all \u2014 they adjust the probability of taking each action based on whether the resulting episode was good or bad. **actor-critic methods** combine the two ideas: a \"critic\" estimates values (like dqn), while an \"actor\" adjusts the policy using the critic's guidance. this gives you lower variance than pure policy gradients and more flexibility than pure value-based methods.\n\nfor **continuous control** \u2014 robotics, motor control, anything where the action is a real number rather than a discrete choice \u2014 variants like ddpg and sac extend these ideas to continuous action spaces.\n\n[[simulation dqn-stability]]\n\n[[simulation replay-buffer-explorer]]\n[[simulation activation-functions]]\n[[simulation cartpole-learning-curves]]\n\n## big ideas\n* the replay buffer is not just an optimization trick \u2014 it is the key to breaking temporal correlations that would otherwise prevent the neural network from generalizing.\n* the target network is a deliberate act of self-deception: you pretend the target is fixed even though it is not, because that pretense is stable enough for gradient descent to make progress.\n* dqn's success on atari was not about the algorithm being perfect \u2014 it was about identifying exactly which instabilities were fatal and engineering specific fixes for each one.\n* the gap between value-based and policy-based methods is not just technical. policy gradients optimize what you care about directly; value methods optimize a proxy. each approach trades off computation, variance, and credit assignme"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement-learning",
      "lessonSlug": "feedback-and-settings",
      "lessonTitle": "Forms of Feedback and Problem Settings",
      "x": 0.7416236996650696,
      "y": 0.45440706610679626,
      "searchText": "forms of feedback and problem settings\n# forms of feedback and problem settings\n\nthe kind of feedback you receive after each decision changes *everything* \u2014 the algorithms you can use, the regret you can achieve, and the difficulty of the problem. this lesson is about understanding those different feedback models, because everything else in the course is just different flavors of the same underlying pain.\n\n## full-information feedback\n\nin the full-information setting, you make a decision and then the world reveals *all* the outcomes \u2014 not just for the action you chose, but for every action you *could* have chosen.\n\nat round $t$, you observe the whole loss vector:\n\n$$\n(\\ell_t(1), \\ell_t(2), \\ldots, \\ell_t(k)).\n$$\n\nthink of it this way: you are betting on a horse race. after the race, you find out not only how your horse did, but the finishing times of *every* horse. you know exactly how much you would have won or lost on each one. that is a huge amount of information.\n\nthis is the setting we call **prediction with expert advice**. you have $n$ experts, each one gives you advice, you make a decision, and then you see how *all* of them would have done. it is the friendliest version of the online learning problem, and it is where algorithms like hedge shine. the regret you can achieve here scales as $o(\\sqrt{t \\log n})$ \u2014 the square root is the signature of a no-regret algorithm.\n\n## bandit (partial) feedback\n\nnow the world gets cruel. you pull the slot-machine lever and the machine only tells you how much you lost on *that* pull. the other levers stay silent. you have no idea what would have happened if you had pulled lever 3 instead of lever 7. that silence is the enemy.\n\nformally, you only observe:\n\n$$\n\\ell_t(a_t).\n$$\n\njust one number. out of potentially hundreds or thousands of actions, you get feedback on exactly one. if you want to know anything about the other actions, you have to *try them*, which means sacrificing rounds where you could have been exploiting what you already know. this is the exploration-exploitation dilemma, and it is the heartbeat of the bandit problem.\n\nthe cost of partial feedback shows up directly in the regret bounds. that $o(\\sqrt{t \\log n})$ from the full-information setting blows up to $o(\\sqrt{kt})$ for bandits, where $k$ is the number of arms. the extra $\\sqrt{k}$ factor is the price you pay for flying blind on all the actions you did not try. you have to explicitly spend rounds exploring, and every exploration round is a round where you might be losing more than you had to.\n\n## contextual feedback\n\nsometimes, before you make your decision, the world hands you a clue. a context vector $x_t$ arrives \u2014 think of it as a patient's medical chart, or a user's browsing history, or the current weather conditions \u2014 and you use that clue to pick your action. then you get bandit feedback: you only see the outcome of the action you chose, given that context.\n\nyou choose $a_t$ using $(x_t, \\text{history})$ and receive only $\\ell_t(a_t)$.\n\nthis is the **contextual bandit** setting, and it is arguably the most practical of the three. a doctor sees a patient (context), prescribes a treatment (action), and observes the outcome (feedback). she never finds out what the other treatments would have done for *this* patient. but the context gives her leverage \u2014 patients who look similar to this one might tell her something useful.\n\n## the spectrum of difficulty\n\nthese three settings \u2014 full information, bandit, contextual \u2014 form a spectrum from easy to hard. in the full-information setting, learning is relatively cheap because you see all the outcomes. in the bandit setting, learning is expensive because you see only one outcome and have to explore. in the contextual setting, you have the bandit's pain but the context's help, and the balance between them determines your regret.\n\nthere are additional complications that make the problem harder still. **delayed feedback** means you do not find out the outcome for several rounds. **noisy feedback** means the loss signal is corrupted. **non-stationary environments** mean the world changes beneath your feet \u2014 the best action today might not be the best action tomorrow. **adversarially chosen outcomes** mean someone is actively trying to make you lose.\n\neach of these variants changes the concentration arguments you can use, the variance of your estimators, and the final regret bounds. but they all build on the same foundation: the type of feedback determines the type of algorithm.\n\n## big ideas\n* feedback is the fundamental resource in online learning. more feedback means faster learning and tighter regret bounds \u2014 but the world rarely hands you a free lunch.\n* the bandit problem is not just a harder version of full-information learning; it requires a qualitatively different approach because you cannot distinguish between \"this arm is bad\" and \"i have not tried it enough.\"\n* context is leverage. knowing something about the current situation before you act is what turns raw bandits into a tool for personalization.\n* delayed, noisy, and non-stationary feedback are not exotic edge cases \u2014 they are the normal condition of most real systems. the clean textbook feedback model is the exception.\n\n## what comes next\n\nnow that we know what feedback looks like, it is time to build something that uses it. the full-information setting \u2014 where you see every outcome after each round \u2014 is the friendliest place to start. it is also where the deepest algorithmic ideas first appear.\n\nthe next lesson introduces follow the leader, which sounds right but fails badly, and then hedge, which fixes ftl's instability using exponential weights. hedge is the blueprint for virtually every no-regret algorithm that follows, so understanding why it works \u2014 and why the potential function argument is so elegant \u2014 pays dividends throughout the rest of the topic.\n\n## check your understanding\n1. why does full-information feedback lead to better regret bounds than bandit fee"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement-learning",
      "lessonSlug": "ftl-and-hedge",
      "lessonTitle": "Follow the Leader and Hedge",
      "x": 0.7109437584877014,
      "y": 0.46821361780166626,
      "searchText": "follow the leader and hedge\n# follow the leader and hedge\n\n## follow the leader: the obvious idea that fails\n\nthe simplest strategy you could imagine is: at each round, look at everything that has happened so far and pick the action that would have been best. formally, ftl chooses\n\n$$\na_t \\in \\arg\\min_{a \\in \\mathcal{a}} \\sum_{s=1}^{t-1}\\ell_s(a).\n$$\n\nin words: you pick the action with the lowest total loss so far. it sounds perfectly reasonable. if expert 2 has been right most often, you follow expert 2.\n\nftl is simple and parameter-free, and when the losses are well-behaved (think strongly convex or stable), it actually works. but here is the problem: against an adversary, ftl can be a disaster. the adversary just keeps flipping the best action back and forth. round 1, expert a is best. round 2, expert b is best. round 3, expert a again. ftl chases back and forth like a dog chasing two squirrels, and its regret grows *linearly* \u2014 which is the worst possible outcome.\n\n[[simulation ftl-instability]]\n\nthe lesson from ftl is profound: you need *stability*. you cannot just pick the current leader \u2014 you need to hedge your bets.\n\n## hedge: the algorithm that hedges\n\nnow we build the algorithm that fixes ftl's problem. imagine you are in a classroom with $n$ students (experts), and each student raises their hand to vote on the answer. at first, you trust everyone equally. but after each round, when you see who was right and who was wrong, you adjust your trust.\n\nhere is the key insight: instead of following one expert, you follow *all of them*, weighted by how much you trust each one. and when an expert is wrong, you do not drop them entirely \u2014 you just reduce their weight. gently. multiplicatively.\n\nsetting: $n$ experts, full-information losses $\\ell_t(i)\\in[0,1]$.\n\nyou start with equal weights: $w_1(i)=1$ for every expert. at each round, you choose a probability distribution over experts based on their weights:\n\n$$\np_t(i)=\\frac{w_t(i)}{\\sum_j w_t(j)}.\n$$\n\nthen you observe all the losses and update:\n\n$$\nw_{t+1}(i)=w_t(i)\\exp(-\\eta \\ell_t(i)).\n$$\n\nthink of each expert as carrying a backpack of confidence. every time an expert is wrong, you punch a hole in their backpack so they get a little lighter. but you do it gently, controlled by the parameter $\\eta$. a large loss means a bigger hole, and over time the bad experts' backpacks deflate while the good experts stay heavy.\n\n## the learning rate: adjusting the gas pedal\n\n*try it yourself: what happens to hedge weights if $\\eta$ is too large? pause and guess before the math.*\n\nwe need a learning rate $\\eta$ that works like adjusting the gas pedal on a long road trip. if you keep the pedal all the way down (large $\\eta$), you react too aggressively to every mistake \u2014 you overshoot the exit. if you barely tap the pedal (tiny $\\eta$), you never get there in time. the sweet spot turns out to be roughly\n\n$$\n\\eta \\asymp \\sqrt{\\frac{\\log n}{t}}.\n$$\n\nthat is the square root of the log of how many experts you have, divided by the total time horizon. with this choice, regret scales as\n\n$$\nr_t = o(\\sqrt{t\\log n}).\n$$\n\nnotice that the dependence on $n$ is only logarithmic. if you have a million experts, $\\log n$ is about 14. so whether you have 10 experts or a million, hedge handles it gracefully. the regret grows with $\\sqrt{t}$, which is sublinear \u2014 your average regret per round goes to zero.\n\n## the potential function: total confidence in the room\n\nthe proof that hedge works revolves around a beautiful idea called the **potential function**. define\n\n$$\n\\phi_t=\\sum_i w_t(i).\n$$\n\nthis is the total weight in the room \u2014 the sum of all the confidence backpacks. at the start, $\\phi_1 = n$ because everyone starts with weight 1.\n\nnow think about what happens each round. when you apply the exponential update, the total weight changes. if most experts did well (small losses), the total weight barely drops. if everyone did badly, it drops a lot. the key is that $\\phi_t$ can never go below the weight of the best expert \u2014 because the best expert's weight is always part of the sum.\n\nby tracking how fast $\\phi_t$ shrinks from above (using the average loss and a second-order correction from the curvature of the exponential) and how slowly it shrinks from below (because the best expert's weight barely changes), you can sandwich the regret between two expressions that both come out to $o(\\sqrt{t \\log n})$.\n\nthe potential function is like measuring the temperature of the room. if the room is getting cold fast, it means everyone is losing. if it is cooling slowly, the good experts are holding up. the gap between the two rates of cooling is the regret.\n\n[[simulation hedge-weights-regret]]\n\n## big ideas\n* ftl fails not because greediness is wrong, but because pure greediness has no inertia \u2014 a tiny adversarial nudge can whipsaw the entire decision.\n* exponential weights are the right answer to instability: multiplicative updates smooth out the influence of any single bad round, and the logarithm in the regret bound reflects exactly how many experts you are hedging across.\n* the potential function proof is not a trick \u2014 it is a way of seeing that the total weight in the room is squeezed between two bounds that both yield $o(\\sqrt{t \\log n})$.\n* the learning rate $\\eta$ mediates a bias-variance trade-off across time: too large and you overreact to noise; too small and you adapt too slowly to identify the best expert.\n\n## what comes next\n\nhedge gives us a complete solution for the full-information setting, but real decisions rarely come with a full report card after every round. the next step is the bandit world, where you only see the outcome of the single action you chose \u2014 the other arms stay silent.\n\nthis silence forces a genuine dilemma: exploiting what you already know versus exploring to reduce uncertainty. the next lesson develops two algorithms that navigate this dilemma from opposite starting assumptions. ucb1 bets that the world is stochastic and uses optimism to balan"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement-learning",
      "lessonSlug": "mdp-and-dp",
      "lessonTitle": "MDPs and Dynamic Programming",
      "x": 0.7822726368904114,
      "y": 0.5512186288833618,
      "searchText": "mdps and dynamic programming\n# mdps and dynamic programming\n\n## the world remembers\n\nup to now, the world had no memory. you pulled a slot-machine lever, you got a payout, and the world reset. your action did not change what happened next. now the world *remembers* what you did yesterday. that changes everything.\n\nif you are a robot navigating a warehouse, and you turn left at the first aisle, you end up in a different part of the building than if you turned right. the state of the world \u2014 your position \u2014 depends on your previous action. and the reward you get (finding the package vs hitting a wall) depends on that state. this is a **markov decision process**: a world where your actions have consequences that ripple forward in time.\n\n## the mdp tuple\n\nan mdp is described by five things:\n\n$$\n\\langle \\mathcal{s}, \\mathcal{a}, p, r, \\gamma \\rangle\n$$\n\n$\\mathcal{s}$ is the set of states \u2014 all the places the world can be. $\\mathcal{a}$ is the set of actions \u2014 all the moves you can make. $p$ is the transition kernel \u2014 for every state and action, it tells you the probability of ending up in each next state. $r$ is the reward function \u2014 what you earn for taking an action in a state. and $\\gamma$ is the discount factor, a number between 0 and 1 that says how much you care about future rewards compared to immediate ones.\n\na **policy** $\\pi$ tells you what to do in each state. it can be deterministic (\"always go left in state 5\") or stochastic (\"go left with probability 0.7, right with probability 0.3\"). the goal is to find the policy that maximizes cumulative discounted reward.\n\n## value functions: scoring a policy\n\nhow good is a policy $\\pi$? we measure it with value functions. the state-value function $v^\\pi(s)$ tells you the expected total discounted reward if you start in state $s$ and follow $\\pi$ forever:\n\n$$\nv^\\pi(s)=\\mathbb{e}^\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_t \\mid s_0=s\\right].\n$$\n\nthe action-value function $q^\\pi(s,a)$ tells you the same thing, but conditioned on taking action $a$ first and then following $\\pi$:\n\n$$\nq^\\pi(s,a)=\\mathbb{e}^\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_t \\mid s_0=s,a_0=a\\right].\n$$\n\nthink of $v^\\pi(s)$ as the \"worth\" of being in state $s$ under policy $\\pi$. a state near the goal with a good policy has high value. a state far from the goal or served by a bad policy has low value.\n\n*try it yourself: in a 3-state chain (start \u2192 middle \u2192 goal), what does the value function look like when $\\gamma = 0$ versus $\\gamma = 0.99$? pause and sketch both before reading on.*\n\n## the bellman equation: the fixed point where ice meets water\n\nthe bellman equation is the most important equation in reinforcement learning. it says: the value of a state is the immediate reward plus the discounted value of wherever you end up next.\n\n$$\nv^*(s)=\\max_a\\left[r(s,a)+\\gamma\\sum_{s'}p(s'|s,a)v^*(s')\\right].\n$$\n\nread it aloud: the optimal value of state $s$ is the best you can do by choosing the action that maximizes your immediate reward plus $\\gamma$ times the expected value of the next state.\n\nthis equation has a **fixed point** \u2014 a unique solution $v^*$ that does not change when you apply the bellman operator. think of it like the temperature where ice and water are happy together: 0\u00b0c. if you heat the ice, it melts and comes back to 0\u00b0. if you cool the water, it freezes and comes back to 0\u00b0. the fixed point is the equilibrium. the bellman equation works the same way \u2014 if you start with any guess for $v$ and keep applying the operator, you converge to $v^*$. the operator is a **contraction** in the discounted case: every application brings you closer to the fixed point. that guarantees convergence and uniqueness.\n\nfor policy evaluation (not optimizing, just scoring a fixed policy), the bellman equation becomes linear: $v^\\pi = r^\\pi + \\gamma p^\\pi v^\\pi$. this you can solve by matrix inversion in small problems, or by iterating the backup operator until convergence.\n\n## dynamic programming: planning when you know the model\n\nwhen you know the transition probabilities and rewards \u2014 when you have a perfect map of the world \u2014 you can solve the mdp with dynamic programming. these are **planning** methods, not learning methods. you are doing math on the model, not interacting with the world.\n\n**policy evaluation** takes a fixed policy $\\pi$ and computes $v^\\pi$ by iteratively applying the bellman backup. you start with an arbitrary guess and keep updating each state's value based on its successors' values. each sweep brings you closer to the true value.\n\n**policy iteration** alternates between two steps: evaluate the current policy (compute $v^\\pi$), then improve it by acting greedily with respect to $v^\\pi$ (pick the action that looks best given the current values). this creates a new, better policy, and you repeat. policy iteration converges in a finite number of steps because there are only finitely many deterministic policies.\n\n**value iteration** is more direct \u2014 you repeatedly apply the bellman *optimality* operator, which combines evaluation and improvement into one step. it converges to $v^*$ as the number of sweeps grows.\n\nthese methods are elegant and exact, but they require knowing $p$ and $r$. in the real world, you usually do not have that luxury. the next lessons are about what happens when you have to *learn* the values by interacting with the environment.\n\n[[simulation gridworld-mdp]]\n[[simulation dp-convergence]]\n[[simulation mdp-simulation]]\n\n### a note on the discount factor\n\nthe discount factor $\\gamma$ appears everywhere in this lesson \u2014 in the value functions, the bellman equations, the contraction argument. it is easy to treat it as just another number and move on. but it deserves a closer look, because it carries a philosophical assumption that is not always justified.\n\nwhy $\\gamma < 1$? the mathematical reason is clear: without discounting, the infinite sum $\\sum_{t=0}^{\\infty} r_t$ can diverge. the discount factor forces convergence by making far-future rewards e"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement-learning",
      "lessonSlug": "monte-carlo-rl",
      "lessonTitle": "Monte Carlo Methods for RL",
      "x": 0.8216111660003662,
      "y": 0.5183308124542236,
      "searchText": "monte carlo methods for rl\n# monte carlo methods for rl\n\n## just play the game and write down the score\n\nmonte carlo is just \"play the game until it ends and write down the score.\" sounds stupid until you realize that is exactly how you learned to ride a bike \u2014 you did not solve differential equations of balance and angular momentum. you got on, you fell off, you got on again, and eventually your brain figured out the pattern from the accumulated experience of many complete attempts.\n\nmonte carlo methods estimate values from **complete episodes**. no transition model is required. you do not need to know the probabilities of anything. you just need to be able to play the game, observe the rewards, and remember what happened.\n\n## the core idea\n\nafter an episode finishes, you look back at every state you visited and compute the **return** \u2014 the total discounted reward from that point forward:\n\n$$\ng_t = \\sum_{k=0}^{t-t-1} \\gamma^k r_{t+k+1}.\n$$\n\nthen you estimate the value of each state by averaging the returns you observed whenever you visited that state:\n\n$$\nv^\\pi(s)\\approx \\frac{1}{n(s)}\\sum_{t:\\,s_t=s} g_t.\n$$\n\nthat is it. play many episodes, average the returns for each state, and the law of large numbers does the rest. as the number of episodes grows, your estimates converge to the true values.\n\n## variants: when to count the visit\n\n*try it yourself: if a state appears three times in one episode, which variant \u2014 first-visit or every-visit \u2014 uses more data, and which gives cleaner statistical guarantees? guess before reading.*\n\n**first-visit mc** only uses the return from the *first* time you visit a state in an episode. if you pass through state $s$ three times in one episode, you only record the return from the first visit. this gives you independent samples (across episodes) and clean convergence guarantees.\n\n**every-visit mc** uses the return from *every* visit to state $s$ within an episode. you get more data per episode, but the samples within an episode are correlated. both variants converge to the right answer, but their finite-sample properties differ.\n\n## on-policy vs off-policy\n\n**on-policy mc control** learns about the policy it is actually following. you play episodes using an epsilon-greedy policy (mostly exploit, sometimes explore), compute returns, update action-values, and improve the policy. the exploration keeps you from getting stuck, and the updates keep improving the policy. it is simple and stable.\n\n**off-policy mc** is trickier but more flexible. you collect episodes using one policy (the behavior policy) and use importance sampling to correct the returns so they reflect a *different* policy (the target policy). this is the same importance-sampling trick we saw in [exp3's loss estimates](./bandits-ucb-exp3) \u2014 correcting for the mismatch between what you observed and what you needed. it is useful when you want to learn about the optimal policy while exploring with a safe, exploratory policy. the importance-sampling correction can have high variance, though, so you need many episodes for the estimates to settle down.\n\n## trade-offs: why mc is both wonderful and frustrating\n\nmonte carlo methods have **low bias**. the return targets are computed from actual rewards, not from estimated values. there is no bootstrapping, no approximation in the target. what you see is what you get.\n\nbut they have **high variance**, especially when episodes are long. a single episode might wander through many states and collect wildly different rewards depending on the random actions taken. averaging over many episodes tames this variance, but it can take a lot of episodes.\n\nmonte carlo is also **naturally episodic**. you have to wait until the episode ends to compute returns. this is fine for games that terminate (chess, go, a round of poker), but it is a problem for continuing tasks that never end (a robot that runs 24/7). for those, you need temporal-difference methods, which is exactly where we are headed next.\n\n[[simulation monte-carlo-convergence]]\n\n[[simulation blackjack-trajectory]]\n\n## big ideas\n* monte carlo is the law of large numbers applied to sequential decisions. no equations, no model \u2014 just averaging enough experience until the truth emerges from the noise.\n* low bias and high variance is the fundamental trade-off. unbiased targets are expensive: you have to wait for the whole episode to finish and then survive the noise of every random decision along the way.\n* the on-policy/off-policy distinction is about whether you are learning what you are doing or what you wish you were doing. importance sampling bridges the gap but amplifies variance as the two policies diverge.\n* monte carlo is episodic by nature. if your task never ends, monte carlo does not directly apply \u2014 which is exactly the pressure that motivates temporal-difference learning.\n\n## what comes next\n\nmonte carlo waits until the episode ends \u2014 clean and unbiased, but maddeningly slow. temporal-difference learning asks: why wait? after a single transition, you can bootstrap \u2014 update right away using the next state's estimated value. the next lesson develops td(0), sarsa, and q-learning: the backbone of modern model-free rl.\n\n## check your understanding\n1. why does first-visit mc produce unbiased estimates of $v^\\pi(s)$, while every-visit mc produces biased estimates within an episode? does every-visit mc still converge to the correct value asymptotically?\n2. monte carlo has low bias but high variance. identify two specific sources of variance in monte carlo return estimates and explain how each one grows with episode length.\n3. off-policy mc uses importance sampling to correct for the mismatch between the behavior and target policies. what happens to the variance of the importance-weighted return when the behavior policy is very different from the target policy?\n\n## challenge\nimplement first-visit monte carlo control on a simple episodic environment (e.g., blackjack from sutton and barto). compare the learning "
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement-learning",
      "lessonSlug": "regret",
      "lessonTitle": "The Notion of Regret",
      "x": 0.7179327607154846,
      "y": 0.4300280809402466,
      "searchText": "the notion of regret\n# the notion of regret\n\n## the weather forecaster story\n\nimagine you are a weather forecaster, and every morning you have to announce \"rain\" or \"sunny.\" you have access to five colleagues \u2014 five experts \u2014 who each give you their prediction. you do not know who is good. you do not know the weather patterns. all you can do is listen to them, pick one prediction, and then wait to see what actually happens.\n\nafter a thousand mornings, you look back. one of those five experts \u2014 let us call her expert 3 \u2014 turned out to be right 820 times. you, following your strategy of mixing and matching their advice, were right 790 times. the gap \u2014 those 30 extra mistakes you made compared to expert 3 \u2014 that is your **regret**.\n\nnow here is the beautiful part. you did not know in advance that expert 3 would be the best. you had to figure it out as you went. and yet your regret was only 30 out of 1000 rounds. that is a tiny fraction. your average per-round regret was essentially zero. that is what we are after in this course: strategies that guarantee your regret stays small, no matter what the world throws at you.\n\n## the formal definition\n\nfor losses, we write external regret over horizon $t$ as\n\n$$\nr_t = \\sum_{t=1}^{t} \\ell_t(a_t) - \\min_{a \\in \\mathcal{a}} \\sum_{t=1}^{t} \\ell_t(a).\n$$\n\nthe first sum is your cumulative loss \u2014 every bad call you made. the second term is the cumulative loss of the single best action in hindsight. the difference is regret: how much worse you did than the best fixed strategy, looking back with perfect knowledge.\n\nif you plot two curves \u2014 your loss growing over time and the best expert's loss growing over time \u2014 the shaded area between them is your regret. a good algorithm keeps that shaded area from growing too fast.\n\n[[simulation regret-growth-comparison]]\n\n## sublinear regret: why it means you are winning\n\n**sublinear regret** means $r_t = o(t)$. in plain language: your regret grows, but it grows slower than the number of rounds. if you play for a thousand rounds and your regret is only 30, that is 3% per round. if you play for a million rounds and your regret is only 1000, that is 0.1% per round. the longer you play, the closer your average performance gets to the best expert. if you are only a tiny bit worse each day, over a thousand days you still look like a genius.\n\nthis is why regret replaces the usual \"generalization error\" from classical machine learning. you do not need any i.i.d. assumption. you do not need the data to come from a nice distribution. the losses can be chosen by an adversary who is actively trying to mess you up, and you can *still* guarantee sublinear regret. that is a remarkable guarantee.\n\n## variants of regret\n\nthere are several flavors of regret, and it helps to know which one you are working with.\n\n**expected regret**, $\\mathbb{e}[r_t]$, averages over any randomness in your algorithm. this is the most common version in stochastic settings.\n\n**high-probability regret** gives bounds that hold with probability at least $1-\\delta$. you pay a small price in the bound (usually a log factor in $1/\\delta$), but you get a guarantee that works on any single run, not just on average.\n\n**pseudo-regret** shows up in stochastic bandits. instead of comparing against the best fixed arm's *realized* loss sequence, you compare against its *expected* loss. it is a slightly weaker benchmark but often easier to analyze.\n\n**external regret** vs **internal (swap) regret**: external regret compares you against the best single action. internal regret asks a harder question \u2014 for every action $a$ you played, would you have done better by swapping it with some other action $a'$? no-internal-regret algorithms are more powerful and connect to game-theoretic equilibria.\n\n## adversarial vs stochastic: two different worlds\n\nin **stochastic bandits**, the losses or rewards come from fixed distributions. you are pulling slot-machine levers, and each lever has a true average payout that does not change. the difficulty is that you do not know these averages.\n\nin **adversarial online learning**, the losses can be anything. an adversary picks the loss sequence, possibly after seeing your strategy. no distributional assumptions at all. this is the harder setting, and the guarantees you get are necessarily weaker \u2014 but they are also more robust.\n\nthe minimax lower bounds tell you the price of feedback. with full information (you see every expert's loss after each round), the best possible regret is $\\omega(\\sqrt{t \\log n})$ where $n$ is the number of experts. with bandit feedback (you only see the loss of the arm you pulled), it jumps to $\\omega(\\sqrt{kt})$ where $k$ is the number of arms. that extra $\\sqrt{k}$ is the tax you pay for partial feedback.\n\n## practical examples\n\nthink of a portfolio manager who invests in stocks every day. at the end of the year, she looks back and compares her cumulative return to the single best stock she could have held all year. her regret is the difference. a no-regret strategy guarantees she is not much worse than that best-in-hindsight stock, even though she could not predict the market.\n\nor think of an email system that routes your messages to different folders using a set of rules (experts). after a year of routing, you compare the system's accuracy to the single best routing rule. regret measures the gap, and a good algorithm makes that gap vanishingly small over time.\n\n## big ideas\n* regret is not about making mistakes \u2014 it is about making mistakes you could have avoided. the benchmark is always the best fixed strategy in hindsight, not perfection.\n* sublinear regret means your per-round error vanishes over time, even against an adversary who knows your algorithm. you do not need a cooperative world to learn well.\n* the price of feedback is real and quantified: full information gives $o(\\sqrt{t \\log n})$ regret while bandit feedback costs an extra $\\sqrt{k}$. that gap is the price of silence.\n* external and internal regre"
    },
    {
      "topicId": "online-reinforcement-learning",
      "topicTitle": "Online Reinforcement Learning",
      "routeSlug": "online-reinforcement-learning",
      "lessonSlug": "td-sarsa-qlearning",
      "lessonTitle": "Temporal-Difference Learning, SARSA, and Q-Learning",
      "x": 0.8545259237289429,
      "y": 0.535525918006897,
      "searchText": "temporal-difference learning, sarsa, and q-learning\n# temporal-difference learning, sarsa, and q-learning\n\n## learning without waiting for the end\n\nmonte carlo said: play the whole game, then learn from the final score. td learning says: why wait? after every single step, you can update your estimate using the reward you just received and your current guess about the next state's value. you are learning as you go, one step at a time.\n\nthis is **bootstrapping** \u2014 using your own estimates as part of the learning target. it sounds circular, and in a way it is. but it works, and it works remarkably well.\n\n## td(0): the one-step update\n\nthe simplest td method updates the state-value estimate after every transition:\n\n$$\nv(s_t)\\leftarrow v(s_t)+\\alpha\\left[r_{t+1}+\\gamma v(s_{t+1})-v(s_t)\\right].\n$$\n\nthe bracketed term is the **td error** \u2014 the difference between what you expected ($v(s_t)$) and a better estimate based on what actually happened ($r_{t+1}+\\gamma v(s_{t+1})$). you nudge your estimate in the direction of this error, scaled by the step size $\\alpha$.\n\nthink of it like this: you are walking to a restaurant and you estimated it would take 20 minutes. after 5 minutes, you have walked a quarter of the way and you now estimate 18 minutes total. td learning says: update your original estimate *right now*, based on the partial information. you do not have to arrive at the restaurant to revise your prediction.\n\n## sarsa: on-policy control\n\nsarsa learns the value of state-action pairs, and it learns about the policy it is actually following. the name comes from the five quantities it uses: $s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}$.\n\n$$\nq(s_t,a_t)\\leftarrow q(s_t,a_t)+\\alpha\\left[r_{t+1}+\\gamma q(s_{t+1},a_{t+1})-q(s_t,a_t)\\right].\n$$\n\nyou are in state $s_t$, you take action $a_t$, you get reward $r_{t+1}$, you land in state $s_{t+1}$, and you take action $a_{t+1}$ (according to your current policy, say epsilon-greedy). then you update $q(s_t,a_t)$ toward the reward plus the discounted value of what you *actually did* next.\n\nbecause sarsa uses the action you *actually took* in the next state, it learns the value of the policy you are following, including its exploration. if your epsilon-greedy policy occasionally walks off a cliff (because it explores), sarsa learns that the cliff-edge state is dangerous \u2014 it bakes in the risk of falling.\n\n## q-learning: off-policy control\n\nq-learning looks almost identical to sarsa, with one critical difference:\n\n$$\nq(s_t,a_t)\\leftarrow q(s_t,a_t)+\\alpha\\left[r_{t+1}+\\gamma \\max_{a'}q(s_{t+1},a')-q(s_t,a_t)\\right].\n$$\n\ninstead of using the action you *actually took* in the next state, q-learning uses the *best* action \u2014 the $\\max$. it does not matter what you actually did next; q-learning assumes you will act optimally from the next state onward.\n\nthis makes q-learning **off-policy**: it learns about the optimal policy regardless of the behavior policy generating the data. you can explore wildly (random actions, curious detours) and q-learning still converges to $q^*$, the optimal action-value function, as long as you visit every state-action pair often enough and your step sizes satisfy the right conditions.\n\n*try it yourself: before reading on, guess \u2014 in a gridworld with a cliff along one edge, which algorithm learns the path along the cliff, and which takes the safer inland route?*\n\nthe difference is sharp. imagine a gridworld with a cliff along one edge. sarsa, following epsilon-greedy, learns a *safe* path that stays away from the cliff \u2014 because its own exploration sometimes sends it tumbling off. q-learning learns the *optimal* path right along the cliff edge \u2014 because it evaluates the optimal policy, which never falls. q-learning finds the faster route, but sarsa finds the route that is actually safe given how you are behaving. neither answer is wrong; they are answering different questions.\n\n[[simulation sarsa-vs-qlearning]]\n\n[[simulation cliff-walking]]\n\n## exploration strategies\n\nyou need exploration to ensure you visit enough state-action pairs, but you want to exploit what you have learned to collect reward. three common approaches:\n\n**epsilon-greedy** is the simplest \u2014 with probability $\\epsilon$, take a random action; otherwise, take the greedy action. easy to implement, easy to tune. but it wastes exploration on clearly bad actions.\n\n**softmax (boltzmann) exploration** assigns probabilities proportional to $\\exp(q(s,a)/\\tau)$, where $\\tau$ is a temperature. high temperature means near-uniform exploration; low temperature means near-greedy. it focuses exploration on actions that look promising rather than wasting pulls on obviously bad ones.\n\n**optimism and ucb-like bonuses** add exploration bonuses to the q-values, similar to how ucb1 works in bandits. this connects the rl exploration problem back to the bandit ideas from earlier in the course.\n\n## the deadly triad: three bad roommates\n\nthere is a stability problem lurking in td methods, and it becomes critical when we scale up. imagine three roommates who are perfectly fine individually but, when you put them together, they burn the house down. the three roommates are:\n\n**function approximation** \u2014 instead of storing one value per state in a table, you use a parameterized function (like a neural network) to generalize across states. perfectly reasonable on its own.\n\n**bootstrapping** \u2014 using your own estimates as targets, as td does. works great in tabular settings.\n\n**off-policy learning** \u2014 learning about one policy from data generated by a different policy. efficient and flexible.\n\nany two roommates are manageable. put all three together and the house burns down \u2014 values diverge, spiraling upward or oscillating wildly as the target changes because the parameters changed, which changes the target again, in a runaway feedback loop. this is the **deadly triad**. tabular q-learning survives because it has no function approximation. dqn, which we meet next lesson, survives because of two clever "
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "atom-field-interaction",
      "lessonTitle": "Atom-Field Interaction",
      "x": 0.169185072183609,
      "y": 0.1423356533050537,
      "searchText": "atom-field interaction\n# atom-field interaction\n__topic 12 keywords__\n* perturbation theory \n* rabi oscillations\n* jaynes-cummings model\n\n# interaction of an atom with a quantized field\n### dipole approximation\nimagine a situation with electromagnetic field act on one atom.\n\nto begin, hamiltonian of an electron bound to an atom in vacuum is\n$$\n    \\hat{h}_\\mathrm{atom}\n    =\n    \\frac{\\hat{p}^2}{2m}\n    +\n    v(\\vec{r})\n$$\nhere, $v(\\vec{r})$ is coulomb potential.\n\nin presence of external fields, the hamiltonian become,\n$$\n    \\hat{h}\n    =\n    \\frac{1}{2m}\n    \\left(\n    \\hat{p}^2\n    +\n    e \\vec{a}(\\vec{r}, t)\n    \\right)^2\n    +\n    v(\\vec{r})\n$$\n\nwe can further simplify the equation.\nthe spatial dependency of electromagnetic field is $e^{i \\vec{k} \\cdot \\vec{r}}$.\nby assuming typical light wavelength as $\\lambda \\sim 500 \\mathrm{nm}$, \nand $\\vec{r}$ is few angstroms, \nthe magnitude of $\\left| \\vec{k} \\cdot \\vec{r} \\right|$ is smaller than one.\n$$\n    \\left| \\vec{k} \\cdot \\vec{r} \\right|\n    \\ll\n    1\n$$\nthus, we can ignore the spatial dependency of electromagnetic field.\n$$\n    \\hat{h}\n    =\n    \\hat{h}_\\mathrm{atom}\n    -\n    \\hat{\\vec{d}} \\cdot \\hat{\\vec{e}} (t)\n$$\nthis is **dipole approximation**.\n\n### 2-level emitter in cavity (perturbation theory)\nimagine a 2-level system in cavity. \nwe apply single optical mode.\nelectric field on atom is \n$$\n    \\hat{\\vec{e}}\n    =\n    \\vec{\\mathcal{e}}\n    \\left( \n        \\hat{a} + \\hat{a}^\\dag\n    \\right)\n$$\nwe are going to solve schr\u00f6dinger equation.\n$$\n    i\\hbar\n    \\frac{\\partial}{\\partial t}\n    \\ket{\\psi(t)}\n    =\n    \\hat{h}\n    \\ket{\\psi(t)}\n$$\nwe use *ansatz* solution. \n$$\n\\begin{aligned}\n    \\ket{\\psi(t)}\n    &=\n    c_i(t) \n    \\cdot\n    \\ket{a} \n    \\ket{n}\n    \\cdot\n    e^{-i e_a t/\\hbar}\n    e^{-i n \\omega t}\n    +\n    c_f(t) \n    \\cdot\n    \\ket{b} \n    \\ket{n-1}\n    \\cdot\n    e^{-i e_b t/\\hbar}\n    e^{-i (n-1) \\omega t}\n    \\\\&=\n    c_i(t) \n    \\cdot\n    \\ket{i} \n    \\cdot\n    e^{-i e_a t/\\hbar}\n    e^{-i n \\omega t}\n    +\n    c_f(t) \n    \\cdot\n    \\ket{f} \n    \\cdot\n    e^{-i e_b t/\\hbar}\n    e^{-i (n-1) \\omega t}\n\\end{aligned}\n$$\nhere, \n$c_i(t)$ is complex coefficient, \n$\\ket{i}$ is initial state with $a$ state and $n$ photon, \n$e^{-i e_a t/\\hbar}$ is time evolution of atom, \nand $e^{-i n \\omega t}$ is time evolution of photon.\n\n\n# the rabi model\n\n# fully quantum-mechanical model; the jaynes-cummings model\n\n## big ideas\n\n* the dipole approximation \u2014 treating the electromagnetic field as spatially uniform over the atom \u2014 is justified because the optical wavelength ($\\sim 500$ nm) dwarfs the atomic size ($\\sim 0.1$ nm), making $|\\vec{k}\\cdot\\vec{r}| \\ll 1$.\n* the atom-field interaction reduces to $-\\hat{\\vec{d}}\\cdot\\hat{\\vec{e}}$: the dot product of the atomic dipole moment and the quantized electric field at the atomic position.\n* the two-level approximation works when the driving field is close to resonance with one particular atomic transition, so all other levels can be ignored.\n* coupling a two-level atom to a quantized field produces a system where energy is exchanged not just in units of $\\hbar\\omega$ (photons) but through entangled atom-field states \u2014 dressed states \u2014 that are neither pure atom nor pure field.\n\n## what comes next\n\nthe stage is set for the jaynes-cummings model, which solves this atom-single-mode-field interaction exactly. the next lesson shows that the quantum nature of the field leaves a distinctive fingerprint in the dynamics: the atom oscillates between excited and ground state at a rate that depends on the square root of the photon number, and a coherent field drives collapses and revivals that no classical model can explain.\n\n## check your understanding\n\n1. the dipole approximation sets $e^{i\\vec{k}\\cdot\\vec{r}} \\approx 1$ for the field at the electron's position. under what physical circumstances would this approximation break down, and what kind of radiation-matter coupling would you need to describe those situations?\n2. the hamiltonian in the dipole approximation is $\\hat{h} = \\hat{h}_\\text{atom} - \\hat{\\vec{d}}\\cdot\\hat{\\vec{e}}$. the minus sign might seem counterintuitive. explain physically why an electric dipole aligned with the field has lower energy, using an analogy from classical electrostatics.\n3. in the two-level approximation, we keep only the ground state $|g\\rangle$ and one excited state $|e\\rangle$. the perturbation theory ansatz for the state is a superposition of $|a, n\\rangle$ and $|b, n-1\\rangle$. what physical process does each term represent, and why is the total excitation number $n + \\text{(atomic excitation)}$ approximately conserved?\n\n## challenge\n\nwrite down the full rabi hamiltonian for a two-level atom coupled to a single cavity mode, including both counter-rotating terms $\\hat{a}^\\dagger\\hat{\\sigma}_+$ and $\\hat{a}\\hat{\\sigma}_-$. show that the total excitation number is not conserved in the full rabi model (unlike in the jaynes-cummings model). under what condition on the coupling strength $g$ and frequency $\\omega$ does the rotating wave approximation become valid, and what physical effect do the counter-rotating terms produce when the rwa breaks down? (this is the bloch-siegert shift.)\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "cavity-qed",
      "lessonTitle": "Cavity QED",
      "x": 0.21461325883865356,
      "y": 0.09819207340478897,
      "searchText": "cavity qed\n# cavity qed\n\n## cavity quantum electrodynamics\n\n**cavity qed** studies the interaction between atoms and photons confined in a high-quality resonator. by trapping the electromagnetic field in a small volume, the atom-photon coupling strength $g$ can be made large relative to the dissipation rates, enabling the observation of fundamentally quantum phenomena.\n\nthe three key rates are:\n* $g$: atom-cavity coupling strength.\n* $\\kappa$: cavity photon loss rate (inverse of cavity lifetime).\n* $\\gamma$: atomic spontaneous emission rate into non-cavity modes.\n\nthe **strong coupling regime** $g \\gg \\kappa, \\gamma$ allows coherent exchange of excitations between atom and field before either decays.\n\n## the purcell effect\n\nwhen an atom is placed in a resonant cavity, its spontaneous emission rate is modified. the **purcell factor** gives the enhancement:\n\n$$\nf_p = \\frac{\\gamma_{\\text{cav}}}{\\gamma_{\\text{free}}} = \\frac{3}{4\\pi^2}\\left(\\frac{\\lambda}{n}\\right)^3 \\frac{q}{v},\n$$\n\nwhere $q$ is the cavity quality factor and $v$ is the mode volume. high-$q$, small-$v$ cavities dramatically enhance emission into the cavity mode.\n\nin the weak coupling regime ($g < \\kappa$), the atom still decays irreversibly but at the enhanced purcell rate $\\gamma_{\\text{cav}} = 4g^2/\\kappa$. in the strong coupling regime, the decay is replaced by reversible rabi oscillations described by the jaynes-cummings model.\n\nconversely, when the cavity is far detuned from the atomic transition, the density of states is suppressed and spontaneous emission is **inhibited**. this was first demonstrated by kleppner (1981) using rydberg atoms between conducting plates.\n\n## experimental platforms\n\n**microwave cavity qed** uses rydberg atoms (with transition frequencies in the ghz range) passing through superconducting microwave cavities. pioneered by haroche and colleagues, these experiments achieve:\n* photon lifetimes of $\\sim 0.1$ seconds in superconducting cavities.\n* strong coupling with $g/2\\pi \\sim 50$ khz.\n* single-atom, single-photon resolution.\n\n**optical cavity qed** uses alkali atoms trapped in high-finesse fabry-perot cavities. pioneered by kimble and colleagues, these experiments work at optical frequencies with:\n* small mode volumes ($\\sim \\lambda^3$).\n* strong coupling with $g/2\\pi \\sim 10{-}100$ mhz.\n* direct single-photon detection.\n\n**circuit qed** replaces atoms with superconducting qubits and cavities with microwave transmission line resonators. the coupling strength is orders of magnitude larger than in natural atoms:\n* $g/2\\pi \\sim 100$ mhz (easily in strong coupling).\n* highly controllable fabrication.\n* the dominant platform for quantum computing (ibm, google).\n\n## cat-state generation\n\na **schrodinger cat state** is a superposition of two macroscopically distinct coherent states:\n\n$$\n|\\text{cat}_\\pm\\rangle = \\mathcal{n}_\\pm(|\\alpha\\rangle \\pm |-\\alpha\\rangle),\n$$\n\nwhere $\\mathcal{n}_\\pm$ is a normalization constant. the even cat $|+\\rangle$ contains only even photon numbers, while the odd cat $|-\\rangle$ contains only odd photon numbers.\n\nin cavity qed, cat states are generated using the **dispersive interaction**. an atom in a superposition $(|e\\rangle + |g\\rangle)/\\sqrt{2}$ interacting dispersively with a coherent state $|\\alpha\\rangle$ creates an entangled state:\n\n$$\n\\frac{1}{\\sqrt{2}}(|e\\rangle|\\alpha e^{i\\chi t}\\rangle + |g\\rangle|\\alpha e^{-i\\chi t}\\rangle).\n$$\n\nat $\\chi t = \\pi/2$, this becomes $\\frac{1}{\\sqrt{2}}(|e\\rangle|i\\alpha\\rangle + |g\\rangle|-i\\alpha\\rangle)$. a subsequent $\\pi/2$ pulse and measurement on the atom projects the cavity into a cat state.\n\nthese experiments were performed by haroche's group and provide direct evidence of quantum superpositions at the mesoscopic scale. the decoherence of cat states (monitored by wigner function tomography) demonstrates the quantum-to-classical transition.\n\n[[simulation vacuum-rabi-oscillation]]\n\n[[simulation wigner-cat-state]]\n\n## decoherence and quantum jumps\n\nreal cavities lose photons at rate $\\kappa$, and atoms decay at rate $\\gamma$. the system dynamics are described by a **master equation**:\n\n$$\n\\frac{d\\hat{\\rho}}{dt} = -\\frac{i}{\\hbar}[\\hat{h}, \\hat{\\rho}] + \\kappa\\mathcal{d}[\\hat{a}]\\hat{\\rho} + \\gamma\\mathcal{d}[\\hat{\\sigma}_-]\\hat{\\rho},\n$$\n\nwhere $\\mathcal{d}[\\hat{o}]\\hat{\\rho} = \\hat{o}\\hat{\\rho}\\hat{o}^\\dagger - \\frac{1}{2}\\{\\hat{o}^\\dagger\\hat{o}, \\hat{\\rho}\\}$ is the lindblad dissipator.\n\nfor cat states, decoherence occurs at a rate $\\gamma_{\\text{decoherence}} = 2\\kappa|\\alpha|^2$, proportional to the \"size\" of the superposition. larger cats decohere faster, consistent with the difficulty of observing quantum effects at macroscopic scales.\n\n**quantum jump** monitoring (measuring the environment) reveals individual photon loss events in real time. between jumps, the system evolves under a non-hermitian effective hamiltonian, and each jump projects the state. this was first observed by nagourney, sauter, and dehmelt (1986) in ion traps.\n\n## big ideas\n\n* cavity qed is defined by the competition between three rates: coupling $g$, cavity decay $\\kappa$, and atomic decay $\\gamma$. strong coupling ($g \\gg \\kappa, \\gamma$) is where the interesting quantum physics lives.\n* the purcell effect shows that spontaneous emission is not a fixed atomic property but an interaction between the atom and its electromagnetic environment \u2014 change the cavity, change the decay rate.\n* schr\u00f6dinger cat states (superpositions of macroscopically distinct coherent states) can be created and observed in cavity qed, and their decoherence rate scales with $|\\alpha|^2$ \u2014 bigger cats die faster.\n* circuit qed extends cavity qed to superconducting chips with coupling strengths so large that strong coupling is easy to achieve, making it the backbone of modern quantum computing.\n\n## what comes next\n\ncavity qed shows us that measurement fundamentally shapes the quantum state \u2014 cat states collapse, photon jumps are observed, and every detection event changes th"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "coherence-functions",
      "lessonTitle": "Coherence Functions",
      "x": 0.20614734292030334,
      "y": 0.0,
      "searchText": "coherence functions\n# coherence functions\n\n## classical coherence\n\n**coherence** describes the ability of light to produce interference. classically, it is quantified by **correlation functions** of the electric field. the first-order correlation function is\n\n$$\ng^{(1)}(\\mathbf{r}_1, t_1; \\mathbf{r}_2, t_2) = \\langle e^*(\\mathbf{r}_1, t_1) e(\\mathbf{r}_2, t_2) \\rangle.\n$$\n\nthe normalized version is the **degree of first-order coherence**:\n\n$$\ng^{(1)}(\\tau) = \\frac{\\langle e^*(t) e(t+\\tau) \\rangle}{\\langle |e(t)|^2 \\rangle}.\n$$\n\nfor a perfectly monochromatic source, $|g^{(1)}(\\tau)| = 1$ for all $\\tau$. for a thermal source with spectral width $\\delta\\nu$, the coherence decays on a timescale $\\tau_c \\sim 1/\\delta\\nu$, called the **coherence time**.\n\n**temporal coherence** measures how well the field correlates with itself at different times at the same point. **spatial coherence** measures correlations between different spatial points at the same time. the **wiener-khintchine theorem** relates $g^{(1)}(\\tau)$ to the power spectral density through a fourier transform.\n\n## quantum coherence functions\n\nin quantum optics, the electric field becomes an operator, and correlation functions involve **normal ordering** (creation operators to the left, annihilation operators to the right). the quantum first-order correlation function is\n\n$$\ng^{(1)}(\\mathbf{r}_1, t_1; \\mathbf{r}_2, t_2) = \\langle \\hat{e}^{(-)}(\\mathbf{r}_1, t_1) \\hat{e}^{(+)}(\\mathbf{r}_2, t_2) \\rangle,\n$$\n\nwhere $\\hat{e}^{(+)}$ and $\\hat{e}^{(-)}$ are the positive and negative frequency parts of the field operator.\n\nfor different quantum states, $g^{(1)}$ behaves differently:\n* **coherent state** $|\\alpha\\rangle$: $|g^{(1)}(\\tau)| = 1$ (perfect first-order coherence, just like a classical field).\n* **number state** $|n\\rangle$: $|g^{(1)}(\\tau)| = 1$ (also perfectly coherent in first order).\n* **thermal state**: $|g^{(1)}(\\tau)|$ decays with $\\tau$, reflecting the broad spectral content.\n\nfirst-order coherence alone cannot distinguish quantum from classical light. the differences emerge at higher orders.\n\n## young's interference with quantum fields\n\nyoung's double-slit experiment illustrates first-order coherence. two pinholes at positions $\\mathbf{r}_1$ and $\\mathbf{r}_2$ sample the field, and the intensity at the observation screen is\n\n$$\ni(\\mathbf{r}) \\propto g^{(1)}(\\mathbf{r}_1, \\mathbf{r}_1) + g^{(1)}(\\mathbf{r}_2, \\mathbf{r}_2) + 2\\operatorname{re}\\left[g^{(1)}(\\mathbf{r}_1, \\mathbf{r}_2) e^{i\\phi}\\right],\n$$\n\nwhere $\\phi$ is the phase difference due to path lengths. the **visibility** of the interference fringes is\n\n$$\n\\mathcal{v} = \\frac{i_{\\max} - i_{\\min}}{i_{\\max} + i_{\\min}} = |g^{(1)}(\\mathbf{r}_1, \\mathbf{r}_2)|.\n$$\n\nremarkably, single photons also produce interference fringes when detected one at a time over many trials. each photon interferes with itself, building up the pattern statistically. this was demonstrated by taylor (1909) and later with more controlled single-photon sources.\n\n## photodetection theory\n\nthe quantum theory of **photodetection** connects correlation functions to measurable quantities. the probability of detecting a photon at position $\\mathbf{r}$ and time $t$ is proportional to\n\n$$\np_1 \\propto \\langle \\hat{e}^{(-)}(\\mathbf{r}, t) \\hat{e}^{(+)}(\\mathbf{r}, t) \\rangle = g^{(1)}(\\mathbf{r}, t; \\mathbf{r}, t).\n$$\n\nthe joint probability of detecting photons at two space-time points involves $g^{(2)}$, the second-order correlation function. normal ordering ensures that these expressions give physically meaningful (non-negative) detection probabilities, consistent with the photoelectric effect.\n\n[[simulation wigner-coherent]]\n\n## big ideas\n\n* coherence is about correlations: first-order coherence $g^{(1)}(\\tau)$ measures how well the field at time $t$ predicts the field at time $t+\\tau$ \u2014 it governs interference fringe visibility.\n* the wiener-khintchine theorem connects time-domain coherence to spectral purity: a narrow-linewidth source has long coherence time, while a broadband source loses coherence quickly.\n* quantum mechanics requires normal ordering (creation operators left, annihilation operators right) in correlation functions so that the photodetection probability is non-negative.\n* first-order coherence cannot distinguish quantum from classical light \u2014 even a single photon produces perfect fringes in young's experiment, because it interferes with itself.\n\n## what comes next\n\nfirst-order coherence and interference tell only part of the story. the next lesson introduces higher-order coherence, particularly the second-order correlation function $g^{(2)}(\\tau)$, which measures photon-photon correlations in time. this is where the real quantum character of light is revealed: thermal light, laser light, and single-photon sources give completely different signatures.\n\n## check your understanding\n\n1. a thermal source with coherence time $\\tau_c$ and a single-mode laser both have $|g^{(1)}(0)| = 1$ at zero delay. how does $g^{(1)}(\\tau)$ differ for the two sources as $\\tau$ increases, and what does this mean for their ability to produce stable interference fringes?\n2. single photons produce interference fringes in young's double-slit experiment when detected one at a time. at first glance this seems paradoxical \u2014 how does a single particle \"go through both slits\"? explain the resolution using the language of superposition and what is actually being measured by the interference pattern.\n3. the photodetection probability is proportional to $\\langle\\hat{e}^{(-)}\\hat{e}^{(+)}\\rangle$ rather than $\\langle\\hat{e}^2\\rangle$. why is normal ordering essential here? what goes wrong with the naive expectation value $\\langle\\hat{e}^2\\rangle$ for a vacuum state?\n\n## challenge\n\nmodel the first-order coherence function $g^{(1)}(\\tau)$ for two physically distinct sources: (a) a lorentzian-lineshape source (single-mode atom undergoing exponential decay) giving $g^{(1)}(\\tau) = e^{-\\gamma|\\tau|/2}$, and (b) a gaussia"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "coherent-states",
      "lessonTitle": "Coherent States: Definition, Statistics, Displacement, and Generation",
      "x": 0.04580428823828697,
      "y": 0.08830529451370239,
      "searchText": "coherent states: definition, statistics, displacement, and generation\n# coherent states: definition, statistics, displacement, and generation\n\n## eigenstates of the annihilation operator and minimum uncertainty states\n### definition of coherent state\ncoherent states are defined as eigenstate of annihilation operator.\n$$\n    \\hat{a}\n    \\ket{\\alpha}\n    =\n    \\alpha\n    \\ket{\\alpha}\n$$\n$$\n    \\hat{a}^\\dag\n    \\bra{\\alpha}\n    =\n    \\alpha^*\n    \\bra{\\alpha}\n$$\n\n> **key intuition.** a coherent state is defined as an eigenstate of the annihilation operator. because the annihilation operator is non-hermitian, the eigenvalue is a complex number, encoding both the amplitude and phase of the field.\n\nsince $\\hat{a}$ is non-hermitian, the eigenvalue **$\\alpha$ is complex\nnumber**.\nnote that $\\hat{n}=\\hat{a}^\\dag\\hat{a}$ is hermitian.\n\nactually, there are three equivalent ways to define the coherent state:\n* right eigenstates of the annihilation operator\n* states that minimize the uncertainty relation for the two orthogonal field quadratures\n* displaced vacuum state\n\nwe will see all three in this lesson.\n\n### expanding coherent state with number state\nlet's expand coherent state with number state.\nfirst, we multiply identity $\\sum_{n=0}^\\infty \\ket{n}\\bra{n}$ to alpha.\nby this operation, we can project coherent state to number state.\n$$\n\\begin{aligned}\n    \\ket{\\alpha}\n    &=\n    \\sum_{n=0}^\\infty\n    \\ket{n}\n    \\braket{n|\\alpha}\n    \\\\&=\n    \\sum_{n=0}^\\infty\n    c_n\n    \\ket{n}\n\\end{aligned}\n$$\nwe replaced the $\\braket{n|\\alpha}=c_n$.\n\napply operator to $\\ket{\\alpha}$ resuts to\n$$\n\\begin{aligned}\n    \\hat{a}\n    \\ket{\\alpha}\n    &=\n    \\alpha\n    \\ket{\\alpha}\n    \\\\&=\n    \\sum_{n=0}^\\infty\n    \\alpha\n    c_n\n    \\ket{n}\n\\end{aligned}\n$$\nthis follows directly from the definition.\nmultiplying $\\hat{a}$ from left gives\n$$\n\\begin{aligned}\n    \\hat{a}\n    \\sum_{n=0}^\\infty\n    c_n\n    \\ket{n}\n    &=\n    \\sum_{n=1}^\\infty\n    \\sqrt{n}\n    c_n\n    \\ket{n-1}\n    \\\\&=\n    \\sum_{n=0}^\\infty\n    \\sqrt{n+1}\n    c_{n+1}\n    \\ket{n}\n\\end{aligned}\n$$\ncoefficient of number state is equal.\n$$\n    \\sqrt{n+1}\n    c_{n+1}\n    =\n    \\alpha\n    c_n\n$$\nthus, we can decide the $c_n$ recursively.\n$$\n\\begin{aligned}\n    c_n\n    &=\n    \\frac\n    {\\alpha}\n    {\\sqrt{n}}\n    c_{n-1}\n    \\\\&=\n    \\frac\n    {\\alpha^2}\n    {\\sqrt{n(n-1)}}\n    c_{n-2}\n    \\\\&=\n    \\cdots\n    \\\\&=\n    \\frac\n    {\\alpha^n}\n    {\\sqrt{n!}}\n    c_{0}\n\\end{aligned}\n$$\nwe almost complete expanding coherent state with number state.\n$$\n    \\ket{\\alpha}\n    =\n    c_0\n    \\sum_{n=0}^\\infty\n    \\frac\n    {\\alpha^n}\n    {\\sqrt{n!}}\n    \\ket{n}\n$$\nwe still need to decide $c_0$. we can do this from normalization condition.\n$$\n\\begin{aligned}\n    1\n    &=\n    \\braket{\\alpha|\\alpha}\n    \\\\&=\n    \\left| c_0 \\right|^2\n    \\sum_{n, n^\\prime}\n    \\frac\n    {\\left| \\alpha \\right|^{2n}}\n    {n!}\n    \\braket{n|n^\\prime}\n    \\left| c_0 \\right|^2\n    \\\\&=\n    \\sum_{n=0}^\\infty\n    \\frac\n    {\\left| \\alpha \\right|^{2n}}\n    {n!}\n    \\\\&=\n    \\left| c_0 \\right|^2\n    e^\n    {\\left| \\alpha \\right|^{2}}\n\\end{aligned}\n$$\nthus,\n$$\n    \\left| c_0 \\right|^2\n    =\n    e^\n    {- \\left| \\alpha \\right|^{2}}\n$$\n$$\n    c_0\n    =\n    e^\n    {- \\left| \\alpha \\right|^{2}/2}\n$$\nfinally coherent coherent state of number state basis is\n$$\n    \\ket{\\alpha}\n    =\n    e^\n    {- \\left| \\alpha \\right|^{2}/2}\n    \\sum_{n=0}^\\infty\n    \\frac\n    {\\alpha^n}\n    {\\sqrt{n!}}\n    \\ket{n}\n$$\n\n> **key intuition.** this expansion shows that a coherent state is a specific superposition of all number states, weighted by a poissonian distribution. the prefactor ensures normalization, and the complex parameter $\\alpha$ fully determines the state.\n\n## electric field averages and photon statistics\n\n### electric field from coherent state viewpoint\nlet's consider the expectation value of electric field operator.\n$$\n    \\hat{e}_x\n    =\n    i \\mathcal{e}_0\n    \\left[\n        \\hat{a}\n        e^{i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n        -\n        \\text{hermitian conjugate}\n    \\right]\n$$\ncoherent state average of electric field is\n$$\n    \\braket{\\alpha | \\hat{e}_x | \\alpha}\n    =\n    i \\mathcal{e}_0 \\alpha\n    e^{i\n    \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n    }\n    +\n    \\text{complex conjugate}\n$$\ncoherent state average of square of electric field is\n$$\n\\begin{aligned}\n    \\braket{\\alpha | \\hat{e}_x^2 | \\alpha}\n    &=\n    * \\mathcal{e}_0^2\n    \\braket{\\alpha |\n    \\left(\n        \\hat{a}\n        e^{i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t \\right)\n        }\n        -\n        \\hat{a}^\\dag\n        e^{-i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n    \\right)^2\n    | \\alpha}\n    \\\\&=\n    * \\mathcal{e}_0^2\n    \\braket{\\alpha |\n    \\left(\n        \\hat{a}^2\n        e^{2i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n        +\n        {\\hat{a}^\\dag }^2\n        e^{-2i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n        -\\hat{a}\\hat{a}^\\dag - \\hat{a}^\\dag\\hat{a}\n    \\right)\n    | \\alpha}\n    \\\\&=\n    * \\mathcal{e}_0^2\n    \\braket{\\alpha |\n    \\left(\n        \\hat{a}^2\n        e^{2i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n        +\n        {\\hat{a}^\\dag }^2\n        e^{-2i\n        \\left( \\vec{k}\\cdot\\vec{r} - \\omega t\\right)\n        }\n        * (1 + \\hat{a}^\\dag\\hat{a})\n        * \\hat{a}^\\dag\\hat{a}\n    \\right)\n    | \\alpha}\n\\end{aligned}\n$$\ndetail of calculation is left for readers ;).\nby writing $\\alpha = |\\alpha|e^{i\\theta}$, we can have sine wave which is **very classical**.\n\nfrom these, the variance become\n$$\n    \\left<\n        \\left( \\delta e_x \\right)^2\n    \\right>_\\alpha\n    =\n    \\mathcal{e}_0\n    =\n    \\frac{\\hbar \\omega}{2 \\epsilon_0 v}\n$$\nwhich **does not depend on $\\alpha$!**\nand notice this is **identical to those for a vacuum state!!** (see section 2.2)\n\n### quadrature operators from coherent state viewpoint\nwe can show that fluctuation of quadrature operator also does not\ndepend on $\\alph"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "higher-order-coherence",
      "lessonTitle": "Higher-Order Coherence",
      "x": 0.2006283849477768,
      "y": 0.02253367379307747,
      "searchText": "higher-order coherence\n# higher-order coherence\n\n## second-order correlation function\n\nthe **second-order correlation function** measures intensity-intensity correlations and reveals the photon statistics of a light source. it is defined as\n\n$$\ng^{(2)}(\\tau) = \\frac{\\langle \\hat{a}^\\dagger \\hat{a}^\\dagger \\hat{a} \\hat{a} \\rangle}{\\langle \\hat{a}^\\dagger \\hat{a} \\rangle^2} = \\frac{\\langle : \\hat{n}(\\hat{n}-1) : \\rangle}{\\langle \\hat{n} \\rangle^2},\n$$\n\nwhere the colons denote normal ordering. at zero delay, $g^{(2)}(0)$ characterizes the photon number fluctuations:\n\n* **coherent light** (poissonian statistics): $g^{(2)}(0) = 1$.\n* **thermal light** (super-poissonian): $g^{(2)}(0) = 2$.\n* **number state** $|n\\rangle$ (sub-poissonian): $g^{(2)}(0) = 1 - 1/n < 1$.\n* **single photon** $|1\\rangle$: $g^{(2)}(0) = 0$.\n\nthe condition $g^{(2)}(0) < 1$ is a signature of **non-classical light** with no classical analogue. classically, the cauchy-schwarz inequality requires $g^{(2)}(0) \\geq 1$.\n\n## the hanbury brown-twiss experiment\n\nin 1956, hanbury brown and twiss measured intensity correlations of starlight using two detectors. they observed **photon bunching**: photons from a thermal source tend to arrive in pairs, giving $g^{(2)}(0) > g^{(2)}(\\tau)$ for $\\tau > 0$.\n\nthe experimental setup splits the light beam with a beam splitter and sends it to two detectors. a correlator measures the coincidence rate as a function of the time delay $\\tau$ between detections. for thermal light:\n\n$$\ng^{(2)}(\\tau) = 1 + |g^{(1)}(\\tau)|^2.\n$$\n\nat $\\tau = 0$, this gives $g^{(2)}(0) = 2$, meaning photons are twice as likely to arrive together as independently. as $\\tau$ increases beyond the coherence time, $g^{(2)}(\\tau) \\to 1$.\n\nthe hbt result was initially controversial because it seemed to imply that photons \"attract\" each other. the resolution is that bunching arises from the bosonic nature of photons and the statistical properties of thermal states, not from photon-photon interactions.\n\n## photon bunching and antibunching\n\n**photon bunching** ($g^{(2)}(0) > 1$) occurs for thermal and chaotic light sources. it is a classical phenomenon explainable by wave interference of many random emitters.\n\n**photon antibunching** ($g^{(2)}(0) < 1$) has no classical explanation and requires a quantum description. it means photons tend to arrive one at a time, with a suppressed probability of simultaneous detection. antibunching was first observed by kimble, dagenais, and mandel (1977) in the fluorescence of a single atom.\n\nthe key distinction is:\n* bunching: $g^{(2)}(\\tau) < g^{(2)}(0)$ (correlations decrease with delay).\n* antibunching: $g^{(2)}(\\tau) > g^{(2)}(0)$ (correlations increase from a minimum at $\\tau = 0$).\n\n## sub-poissonian statistics\n\nclosely related to antibunching is the concept of **sub-poissonian** photon statistics, where the variance of the photon number is below the poissonian value:\n\n$$\n\\langle (\\delta n)^2 \\rangle < \\langle n \\rangle.\n$$\n\nthe **mandel q parameter** quantifies the deviation:\n\n$$\nq = \\frac{\\langle (\\delta n)^2 \\rangle - \\langle n \\rangle}{\\langle n \\rangle} = \\langle n \\rangle (g^{(2)}(0) - 1).\n$$\n\n$q = 0$ for coherent light, $q > 0$ for super-poissonian (classical) light, and $q < 0$ for sub-poissonian (non-classical) light. number states have $q = -1$, the minimum possible.\n\n## higher-order correlations\n\nthe hierarchy extends to arbitrary order. the $n$-th order correlation function is\n\n$$\ng^{(n)}(\\tau_1, \\ldots, \\tau_{n-1}) = \\frac{\\langle (\\hat{a}^\\dagger)^n \\hat{a}^n \\rangle}{\\langle \\hat{a}^\\dagger \\hat{a} \\rangle^n}.\n$$\n\nfor thermal light, $g^{(n)}(0) = n!$, while for coherent light, $g^{(n)}(0) = 1$ for all $n$. these higher-order functions provide increasingly stringent tests of non-classicality and are relevant for multi-photon experiments and quantum information protocols.\n\n## big ideas\n\n* the second-order correlation $g^{(2)}(0)$ is the single most powerful number for classifying a light source: $= 1$ for coherent, $= 2$ for thermal, $< 1$ for non-classical, and $= 0$ for a true single photon.\n* the classical cauchy-schwarz inequality demands $g^{(2)}(0) \\geq 1$; any measurement giving $g^{(2)}(0) < 1$ is direct proof that quantum mechanics is necessary to describe the light.\n* photon bunching (thermal light) is not because photons attract each other \u2014 it arises from the bosonic statistics of identical particles conspiring to favor joint detection.\n* antibunching ($g^{(2)}(0) < 1$) and sub-poissonian statistics are related but distinct: antibunching is a statement about temporal correlations, sub-poissonian is about the integrated photon number distribution.\n\n## what comes next\n\nwe now have a complete statistical toolkit for characterizing any light source. the next lesson turns this into practice: single-photon experiments at the level of individual quanta, using beam splitters and coincidence detection to observe antibunching, hong-ou-mandel interference, and the genuinely quantum behavior of light particle by particle.\n\n## check your understanding\n\n1. thermal light has $g^{(2)}(0) = 2$, meaning two photons are twice as likely to arrive simultaneously as a coherent beam with the same mean intensity. where does this extra factor of 2 come from? what is physically happening in the thermal source that produces this bunching?\n2. the condition $g^{(2)}(0) < 1$ cannot be satisfied by any classical field, yet a single atom emitting fluorescence photons achieves $g^{(2)}(0) = 0$. explain physically why a single two-level atom cannot emit two photons at once, and what this tells you about the photon statistics immediately after a detection event.\n3. the mandel q parameter is $q = \\langle n \\rangle(g^{(2)}(0) - 1)$. a coherent state gives $q = 0$, a thermal state gives $q = \\langle n \\rangle > 0$, and a number state gives $q = -1$. interpret the sign of $q$ in terms of whether the photon arrivals are more or less \"regular\" than random poissonian events.\n\n## challenge\n\nthe hanbury bro"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "interferometry",
      "lessonTitle": "Interferometry",
      "x": 0.12637516856193542,
      "y": 0.03423166647553444,
      "searchText": "interferometry\n# interferometry\n\n## input-output relations\n\noptical networks of beam splitters and phase shifters are described by **input-output relations** that map input mode operators to output mode operators. for a network with $m$ modes, the transformation is\n\n$$\n\\hat{a}_{\\text{out}, i} = \\sum_{j=1}^{m} u_{ij} \\hat{a}_{\\text{in}, j},\n$$\n\nwhere $u$ is a unitary matrix. any unitary transformation on $m$ modes can be decomposed into a sequence of beam splitters and phase shifters (reck decomposition). this universality makes linear optics a powerful platform for quantum information processing.\n\n## homodyne detection\n\n**homodyne detection** measures a single quadrature of the electromagnetic field by interfering the signal with a strong local oscillator (lo) at the same frequency. the signal mode $\\hat{a}$ and the lo mode $\\hat{b} \\approx |\\beta|e^{i\\theta}$ are combined on a 50:50 beam splitter, and the photocurrents from two detectors are subtracted:\n\n$$\n\\hat{i}_- \\propto \\hat{a} e^{-i\\theta} + \\hat{a}^\\dagger e^{i\\theta} = \\hat{x}_\\theta,\n$$\n\nwhere $\\hat{x}_\\theta$ is the quadrature operator at phase angle $\\theta$. by varying the lo phase $\\theta$, any quadrature can be measured. this enables full reconstruction of the quantum state via **quantum state tomography**.\n\nfor a coherent state $|\\alpha\\rangle$, homodyne detection yields gaussian noise centered at $2|\\alpha|\\cos(\\theta - \\phi_\\alpha)$ with vacuum-level variance. for squeezed states, the variance is below vacuum for one quadrature.\n\n## heterodyne detection\n\n**heterodyne detection** simultaneously measures both quadratures ($\\hat{x}$ and $\\hat{p}$) by using a local oscillator detuned from the signal frequency. this is equivalent to a joint measurement of non-commuting observables and necessarily adds half a quantum of noise to each quadrature (from the heisenberg uncertainty principle):\n\n$$\n\\delta x \\cdot \\delta p \\geq \\frac{1}{2}.\n$$\n\nheterodyne detection projects onto coherent states and is equivalent to measuring the husimi q-function of the field.\n\n## mach-zehnder interferometer\n\nthe **mach-zehnder interferometer** (mzi) is the workhorse of optical interferometry. two beam splitters enclose two paths with a relative phase shift $\\phi$. the full unitary transformation for a balanced mzi is\n\n$$\nu_{\\text{mzi}} = u_{\\text{bs}} \\cdot u_\\phi \\cdot u_{\\text{bs}} = \\begin{pmatrix} \\cos(\\phi/2) & i\\sin(\\phi/2) \\\\ i\\sin(\\phi/2) & \\cos(\\phi/2) \\end{pmatrix},\n$$\n\nup to global phases. the output intensities oscillate sinusoidally with $\\phi$, allowing precise phase measurements.\n\n## quantum-enhanced interferometry\n\nclassical interferometry with coherent light achieves a phase sensitivity at the **shot-noise limit**:\n\n$$\n\\delta\\phi_{\\text{snl}} = \\frac{1}{\\sqrt{\\bar{n}}},\n$$\n\nwhere $\\bar{n}$ is the mean photon number. this limit arises from the poissonian photon statistics of coherent states.\n\nquantum states can beat this limit. the ultimate bound set by quantum mechanics is the **heisenberg limit**:\n\n$$\n\\delta\\phi_{\\text{hl}} = \\frac{1}{\\bar{n}}.\n$$\n\nstrategies to approach the heisenberg limit include:\n* **squeezed vacuum** injected into the unused port of the mzi reduces the noise in the measured quadrature. this was implemented in ligo to improve gravitational wave sensitivity.\n* **noon states** $|n,0\\rangle + |0,n\\rangle$ achieve heisenberg-limited sensitivity but are fragile and difficult to prepare for large $n$.\n* **twin-fock states** $|n,n\\rangle$ provide robustness against losses compared to noon states.\n\n## sagnac interferometer\n\nthe **sagnac interferometer** uses a common path traversed in opposite directions. rotation of the interferometer introduces a phase shift proportional to the enclosed area and angular velocity (sagnac effect):\n\n$$\n\\delta\\phi = \\frac{8\\pi a \\omega}{c\\lambda},\n$$\n\nwhere $a$ is the enclosed area and $\\omega$ is the rotation rate. fiber-optic gyroscopes and ring laser gyroscopes exploit this principle for navigation and geophysics.\n\n## big ideas\n\n* any linear-optical network \u2014 no matter how complex \u2014 is described by a unitary matrix acting on the mode operators, and any unitary can be built from beam splitters and phase shifters alone.\n* homodyne detection projects the field onto a single quadrature by interfering it with a strong local oscillator; rotating the local oscillator phase rotates the measurement axis in phase space, enabling full state tomography.\n* the shot-noise limit $\\delta\\phi = 1/\\sqrt{\\bar{n}}$ is not fundamental \u2014 it is merely the limit for coherent-state inputs with poissonian noise. squeezing or entanglement can breach it.\n* the heisenberg limit $\\delta\\phi = 1/\\bar{n}$ is the fundamental quantum limit: beating it would require measuring more information than the uncertainty principle permits.\n\n## what comes next\n\nphase measurement is the gateway to understanding why squeezed light matters. the next lesson introduces [squeezed states](./squeezed-states) \u2014 states where one quadrature has noise below the vacuum level \u2014 and shows how they are generated, characterized, and used to push interferometric sensitivity toward the heisenberg limit. in particular, injecting squeezed vacuum into the dark port of a mach-zehnder interferometer improves the phase sensitivity from $\\delta\\phi = 1/\\sqrt{\\bar{n}}$ to $\\delta\\phi = e^{-r}/\\sqrt{\\bar{n}}$ \u2014 the principle behind ligo's quantum noise reduction.\n\n## check your understanding\n\n1. homodyne detection measures $\\hat{x}_\\theta$ by interfering the signal with a strong local oscillator at phase $\\theta$. by varying $\\theta$, you sample different quadratures. explain why you cannot measure $\\hat{x}$ and $\\hat{p}$ simultaneously with a single homodyne setup, and what the uncertainty principle has to say about it.\n2. the shot-noise limit scales as $1/\\sqrt{\\bar{n}}$: doubling the photon number improves precision by $\\sqrt{2}$. the heisenberg limit scales as $1/\\bar{n}$: doubling the photon number doubles the precision. what kind of quantum state a"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "jaynes-cummings",
      "lessonTitle": "Jaynes-Cummings Model",
      "x": 0.18547745048999786,
      "y": 0.12572725117206573,
      "searchText": "jaynes-cummings model\n# jaynes-cummings model\n\n## the model\n\nthe **jaynes-cummings model** describes the simplest quantum interaction between a single two-level atom and a single mode of the electromagnetic field. the hamiltonian is\n\n$$\n\\hat{h} = \\hbar\\omega_c \\hat{a}^\\dagger \\hat{a} + \\frac{\\hbar\\omega_a}{2}\\hat{\\sigma}_z + \\hbar g(\\hat{a}^\\dagger \\hat{\\sigma}_- + \\hat{a}\\hat{\\sigma}_+),\n$$\n\nwhere $\\omega_c$ is the cavity frequency, $\\omega_a$ is the atomic transition frequency, $g$ is the coupling strength, and $\\hat{\\sigma}_\\pm$ are the atomic raising/lowering operators.\n\nthe interaction term $\\hat{a}^\\dagger \\hat{\\sigma}_-$ describes the atom emitting a photon (going from excited to ground) while creating a cavity photon, and $\\hat{a}\\hat{\\sigma}_+$ describes absorption. this is the **rotating wave approximation** (rwa), valid when $g \\ll \\omega_c, \\omega_a$.\n\n## energy levels and dressed states\n\nthe total excitation number $\\hat{n} = \\hat{a}^\\dagger\\hat{a} + \\hat{\\sigma}_+\\hat{\\sigma}_-$ is conserved, so the hilbert space splits into independent two-dimensional subspaces spanned by $\\{|n, e\\rangle, |n+1, g\\rangle\\}$ for each $n$.\n\nwithin each subspace, diagonalizing gives the **dressed states**:\n\n$$\n|+, n\\rangle = \\cos\\theta_n |n, e\\rangle + \\sin\\theta_n |n+1, g\\rangle,\n$$\n$$\n|-, n\\rangle = -\\sin\\theta_n |n, e\\rangle + \\cos\\theta_n |n+1, g\\rangle,\n$$\n\nwhere $\\tan(2\\theta_n) = 2g\\sqrt{n+1}/\\delta$ and $\\delta = \\omega_a - \\omega_c$ is the detuning. the dressed-state energies are\n\n$$\ne_{\\pm, n} = \\hbar\\omega_c(n + \\tfrac{1}{2}) \\pm \\frac{\\hbar}{2}\\sqrt{\\delta^2 + 4g^2(n+1)}.\n$$\n\nthe splitting $\\hbar\\omega_n = \\hbar\\sqrt{\\delta^2 + 4g^2(n+1)}$ between the dressed states is the **vacuum rabi splitting** when $n = 0$.\n\n## rabi oscillations\n\nif the atom starts in the excited state with $n$ photons, $|\\psi(0)\\rangle = |n, e\\rangle$, the state evolves as\n\n$$\n|\\psi(t)\\rangle = \\cos(\\omega_n t/2)|n, e\\rangle - i\\sin(\\omega_n t/2)|n+1, g\\rangle,\n$$\n\nat resonance ($\\delta = 0$), where $\\omega_n = 2g\\sqrt{n+1}$ is the **$n$-photon rabi frequency**. the atomic inversion oscillates:\n\n$$\n\\langle \\hat{\\sigma}_z(t) \\rangle = \\cos(\\omega_n t).\n$$\n\nthe $\\sqrt{n+1}$ dependence is a purely quantum effect. for a coherent state input $|\\alpha\\rangle$ with $\\bar{n} = |\\alpha|^2$ photons, different fock components oscillate at different frequencies $\\omega_n$, leading to **collapse and revival** of the rabi oscillations. the initial oscillations collapse on a timescale $t_c \\sim 1/(g\\sqrt{\\bar{n}})$ due to dephasing, then revive at $t_r \\sim 2\\pi\\sqrt{\\bar{n}}/g$ when the phases realign.\n\n[[simulation jaynes-cummings-revival]]\n\n## dispersive regime\n\nwhen the detuning is large ($|\\delta| \\gg g\\sqrt{n+1}$), the atom and field exchange only virtual excitations. perturbation theory gives an effective hamiltonian\n\n$$\n\\hat{h}_{\\text{disp}} \\approx \\hbar(\\omega_c + \\chi\\hat{\\sigma}_z)\\hat{a}^\\dagger\\hat{a} + \\frac{\\hbar(\\omega_a + \\chi)}{2}\\hat{\\sigma}_z,\n$$\n\nwhere $\\chi = g^2/\\delta$ is the **dispersive shift**. the cavity frequency shifts by $\\pm\\chi$ depending on the atomic state, and the atomic frequency shifts by $\\chi$ per photon (**ac stark shift** or **light shift**).\n\nthis regime enables **quantum non-demolition** (qnd) measurement of photon number: measuring the atomic phase shift reveals $n$ without absorbing photons. it is the foundation of circuit qed readout in superconducting qubits.\n\n## spontaneous emission\n\nin free space, a two-level atom coupled to a continuum of modes decays irreversibly at the **wightman-weisskopf** rate\n\n$$\n\\gamma = \\frac{\\omega_a^3 d^2}{3\\pi\\epsilon_0\\hbar c^3},\n$$\n\nwhere $d$ is the transition dipole moment. the excited-state population decays exponentially: $p_e(t) = e^{-\\gamma t}$.\n\nin a cavity, spontaneous emission is modified by the density of states. when the cavity is resonant, the enhanced rate is $\\gamma_{\\text{cav}} = 4g^2/\\kappa$ (where $\\kappa$ is the cavity decay rate), leading to the **purcell effect** discussed in the cavity qed topic.\n\n## big ideas\n\n* the jaynes-cummings model is exactly solvable because the total excitation number $\\hat{n} = \\hat{a}^\\dagger\\hat{a} + \\hat{\\sigma}_+\\hat{\\sigma}_-$ is conserved, breaking the full hilbert space into independent two-dimensional subspaces.\n* the $\\sqrt{n+1}$ scaling of the rabi frequency with photon number is a purely quantum effect: a classical field would drive oscillations at a fixed frequency independent of field strength at fixed amplitude, not this square-root dependence.\n* collapse and revival of rabi oscillations with a coherent field input is the clearest signature of field quantization: different photon-number components oscillate at incommensurable frequencies, dephase, then rephase with quantum periodicity.\n* the dispersive regime ($|\\delta| \\gg g$) turns the atom into a phase meter for the cavity photon number, enabling quantum non-demolition measurements without ever absorbing the photons.\n\n## what comes next\n\nthe jaynes-cummings model describes a perfect, isolated atom-cavity system. real experiments face photon loss from the cavity and atomic decay into unwanted modes. the next lesson examines cavity qed \u2014 the regime where coherent coupling competes with dissipation \u2014 and shows how this competition determines whether you observe reversible rabi oscillations or irreversible purcell-enhanced decay.\n\n## check your understanding\n\n1. the dressed states $|\\pm, n\\rangle$ are superpositions of $|n, e\\rangle$ and $|n+1, g\\rangle$ \u2014 entangled atom-field states. on resonance ($\\delta = 0$), what are the dressed states explicitly, and what does the splitting $2g\\sqrt{n+1}$ between them tell you experimentally if you sweep a probe laser through the resonance?\n2. for a coherent state input with mean photon number $\\bar{n}$, the rabi oscillations collapse on a timescale $t_c \\sim 1/(g\\sqrt{\\bar{n}})$ and revive at $t_r \\sim 2\\pi\\sqrt{\\bar{n}}/g$. show that $t_r/t_c \\sim 2\\pi\\sqrt{\\bar{n}}$, which grows with photon n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "multimode-fields",
      "lessonTitle": "Multimode Fields",
      "x": 0.13794127106666565,
      "y": 0.09894106537103653,
      "searchText": "multimode fields\n# multimode fields\n__topic 2 keywords__\n* multi-mode fields\n* thermal states-density of states\n* planck formula\n* density operators\n* lamb shift\n* casimir forces\n\n## multimode fields\nin the textbook, they use the vector potential. \nwe can do same things in different way.\nconsider the hamiltonian of multi-mode wave.\n$$\n\\begin{aligned}\n    \\hat{h}\n    &=\n    \\sum_{\\vec{k}, s}\n    \\hbar \\omega_k\n    \\left(\n        \\hat{a}^\\dag_{\\vec{k},s}\n        \\hat{a}_{\\vec{k},s}\n        +\n        \\frac{1}{2}\n    \\right)\n    \\\\&=\n    \\sum_j\n    \\hbar \\omega_j\n    \\left(\n        \\hat{n}_j\n        +\n        \\frac{1}{2}\n    \\right)\n\\end{aligned}\n$$\nhere, paramters are follows.\n* $\\omega_k=kc$ is frequency\n* $\\vec{k}$ is the wave vetor\n* $s$ is the polarization index\n\nby using index $j$ we can simply write the eigenstate of this hamiltonian.\nthe eigenstate would be product of number states.\n$$\n    \\ket{\\left\\{n_j\\right\\}}\n    =\n    \\ket{n_1, n_2, \\cdots, n_j, \\cdots}\n$$\nthis state means how many photon in each mode.\n\nwe can also think the annihilation operator of multi-mode state.\n$$\n    \\hat{a}_i\n    \\ket{\\left\\{n_j\\right\\}}\n    =\n    \\sqrt{n_i}\n    \\ket{n_1, n_2, \\cdots, n_i-1, \\cdots}\n$$\n\n## thermal fields\n### thermal light\nso far we consider the zero-temperature box.\nwe can think interaction between photon and thermal wall.\nin thermal equilibrium, the density matrix is\n$$\n    \\hat{\\rho}_\\mathrm{th}\n    =\n    \\frac\n    {e^{ -\\hat{h} / k_\\mathrm{b}t}}\n    {\\operatorname{tr}~e^{- \\hat{h} / k_\\mathrm{b}t}}\n$$\nremember that operator on napier number is symbolic representation of maclaurin \nseries.\n$$\n    e^{ -\\hat{h} / k_\\mathrm{b}t}\n    =\n    1 \n    * \n    \\frac{\\hat{h}}{k_\\mathrm{b}t}\n    +\n    \\frac{1}{2}\n    \\left(\n        \\frac{\\hat{h}}{k_\\mathrm{b}t}\n    \\right)^2\n    \\cdots\n$$\nprobability of occupying $\\ket{n}$ is\n$$\n    p_n \n    =\n    \\braket{\n        n|\n        \\hat{\\rho}_\\mathrm{th}\n        |n\n    }\n$$\nexpectation number of photon is \n$$\n    \\boxed{\n    \\left< n \\right>\n    =\n    \\sum_n\n    n p_n\n    =\n    \\frac\n    {1}\n    {e^{\\hbar \\omega / k_\\mathrm{b}t}  - 1 }\n    }\n$$\nlet's check the order of this number.\nenergy of visible light is \n$$\n    \\hbar \\omega\n    \\sim\n    1\n    \\mathrm{ev}\n$$\nenergy of room temperature is\n$$\n    k_\\mathrm{b}t\n    \\sim\n    \\frac{1}{40}\n    \\mathrm{ev}\n$$\nby putting these number into equation, we see\n$$\n    \\left< \\hat{n} \\right>\n    \\sim\n    0\n$$\nso photon does not care temperature. this is different from quantum computer.\n**temperature does not play a role in quantum optics.**\n\nthermal fluctuation is\n$$\n\\begin{aligned}\n    \\left< \\left( \\delta n \\right)^2 \\right>\n    &=\n    \\left< \\hat{n}^2 \\right>\n    -\n    \\left< \\hat{n} \\right>^2\n    \\\\&=\n    \\bar{n}\n    +\n    \\bar{n}^2\n\\end{aligned}\n$$\nthis is super-poisonian distribution. variance is larger than that of poisonian.\n### planck's radiation law\nconsider the light in the box with each length $l$ in thermal equilibrium.\nfrom boundary condition,\n$$\n    e^{i k_i x_i}\n    =\n    e^{i k_i (x_i+l)}\n$$\n$$\n    k_i \n    =\n    \\frac{2\\pi}{l} \n    m_i\n$$\nthe number of state is \n$$\n\\begin{aligned}\n    \\delta m\n    &=\n    \\delta m_x\n    \\delta m_y\n    \\delta m_z\n    \\\\&=\n    2\n    \\frac{v}{2\\pi}\n    \\delta k_x\n    \\delta k_y\n    \\delta k_z\n\\end{aligned}\n$$\nhere multiplication of 2 is number of polarization.\n\nwe can think this in spherical coordinate.\n$$\n\\begin{aligned}\n    \\mathrm{d}m\n    &=\n    \\frac{v}{4\\pi^3}\n    \\mathrm{d}^3 k\n    \\\\&=\n    \\frac{v}{4\\pi^3}\n    k^2\n    \\mathrm{d} k\n    \\mathrm{d} \\omega\n    \\\\&=\n    \\frac{v}{4\\pi^3}\n    \\frac{\\omega^2}{c^3}\n    \\mathrm{d} \\omega\n    \\mathrm{d} \\omega\n\\end{aligned}\n$$\ndensity of state is \n$$\n    \\rho(\\omega)\n    =\n    \\frac{\\omega^2}{\\pi^2 c^3}\n$$\ndimension of density of state is \n$\\frac{\\text{\\# of state}}{\\text{frequency} \\cdot \\text{volume}}$\n\nenergy density become\n$$\n\\begin{aligned}\n    \\bar{u}\n    &=\n    \\hbar \\omega \\bar{n} \\rho (\\omega)\n    \\\\&=\n    \\frac\n    {\\hbar\\omega}\n    {e^{\\hbar\\omega/k_\\mathrm{b}t} - 1}\n    \\frac\n    {\\omega^2}\n    {\\pi^2 c^3}\n\\end{aligned}\n$$\n\n## big ideas\n\n* a realistic field is a superposition of infinitely many modes; quantizing each independently gives a multimode fock state labeled by how many photons occupy each mode.\n* thermal light at room temperature contains essentially zero photons per visible mode \u2014 temperature is irrelevant for optical quantum optics in a way it is not for microwave quantum computing.\n* planck's blackbody law emerges directly from the quantum statistics of photons: each mode contributes its bose-einstein average $\\bar{n} = 1/(e^{\\hbar\\omega/k_\\mathrm{b}t}-1)$ weighted by the density of states.\n* thermal photon fluctuations are super-poissonian, with variance $\\bar{n} + \\bar{n}^2$ \u2014 noisier than a coherent laser.\n\n## what comes next\n\nwe have seen that the quantum vacuum is not empty: every mode carries zero-point energy $\\frac{1}{2}\\hbar\\omega$. the next lesson takes this seriously and asks what observable consequences that vacuum energy has \u2014 because it turns out you can measure it directly in the lab, in phenomena ranging from spontaneous emission shifts to forces between uncharged metal plates.\n\n## check your understanding\n\n1. at optical frequencies ($\\hbar\\omega \\sim 1$ ev) and room temperature ($k_\\mathrm{b}t \\sim \\frac{1}{40}$ ev), the mean photon number is essentially zero. what does this tell you about the usefulness of cooling an optical experiment versus a microwave experiment for eliminating thermal photons?\n2. thermal light has $\\langle(\\delta n)^2\\rangle = \\bar{n} + \\bar{n}^2$. explain why the $\\bar{n}^2$ term is a signature of photon bunching, and what it implies about the probability of detecting two photons close together in time.\n3. the density of states $\\rho(\\omega) = \\omega^2/(\\pi^2 c^3)$ grows as $\\omega^2$. what would happen to the total energy of blackbody radiation if the photon were not subject to bose-einstein statistics (suppose instead it obeyed boltzmann statistics)? this "
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "phase-space-representations",
      "lessonTitle": "Phase-Space Representations",
      "x": 0.054581064730882645,
      "y": 0.04784899950027466,
      "searchText": "phase-space representations\n# phase-space representations\n\n> *three quasi-probability distributions \u2014 q, p, and wigner \u2014 each paint a different portrait of the same quantum state. together they form the complete toolkit for visualizing quantum light in phase space.*\n\n## coherent and number states in phase space\n\nrecall the quadrature operators:\n$$\n    \\hat{x_1}\n    =\n    \\frac{1}{2}\n    \\left(\n        \\hat{a} + \\hat{a}^\\dag\n    \\right), \\qquad\n    \\hat{x_2}\n    =\n    \\frac{1}{2i}\n    \\left(\n        \\hat{a} - \\hat{a}^\\dag\n    \\right).\n$$\n\nfor a coherent state $|\\alpha\\rangle$, the quadrature expectations are $\\langle\\hat{x}_1\\rangle = \\text{re}(\\alpha)$ and $\\langle\\hat{x}_2\\rangle = \\text{im}(\\alpha)$, with equal variances $\\langle(\\delta\\hat{x}_{1,2})^2\\rangle = \\frac{1}{4}$. on the quadrature plane, a coherent state is a fuzzy dot of radius $\\frac{1}{2}$ centered at $\\alpha$, with distance from the origin equal to $\\sqrt{\\bar{n}}$.\n\na number state $|n\\rangle$ has $\\langle\\hat{x}_{1,2}\\rangle = 0$ and variances $\\frac{1}{2}(n + \\frac{1}{2})$ \u2014 a ring centered at the origin with no angular localization. the number state knows exactly how many photons it has but nothing about the oscillation phase.\n\n[[simulation phase-space-states]]\n\nas $|\\alpha| \\to \\infty$ for a coherent state, the angular uncertainty $\\delta\\theta \\to 0$ \u2014 very bright coherent light has a well-defined phase, recovering classical behavior.\n\n---\n\n## the husimi q function\n\nthe simplest phase-space distribution is the **husimi q function**:\n\n$$\nq(\\alpha) = \\frac{1}{\\pi}\\langle\\alpha|\\hat{\\rho}|\\alpha\\rangle.\n$$\n\nthe q function is always non-negative (it is the overlap of the state with a coherent state, hence a probability). it gives a smoothed picture of the quantum state but never reveals sub-vacuum structure because the coherent-state projection acts as a gaussian blur.\n\n---\n\n## the glauber-sudarshan p function\n\n### definition\n\nthe coherent states form an overcomplete basis, so any density operator can be expanded as\n\n$$\n    \\hat{\\rho}\n    =\n    \\int \\mathrm{d}^2 \\alpha\\,\n    p(\\alpha)\n    |\\alpha\\rangle\n    \\langle\\alpha|.\n$$\n\nthe weight function $p(\\alpha)$ is the **glauber-sudarshan p function**. it satisfies $\\int p(\\alpha)\\,\\mathrm{d}^2\\alpha = 1$, so it looks like a probability distribution \u2014 but it need not be non-negative.\n\n### computing p via fourier transform\n\nusing the coherent-state overlap $\\langle\\beta|\\alpha\\rangle = e^{-|\\alpha|^2/2 - |\\beta|^2/2 + \\beta^*\\alpha}$, one obtains\n\n$$\n    p(\\alpha)\n    =\n    \\frac{e^{|\\alpha|^2}}{\\pi^2}\n    \\int \\mathrm{d}^2 u\\,\n    e^{|u|^2}\n    \\langle -u|\\hat{\\rho}|u\\rangle\\,\n    e^{u^*\\alpha - \\alpha^* u}.\n$$\n\n### example: coherent state\n\nfor a pure coherent state $|\\beta\\rangle$, the p function is a dirac delta: $p(\\alpha) = \\delta^2(\\alpha - \\beta)$. this is a single point in phase space \u2014 maximally classical.\n\n### non-classicality criterion\n\nthe p function is what separates classical from non-classical light. if $p(\\alpha)$ can be interpreted as a genuine (non-negative, non-singular) probability distribution, the state is classical. if $p$ is negative or more singular than a delta function, the state has no classical analogue.\n\nfor a **number state** $|n\\rangle$ with $n \\geq 1$, the p function involves derivatives of the delta function and cannot be interpreted as a probability distribution. this mathematical pathology is the fingerprint of a genuinely quantum state: a number state has zero photon-number uncertainty, which is impossible for any classical or semi-classical light source. no mixture of coherent states \u2014 no matter how cleverly weighted \u2014 can reproduce the sharp, deterministic photon count of a fock state.\n\n### the optical equivalence theorem\n\nfor any normally ordered operator $\\hat{g}^{(n)}(\\hat{a}, \\hat{a}^\\dag) = \\sum_{n,m} c_{nm}\\,{\\hat{a}^\\dag}^n \\hat{a}^m$:\n\n$$\n    \\langle \\hat{g}^{(n)} \\rangle\n    =\n    \\int p(\\alpha)\\,\\mathrm{d}^2\\alpha\\,\n    g^{(n)}(\\alpha, \\alpha^*).\n$$\n\nreplace operators with complex numbers, weight by $p(\\alpha)$, and integrate \u2014 just as in classical statistical mechanics.\n\n---\n\n## the wigner function\n\n### definition\n\nthe **wigner function** is defined as\n\n$$\n    w(q, p)\n    =\n    \\frac{1}{2\\pi\\hbar}\n    \\int_{-\\infty}^\\infty\n    \\mathrm{d}x\\,\n    \\langle q + \\tfrac{x}{2}|\\hat{\\rho}|q - \\tfrac{x}{2}\\rangle\\,\n    e^{-ipx/\\hbar}.\n$$\n\nthe wigner function is always real, normalized to one, and its marginals give the correct position and momentum probability densities:\n\n$$\n\\int w(q,p)\\,\\mathrm{d}p = |\\psi(q)|^2, \\qquad \\int w(q,p)\\,\\mathrm{d}q = |\\tilde{\\psi}(p)|^2.\n$$\n\n### negativity as non-classicality\n\nunlike the q function, the wigner function **can be negative**. negative values are a direct signature of non-classicality:\n\n* **coherent states**: non-negative gaussian $w$ \u2014 classical behavior.\n* **fock states** ($n \\geq 1$): oscillating $w$ with $w(0,0) < 0$ \u2014 no classical analogue.\n* **cat states**: interference fringes between two coherent-state lobes \u2014 visible only in $w$, invisible in $q$.\n\nthe wigner function sits between the p function (too singular for non-classical states) and the q function (always positive but smeared): it gives the sharpest accessible picture of quantum phase space.\n\n[[simulation phase-space-explorer]]\n\n---\n\n## big ideas\n\n* a coherent state $|\\alpha\\rangle$ is a fuzzy dot centered at $\\alpha$ in the quadrature plane with uncertainty radius $\\frac{1}{2}$; a number state $|n\\rangle$ is a ring centered at the origin \u2014 it knows how many photons it has but not where in the oscillation cycle it is.\n* the p function classifies light: if $p(\\alpha) \\geq 0$ everywhere (and is no more singular than a delta function), the state is classical; if $p$ goes negative or becomes hyper-singular, the state is genuinely quantum.\n* the optical equivalence theorem reduces quantum expectation values to classical-looking integrals weighted by $p(\\alpha)$, but the \"probability\" weights can fail to be probabilities \u2014 and that "
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "quantization-single-mode",
      "lessonTitle": "Quantization of Single Mode Field",
      "x": 0.09326527267694473,
      "y": 0.13889223337173462,
      "searchText": "quantization of single mode field\n# quantization of single mode field\n\n## why do we need to quantize?\n* so far, we learned that electron in high-energy state decays back to ground state with the emission of light.\n* however, according to the schr\u00f6dinger equation $$\\hat{h} \\ket{\\psi_m} = e_m \\ket{\\psi_m}$$, electron is stable in excited state i.e. *electron stays in high energy state*.\n* how do the decay? something goes wrong. why does spontaneous emission happen?\n* the answer is **fluctuations**. \n* the new question is *what kind of fluctuations do they have*?\n\n## can classical electromagnetism describe these fluctuations?\nin classical electromagnetic, energy of electron in electric field $e(t)$ is \n$$ e \\cdot e(t) $$\nthus, when there is no light, there is no fluctuation.\nhowever, it is known that even without light, there is a fluctuation i.e. quantum\nmechanical fluctuation.\nwe need to use quantum mechanics.\n\n## quantization of single mode field\n### motivation\nlet's start our quantization of electromagnetic waves.\nthe quantity we quantize is **energy**.\n### situation\nimagine a three-dimensional box and there are perfectly conducting walls at \n$z=0$ and $z=l$. \nwe consider **single mode field** i.e. we only take care one mode.\nboundary conditions are\n* $\\vec{e}(z=0)=0$\n* $\\vec{e}(z=l)=0$\n### energy\nelectromagnetic energy is\n$$\n    h \n    = \n    \\frac{1}{2} \n    \\int \\mathrm{d} v\n    \\left[\n        \\epsilon_0 \\left| \\vec{e}(z, t) \\right|^2\n        + \n        \\frac{1}{\\mu_0} \\left| \\vec{b}(z, t) \\right|^2\n    \\right]^2\n$$\n### electric field\nfrom boundary condition, we can determine the spatial part of electric field.\n$$\n    \\vec{e}(z, t)\n    =\n    a \\cdot q(t) \\cdot \\sin(kz) \\cdot \\vec{e_x}\n$$\nhere, parameters are described as follows.\n* $a$ is a constant and we can decide. \n* $q(t)$ is the time dependent part and we will see why we use $q$ as symbol.\n* $k=\\frac{2\\pi}{\\lambda}=\\frac{\\pi}{l}m$ is a wave vector and $m$ is integer ($m=0, \\pm1, \\pm2, \\cdots$).\n* $\\vec{e_x}$ is an unit vector of $x$-direction (we decide the electric field has component on $x$ direction).\nwe decide $a$ as follows.\n$$\n    a\n    =\n    \\sqrt{\\frac{2\\omega^2}{\\epsilon_0 v}}\n$$\nhere, $\\omega=kc$ is frequency.\n### magnetic field\nso, we got electric field. how can we get magnetic field? we use **maxwell equations**.\n$$\n    \\vec{\\nabla} \\times \\vec{b}\n    =\n    \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{e}}{\\partial t}\n$$\nto calculate left-hand side term, we can use the table.\n$$\n    \\vec{\\nabla} \\times \\vec{b}\n    =\n    \\begin{vmatrix}\n        \\vec{e_x} & \\vec{e_y} & \\vec{e_z}\\\\\n        \\partial_x & \\partial_y & \\partial_z\\\\\n        b_x & b_y & b_z\\\\\n    \\end{vmatrix}\n$$\nphysicist can typically notice that\nwe need to take care only $y$ direction of magnetic field..\nrecall that we set the electric field along the $x$ direction.\nthus, \n$$\n    \\vec{\\nabla} \\times \\vec{b}\n    =\n    -\\frac{\\partial b_y}{\\partial z} \\vec{e_x}\n$$\nthe right-hand side is straightforward.\n$$\n    \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{e}}{\\partial t}\n    =\n    a \\cdot \\dot{q}(t) \\cdot \\sin(kz) \\cdot \\vec{e_x}\n$$\nthen, maxwell equation become\n$$\n    -\\frac{\\partial b_y}{\\partial z} \\vec{e_x}\n    =\n    a \\cdot \\dot{q}(t) \\cdot \\sin(kz) \\cdot \\vec{e_x}\n$$\nthus, \n$$\n\\begin{aligned}\n    \\vec{b}(z, t)\n    &=\n    \\frac{mu_0 \\epsilon_0}{k} \\cdot \n    a \\cdot \\dot{q}(t) \\cdot \\cos(kz) \\cdot \\vec{e_y}\n    \\\\&=\n    \\frac{mu_0 \\epsilon_0}{k} \\cdot \n    a \\cdot p(t) \\cdot \\cos(kz) \\cdot \\vec{e_y}\n\\end{aligned}\n$$\nhere, we introduced $p(t)=\\dot{q}(t)$. the reason for this choice of symbol will become clear shortly.\n### finally, let's calculate energy!\nwe have electric field and magnetic field so we can calculate energy.\n\n[[simulation wigner-number-state]]\n\nhowever, as you can see the difficulty is $\\int \\mathrm{d} v$. \nhow do we process this?\nwe introduce new variable cross section $\\sigma$.\n$$\n    v \n    =\n    \\sigma \\cdot l\n$$\nenergy function become\n$$\n    h \n    = \n    \\frac{1}{2} \n    \\sigma\n    \\int_0^l \\mathrm{d} z\n    \\left[\n        \\epsilon_0 \\left| \\vec{e}(z, t) \\right|^2\n        + \n        \\frac{1}{\\mu_0} \\left| \\vec{b}(z, t) \\right|^2\n    \\right]^2\n$$\nby using the math trick\n$$\n\\begin{aligned}\n    \\int_0^l \\mathrm{d}z \\sin^2(kz) \n    &=\n    \\int_0^l \\mathrm{d}z \\sin^2\\left(\\frac{\\pi}{l}mz\\right) \n    \\\\&=\n    \\frac{l}{2}\n\\end{aligned}\n$$\nwe see the simple form of energy function.\n$$\n    h\n    =\n    \\frac{1}{2} \\omega^2 q^2(t)\n    +\n    \\frac{1}{2} p^2(t)\n$$\nas you notice this energy function is same as that of unit mass classical \nharmonic oscillator.\nand our way of symbolizing make sense.\n### warning\ncan we say $q$ as position of photon and $p$ as momentum of photon? no, we can't.\nthis is just mathematical formulation and analogy to classical harmonic oscillator.\n### quantization\nnow we return to the quantum description.\nto quantize the energy function, we need to use operators.\nwe introduce three operators $\\hat{h}, \\hat{q}, \\hat{p}$.\n$$\n    \\hat{h}\n    =\n    \\frac{1}{2} \\omega^2 \\hat{q}^2(t)\n    +\n    \\frac{1}{2} \\hat{p}^2(t)\n$$\nto quantize we use commutation relation to operators $\\hat{q}$ and $\\hat{p}$.\n$$\n    \\left[\n        \\hat{q}, \\hat{p}\n    \\right]\n    =\n    i\\hbar\n$$\nnotice that $\\hat{q}$ and $\\hat{p}$ are hermitian i.e. observable.\n### new operators\nin addition to these operators, we introduce other two non-hermitian operators.\nnamely, **annihilation operator** $\\hat{a}$ \nand **creation operator** $\\hat{a}^\\dag$ .\n$$\n    \\hat{a} \n    =\n    \\frac{1}{2\\hbar\\omega}\n    \\left(\n        \\omega \\hat{q} \n        + \n        i\\hat{p} \n    \\right)\n$$\n$$\n    \\hat{a}^\\dag \n    =\n    \\frac{1}{2\\hbar\\omega}\n    \\left(\n        \\omega \\hat{q} \n        * \n        i\\hat{p} \n    \\right)\n$$\nthese operators satisfy the commutation relation\n$$\n    \\left[\n        \\hat{a}, \\hat{a}^\\dag\n    \\right]\n    =\n    1\n$$\nthe result is simply $1$, which greatly simplifies calculations.\n### quantized electric field, magnetic fields and hamiltonian using $\\hat{a}$ and $\\hat{a}^\\dag"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "quantum-fluctuations",
      "lessonTitle": "Quantum Fluctuations and Quadrature Operators",
      "x": 0.08585862070322037,
      "y": 0.08882088959217072,
      "searchText": "quantum fluctuations and quadrature operators\n# quantum fluctuations and quadrature operators\n\n## the vacuum is not empty\n\nthe most startling prediction of quantum field theory is that \"nothing\" is not silent. strip away every photon from a cavity \u2014 cool it, shield it, pump it down to the absolute quantum ground state \u2014 and the electric field is still fluctuating. not because of stray photons leaking in, and not because of thermal noise, but because the uncertainty principle forbids the field from sitting perfectly still. the vacuum state $|0\\rangle$ has zero average field yet non-zero field variance. those irreducible fluctuations drive the casimir effect, set the noise floor for every optical measurement, and determine the fundamental sensitivity limit of interferometers like ligo.\n\n## quantum fluctuations\ntime average of electric field is\n$$\n\\begin{aligned}\n    \\left<\n        \\hat{\\vec{e}}\n    \\right>\n    &=\n    \\braket{n|\\hat{\\vec{e}}|n}\n    \\\\&=\n    \\sqrt{\n        \\frac{\\hbar \\omega}{\\epsilon_0 v}\n    }\n    \\sin (kz)\n    \\braket{n|\\left(\\hat{a}+\\hat{a}^\\dag\\right)|n}\n    \\\\&=\n    0\n\\end{aligned}\n$$\ni see. how about variance?\n$$\n\\begin{aligned}\n    \\left<\n        \\hat{\\vec{e}}^2\n    \\right>\n    &=\n    \\braket{n|\\hat{\\vec{e}}^2|n}\n    \\\\&=\n    \\frac{\\hbar \\omega}{\\epsilon_0 v}\n    \\sin^2 (kz)\n    \\braket{\n        n|\n        \\left(\n        \\hat{a}^2 + {\\hat{a}^\\dag}^2 + \\hat{a}\\hat{a}^\\dag + \\hat{a}^\\dag\\hat{a}\n        \\right)\n        |n\n    }\n    \\\\&=\n    \\frac{\\hbar \\omega}{\\epsilon_0 v}\n    \\sin^2 (kz)\n    \\left(\n        n+\\frac{1}{2}\n    \\right)\n\\end{aligned}\n$$\nwow! even in the vacuum state ($n=0$), electric field fluctuates!!\n\n## quadrature operators\nsimilar to $\\hat{q}$ and $\\hat{p}$ operators, we introduce quadrature operators\n$\\hat{x_1}$ and $\\hat{x_2}$..\n$$\n    \\hat{x_1}\n    =\n    \\frac{\\hat{a} + \\hat{a}^\\dag}{2}\n    \\propto\n    \\hat{q}(t)\n$$\n$$\n    \\hat{x_2}\n    =\n    -i\n    \\frac{\\hat{a} - \\hat{a}^\\dag}{2}\n    \\propto\n    \\hat{p}(t)\n$$\n$$\n    \\left[\n        \\hat{x_1}, \\hat{x_2}\n    \\right]\n    =\n    \\frac{1}{2}\n$$\nnotice that quadrature operators are observable and homodyne technique is used for\nthat.\nand these operators does not commute. this is because heisenberg's uncertainty.\n\nwe can rewrite electric field with quadrature operators.\n$$\n\\begin{aligned}\n    \\hat{e_x}\n    &=\n    \\mathcal{e}_0\n    \\left[\n        \\hat{a}e^{-i\\omega t}\n        +\n        \\hat{a}^\\dag e^{i\\omega t}\n    \\right]\n    \\sin (kz)\n    \\\\&=\n    \\mathcal{e}_0\n    \\sin (kz)\n    \\left[\n        \\left(\n            \\hat{a}\n            +\n            \\hat{a}^\\dag\n        \\right)\n        \\cos \\omega t\n        -\n        i\n        \\left(\n            \\hat{a}\n            -\n            \\hat{a}^\\dag\n        \\right)\n        \\sin \\omega t\n    \\right]\n    \\\\&=\n    2\n    \\mathcal{e}_0\n    \\sin (kz)\n    \\left[\n        \\hat{x_1}\n        \\cos \\omega t\n        +\n        \\hat{x_2}\n        \\sin \\omega t\n    \\right]\n\\end{aligned}\n$$\nimportant staff of quadrature operators is their uncertainty.\n$$\n    \\left<\n        \\left(\n            \\delta \\hat{x}_1\n        \\right)^2\n    \\right>\n    \\left<\n        \\left(\n            \\delta \\hat{x}_2\n        \\right)^2\n    \\right>\n    \\ge\n    \\frac{1}{4}\n    \\left|\n        \\left<\n            \\left[\n                \\hat{x}_1, \\hat{x}_2\n            \\right]\n        \\right>\n    \\right|^2\n    \\ge\n    \\frac{1}{16}\n$$\n\n## big ideas\n\n* a number state has zero average electric field but non-zero variance \u2014 the field is fluctuating even when its mean is completely silent.\n* the quadrature operators $\\hat{x}_1$ and $\\hat{x}_2$ decompose the field into two orthogonal oscillation components; measuring one unavoidably disturbs the other.\n* the uncertainty product $\\langle(\\delta\\hat{x}_1)^2\\rangle\\langle(\\delta\\hat{x}_2)^2\\rangle \\geq \\frac{1}{16}$ is not a limitation of our instruments \u2014 it is a structural feature of the quantum field.\n\n## what comes next\n\nwe now know that number states fluctuate in both quadratures, and that those fluctuations cannot be reduced below the uncertainty bound. the next lesson asks: are there states of the field that look as classical as possible \u2014 states that achieve the minimum uncertainty and whose average field behaves like a classical sine wave? the answer is yes, and they are called coherent states.\n\n## check your understanding\n\n1. for a number state $|n\\rangle$ the mean electric field is zero, yet the mean-square field grows with $n$. explain physically what is fluctuating and why more photons produce larger fluctuations rather than a more definite field value.\n2. the quadrature operators are defined as specific linear combinations of $\\hat{a}$ and $\\hat{a}^\\dagger$. why do we call them quadratures, and what is the physical interpretation of measuring $\\hat{x}_1$ versus $\\hat{x}_2$?\n3. the heisenberg uncertainty product $\\langle(\\delta\\hat{x}_1)^2\\rangle\\langle(\\delta\\hat{x}_2)^2\\rangle \\geq \\frac{1}{16}$ holds for all states. which states saturate the inequality, and what special property does that saturation imply?\n\n## challenge\n\nthe vacuum state $|0\\rangle$ satisfies $\\hat{a}|0\\rangle = 0$. use this to prove directly that $\\langle(\\delta\\hat{x}_1)^2\\rangle = \\langle(\\delta\\hat{x}_2)^2\\rangle = \\frac{1}{4}$ for the vacuum, confirming it is a minimum-uncertainty state with equal noise in both quadratures. then construct any state of the form $|\\psi\\rangle = c_0|0\\rangle + c_1|1\\rangle$ (normalized) and compute both quadrature variances. under what conditions on $c_0, c_1$ does the state achieve minimum uncertainty?\n"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "quantum-measurements",
      "lessonTitle": "Quantum Measurements",
      "x": 0.16478481888771057,
      "y": 0.06269019097089767,
      "searchText": "quantum measurements\n# quantum measurements\n\n## measurement in quantum mechanics\n\nin quantum optics, measurement plays a fundamental role: the act of detection irreversibly alters the quantum state of light. the measurement formalism connects the abstract quantum state to experimentally observable quantities like photon counts, quadrature values, and correlation functions.\n\na general quantum measurement is described by a set of **measurement operators** $\\{\\hat{m}_m\\}$ satisfying $\\sum_m \\hat{m}_m^\\dagger \\hat{m}_m = \\hat{i}$. the probability of outcome $m$ given state $\\hat{\\rho}$ is\n\n$$\np(m) = \\operatorname{tr}[\\hat{m}_m^\\dagger \\hat{m}_m \\hat{\\rho}],\n$$\n\nand the post-measurement state is $\\hat{\\rho}_m = \\hat{m}_m \\hat{\\rho} \\hat{m}_m^\\dagger / p(m)$.\n\n## projective vs. generalized measurements\n\n**projective (von neumann) measurements** use orthogonal projectors $\\hat{\\pi}_m = |m\\rangle\\langle m|$ satisfying $\\hat{\\pi}_m \\hat{\\pi}_n = \\delta_{mn}\\hat{\\pi}_m$. photon number detection is a projective measurement in the fock basis $\\{|n\\rangle\\}$.\n\n**generalized measurements** (povms) allow non-orthogonal outcomes and describe realistic detectors. a **positive operator-valued measure** consists of positive operators $\\hat{e}_m \\geq 0$ with $\\sum_m \\hat{e}_m = \\hat{i}$, without requiring orthogonality. heterodyne detection is a povm with elements $\\hat{e}_\\alpha = \\frac{1}{\\pi}|\\alpha\\rangle\\langle\\alpha|$ that projects onto coherent states.\n\n## photon counting\n\nideal **photon-number-resolving** (pnr) detectors measure the fock state projectors. for a state $\\hat{\\rho}$, the probability of detecting $n$ photons is $p(n) = \\langle n|\\hat{\\rho}|n\\rangle$.\n\nreal single-photon detectors (avalanche photodiodes, superconducting nanowire detectors) are typically **threshold detectors**: they distinguish \"no click\" from \"one or more clicks\" but cannot resolve the exact photon number. the povm elements are\n\n$$\n\\hat{e}_0 = |0\\rangle\\langle 0|, \\qquad \\hat{e}_{\\text{click}} = \\hat{i} - |0\\rangle\\langle 0|,\n$$\n\nwith detector efficiency $\\eta < 1$ modeled as a beam splitter loss before an ideal detector.\n\ntransition-edge sensors and superconducting nanowire arrays can resolve photon numbers up to $\\sim 10{-}20$, enabling direct measurement of photon statistics and wigner function negativity.\n\n## quantum state tomography\n\n**quantum state tomography** reconstructs the full density matrix $\\hat{\\rho}$ from a set of measurements on many identically prepared copies. for optical fields, this is achieved by:\n\n1. **homodyne tomography**: measure the quadrature $\\hat{x}_\\theta$ for many phase angles $\\theta$. the marginal distributions $p(x_\\theta)$ are projections of the wigner function. the state is reconstructed via the inverse radon transform (same mathematics as medical ct scanning).\n\n2. **maximum likelihood estimation**: numerically find the density matrix that maximizes the likelihood of the observed data, subject to the constraints that $\\hat{\\rho}$ is positive semidefinite with unit trace.\n\nthe wigner function provides a complete phase-space representation:\n\n$$\nw(x, p) = \\frac{1}{\\pi\\hbar}\\int_{-\\infty}^{\\infty} \\langle x + y|\\hat{\\rho}|x - y\\rangle e^{-2ipy/\\hbar} dy.\n$$\n\nnegative values of $w$ indicate non-classical states (e.g., fock states, cat states).\n\n[[simulation homodyne-tomography]]\n\n## quantum non-demolition measurements\n\na **quantum non-demolition** (qnd) measurement extracts information about an observable without disturbing it. for photon number, the key requirement is $[\\hat{n}, \\hat{h}_{\\text{int}}] = 0$: the interaction used for measurement commutes with the measured observable.\n\nin cavity qed, the dispersive interaction shifts the atomic phase by an amount proportional to $n$ without absorbing photons. by sending probe atoms through the cavity and measuring their phase shift, one can determine $n$ repeatedly and observe quantum jumps as individual photons are lost.\n\nthis was demonstrated by haroche's group, who tracked the photon number in a microwave cavity decaying from $n = 7$ to $n = 0$ one photon at a time, directly observing the quantum trajectory of the field state.\n\n## back-action and the uncertainty principle\n\nevery measurement has **back-action**: gaining information about one observable increases uncertainty in conjugate observables. for homodyne detection of $\\hat{x}$, the back-action appears as increased noise in $\\hat{p}$.\n\nthe **standard quantum limit** (sql) for monitoring the position of a free mass is\n\n$$\n\\delta x_{\\text{sql}} = \\sqrt{\\frac{\\hbar t}{2m}},\n$$\n\narising from the trade-off between measurement imprecision and radiation-pressure back-action. beating the sql requires correlating the measurement and back-action noise, achievable with squeezed light or back-action evasion techniques.\n\n## big ideas\n\n* measurement in quantum optics is not passive reading \u2014 every detection event irreversibly alters the state of the field, and the post-measurement state is as important as the probability of each outcome.\n* povms (positive operator-valued measures) capture the full range of realistic detectors: threshold detectors, homodyne receivers, and heterodyne detectors each correspond to a different set of povm elements.\n* quantum state tomography reconstructs the full density matrix from quadrature histograms using the inverse radon transform \u2014 the same mathematics that reconstructs a ct scan from x-ray projections.\n* back-action is unavoidable: learning about $\\hat{x}$ necessarily disturbs $\\hat{p}$, and beating the standard quantum limit requires cleverly correlating that disturbance rather than trying to eliminate it.\n\n## what comes next\n\nthis is where the quantum optics journey culminates. the thread that began with a single field mode in a box has led us through vacuum fluctuations, coherent and squeezed states, photon statistics, atom-field coupling, and cavity qed \u2014 all converging on the fundamental question of how information is extracted from a quantum field. the "
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "single-photon-experiments",
      "lessonTitle": "Single Photon Experiments",
      "x": 0.15628820657730103,
      "y": 0.012760458514094353,
      "searchText": "single photon experiments\n# single photon experiments\n\n## the beam splitter\n\na **beam splitter** is the fundamental optical element for single-photon experiments. it partially reflects and partially transmits incoming light. quantum mechanically, a lossless beam splitter with reflectivity $r$ and transmissivity $t = 1 - r$ transforms the input mode operators as\n\n$$\n\\begin{pmatrix} \\hat{a}_{\\text{out}} \\\\ \\hat{b}_{\\text{out}} \\end{pmatrix}\n=\n\\begin{pmatrix} t & r \\\\ r' & t' \\end{pmatrix}\n\\begin{pmatrix} \\hat{a}_{\\text{in}} \\\\ \\hat{b}_{\\text{in}} \\end{pmatrix},\n$$\n\nwhere $|t|^2 + |r|^2 = 1$ and the matrix must be unitary to preserve the commutation relations $[\\hat{a}, \\hat{a}^\\dagger] = 1$. for a symmetric beam splitter: $t = t' = \\cos\\theta$ and $r = -r'^* = i\\sin\\theta$.\n\n## single photon at a beam splitter\n\nwhen a single photon $|1\\rangle_a |0\\rangle_b$ enters one port of a 50:50 beam splitter, the output state is\n\n$$\n|1\\rangle_a |0\\rangle_b \\xrightarrow{\\text{bs}} \\frac{1}{\\sqrt{2}}\\left(|1\\rangle_c |0\\rangle_d + i|0\\rangle_c |1\\rangle_d\\right).\n$$\n\nthe photon exits through one port or the other but is never split: a single detector click occurs in output $c$ or output $d$, never both simultaneously. this was demonstrated by grangier, roger, and aspect (1986), confirming the particle nature of single photons.\n\nthe joint detection probability at both outputs is $p_{cd} = 0$ for a true single photon, while for a classical wave it would be $p_{cd} > 0$. the **anticorrelation parameter** $\\alpha = p_{cd}/(p_c p_d)$ equals zero for a single photon, providing a clean test of the quantum nature of light.\n\n## coherent state at a beam splitter\n\nfor a coherent state input $|\\alpha\\rangle_a |0\\rangle_b$, the output is a product of coherent states:\n\n$$\n|\\alpha\\rangle_a |0\\rangle_b \\xrightarrow{\\text{bs}} |t\\alpha\\rangle_c |r\\alpha\\rangle_d.\n$$\n\nunlike the single-photon case, the outputs are uncorrelated and each port independently contains a coherent state. this factorization property is unique to coherent states and reflects their classical nature.\n\na 50:50 beam splitter splits a coherent state $|\\alpha\\rangle$ into two coherent states $|\\alpha/\\sqrt{2}\\rangle$, with the total mean photon number preserved: $|\\alpha/\\sqrt{2}|^2 + |\\alpha/\\sqrt{2}|^2 = |\\alpha|^2$.\n\n## two-photon interference: hong-ou-mandel effect\n\nwhen **two indistinguishable single photons** enter a 50:50 beam splitter from different ports, they always exit together through the same port:\n\n$$\n|1\\rangle_a |1\\rangle_b \\xrightarrow{\\text{bs}} \\frac{1}{\\sqrt{2}}\\left(|2\\rangle_c |0\\rangle_d - |0\\rangle_c |2\\rangle_d\\right).\n$$\n\nthe coincidence rate at the two outputs drops to zero, known as the **hong-ou-mandel (hom) dip**. this two-photon quantum interference effect has no classical analogue and was first demonstrated by hong, ou, and mandel (1987).\n\nthe dip visibility depends on the indistinguishability of the photons. if the photons differ in frequency, polarization, or arrival time, the dip becomes shallower. the hom effect is the basis for **bell-state measurements** and linear-optical quantum computing schemes.\n\n## mach-zehnder interferometer\n\na **mach-zehnder interferometer** consists of two beam splitters with a phase shift $\\phi$ in one arm. for a single-photon input, the detection probabilities at the two outputs are\n\n$$\np_c = \\cos^2(\\phi/2), \\qquad p_d = \\sin^2(\\phi/2).\n$$\n\nthis demonstrates single-photon interference: the photon interferes with itself as it traverses both paths simultaneously in superposition. the which-path information determines whether interference is observed, illustrating the complementarity principle.\n\nfor $n$-photon states or squeezed light inputs, the interferometer can achieve phase sensitivity beyond the **shot-noise limit** $\\delta\\phi \\sim 1/\\sqrt{n}$, approaching the **heisenberg limit** $\\delta\\phi \\sim 1/n$.\n\n## big ideas\n\n* a single photon at a beam splitter never splits: one detector clicks, never both. this anticorrelation ($g^{(2)}(0) = 0$ across the two output ports) is proof that light comes in discrete quanta.\n* a coherent state, by contrast, gives independent poissonian clicks at both outputs \u2014 it is the closest classical behavior possible, with no photon-photon correlations.\n* the hong-ou-mandel effect is bunching with a vengeance: two indistinguishable photons entering separate ports always exit together, making the coincidence rate vanish. the hom dip is the most sensitive tool we have for measuring photon indistinguishability.\n* a mach-zehnder interferometer reveals wave-particle duality in the starkest form: close the interferometer and the photon shows wave-like interference; ask which path it took and the fringes disappear.\n\n## what comes next\n\nsingle-photon experiments demonstrate the quantum limits of detecting and routing light. the next lesson zooms out to ask how precisely we can measure a phase with many photons \u2014 and how quantum resources like squeezing and entangled states push the fundamental limits of interferometric sensitivity far beyond what classical light can achieve.\n\n## check your understanding\n\n1. in the single-photon beam-splitter experiment, the two output detectors never click simultaneously ($p_{cd} = 0$). for a classical wave of the same average intensity, what would $p_{cd}$ be, and why does the quantum result differ so dramatically?\n2. the hong-ou-mandel dip depth (visibility of the zero-coincidence feature) measures photon indistinguishability. if two photons are identical in every degree of freedom, visibility = 1. what would reduce the visibility, and why does any partial distinguishability (in frequency, polarization, or arrival time) restore some coincidences?\n3. in a mach-zehnder interferometer with a single photon, the detection probabilities at the two outputs are $\\cos^2(\\phi/2)$ and $\\sin^2(\\phi/2)$. there is no \"which-path\" information available at either output. if you inserted a detector inside the interferometer to determine which path"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "squeezed-states",
      "lessonTitle": "Squeezed States",
      "x": 0.08262771368026733,
      "y": 0.005228184629231691,
      "searchText": "squeezed states\n# squeezed states\n\n> *coherent states share the vacuum's noise equally between quadratures. squeezed states break that symmetry \u2014 reducing noise below the vacuum in one quadrature at the expense of the other \u2014 and this redistribution is the key to [quantum-enhanced interferometry](./interferometry).*\n\n## quadrature operators\n\nthe electromagnetic field quadratures are defined as\n\n$$\n\\hat{x} = \\frac{1}{2}(\\hat{a} + \\hat{a}^\\dagger), \\qquad \\hat{p} = \\frac{1}{2i}(\\hat{a} - \\hat{a}^\\dagger),\n$$\n\nsatisfying $[\\hat{x}, \\hat{p}] = i/2$. the heisenberg uncertainty relation gives $\\delta x \\cdot \\delta p \\geq 1/4$. for vacuum and coherent states, the uncertainties are equal: $\\delta x = \\delta p = 1/2$ (minimum uncertainty states with symmetric noise).\n\n**squeezed states** redistribute the quantum noise between the two quadratures. one quadrature has reduced fluctuations below the vacuum level at the expense of increased fluctuations in the conjugate quadrature, while still satisfying the uncertainty relation.\n\n## the squeezing operator\n\nthe single-mode **squeezing operator** is\n\n$$\n\\hat{s}(\\xi) = \\exp\\left[\\frac{1}{2}(\\xi^* \\hat{a}^2 - \\xi \\hat{a}^{\\dagger 2})\\right],\n$$\n\nwhere $\\xi = r e^{i\\theta}$ is the complex squeezing parameter. the magnitude $r$ determines the degree of squeezing. the squeezed vacuum state is $|\\xi\\rangle = \\hat{s}(\\xi)|0\\rangle$.\n\nunder squeezing, the quadrature variances become\n\n$$\n(\\delta x_\\theta)^2 = \\frac{1}{4}e^{-2r}, \\qquad (\\delta x_{\\theta+\\pi/2})^2 = \\frac{1}{4}e^{+2r}.\n$$\n\nthe noise reduction is characterized in decibels: squeezing of $r$ corresponds to $-10\\log_{10}(e^{-2r}) \\approx 8.69 r$ db. current experiments achieve over 15 db of squeezing.\n\n## photon statistics of squeezed vacuum\n\nthe squeezed vacuum contains only **even photon numbers**:\n\n$$\n|\\xi\\rangle = \\frac{1}{\\sqrt{\\cosh r}} \\sum_{n=0}^{\\infty} \\frac{(-e^{i\\theta} \\tanh r)^n \\sqrt{(2n)!}}{2^n n!} |2n\\rangle.\n$$\n\nthe mean photon number is $\\langle n \\rangle = \\sinh^2 r$, and the photon number distribution is super-poissonian despite the sub-vacuum noise in one quadrature. the even-photon-number signature is a distinctive feature observable in photon-number-resolving measurements.\n\n## generation by parametric down-conversion\n\nthe primary method for generating squeezed light is **optical parametric down-conversion** (pdc). a nonlinear crystal with $\\chi^{(2)}$ nonlinearity is pumped by a strong laser at frequency $\\omega_p$. the pump photons are converted into pairs of photons (signal and idler) satisfying energy and momentum conservation:\n\n$$\n\\omega_p = \\omega_s + \\omega_i, \\qquad \\mathbf{k}_p = \\mathbf{k}_s + \\mathbf{k}_i.\n$$\n\nwhen the signal and idler are in the same mode (**degenerate** pdc), the output is a squeezed vacuum state. the hamiltonian describing this process is\n\n$$\n\\hat{h}_{\\text{pdc}} = i\\hbar\\kappa(\\hat{a}^{\\dagger 2} - \\hat{a}^2),\n$$\n\nwhere $\\kappa$ is proportional to the pump amplitude and the nonlinear coefficient. this is exactly the squeezing hamiltonian.\n\nin an **optical parametric oscillator** (opo), the nonlinear crystal is placed inside a cavity. below threshold, the opo produces continuous-wave squeezed light with narrow bandwidth determined by the cavity linewidth. above threshold, it oscillates and produces a coherent output.\n\n---\n\n## displaced squeezed states\n\na **displaced squeezed state** is obtained by first squeezing the vacuum and then displacing it in phase space:\n\n$$\n|\\alpha, \\xi\\rangle = \\hat{d}(\\alpha) \\hat{s}(\\xi) |0\\rangle,\n$$\n\nwhere $\\hat{d}(\\alpha) = \\exp(\\alpha \\hat{a}^\\dagger - \\alpha^* \\hat{a})$ is the displacement operator. the ordering matters: $\\hat{d}(\\alpha)\\hat{s}(\\xi) \\neq \\hat{s}(\\xi)\\hat{d}(\\alpha)$ in general. the result is a **minimum uncertainty state** centered at $\\alpha$ in phase space with an elliptical noise distribution.\n\nthe mean values and variances are\n\n$$\n\\langle \\hat{x}_\\phi \\rangle = |\\alpha|\\cos(\\phi - \\phi_\\alpha), \\qquad (\\delta x_\\phi)^2 = \\frac{1}{4}(e^{-2r}\\cos^2\\psi + e^{2r}\\sin^2\\psi),\n$$\n\nwhere $\\psi = \\phi - \\theta/2$ and $\\phi_\\alpha = \\arg(\\alpha)$.\n\n### amplitude and phase squeezed light\n\nthe relative orientation of the squeezing ellipse and the coherent amplitude determines the type of squeezing:\n\n**amplitude squeezed light** has reduced intensity fluctuations. the squeezing axis is aligned with the coherent amplitude, so the radial (amplitude) quadrature has sub-vacuum noise: $(\\delta n)^2 < \\langle n \\rangle$ (sub-poissonian). this is useful for precision intensity measurements.\n\n**phase squeezed light** has reduced phase fluctuations. the squeezing axis is perpendicular to the coherent amplitude: $\\delta\\phi < 1/(2\\sqrt{\\langle n \\rangle})$ (below shot noise). this is advantageous for interferometric measurements where phase sensitivity is the limiting factor.\n\nthe wigner function of a displaced squeezed state is a gaussian centered at $\\alpha$ with principal axes tilted by $\\theta/2$:\n\n$$\nw(x, p) = \\frac{1}{\\pi} \\exp\\left[-e^{2r}(x' - x_0')^2 - e^{-2r}(p' - p_0')^2\\right],\n$$\n\nwhere $(x', p')$ are coordinates rotated by the squeezing angle.\n\n[[simulation wigner-squeezed]]\n\n---\n\n## two-mode squeezing and entanglement\n\n**two-mode squeezing** correlates two distinct field modes and is the key resource for continuous-variable entanglement. the two-mode squeezing operator is\n\n$$\n\\hat{s}_2(\\xi) = \\exp(\\xi^* \\hat{a}\\hat{b} - \\xi \\hat{a}^\\dagger \\hat{b}^\\dagger),\n$$\n\nwhich creates photon pairs: one in mode $a$ and one in mode $b$. the two-mode squeezed vacuum (tmsv) state is\n\n$$\n|\\text{tmsv}\\rangle = \\frac{1}{\\cosh r} \\sum_{n=0}^{\\infty} (-e^{i\\theta}\\tanh r)^n |n\\rangle_a |n\\rangle_b.\n$$\n\nthe modes are perfectly correlated in photon number but individually each mode is a thermal state with $\\langle n \\rangle = \\sinh^2 r$.\n\nthe sum and difference quadratures satisfy\n\n$$\n\\delta(x_a - x_b)^2 = \\frac{1}{2}e^{-2r}, \\qquad \\delta(p_a + p_b)^2 = \\frac{1}{2}e^{-2r},\n$$\n\nboth vanishing in the limit $r \\to \\infty"
    },
    {
      "topicId": "quantum-optics",
      "topicTitle": "Quantum Optics",
      "routeSlug": "quantum-optics",
      "lessonSlug": "vacuum-fluctuations",
      "lessonTitle": "Vacuum Fluctuations and Observable Effects",
      "x": 0.13291992247104645,
      "y": 0.18067874014377594,
      "searchText": "vacuum fluctuations and observable effects\n# vacuum fluctuations and observable effects\n\n## vacuum fluctuations and the zero-point energy\nvacuum energy and fluctuations actually give rise to observable effects such as:\n* spontaneous emission\n* lamb shift\n* casimir effect\n### lamb shift\nthe lamb shift is a discrepancy between experiment and the dirac relativistic\ntheory of the hydrogen atom.\n* the theory predicts that the $2^2s_{1/2}$ and $2^2p_{1/2}$ levels should be degenerate.\n* optical experiment suggest that these states were not degenerate.\n\nthis discrepancy explained by bethe. here we use the welton's intuitive\ninterpretation.\n\nfirst the potential energy of electron of hydrogen atom is\n$$\n    v(r)\n    =\n    -\n    \\frac{e^2}{r}\n    +\n    v_\\mathrm{vac}\n$$\nwe added the small term $v_\\mathrm{vac}$ to include vacuum energy.\nsmall displacement of potential energy is\n$$\n    \\delta v\n    =\n    \\delta \\vec{r}\n    \\cdot\n    \\vec{\\nabla} v\n    +\n    \\sum_{i=1}^3\n    \\frac{1}{2}\n    \\left(\n        \\delta x_i\n    \\right)^2\n    \\frac\n    {\\partial^2 v}\n    {\\partial x_i^2}\n$$\nwe assume that fluctuation is uniform in all direction in sufficiently long time.\nso we set\n$$\n    \\left< \\delta \\vec{r} \\right> = 0\n$$\nalso, we assume that the displacement is same in all direction.\n$$\n    \\left<\n        \\left(\n            \\delta x_i\n        \\right)^2\n    \\right>\n    =\n    \\frac{1}{3}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n$$\nso the time-average potential energy displacement is\n$$\n\\begin{aligned}\n    \\left<\n        \\delta v\n    \\right>\n    &=\n    \\sum_{i=1}^3\n    \\frac{1}{2}\n    \\left<\n        \\left(\n            \\delta x_i\n        \\right)^2\n    \\right>\n    \\frac\n    {\\partial^2 v}\n    {\\partial x_i^2}\n    \\\\&=\n    \\sum_{i=1}^3\n    \\frac{1}{2}\n    \\frac{1}{3}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    \\frac\n    {\\partial^2 v}\n    {\\partial x_i^2}\n    \\\\&=\n    \\frac{1}{6}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    \\sum_{i=1}^3\n    \\frac\n    {\\partial^2 v}\n    {\\partial x_i^2}\n    \\\\&=\n    \\frac{1}{6}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    \\nabla^2 v\n    \\\\&=\n    \\frac{1}{6}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    4 \\pi e^2 \\delta(\\vec{r})\n\\end{aligned}\n$$\nhere, $\\delta(\\vec{r})$ is dirac delta function.\n\nwe can calculate quantum mechanical energy shift.\n$$\n\\begin{aligned}\n    \\delta e\n    &=\n    \\braket{nl|\\delta v|nl}\n    \\\\&=\n    \\frac{4\\pi e^2}{6}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    \\int\n    \\psi^*_{nl}(\\vec{r})\n    \\delta(\\vec{r})\n    \\psi_{nl}(\\vec{r})\n    \\mathrm{d}\\vec{r}\n    \\\\&=\n    \\frac{2\\pi e^2}{3}\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n    \\left|\n        \\psi_{nl}(\\vec{r}=0)\n    \\right|^2\n\\end{aligned}\n$$\n\nwe need to calculate\n$\n    \\left<\n        \\left(\n            \\delta r\n        \\right)^2\n    \\right>\n$.\nto obtain this, we assume that the important field frequencies exceed the atomic\nresonance frequencies.\nthe displacement $\\delta r_\\nu$ induced with frequency between $\\nu$ and\n$\\nu+\\mathrm{d}\\nu$ is determined by\n$$\n    m\n    \\frac\n    {\\mathrm{d}^2 \\delta r_\\nu}\n    {\\mathrm{d}t^2}\n    =\n    e e_\\mathrm{vac} e^{2\\pi i \\nu t}\n$$\ntotal vacuum energy is\n$$\n\\begin{aligned}\n    \\text{total vacuum energy}\n    &=\n    \\underbrace{\n    \\frac\n    {8\\pi \\nu^2}\n    {c^3}\n    }_\\text{density of state}\n    \\cdot\n    \\overbrace{\n    v\n    }^\\text{volume}\n    \\cdot\n    \\overbrace{\n    \\frac{1}{2}\n    h \\nu\n    }^\\text{vacuum energy}\n    \\\\&=\n    \\frac{1}{8\\pi}\n    \\int\n    \\left(\n        e_{\\mathrm{vac}, \\nu}^2\n        +\n        \\overbrace{\n            \\cancel{\n                b_{\\mathrm{vac}, \\nu}^2\n            }\n        }^{e_\\nu \\gg b_\\nu}\n    \\right)\n    \\mathrm{d}v\n\\end{aligned}\n$$\nthus energy difference become\n$$\n    \\delta e\n    =\n    \\chi\n    \\left|\n        \\psi_{nl}(\\vec{r}=0)\n    \\right|^2\n    \\int_0^\\infty\n    \\mathrm{d} \\nu\n    \\frac{1}{\\nu}\n    \\longrightarrow\n    \\infty\n$$\n\n### casimir effect\nfrom the vacuum electric field, we can show that two conducting plane attract\neach other.\nconsider the box of dimension $l\\times l \\times d$.\nthe total vacuum energy is\n$$\n    e_0(d)\n    =\n    \\sum_{lmn}\n    2\n    \\frac{1}{2}\n    \\hbar \\omega_{lmn}\n$$\ndue to two independent polarizations, we multiply two.\nhere $\\omega$ can be calculated from periodic boundary conditions.\n$$\n    \\omega_{lmn}\n    =\n    \\pi c\n    \\sqrt{\n        \\frac{l^2}{l^2}\n        +\n        \\frac{m^2}{l^2}\n        +\n        \\frac{n^2}{d^2}\n    }\n$$\nwe will conduct several approximations listed below:\n* calculate $e_0(d)$. we are interested in $l \\gg d$, so we can replace the sums of $l$ and $m$ by integrals.\n* calculate $e(\\infty)$. we assume that $d$ is arbitrarily large, so we can replace the sum by integral.\n* calculate $u(d)=e_0(d)-e_0(\\infty)$, which is energy required to bring the plates from infinity to a distance $d$.\n* to transform $u(d)$ further, we need to introduce polar coordinates in the $x$-$y$ plane.\n* to estimate the sum and integral, we use euler-maclaurin formulae. we keep the terms until third order.\n\nfrom these intensive calculations, we can show\n$$\n    u(d)\n    =\n    -\n    \\frac\n    {\\pi^2 \\hbar c}\n    {720}\n    \\frac\n    {l^2}\n    {d^3}\n$$\nwhich means there is an attractive force (casimir force) between two plates.\n\n## big ideas\n\n* the vacuum is not nothing \u2014 zero-point fluctuations of the electromagnetic field jiggle electrons inside atoms, shifting energy levels in ways that dirac's otherwise exact theory cannot predict.\n* the lamb shift tells us that the $2s_{1/2}$ and $2p_{1/2}$ levels of hydrogen, predicted to be degenerate, are split by roughly 1 ghz because the electron \"trembles\" in the vacuum field.\n* two neutral, uncharged conducting plates attract each other simply because the vacuum modes between them are restricted to a discre"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "bounding-errors",
      "lessonTitle": "Bounding Errors",
      "x": 0.44171422719955444,
      "y": 0.8247589468955994,
      "searchText": "bounding errors\n# bounding errors\n\n## the first surprise\n\nsuppose you try to compute $\\sqrt{2}$ on a calculator. you punch in 2, hit the square root button, and get 1.41421356... that looks pretty good. but here's the thing: your calculator is lying to you. not maliciously \u2014 it's doing its best with only a finite number of digits. the true $\\sqrt{2}$ goes on forever, and the calculator has to stop somewhere. every number it gives you is a tiny lie.\n\nthe whole game of scientific computing starts right here: **understanding how much your computer is lying, and making sure the lies stay small enough that your answers are still useful.**\n\n## sources of error and error definitions\n\n**sources of approximation** include:\n* modelling (simplifications of the physical system)\n* empirical measurements\n* previous computations\n* truncation/discretization\n* rounding\n\n**absolute error** and **relative error** are different in the obvious manner, i.e., abs. error\n= approx. value - true value, and rel. error = abs. error / true value.\n\n**data error and computational error**: the hats indicate an approximation;\n\n$$\n\\begin{align*}\n\\text{total error} &= \\hat{f}(\\hat{x})-f(x)\\\\\n&= \\left(\\hat{f}(\\hat{x})-f(\\hat{x})\\right) &+&\\left(f(\\hat{x})-f(x)\\right)\\\\\n&= \\text{computational error} &+& \\text{propagated data error}\\\\\ne_\\text{tot} &= e_\\text{comp} &+& e_\\text{data}\n\\end{align*}\n$$\n\n*in plain english: the total error is the sum of (a) mistakes the algorithm makes and (b) mistakes already baked into the input data.*\n\nhere $\\hat{x}$ is the approximate input, $\\hat{f}$ is the approximate function, $f$ is the true function, and $x$ is the true input.\n\ncan these two errors cancel each other out? yes, sometimes they do! but you can't count on it. that's like hoping two wrongs make a right \u2014 occasionally true, but a terrible strategy.\n\n## truncation error vs rounding error\ncomputational error can be split into truncation error $e_\\text{trunc}$ and rounding error $e_\\text{round}$.\n\n**truncation error** stems from:\n* simplifications of the physical model (frictionless, etc.)\n* finite basis sets\n* truncations of infinite series (replacing derivatives with finite differences)\n\n**rounding error** contains everything that comes from working on a finite computer:\n* accumulated rounding error (from finite arithmetic)\n\n**forward vs. backward error**\n\nforward error is the error in the output, backward error is the error in the input.\n\n*think of it this way: forward error asks \"how wrong is my answer?\" backward error asks \"what slightly different question did i actually solve perfectly?\"*\n\n\n## sensitivity, conditioning, and floating point\n**sensitivity and conditioning**\n\nthink of the condition number as an **error amplifier** built into the problem itself. before you choose an algorithm, before you write a single line of code, the mathematics already has an opinion about how many of your input digits will survive to the output. a condition number of 1 means every input digit comes through intact. a condition number of $10^6$ means you lose six decimal digits on the spot \u2014 not because the algorithm is bad, but because the problem is inherently sensitive. the whole point of learning about conditioning first is that no clever algorithm can rescue digits the problem has already destroyed.\n\ncondition number: $\\text{cond}(f) \\equiv \\frac{|\\delta y / y|}{|\\delta x / x|} = \\frac{|x \\delta y|}{|y \\delta x|}$\n\nin the limit $\\delta x \\to 0$, this becomes $\\text{cond}(f) = \\left|\\frac{x f'(x)}{f(x)}\\right|$, which measures how sensitive the relative output is to relative changes in input.\n\n*this says: the condition number is how much the problem itself amplifies relative errors. a condition number of 100 means a 1% input error could become a 100% output error. yikes.*\n\nis a bad condition number the computer's fault? no! conditioning is a property of the *problem*, not the algorithm. even a perfect computer with infinite precision would struggle with an ill-conditioned problem. it's like trying to balance a pencil on its tip \u2014 the physics makes it hard, not your fingers.\n\n**stability and accuracy**\n\n* fixed points have each bit correspond to a specific scale.\n* floating point (32 bit) has: 1 sign bit (0=positive, 1=negative), 8 exponent bits, and 23 mantissa bits. machine epsilon is $\\epsilon \\approx 2^{-23} \\approx 1.2 \\times 10^{-7}$ for single precision.\n\noverflow and underflow refer to the largest and smallest numbers that can be contained in a floating point representation.\n\n\n## example: finite difference error trade-off\n\nhere's the beautiful part \u2014 watch what happens when two types of error fight each other.\n\ncomputational error of first order finite difference: $$ f'(x) \\approx \\frac{f(x+h) - f(x)}{h} \\equiv \\hat{f'}(x) $$\n\n*this says: estimate the slope by looking at two nearby points and computing rise over run.*\n\ntaylor expand:\n$$ f(x+h) = f(x) + h f'(x) + \\frac{h^2}{2} f''(\\theta), \\qquad \\lvert \\theta - x \\rvert \\leq h $$\n$$ \\frac{f(x+h) - f(x)}{h} = f'(x) + \\frac{h}{2} f''(\\theta) $$\n$$ \\hat{f'}(x) - f'(x) = \\frac{h}{2} f''(\\theta) $$\n$$ m \\equiv \\sup_{|\\theta - x| \\leq h} \\lvert f''(\\theta) \\rvert $$\n$$ e_\\text{trunc} = |\\hat{f'}(x) - f'(x)| \\leq \\frac{m}{2} h \\quad \\sim o(h) $$\n\n*truncation error shrinks as h gets smaller \u2014 make the step tiny, get a better derivative.*\n\nwhat about rounding error? when we compute $f(x+h)$ and $f(x)$ in floating point, each has a relative error bounded by machine epsilon $\\epsilon$. the subtraction $f(x+h) - f(x)$ can lose significant digits (cancellation), and dividing by $h$ amplifies this. the result is:\n$$ e_\\text{round} \\leq \\frac{2\\epsilon}{h} \\quad \\sim o\\left(\\frac{1}{h}\\right) $$\n\n*rounding error grows as h gets smaller \u2014 the very thing that helps truncation error makes rounding error worse!*\n\nthe factor of 2 arises because we subtract two quantities, each with rounding error up to $\\epsilon |f|$.\n\nif you decrease $h$, you decrease truncation error but increase rounding error:\n\n$$ e_"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "eigen-systems",
      "lessonTitle": "Eigenvalue Problems",
      "x": 0.4927310645580292,
      "y": 1.0,
      "searchText": "eigenvalue problems\n# eigenvalue problems\n\n> *every matrix has a set of special directions that it only stretches \u2014 never rotates. these are the eigenvectors, and the stretch factors are the eigenvalues. together, they reveal the dna of the matrix.*\n\nany stable physical/chemical system can be described with an eigensystem. wavefunctions in quantum mechanics are a typical example.\n\n$$\nax = \\lambda x\n$$\n\nwhere $a$ is the operator (mapping $\\mathbb{c}^n \\rightarrow \\mathbb{c}^n$), $x$ an eigenvector, and $\\lambda$ an eigenvalue. the question is: **how do we obtain the eigenvalues and eigenvectors?**\n\n[[simulation eigen-transformation]]\n\n## why eigenvalues matter\n\neigenvalues appear throughout physics and engineering:\n\n* **quantum mechanics**: energy levels are eigenvalues of the hamiltonian\n* **vibrations**: natural frequencies of structures\n* **stability analysis**: system behavior near equilibrium points\n* **principal component analysis**: dimensionality reduction in data science\n* **google pagerank**: largest eigenvector of the web graph\n\n*every time you ask \"what are the natural modes of this system?\" you're asking an eigenvalue question.*\n\n[[figure eigen-applications]]\n\n---\n\n## mathematical foundations\n\n### the characteristic polynomial\n\nfor any fixed $\\lambda$, we have a linear system:\n\n$$\n(a-\\lambda i)x = 0\n$$\n\nthis has **non-trivial** solutions ($x\\neq0$) if and only if:\n\n$$\n\\det(a-\\lambda i) = 0\n$$\n\n*this says: eigenvalues are the values of $\\lambda$ that make the matrix $(a - \\lambda i)$ singular. setting the determinant to zero gives us a polynomial to solve.*\n\n[[simulation characteristic-polynomial]]\n\n### eigenspaces and multiplicity\n\neigenvectors form subspaces called **eigenspaces**:\n\n$$\ne_\\lambda = \\{x\\in \\mathbb{c}^n \\mid ax=\\lambda x\\}\n$$\n\nthe characteristic polynomial $p(\\lambda) = \\det(a-\\lambda i)$ is of degree $n$, and by the **fundamental theorem of algebra**, has exactly $n$ complex roots (counting multiplicity).\n\n**two types of multiplicity:**\n\n1. **algebraic multiplicity**: how many times $\\lambda_i$ appears in the characteristic polynomial\n2. **geometric multiplicity**: dimension of the eigenspace $e_\\lambda$\n\n[[figure multiplicity-diagram]]\n\n**key inequality**: geometric multiplicity $\\leq$ algebraic multiplicity\n\n### defective vs non-defective matrices\n\n* **non-defective**: $\\sum_{\\lambda\\in sp(a)} \\dim(e_\\lambda) = n$ \u2014 we have a complete eigenbasis\n* **defective**: $\\sum_{\\lambda\\in sp(a)} \\dim(e_\\lambda) < n$ \u2014 not enough eigenvectors\n\n**non-defectiveness is guaranteed if:**\n* $a$ is **normal**: $a^h a = aa^h$\n* all eigenvalues are distinct\n* $a$ is **hermitian**: $a=a^h$ (best case \u2014 guarantees orthonormal eigenbasis)\n\nfor hermitian matrices: $a = u \\lambda u^h$\n\nwhy do we care if a matrix is defective? because if it's non-defective, we can diagonalize it \u2014 write $a = t\\lambda t^{-1}$ \u2014 which makes everything easy. computing $a^{100}$? just compute $\\lambda_i^{100}$. solving $e^{at}$? just compute $e^{\\lambda_i t}$. defective matrices are the troublemakers that make life harder.\n\n[[simulation hermitian-demo]]\n\n---\n\n## part i: simple methods (finding one eigenvalue at a time)\n\n### the power method \u2014 the world's laziest way to find the boss eigenvalue\n\nhere's the idea: take any vector and multiply it by $a$ over and over. the dominant eigenvalue's eigenvector will gradually take over, like the loudest voice in a room eventually drowning out everyone else.\n\nlet $a$ be non-defective with $a=t\\lambda t^{-1}$. order the eigenvalues by magnitude:\n\n$$\n|\\lambda_1| \\geq |\\lambda_2| \\geq \\dots \\geq |\\lambda_n|\n$$\n\n### derivation\n\nfor any $x \\in \\mathbb{c}^n$, expressed in the eigenbasis:\n\n$$\nx = \\tilde{x}_1 t_1 + \\tilde{x}_2 t_2 + \\dots + \\tilde{x}_n t_n\n$$\n\napplying $a$ repeatedly:\n\n$$\na^k x = \\tilde{x}_1 \\lambda_1^k t_1 + \\tilde{x}_2 \\lambda_2^k t_2 + \\dots + \\tilde{x}_n \\lambda_n^k t_n\n$$\n\n$$\n= \\lambda_1^k \\left(\\tilde{x}_1 t_1 + \\tilde{x}_2 \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k t_2 + \\dots + \\tilde{x}_n \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k t_n\\right)\n$$\n\nsince $|\\lambda_1|$ is strictly the largest, the ratios approach zero:\n\n$$\n\\lim_{k\\to\\infty}\\frac{a^k x}{\\|a^k x\\|} = t_1, \\quad \\text{if } \\tilde{x}_1 \\neq 0 \\text{ and } |\\lambda_2| < |\\lambda_1|\n$$\n\n*this says: keep multiplying and normalizing. the biggest eigenvalue wins because its ratio keeps growing while all others shrink to zero. convergence speed depends on how much bigger $\\lambda_1$ is than $\\lambda_2$.*\n\n**important**: this requires the dominant eigenvalue to be strictly larger in magnitude than the second largest. if $|\\lambda_1| = |\\lambda_2|$, the method fails to converge. convergence rate is $o(|\\lambda_2/\\lambda_1|^k)$, so a small spectral gap means slow convergence.\n\n[[simulation power-method-animation]]\n\n### power method algorithm\n\n```python\ndef power_iterate(a, x0, tol=1e-10, max_iter=1000):\n    \"\"\"\n    find the dominant eigenvalue and eigenvector.\n    the laziest eigenvalue finder: just keep multiplying and normalizing.\n    \"\"\"\n    x = x0 / np.linalg.norm(x0)       # start with any unit vector\n\n    for i in range(max_iter):\n        y = a @ x                       # multiply by a\n        x_new = y / np.linalg.norm(y)   # normalize so it doesn't blow up\n\n        # rayleigh quotient for eigenvalue estimate\n        eigenvalue = x_new.conj() @ a @ x_new\n\n        if np.linalg.norm(x_new - x) < tol or np.linalg.norm(x_new + x) < tol:\n            return eigenvalue, x_new, i + 1\n\n        x = x_new\n\n    return eigenvalue, x, max_iter\n```\n\n### limitations\n\n* only finds the **dominant eigenvalue** (largest magnitude)\n* fails if the dominant eigenvalue is not unique\n* slow convergence when $|\\lambda_1| \\approx |\\lambda_2|$ (small spectral gap)\n\n[[figure convergence-comparison]]\n\n---\n\n### inverse iteration \u2014 finding eigenvalues near a target\n\nto find an eigenvalue **closest to a shift** $\\sigma$, apply the power method to $(a - \\sigma i)^{-1}$:\n\nthe eigenvalues of $(a - \\sigma i)^{-1}$ are $(\\lamb"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "fft",
      "lessonTitle": "Fast Fourier Transform",
      "x": 0.38646817207336426,
      "y": 0.9865469336509705,
      "searchText": "fast fourier transform\n# fast fourier transform\n\n> *eigenvalues revealed the personality of matrices. the fft reveals the personality of signals \u2014 which frequencies are hiding inside any chunk of data. and it connects back to pdes at the summit of our course.*\n\n## suppose you have a song\n\nyou hear a chord on a piano \u2014 three notes played together. your ear somehow picks out the individual notes from the combined sound wave. the **fourier transform** is the mathematical version of your ear: it takes a signal (the combined wave) and tells you exactly which frequencies (notes) are inside it, and how loud each one is.\n\nthe **fast fourier transform** is the algorithm that makes this practical. without it, analyzing a million-sample signal would take a trillion operations. with it, twenty million. that's the difference between \"impossible\" and \"done before you blink.\"\n\n## the discrete fourier transform\n\nthe **discrete fourier transform** (dft) converts a sequence of $n$ samples in the time domain into a sequence of $n$ complex coefficients in the frequency domain:\n\n$$\nx_k = \\sum_{n=0}^{n-1} x_n \\, e^{-2\\pi i \\, kn/n}, \\qquad k = 0, 1, \\ldots, n-1.\n$$\n\n*this says: for each frequency $k$, multiply the signal by a complex sinusoid at that frequency and add everything up. the result tells you how much of that frequency is present.*\n\nthe inverse transform recovers the original signal:\n\n$$\nx_n = \\frac{1}{n} \\sum_{k=0}^{n-1} x_k \\, e^{2\\pi i \\, kn/n}.\n$$\n\neach coefficient $x_k$ represents the amplitude and phase of a sinusoidal component at frequency $k/n$ cycles per sample. the **power spectrum** $|x_k|^2$ reveals the dominant frequencies in the signal.\n\nremember that the dft implicitly treats your finite signal as one period of an infinite periodic extension \u2014 forgetting this is the source of spectral leakage.\n\nwhy complex exponentials instead of sines and cosines? because $e^{i\\theta} = \\cos\\theta + i\\sin\\theta$ packages both into one elegant expression. the real part gives you cosines, the imaginary part gives you sines, and the math becomes cleaner. it's like writing coordinates as $(x, y)$ instead of \"x units east and y units north.\"\n\n## the fft algorithm\n\ncomputing the dft directly requires $o(n^2)$ operations. the **fast fourier transform** (the cooley-tukey algorithm) reduces this to $o(n \\log n)$ by exploiting the symmetry and periodicity of the complex exponentials.\n\nhere's the trick \u2014 split the signal into even and odd samples:\n\n$$\nx_k = \\underbrace{\\sum_{m=0}^{n/2-1} x_{2m} \\, e^{-2\\pi i \\, k(2m)/n}}_{\\text{even-indexed dft}} + e^{-2\\pi i \\, k/n} \\underbrace{\\sum_{m=0}^{n/2-1} x_{2m+1} \\, e^{-2\\pi i \\, k(2m)/n}}_{\\text{odd-indexed dft}}.\n$$\n\n*this says: one big dft of size $n$ equals two half-size dfts (even samples and odd samples) plus $n$ multiplications to combine them. apply this recursively and you go from $n^2$ to $n \\log n$.*\n\nthis splits one $n$-point dft into two $n/2$-point dfts plus $o(n)$ multiplications. applying this recursively yields the $o(n \\log n)$ complexity.\n\n**practical impact**: for $n = 10^6$, the fft is roughly $50{,}000$ times faster than the direct dft.\n\nis this really that big a deal? yes! the fft is arguably one of the most important algorithms ever invented. it makes real-time audio processing, medical imaging (mri), radio astronomy, and modern telecommunications possible. without it, your phone couldn't decode a wifi signal fast enough to be useful.\n\n## the convolution theorem\n\none of the most powerful properties of the dft is the **convolution theorem**:\n\n$$\n\\mathcal{f}\\{f * g\\} = \\mathcal{f}\\{f\\} \\cdot \\mathcal{f}\\{g\\},\n$$\n\nwhere $*$ denotes convolution and $\\cdot$ denotes pointwise multiplication. this means that convolution in the time domain becomes multiplication in the frequency domain.\n\n*think of it like this: instead of stirring the whole pot for hours (direct convolution), transform both recipes into ingredient lists, multiply the ingredients together, and transform back. same result, fraction of the time.*\n\n**practical consequence**: convolving two sequences of length $n$ via the fft costs $o(n \\log n)$, compared to $o(n^2)$ for direct convolution. this speedup is exploited in:\n\n* polynomial multiplication.\n* cross-correlation and matched filtering.\n* solving pdes with periodic boundary conditions (spectral methods).\n\n## applications in signal processing\n\nthe dft and fft are ubiquitous in scientific computing:\n\n* **spectral analysis**: identify periodic components in time series (e.g., tidal data, heart rhythms, seismic signals).\n* **filtering**: multiply the spectrum by a transfer function to remove noise or isolate frequency bands.\n* **interpolation and zero-padding**: increasing $n$ by appending zeros refines the frequency resolution.\n* **image processing**: the 2d dft decomposes images into spatial frequencies for compression (jpeg) and enhancement.\n\n## aliasing and the nyquist frequency\n\nthe dft assumes the signal is periodic with period $n$. if the signal contains frequencies above the **nyquist frequency** $f_{\\text{nyq}} = f_s / 2$ (where $f_s$ is the sampling rate), those components are **aliased** into lower frequencies.\n\n*this says: if you don't sample fast enough, high frequencies disguise themselves as low frequencies. it's like a spinning wheel that looks like it's going backwards in a movie \u2014 the camera wasn't sampling fast enough.*\n\nthe **sampling theorem** (shannon-nyquist) states that a bandlimited signal can be perfectly reconstructed from its samples if and only if the sampling rate exceeds twice the highest frequency present.\n\n[[simulation aliasing-slider]]\n\n## windowing\n\nreal signals are finite in duration. truncation introduces spectral **leakage**, spreading energy from a true frequency into neighboring bins. **window functions** (hann, hamming, blackman) taper the signal at the edges to reduce leakage at the cost of slightly reduced frequency resolution.\n\n> **challenge.** generate a signal with two frequencies: `x = sin(2"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "initial-value-problems",
      "lessonTitle": "Initial Value Problems",
      "x": 0.4201558828353882,
      "y": 0.9065698981285095,
      "searchText": "initial value problems\n# initial value problems\n\n> *the [eigenvalues](./eigen-systems) show up again here \u2014 the eigenvalues of the jacobian determine whether your problem is stiff, and stiffness determines which solver you need.*\n\n## introduction\n\nan **initial value problem** (ivp) asks us to find $y(t)$ satisfying an ordinary differential equation $y' = f(t, y)$ with a given initial condition $y(t_0) = y_0$. most ode systems arising in physics and engineering cannot be solved analytically, so we rely on numerical methods that advance the solution step by step through discrete time increments.\n\nthe key challenges are **accuracy** (how close the numerical solution tracks the true one), **stability** (whether errors grow or decay), and **efficiency** (how much computation is needed for a given accuracy). these three concerns drive the design of all ivp solvers.\n\n## the method of lines \u2014 turning movies into photographs\n\nbefore we dive into time-stepping, here's the big picture of why ode solvers matter beyond just odes.\n\nimagine you have a pde \u2014 say, heat spreading across a metal plate. that's a function of both space and time. the **method of lines** says: discretize space (replace the smooth plate with a grid of points) but leave time continuous. now instead of one pde, you have a *system of odes* \u2014 one for each grid point.\n\n*it's like taking a movie and turning each frame into a still photo you can process one by one. each \"photo\" is a snapshot of the temperature at all grid points, and the ode system tells you how to get from one frame to the next.*\n\n$$\n\\frac{d\\mathbf{u}}{dt} = \\mathbf{f}(\\mathbf{u}),\n$$\n\nthis is why everything in this lesson matters for pde solving too \u2014 it's not just about odes in isolation.\n\n## euler's method\n\nthe simplest approach discretizes the derivative directly. given a step size $h$, **forward euler** updates the solution as\n\n$$\ny_{n+1} = y_n + h f(t_n, y_n).\n$$\n\n*this says: look at the slope where you are now, and walk in that direction for a step of size $h$. hope you don't walk off a cliff.*\n\nthis is a first-order method: the local truncation error per step is $o(h^2)$, giving a global error of $o(h)$. while easy to implement, euler's method requires very small step sizes for acceptable accuracy and is unstable for stiff problems.\n\n**backward euler** replaces the right-hand side with the function evaluated at the new time:\n\n$$\ny_{n+1} = y_n + h f(t_{n+1}, y_{n+1}).\n$$\n\n*this says: look at the slope where you're going to end up, not where you are now. this requires solving an equation (since $y_{n+1}$ appears on both sides), but the payoff is much better stability.*\n\nthis is an **implicit** method since $y_{n+1}$ appears on both sides and generally requires solving a nonlinear equation at each step. the payoff is greatly improved stability for stiff systems.\n\nif backward euler needs to solve a nonlinear equation at every step, isn't that expensive? yes, it is! you typically need [newton's method](./nonlinear-equations) at each time step. but for stiff problems, forward euler would need *billions* of tiny steps to stay stable, so solving one nonlinear equation per large step is a bargain.\n\n## runge-kutta methods\n\n**runge-kutta methods** achieve higher accuracy by evaluating $f$ at intermediate points within each step. the classical fourth-order method (rk4) computes\n\n$$\n\\begin{aligned}\nk_1 &= f(t_n, y_n), \\\\\nk_2 &= f(t_n + h/2, \\; y_n + h k_1/2), \\\\\nk_3 &= f(t_n + h/2, \\; y_n + h k_2/2), \\\\\nk_4 &= f(t_n + h, \\; y_n + h k_3),\n\\end{aligned}\n$$\n\nand then advances with\n\n$$\ny_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4).\n$$\n\n*this says: instead of looking at the slope at just one point (euler), sample the slope at four different places within the step and take a weighted average. it's like getting four weather forecasts and combining them instead of relying on just one.*\n\nrk4 has a local truncation error of $o(h^5)$ and a global error of $o(h^4)$, offering an excellent balance of accuracy and simplicity. it is the workhorse method for non-stiff problems.\n\na general $s$-stage runge-kutta method is defined by a **butcher tableau** specifying the coefficients $a_{ij}$, $b_i$, and $c_i$. the method is explicit if $a_{ij} = 0$ for $j \\geq i$ and implicit otherwise.\n\nwhy is rk4 so popular instead of, say, rk8? diminishing returns. going from order 1 to order 4 is a huge accuracy gain for moderate extra work. going from order 4 to order 8 requires many more function evaluations per step, and you usually get better results by using rk4 with a smaller step size instead.\n\n## multistep methods\n\nrather than using multiple evaluations within a single step, **multistep methods** use information from several previous steps. an $s$-step **adams-bashforth** method (explicit) takes the form\n\n$$\ny_{n+1} = y_n + h \\sum_{j=0}^{s-1} \\beta_j f(t_{n-j}, y_{n-j}).\n$$\n\nthe two-step version ($s = 2$) is\n\n$$\ny_{n+1} = y_n + \\frac{h}{2}\\bigl(3f_n - f_{n-1}\\bigr).\n$$\n\n*this says: use the slopes from the last two steps (not just the current one) to predict where to go next. you're using your memory \u2014 no extra function evaluations needed.*\n\n**adams-moulton** methods are the implicit counterparts and include $f_{n+1}$ on the right-hand side. in practice, a **predictor-corrector** scheme uses adams-bashforth to predict and adams-moulton to correct, combining the efficiency of explicit methods with the improved stability of implicit ones.\n\n## stability analysis\n\nto study stability, we apply numerical methods to the **test equation** $y' = \\lambda y$ where $\\lambda \\in \\mathbb{c}$. the **stability region** of a method is the set of values $h\\lambda$ for which the numerical solution does not grow without bound.\n\nfor forward euler, the stability region is the disk $|1 + h\\lambda| \\leq 1$ in the complex plane. for backward euler, the stability region is the complement of $|1 - h\\lambda| < 1$, which includes the entire left half-plane. a method is **a-stable** if its stability region contains the "
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "linear-equations",
      "lessonTitle": "Linear Equations",
      "x": 0.5282241106033325,
      "y": 0.9730005860328674,
      "searchText": "linear equations\n# linear equations\n\n> *remember the [condition number](./bounding-errors)? watch how it shows up again here \u2014 this time deciding whether your linear system's answer is trustworthy or garbage.*\n\n## why linear systems matter\nlinear systems are special: they're something we can solve really well, and they cover an enormous amount of problems. even when you've got a non-linear system, you can isolate the linearity and solve that, and treat the non-linearity in another way.\n\n## from abstract linear systems to matrices\n### going from abstract linear systems to matrices\n\na **linear function** is anything that behaves in the way we call linear. they exist in vector spaces: a mapping $f: x \\rightarrow y$ (abstract, eg: wave functions) is linear if $f(ax + bx') = af(x) + bf(x')$.\n\n*this says: linearity means \"scaling and adding inputs does the same thing as scaling and adding outputs.\" it's the nicest property a function can have.*\n\nlinear functions and matrices are the same thing (in finite dimensions, once bases are chosen)!\n* let's say you have a basis for $x$, eg: $\\{e_{1}, e_2, \\dots e_n\\}$, then any vector $x \\in x$ can be written uniquely as $x = a_1e_1 + \\dots + a_n e_n$\n* let $\\{f_{1}, f_2, \\dots f_n\\}$ form a basis for $y$, any vector $y \\in y$ can be written as $y = b_1f_1 + \\dots + b_m f_m$\n* $$f(e_j)=b_{1j}f_1 + \\dots + b_{mj}f_m$$\n\twe can write it because of linearity, but you'll notice that it's just matrix multiplication.\n* $$ f(x) = f(a_1e_1 + \\dots + a_ne_n) = a_1f(e_1) + \\dots + a_nf(e_n) $$\n\t$$ = \\sum_{j=1}^n a_j \\sum_{i=1}^m b_{ij} f_i $$\n\t$$ = \\sum_{i=1}^m f_i \\left(\\sum_{j=1}^n a_j b_{ij} \\right) $$\n\tthe thing on the right is nothing more than a matrix product. think of the total thing as:  representation of $f$ in $e,f$ basis $*$ (coordinates of $x$ in $e$ basis) = coordinates of $y$ in $f$ basis\n\nwe've shown how an abstract linear problem can be represented with matrices.\n\nwe're missing one thing: how do you find $b_{ij}$, the components of the matrix?\n$$b_{ij} = \\left< f_i \\vert f e_j \\right>$$\nfor example, $\\int f_i(\\alpha)(fe_j)(\\alpha) d\\alpha$, if $y$ is a space of functions $f(\\alpha)$\n\n**punch line:** the actions of $f: x\\rightarrow y$ are exactly the same as $f: \\mathbb{c}^m \\rightarrow \\mathbb{c}^n$.\n\n### how to solve linear systems of equations\n\nfind all $x \\in x$ such that $f(x) = y$ (some known $y\\in y$)\n1. pick basis sets $\\{e_1, \\dots, e_n\\}$ for $x$, $\\{f_1, \\dots, f_m\\}$ for $y$\n2. write down matrix $f$ in $e,f$ basis and rhs $y$ in $f$ basis\n3. solve matrix equation $fx=y$, where $f$ and $y$ are known\n4. dot result $x$ with $e$ basis to get the result in the problem domain $x=x_1e_1+\\dots+x_ne_n$\n\n## existence of solutions\n### can we solve $f(x)=y$?\nyes, when solutions exist!\n\ntoday, we'll use the case where $m=n$, i.e, matrix $\\underline{f}$ is square.\n\nif you apply $f$ to all the vectors in the source space, you get a subset (not proper) of $y$, which is called the _image_. $im(f) = \\{f(x) \\vert x \\in x\\}$.\n\nin the non-abstract space, it's called the _span_: $span(\\underline{f}) = \\{\\underline{f} \\,\\underline{x} \\vert \\underline{x} \\in \\mathbb{c}\\}$\n\nthree cases when $n=m$\n\n* **$f$ is non-singular**: (if one of them holds, all of them hold: they're equivalent)\n* $im(f) = y$ (dimension of source space $x$ = dimension of target space $y$)\n* $span(\\underline{f} = \\mathbb{c}^n)$\n* $rank(\\underline{f}) = n$\n* $det(\\underline{f}) \\neq 0$\n* $\\underline{f}\\,\\underline{x} = 0 \\leftrightarrow \\underline{x}=0$ (trivial kernel)\n* $\\underline{f}$ is invertible (deterministically find unique solution)\n* **$f$ is singular**:\n* $im(f) \\not\\subseteq y$\n* $span(\\underline{f} \\not\\subseteq \\mathbb{c}^n)$\n* $rank(\\underline{f}) < n$\n* $det(\\underline{f}) = 0$\n* there is a non-trivial subspace $ker(\\underline{f})$ such that $\\underline{f}(\\underline{x})=0$ for $\\underline{x}\\in ker(\\underline{f})$\n\nsingular $f$ splits into two sub-cases:\n1. $y \\not\\in im(f) \\implies$ there are no solutions\n2. $y \\in im(f) \\implies$ infinitely many solutions (because we can add something from the kernel and get another solution).\n\nthat was the mathematical part: now we're going to look at a case where we have exact solutions: when are the solutions stable, and when do small perturbations cause it to blow up?\n\n## condition number interactive demo\n\n[[simulation condition-number-demo]]\n\n## sensitivity of a linear system of equations\n\nthe more orthogonal the matrix is, the lower the condition number. it's ~1 for orthogonal, but as they get closer to one another, i.e, they get closer to being linearly dependent on one another, condition number increases. the webpage has a calculation of the exact condition number.\n\n## interactive gaussian elimination and lu decomposition\n\n[[simulation gaussian-elim-demo]]\n\n[[simulation lu-decomp-demo]]\n\n## how to build the algorithms from scratch\n\nconstruct algorithms that transform $b=ax$ into $x$ using a modest number of operations that are linear, invertible, and simple to compute.\n\n**fundamental property:** if $m$ is invertible, then $max = mb$ has the same solutions as $ax=b$\n\n*this says: multiplying both sides of the equation by the same invertible matrix doesn't change the answer. it's like weighing something on a different scale \u2014 the object doesn't change.*\n\nwhat we know:\n1. our algorithm must have the same effect as multiplying by the inverse. we avoid explicitly computing $a^{-1}$ and multiplying because inverses amplify rounding errors, especially for ill-conditioned matrices: $a \\rightarrowtail i; i \\rightarrowtail a; b \\rightarrowtail x$\n2. each step must be linear, invertible\n3. we first want $a = lu$ (lower and upper triangular matrices)\n4. every step, we want to take an $n\\times n$ matrix, and reduce the leftmost column to be zero below the first element. then, we just recursively continue for an $(n-1)\\times(n-1)$ matrix\n5. row scaling (it's linear and invertible)\n6. row addition and subtraction is also linear and invertible\n\nwe "
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "linear-least-squares",
      "lessonTitle": "Linear Least Squares",
      "x": 0.5472632646560669,
      "y": 0.9802916049957275,
      "searchText": "linear least squares\n# linear least squares\n\n> *remember the [condition number](./bounding-errors)? here's where it bites us \u2014 the normal equations square it, and householder reflections save us.*\n\n## overdetermined systems: m > n\n\nlarge number of equations, small number of unknowns. in general, there are _no exact solutions_.\n\nall the places you can reach are driven by $im(a)$, the image. there's no $x$ that you can feed in that results in you ending up outside the image.\n\n### how would a least squares approximate solution have to look?\n\n**write** $\\mathbb{c}^m = im(a) \\oplus im(a)^\\perp$\nany vector space with a subspace can be broken down into the subspace and a space that's orthogonal to the subspace.\n**means** $b \\in \\mathbb{c}^m$ can be uniquely written $b = \\tilde{b} + b^\\perp$, with $\\tilde{b} \\in im(a), b^\\perp \\in im(a)^\\perp$\n**where**\n$$im(a)^\\perp = \\{x\\in \\mathbb{c}^m \\mid x^t x' = 0 \\;\\forall\\; x' \\in im(a)\\}$$\n\n$$=\\{x\\in \\mathbb{c}^m \\mid a^t_i x=0 \\text{ for } 1 \\leq i \\leq m \\}$$\n\nwatch this trick \u2014 pythagoras makes the whole thing click:\n\n$$||r||^2 = ||b-ax||^2$$\n$$= ||\\tilde{b} + b^\\perp - ax||^2 = ||(\\tilde{b}-ax)+b^\\perp||^2$$\nsince $(\\tilde{b}-ax) \\in im(a)$ and $b^\\perp \\in im(a)^\\perp$, these two components are orthogonal. by pythagoras (valid in $l^2$ norm because orthogonal vectors satisfy $\\|u+v\\|^2 = \\|u\\|^2 + \\|v\\|^2$):\n$$ = ||\\tilde{b}-ax||^2 + || b^\\perp ||^2$$\n\n*this says: the total error splits into a part we can control (by choosing x) and a part we can't (the component of b perpendicular to the image). minimize the first, accept the second.*\n\nthe first term is in the image, the second is in the perpendicular subspace. we can minimize the first term to zero (by choosing $ax = \\tilde{b}$), and the second term is a constant independent of $x$.\n\n**so:** the residual of least squares is perpendicular to $im(a)$.\n\n[[simulation geometric-projection]]\n\n## the least squares problem\nwith given data and a desired function, determine the parameters of the function to minimize the distance to data points.\n\n\n## least squares solution\n* **always exists:** the solution to $ax=\\tilde{b}$ is the least squares solution.\n$\\tilde{b}$ is the projection on the image space, and $b^\\perp$\nthe projection on the orthogonal component.\n* **unique if rank(a) = n:** if the rank is less than $n$ (columns are linearly dependent), there are\ninfinitely many solutions: you can find one and add the kernel space\nto it to get them all.\n* **normal equations**:\n$a^t_i r=0 \\rightarrow a^t r=0 \\implies a^t(b-ax)=a^tb-a^tax=0$.\nthe fact that the residual is orthogonal to the image means that\nfor all the rows of $a$, the dot product with the residual has to be 0.\n* **$a^ta$ is a small square matrix**\n* the only problem is that this has a large condition number: $\\text{cond}(a^ta) = \\text{cond}(a)^2$\n\n*this says: forming the normal equations squares the condition number. if your matrix was a bit ill-conditioned (say cond = 1000), the normal equations make it horrifically ill-conditioned (cond = 1,000,000). that's like photocopying a blurry photo \u2014 every generation makes it worse.*\n\nhere is a more visceral way to see the damage. in double precision you start with about 16 significant digits. the condition number tells you how many of those digits evaporate. if $\\text{cond}(a) \\approx 10^8$, the normal equations square that to $10^{16}$, and **all sixteen digits disappear** \u2014 the answer is pure noise. the normal equations are a photocopier that runs the original through twice: the first pass blurs the image; the second pass makes the blur unreadable.\n\nwe're almost at our goal, but not there yet. we've found an efficient solution (the normal equations) which are great for mathematical calculations, but we can't use them because a small error will ruin us because of the squared condition number.\n\n### how do we save our significant digits (hopefully without too much work?)\n\nnote that $im(a)^\\perp = ker(a^t) \\implies \\mathbb{c}^m = im(a) \\oplus ker(a^t)$.\n\n### recap\nwe decomposed the target space $\\mathbb{c}^m$ into the image of $a$ and the kernel of $a^t$, two orthogonal subspaces.\n\n$\\text{cond}(a^ta) = \\text{cond}(a)^2$, but $\\text{cond}(q^ta) = \\text{cond}(a)$.\n\nwe just need to construct the effect of multiplying our matrix by $q^t$. in general, you never want to calculate $q$ because it can be really large (million x million).\n\nwhy does multiplying by $q^t$ not mess up the condition number, but multiplying by $a^t$ does? because $q$ is orthogonal \u2014 it's a pure rotation/reflection that doesn't stretch anything. multiplying by $q^t$ is like turning your head to look at the problem from a different angle. the problem doesn't change, just your viewpoint. multiplying by $a^t$ is like squishing the problem through a funnel.\n\n## building a least squares algorithm from scratch\n\nnote: norm $\\left(\\,||x||\\,\\right)$ always refers to the euclidean norm $\\left(\\,||x||_2\\,\\right)$ when we're talking about least squares.\n\n**goal:** construct the effect of $q^t$ such that $q^t a = r$, where $r$ is a matrix with the bottom part being 0 and the upper part being upper triangular.\n\n### the householder reflection: bouncing light off a mirror\n\nhere's the beautiful part. imagine you're holding a flashlight (that's your column vector $a$) and you want to redirect all the light so it points straight along the first axis $e_1$ (that's the target direction). how do you do it? **place a mirror at exactly the right angle between the current direction and the target direction, and bounce the light off it.** that's exactly what a householder reflection does \u2014 it's a mirror that zeros out everything below the diagonal in one bounce.\n\n**building blocks:** we want to perform _unitary_ operations: i.e., do things that don't change the length of vectors: operations like rotation, reflection (not translations, because although translations don't change lengths, they change the norm).\n* 2d: rotations and reflections\n* 3d: rotations, refl"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "nonlinear-equations",
      "lessonTitle": "Nonlinear Equations",
      "x": 0.4835597574710846,
      "y": 0.9111712574958801,
      "searchText": "nonlinear equations\n# nonlinear equations\n\n> *we just mastered linear systems where everything was guaranteed. now those guarantees vanish. welcome to the real world.*\n\n# root finding in one dimension\n## notation:\n\n* $f(x) = 0$: 1 dimensional stuff. $f:\\mathbb{r} \\to \\mathbb{r}$\n* $f(x) = 0$: higher dimensional stuff, $f$ is a matrix an $x$ a vector. $f:\\mathbb{r}^n\\to\\mathbb{r}^m$\n\ncomplex numbers are complicated, so we'll only work with reals.\n\n\n## introduction\n\nemergent macroscopic behaviour comes out of high dimensionality of linear systems. for example, you don't figure out the aerodynamics of a plane by using the schrodinger equation for every atom. you can make a simpler theory based on the emergent behaviour, which could be non-linear despite the underlying rules being linear. you could also just make up a non-linear problem, like in economics.\n\n**linear systems:**\n* we know exactly how many solutions exist (by looking at the matrix's rank)\n* we have methods to find exact solutions (if they exist) or approximate solutions (if they don't exist)\n* we can find the full solution space of the problem by adding the kernel\n* we can routinely solve for billions of dimensions: it's very efficient\n\n**non-linear systems:**\n* no idea how many (if any) solutions. all we can hope for is rules of thumb, heuristics, and we can look for something that works as much as possible and fails rarely. we won't get great results in a finite number of steps; sometimes it gets closer and sometimes it doesn't.\n* no fail-proof solvers\n* no way of knowing if we've found all the solutions\n* even 1 dimensional solutions can take ages\n\n## how many solutions?\n\nglobally, *anything is possible*. $e^{-x} = 0$ has no solutions, but $(e^{-x}-\\delta) = 0$ (where $\\delta$ is a small positive number) does. $\\sin(x) = 0$ has countably infinite solutions. even a simple-looking polynomial like $x^5 - x = 0$ has multiple roots that are hard to predict without analysis.\n\nlocally, we can sometimes work it out. in 1d, we can look at where $f(x)$ changes sign, and we can assume that there's a root in between (intermediate value theorem, assuming the function is continuous). in particular, there are $>1$ roots in the region, and there are an odd number of roots.\n\n## general algorithm construction scheme\n\nhere's a powerful recipe for inventing algorithms:\n1. find invariant that guarantees existence of a solution in the search space\n2. design operation that preserves 1. and shrinks search space\n\n*this says: first make sure you're looking in the right place, then systematically make that place smaller. it's a tried and true scheme \u2014 euclid did it thousands of years ago, and we'll do it now.*\n\nlet's use this to build an equation solver on a bracket. (a *bracket* is an interval in which $f(x)$ changes sign).\n\n## bisection method\n\n**step 1:** establish the invariant: $a<b$ and $\\text{sign}(f(a)) \\neq \\text{sign}(f(b))$\n\n*the function crosses zero somewhere between a and b. we're sure of it.*\n\n**step 2:** cut the interval in half: set $m = \\frac{a+b}{2}$ and evaluate $s_m = \\text{sign}(f(m))$\n\n**step 3:** keep the half that still brackets the root:\n* if $s_m == s_a$: the root is in the right half, so $a=m$\n* if $s_m == s_b$: the root is in the left half, so $b=m$\n* if $s_m == 0$: we've found the root\n\nnote: you should not use $m = \\frac{a+b}{2}$ because of floating point error: use  $m = a + \\frac{b-a}{2}$\n\n```python\ndef bisection(f, a, b, tolerance):\n    n_steps = int(np.ceil(np.log2((b - a) / tolerance)))\n    s_a = np.sign(f(a))\n    for i in range(n_steps):\n        m = a + (b - a) / 2      # midpoint without overflow\n        s_m = np.sign(f(m))\n        if s_m == s_a:\n            a = m                 # root is in right half\n        else:\n            b = m                 # root is in left half\n    return m\n```\n\n*bisection gains exactly one bit of accuracy per step. slow but sure \u2014 the tortoise of root-finding.*\n\n## conditionings\n\nthe conditioning for evaluating $f(x)$ is approximately $\\left| \\frac{x f'(x)}{f(x)} \\right|$ (from taylor expansion), and the absolute error is $|f'(x)|$. when evaluating $f^{-1}$, the condition number is $\\approx \\left|  \\frac{f(x)}{x f'(x)} \\right|$ and the absolute error is $\\left|\\frac{1}{f'(x)} \\right|$.\n\n*this says: if a function has a steep slope near the root, the root is easy to find (well-conditioned). if the function barely grazes zero (shallow crossing), the root is hard to pin down.*\n\nas you get a high sensitivity in the inverse, you get a low sensitivity in the inversion and vice-versa. the function doesn't need to have an inverse in order to find the inverse in a local region.\n\nwe're trying to look for $f(x)=0$: when we're close to zero, we never use the relative accuracy but always the absolute one.\n\n## convergence\n\n$e_{k}$ (the error at the $k^{\\text{th}}$ step) $= x_k - x^*$\n\n$$e^k_{rel} = \\frac{e_k}{x^*}  = \\frac{x_k - x^*}{x^*}$$\n\nwe need to look at the number of significant bits, because it's exact unlike significant decimal digits.\n\n$$\n\\begin{align*}\n\\text{bits/step} &= -\\log_2(e^{k+1}_{rel}) - \\left(-\\log_2(e^k_{rel}) \\right)   \\\\\n& = \\log_2\\left(\\frac{\\frac{x_k - x^*}{x^*}}{\\frac{x_{k+1} - x^*}{x^*}} \\right) \\\\\n& = \\log_2\\left(\\frac{\\vert x_k - x^*\\vert}{\\vert x_{k+1} - x^*\\vert} \\right)   \\\\\n& = \\log_2\\left(\\frac{\\vert e_k\\vert}{\\vert e_{k+1}\\vert}\\right) \t\t\t\t\\\\\n& = -\\log_2\\left(\\frac{\\vert e_{k+1}\\vert}{\\vert e_k\\vert}\\right)\n\\end{align*}\n$$\n\nif $\\lim_{k\\to 0} \\frac{\\vert e_{k+1}\\vert}{\\vert e_k\\vert^r} = c$, and $0 \\leq c \\lt 1$, method converges with rate r=1 $\\implies$ linear, r=2 $\\implies$ quadratic, etc\n\n**how fast is fast?** think of convergence rates like this:\n* **linear (r=1):** like earning simple interest \u2014 you gain a fixed number of correct digits each step. bisection does this: one bit per step, steady and reliable.\n* **quadratic (r=2):** like compound interest on steroids \u2014 the number of correct digits *doubles* each step. start with 3 good digits, then 6, then 12, th"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "nonlinear-systems",
      "lessonTitle": "Nonlinear Systems",
      "x": 0.5190824270248413,
      "y": 0.888023316860199,
      "searchText": "nonlinear systems\n# nonlinear systems\n\n> *everything we learned about newton's method in 1d carries over \u2014 but now the derivative is a jacobian matrix, and we solve a linear system at every step. remember [linear equations](./linear-equations)? you'll need them here.*\n\n## nonlinear solvers\n\n## fixed point iteration (recap)\n\nrecall from the previous page: we can transform $f(x)=0$ to $g(x)=x$ and iterate $x_{k+1} = g(x_k)$. the key results were:\n\n* if $|g'(x^*)| < 1$, $g$ is a contraction near $x^*$ and we get **linear convergence**: $|e_{k+1}| \\leq c^k |e_0|$\n* if $g'(x^*) = 0$, we get **quadratic convergence**: $|e_{k+1}| \\leq \\bar{c}^k |e_0|^{2^k}$\n\nthis all generalizes directly to higher dimensions, replacing $|g'|$ with $\\vert g' \\vert$ (the jacobian norm).\n\n\n## newton's method\n\na quadratically convergent fixed point iteration solver. take a point, take the tangent of the curve at that point, your new point is the tangent's x-intercept. intuition: find zero for linear approximation, set as next $x$.\n\ntaylor expand:\n$$ f(x_{k+1}) = f(x_k) + f'(x_k) (x_{k+1}-x_k) + \\mathcal{o}(\\vert x_{k+1}-x_k\\vert^2)$$\n$$ \\approx f(x_k) + f'(x_k) (x_{k+1}-x_k)$$\n$$ 0 \\approx f(x_k) + f'(x_k) \\delta x_{k+1}$$\n$$ f'(x_k) \\delta x_{k+1} = f(x_k)$$\n\n*this says: at each step, pretend the world is linear (use the tangent), solve the linear problem to find the step, then take that step.*\n\nremember the above line, cause we'll use it in higher dimensions as it's solving linear systems\n$$ \\delta x_k = -\\frac{f(x_k)}{f'(x_k)}  $$\nthis works in 1d, but is not general because we're dividing by a matrix (the jacobian) and not a number\n$$ \\implies g(x_k) = x_k + \\delta x_k = x_k - \\frac{f(x_k)}{f'(x_k)}$$\n\nif $f(x^*) = 0$,\n$$g(x^*) = x^* - \\frac{f(x)}{f'(x)} = x^*$$\nalso,\n$$ g'(x) = \\frac{d}{dx}\\left(x - \\frac{f(x)}{f'(x)}\\right)$$\n$$ 1 - \\frac{f'(x)f'(x) + f(x)f''(x)}{f'(x)^2} $$\n$$  = - \\frac{f(x)f''(x)}{f'(x)^2}$$\n$$ \\to 0 \\quad \\text{when}\\quad f(x)=0$$\n\n## quasi-newton/secant methods\n\nthese are methods that try to replicate the wonderful properties of newton's method, but without having to evaluate the derivative. in higher dimensions, you don't want to be evaluating the derivative cause it's a massive matrix.\n\n**method**: use secant (finite difference) instead of tangent (derivative).\n\n$$f'(x) \\approx \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}$$\n\n*this says: instead of knowing the exact slope, estimate it from the last two points. you lose a bit of convergence speed but save a huge amount of computation.*\n\nin 1d, we have the following equation for $\\delta x$:\n$$\\delta x_{k+1} = -\\frac{f(x_k)}{\\hat{f}'(x_k)}$$\nwhere\n$$\\hat{f}'(x_k) = \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}$$\nfor higher dimensions, we want to write it like\n$$ \\hat{f}'(x_k) \\delta x_{k+1} = - f(x_k)$$\ncause we'll solve a linear system instead\n\n## interactive visualizations\n\n### newton's method in 1d: cobweb diagram\n\nexplore newton's method interactively. select functions with known roots, adjust initial guess `x\u2080`, and step through iterations. the cobweb diagram visualizes the fixed-point iteration `x = g(x)` where `g(x) = x - f(x)/f'(x)`.\n\n```tsx\nimport newton1d from '@/components/visualization/nonlinear-equations/newton1d';\n&lt;newton1d /&gt;\n```\n\n<newton1d />\n\nobserve quadratic convergence near the root and potential overshoots or divergences.\n\n### 2d nonlinear optimization: contour descent and basins of attraction\n\nthe himmelblau function has four minima:\n* \u2248 (3.0, 2.0)\n* \u2248 (-2.8, 3.1)\n* \u2248 (-3.8, -3.3)\n* \u2248 (3.6, -1.8)\n\ncompare gradient descent (gd) and newton's method. toggle basins to see attraction regions from initial points grid. newton's method has smaller, more precise basins due to curvature info.\n\n```tsx\nimport himmelblau2d from '@/components/visualization/nonlinear-equations/himmelblau2d';\n&lt;himmelblau2d /&gt;\n```\n\n<himmelblau2d />\n\n**research insights & gaps filled:**\n* **basin attractors**: visualizes fractal-like boundaries in practice, sensitive to method.\n* **contour descent**: paths show gd zigzagging (rosenbrock-like valley), newton direct.\n* **interactivity**: sliders for params/init, animation reveals dynamics.\n* **gaps**: few web interactives compare newton/secant(quasi) in 2d basins; this adds newton vs gd proxy for secant ideas.\n\nfor secant methods like broyden/bfgs, paths approximate newton without exact derivs/hessians.\n\n## going to higher dimensions\n\nimagine you're a blind mountain climber feeling the slope under your boots. in one dimension, you only feel \"uphill\" or \"downhill.\" but in $n$ dimensions, the slope has a direction \u2014 it's a vector (the gradient), and the rate of change along any particular direction is the directional derivative. the jacobian matrix collects all these directional sensitivities into one object.\n\nnotation: $f: \\mathbb{r}\\to\\mathbb{r}$, $f: \\mathbb{r}^n\\to\\mathbb{r}^n$\n\n1d taylor expansion:\n$$f(x_{k+1}) = f(x_k + \\delta x_{k+1})$$\n$$f(x_{k+1}) = f(x_k) + f'(x_k)\\delta x_{k+1} + \\mathcal{o}(|\\delta x_{k+1}|^2)$$\n\nn-dimensional case:\n$$f(x_{k+1}) = f(x_k + \\delta x_{k+1})$$\n$$f(x_{k+1}) = f(x_k) + f'(x_k)\\delta x_{k+1} + \\mathcal{o}(|\\delta x_{k+1}|^2)$$\n\n*here $f'(x_k)$ is the jacobian \u2014 an $n \\times n$ matrix of all partial derivatives. each row tells you how one output component changes with all the inputs.*\n\nto go to the next step, we don't divide by $f'(x)$ like we do in the 1d case, but instead we solve the linear system\n$$f'(x_k) \\delta x_{k+1} = - f(x)$$\nwhere we know the first and last terms and want to find $\\delta x_{k+1}$\n\n*this is the key connection: each newton step in n dimensions requires solving a [linear system](./linear-equations). the jacobian plays the role of the coefficient matrix, and $-f(x_k)$ is the right-hand side.*\n\nin the secant method, we just use $\\hat{f}'(x)$ instead. this raises an extra problem: we need to find a good $\\hat{f}'$\n\nhow expensive is computing the jacobian? for $n$ unknowns, the jacobian has $n^2$ entries. if each partial derivative requires a "
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "optimization",
      "lessonTitle": "Optimization Methods",
      "x": 0.5534055233001709,
      "y": 0.870559573173523,
      "searchText": "optimization methods\n# optimization methods\n\n> *the quasi-newton ideas from [broyden's method](./nonlinear-systems) show up again here as bfgs \u2014 the same trick of building up curvature knowledge step by step, but now for finding minima instead of zeros.*\n\n## non-linear optimization\n\n### the party in the dark\n\nimagine you're at a party in a pitch-dark room and you're trying to find the coldest spot (next to the air conditioning). you can feel the temperature where you're standing, and maybe you can feel which direction is cooler. that's gradient descent \u2014 just follow the chill.\n\nbut what if the room has multiple cold spots? you might get stuck in a corner that's cool but not the *coolest*. that's a local minimum. now imagine the room is full of people, all searching independently, and they shout to each other when they find somewhere cold. that's metaheuristic optimization \u2014 collective exploration beats individual greedy search.\n\nthe big question is always: **how much do you know about the landscape, and how much can you afford to explore?**\n\n## when to use what:\n\n### questions that'll help decide on what to use\n1. how slow are function evaluations (and gradients)?\n2. how big is your space?\n3. how ugly is your energy landscape? convex, or many minima?\n\n### rules of thumb:\n1. fast function evaluations:\n   1.  you can just take a linspace, evaluate the function at every point, and either use the minimum or feed that to a newton raphson method.  both pretty and ugly energy landscapes works, it just changes the size of the linspace.\n   2.  for medium dimensions (up to 100), you want to use bfgs if the energy landscape is simple. if it's complicated, you want to use bfgs + exploration, as you need some way to escape local minima to find a global minima.\n   3.  for high dimensions (up to 1m), use conjugated gradients. takes longer to converge than bfgs, but you don't have to represent the high dimensional hessian matrix and so it works up to millions of dimensions. for a convoluted energy landscape, you want to use exploration as well.\n\n2. slow function evaluations\n   1. low to medium dimensions: if the energy landscape is simple, bfgs. even if you have a complicated energy landscape, your search area is small enough that you can use bfgs with exploration.\n   2. high dimensions: simple energy landscape, use conjugate gradients. when it's expensive to evaluate the function, and you're in a high-dimensional complex landscape, the search space is too big for you to get anywhere. here you need to think, and tailor your solution to fit your problem. generally, you can try to use some sort of symmetry or structure of your problem and then use a metaheuristic to guide your solutions\n\nwhy can't you just use gradient descent for everything? because gradient descent is like always walking downhill in the steepest direction \u2014 it zigzags in narrow valleys and takes forever to converge. bfgs and conjugate gradients are smarter: they learn the shape of the valley and take much better steps.\n\n[[simulation rosenbrock-banana]]\n\n## bfgs (broyden-fletcher-goldfarb-shanno)\n\nbfgs is a quasi-newton method for minimizing $f:\\mathbb{r}^n\\to\\mathbb{r}$. newton's method for optimization uses the hessian $h$ to find the step:\n\n$$h_k \\delta x_k = -\\nabla f(x_k)$$\n\n*this says: the hessian tells you the curvature of the landscape, and newton uses it to jump directly to the bottom of the local bowl. but computing the full hessian is expensive.*\n\ncomputing the full hessian is expensive ($o(n^2)$ storage, $o(n^3)$ to solve). bfgs builds an approximation $b_k \\approx h_k^{-1}$ using only gradient information, similar to how broyden's method approximates the jacobian.\n\n**bfgs update rule:** given step $s_k = x_{k+1} - x_k$ and gradient change $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$:\n\n$$b_{k+1} = \\left(i - \\frac{s_k y_k^t}{y_k^t s_k}\\right) b_k \\left(i - \\frac{y_k s_k^t}{y_k^t s_k}\\right) + \\frac{s_k s_k^t}{y_k^t s_k}$$\n\n*this says: update your curvature estimate using the step you just took and how much the gradient changed. each step teaches you a bit more about the shape of the landscape.*\n\nthis is a rank-2 update (two outer products), so each step costs $o(n^2)$ instead of $o(n^3)$.\n\n**algorithm \u2014 a numbered story:**\n1. **start ignorant:** set $b_0 = i$ (pretend the landscape is a simple bowl)\n2. **feel the slope:** compute search direction $\\delta x_k = -b_k \\nabla f(x_k)$\n3. **walk carefully:** line search to find step size $\\alpha_k$ along $\\delta x_k$ (wolfe conditions)\n4. **take the step:** update $x_{k+1} = x_k + \\alpha_k \\delta x_k$\n5. **learn from the step:** update $b_{k+1}$ using the formula above\n6. **repeat** until $\\vert \\nabla f \\vert < \\text{tol}$\n\nbfgs achieves **superlinear convergence** near a minimum and works well up to medium dimensions (~100). beyond that, storing the $n\\times n$ matrix $b_k$ becomes prohibitive.\n\n**l-bfgs** (limited-memory bfgs) avoids storing the full matrix by keeping only the last $m$ pairs $(s_k, y_k)$ and reconstructing the matrix-vector product implicitly. this reduces storage from $o(n^2)$ to $o(mn)$ and works up to millions of dimensions, though it converges somewhat slower than full bfgs.\n\n## conjugate gradient method\n\nfor high-dimensional problems where even $o(n^2)$ storage is too much, the conjugate gradient (cg) method uses only $o(n)$ storage by maintaining just a search direction vector.\n\n**key idea:** instead of steepest descent (which zigzags), choose search directions that are _conjugate_ with respect to the hessian: $d_i^t h d_j = 0$ for $i \\neq j$. this guarantees that progress made in one direction is not undone by later steps.\n\n*think of it like this: if steepest descent is a drunk stumbling downhill (zigzagging back and forth across a valley), conjugate gradients is a sober hiker who remembers where they've already been and never backtracks.*\n\n**nonlinear cg (fletcher-reeves):**\n1. **start steep:** set initial direction $d_0 = -\\nabla f(x_0)$\n2. **slide downhill"
    },
    {
      "topicId": "scientific-computing",
      "topicTitle": "Scientific Computing",
      "routeSlug": "scientific-computing",
      "lessonSlug": "partial-differential-equations",
      "lessonTitle": "Partial Differential Equations",
      "x": 0.40132755041122437,
      "y": 0.9206953048706055,
      "searchText": "partial differential equations\n# partial differential equations\n\n> *this is the summit. everything comes together here: finite differences ([error analysis](./bounding-errors)), [linear systems](./linear-equations), [eigenvalues](./eigen-systems) for stability, the [fft](./fft) for spectral methods, and [ode solvers](./initial-value-problems) via method of lines.*\n\n## classification\n\npartial differential equations (pdes) are classified by the nature of their highest-order terms. a second-order linear pde in two variables has the general form\n\n$$\na u_{xx} + 2b u_{xy} + c u_{yy} + \\text{lower order terms} = 0.\n$$\n\nthe **discriminant** $b^2 - ac$ determines the type:\n\n* **elliptic** ($b^2 - ac < 0$): e.g., laplace equation $\\nabla^2 u = 0$. describes equilibrium states.\n* **parabolic** ($b^2 - ac = 0$): e.g., heat equation $u_t = \\alpha \\nabla^2 u$. describes diffusion processes.\n* **hyperbolic** ($b^2 - ac > 0$): e.g., wave equation $u_{tt} = c^2 \\nabla^2 u$. describes wave propagation.\n\n*think of it this way: elliptic pdes describe things that have settled down (a rubber sheet draped over a frame), parabolic pdes describe things that are settling down (heat spreading until everything is the same temperature), and hyperbolic pdes describe things that are sloshing around (waves in a pond).*\n\neach type requires different numerical strategies and boundary conditions. elliptic problems need boundary values on the entire domain boundary. parabolic and hyperbolic problems need initial conditions plus boundary conditions.\n\n## finite difference methods\n\nthe fundamental idea is to replace continuous derivatives with **discrete approximations** on a grid. for a uniform grid with spacing $\\delta x$, the standard finite difference formulas are\n\n$$\n\\frac{\\partial u}{\\partial x} \\approx \\frac{u_{i+1} - u_{i-1}}{2\\delta x} \\quad \\text{(central, } o(\\delta x^2)\\text{)},\n$$\n\n$$\n\\frac{\\partial^2 u}{\\partial x^2} \\approx \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\delta x^2} \\quad \\text{(} o(\\delta x^2)\\text{)}.\n$$\n\n*this says: estimate derivatives by looking at neighboring grid points. first derivative = slope between neighbors. second derivative = curvature: how much the value at a point deviates from the average of its neighbors.*\n\nthese replace the pde at each interior grid point with an algebraic equation, producing a system of equations that can be solved by linear algebra techniques.\n\nhow fine does the grid need to be? that depends on the solution. smooth solutions need fewer points; solutions with sharp gradients or boundary layers need more. the error is $o(\\delta x^2)$ for these central differences, so halving the grid spacing quarters the error \u2014 but also quadruples the number of equations.\n\n## the heat equation\n\nthe one-dimensional heat equation $u_t = \\alpha u_{xx}$ is the prototypical parabolic pde. the **ftcs scheme** (forward time, central space) discretizes as\n\n$$\n\\frac{u_i^{n+1} - u_i^n}{\\delta t} = \\alpha \\frac{u_{i+1}^n - 2u_i^n + u_{i-1}^n}{\\delta x^2},\n$$\n\ngiving the explicit update\n\n$$\nu_i^{n+1} = u_i^n + r(u_{i+1}^n - 2u_i^n + u_{i-1}^n),\n$$\n\nwhere $r = \\alpha \\delta t / \\delta x^2$. this scheme is **conditionally stable**: the solution blows up unless $r \\leq 1/2$ (the **cfl condition** for this problem).\n\n*this says: the ratio $r$ controls everything. make the time step too big relative to the grid spacing and the solution explodes. it's a direct echo of the truncation-rounding tradeoff from [bounding errors](./bounding-errors) \u2014 you can't be reckless with step sizes.*\n\nthe **implicit (backward euler)** scheme evaluates spatial derivatives at the new time level:\n\n$$\nu_i^{n+1} - r(u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}) = u_i^n.\n$$\n\nthis is unconditionally stable but requires solving a tridiagonal linear system at each time step. the **crank-nicolson** scheme averages the explicit and implicit forms, achieving $o(\\delta t^2, \\delta x^2)$ accuracy while remaining unconditionally stable.\n\n## the wave equation\n\nthe one-dimensional wave equation $u_{tt} = c^2 u_{xx}$ is discretized with central differences in both time and space:\n\n$$\n\\frac{u_i^{n+1} - 2u_i^n + u_i^{n-1}}{\\delta t^2} = c^2 \\frac{u_{i+1}^n - 2u_i^n + u_{i-1}^n}{\\delta x^2}.\n$$\n\nthis leapfrog scheme is second-order accurate and stable when the **courant number** satisfies $c = c\\delta t / \\delta x \\leq 1$. the courant condition ensures that the numerical domain of dependence contains the physical domain of dependence.\n\n*this says: information in the wave equation travels at speed $c$. your time step must be small enough that the numerical scheme can \"keep up\" with the wave. if the wave moves faster than one grid cell per time step, the scheme misses information and explodes.*\n\n## elliptic problems\n\nfor laplace's equation $\\nabla^2 u = 0$ on a 2d grid, the five-point stencil gives\n\n$$\nu_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j} = 0.\n$$\n\n*this says: at equilibrium, every point equals the average of its four neighbors. if it didn't, heat would flow and the system wouldn't be in equilibrium.*\n\nthis produces a large sparse linear system $a\\mathbf{u} = \\mathbf{b}$ where boundary values enter through the right-hand side. direct solvers (lu decomposition) work for moderate grids, but **iterative methods** are preferred for large problems:\n\n* **jacobi iteration**: update each point from its neighbors' previous values.\n* **gauss-seidel**: use updated values as soon as available (faster convergence).\n* **successive over-relaxation (sor)**: accelerates gauss-seidel with a relaxation parameter $\\omega$.\n\nfor poisson's equation $\\nabla^2 u = f$, the same stencil applies with $f_{i,j}$ on the right-hand side.\n\nhow big can these linear systems get? for a 2d grid with $n$ points per side, you have $n^2$ unknowns. for $n = 1000$, that's a million unknowns and a million-by-million matrix. it's sparse (mostly zeros), which is the only reason it's tractable. this is exactly where the iterative methods and sparse solvers from [linear equat"
    }
  ]
}