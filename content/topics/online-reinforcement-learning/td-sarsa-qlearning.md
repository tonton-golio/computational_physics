# Temporal-Difference Learning, SARSA, and Q-Learning

## Learning without waiting for the end

Monte Carlo said: play the whole game, then learn from the final score. TD learning says: why wait? After every single step, update your estimate using the reward you just got and your current guess about the next state's value.

This is **bootstrapping** -- using your own estimates as part of the learning target. It sounds circular. But it works, and remarkably well.

## TD(0): the one-step update

The simplest TD method updates after every transition. Here's the rule:

$$
V(s_t)\leftarrow V(s_t)+\alpha\left[r_{t+1}+\gamma V(s_{t+1})-V(s_t)\right].
$$

The bracketed term is the **TD error** -- the difference between what you expected ($V(s_t)$) and a better estimate based on what actually happened ($r_{t+1}+\gamma V(s_{t+1})$). You nudge your estimate in the direction of this error, scaled by step size $\alpha$.

Think of it like this: you're walking to a restaurant and you estimated 20 minutes. After 5 minutes you've walked a quarter of the way and now estimate 18 minutes total. TD says: update your original estimate *right now*, based on the partial information. You don't have to arrive to revise your prediction.

## SARSA: the cautious accountant

SARSA learns the value of state-action pairs, and it learns about the policy it's actually following. SARSA is a cautious accountant who only trusts the policy he's actually using. The name comes from the five things it uses: $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$.

$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha\left[r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)\right].
$$

You're in state $s_t$, take action $a_t$, get reward $r_{t+1}$, land in $s_{t+1}$, and take action $a_{t+1}$ (according to your current policy, say epsilon-greedy). You update toward the reward plus the discounted value of what you *actually did* next.

Because SARSA uses the action you *actually took*, it bakes in the risk of exploration. If your epsilon-greedy policy occasionally walks off a cliff, SARSA learns the cliff-edge state is dangerous.

## Q-Learning: the optimist

Q-learning looks almost identical, with one critical difference. Q-learning is an optimist who keeps dreaming about the best possible world.

$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha\left[r_{t+1}+\gamma \max_{a'}Q(s_{t+1},a')-Q(s_t,a_t)\right].
$$

Instead of using the action you *actually took* next, Q-learning uses the *best* action -- the $\max$. It doesn't matter what you actually did; Q-learning assumes you'll act optimally from here on out.

This makes Q-learning **off-policy**: it learns about the optimal policy regardless of the behavior policy generating the data. Explore wildly, and Q-learning still converges to $Q^*$ as long as you visit every state-action pair often enough.

## The cliff: where SARSA and Q-learning disagree

Imagine a gridworld with a cliff along one edge. SARSA, following epsilon-greedy, learns a *safe* path that stays away from the cliff -- because its own exploration sometimes sends it tumbling off. Q-learning learns the *optimal* path right along the cliff edge -- because it evaluates the optimal policy, which never falls.

Q-learning finds the faster route. SARSA finds the route that's actually safe given how you're behaving. Neither is wrong; they're answering different questions.

[[simulation sarsa-vs-qlearning]]

[[simulation cliff-walking]]

## Exploration strategies

You need exploration to visit enough state-action pairs, but you want to exploit what you've learned. Three common approaches:

**Epsilon-greedy**: with probability $\epsilon$, random action; otherwise, greedy. Simple but wastes exploration on clearly bad actions.

**Softmax (Boltzmann)**: probabilities proportional to $\exp(Q(s,a)/\tau)$, with temperature $\tau$. Focuses exploration on promising actions rather than wasting pulls on obviously bad ones.

**Optimism and UCB-like bonuses**: add exploration bonuses to Q-values, connecting RL exploration back to the [bandit ideas from earlier](./bandits-ucb-exp3).

## The deadly triad: three bad roommates

There's a stability problem lurking in TD methods. Imagine three roommates who are fine individually but, put together, burn the house down:

**Function approximation** -- using a neural network instead of a table to generalize across states.

**Bootstrapping** -- using your own estimates as targets, which TD does.

**Off-policy learning** -- learning about one policy from data generated by a different one.

Any two roommates are manageable. All three together? Values diverge, spiraling upward or oscillating wildly as the target shifts because the parameters changed, which shifts the target again, in a runaway feedback loop. That's the **deadly triad**.

Tabular Q-learning survives because it has no function approximation. DQN, which we meet next, survives because of two clever engineering patches. Modern deep RL is mostly about inventing better patches.

[[simulation concentration-bounds]]
[[simulation stochastic-approximation]]

## What Comes Next

TD methods and Q-learning work beautifully when the state space fits in a table, but most interesting problems have state spaces too large for any table. The next lesson replaces the lookup table with a neural network -- and that changes everything. The replay buffer and the target network are engineering patches on the deadly triad, and understanding what each one fixes is the central payoff.

## Challenge (Optional)

Implement both SARSA and Q-learning on cliff-walking (4x12 grid, cliff along the bottom, start bottom-left, goal bottom-right). Run both with $\epsilon = 0.1$ for 500 episodes. Plot per-episode reward. Explain the gap between curves, verify Q-learning's policy is closer to optimal while SARSA's is safer, and show what happens when $\epsilon$ drops to 0.01 after episode 500.
